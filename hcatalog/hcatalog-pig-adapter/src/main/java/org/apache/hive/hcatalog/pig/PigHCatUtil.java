begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/**  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing,  * software distributed under the License is distributed on an  * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY  * KIND, either express or implied.  See the License for the  * specific language governing permissions and limitations  * under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hive
operator|.
name|hcatalog
operator|.
name|pig
package|;
end_package

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|sql
operator|.
name|Date
import|;
end_import

begin_import
import|import
name|java
operator|.
name|sql
operator|.
name|Timestamp
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Arrays
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Calendar
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
operator|.
name|Entry
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Properties
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configuration
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|type
operator|.
name|HiveChar
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|type
operator|.
name|HiveDecimal
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|type
operator|.
name|HiveVarchar
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|HiveMetaStoreClient
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|MetaStoreUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|NoSuchObjectException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|Table
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|Job
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hive
operator|.
name|hcatalog
operator|.
name|common
operator|.
name|HCatConstants
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hive
operator|.
name|hcatalog
operator|.
name|common
operator|.
name|HCatException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hive
operator|.
name|hcatalog
operator|.
name|common
operator|.
name|HCatUtil
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hive
operator|.
name|hcatalog
operator|.
name|data
operator|.
name|HCatRecord
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hive
operator|.
name|hcatalog
operator|.
name|data
operator|.
name|Pair
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hive
operator|.
name|hcatalog
operator|.
name|data
operator|.
name|schema
operator|.
name|HCatFieldSchema
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hive
operator|.
name|hcatalog
operator|.
name|data
operator|.
name|schema
operator|.
name|HCatFieldSchema
operator|.
name|Type
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hive
operator|.
name|hcatalog
operator|.
name|data
operator|.
name|schema
operator|.
name|HCatSchema
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|pig
operator|.
name|LoadPushDown
operator|.
name|RequiredField
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|pig
operator|.
name|PigException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|pig
operator|.
name|ResourceSchema
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|pig
operator|.
name|ResourceSchema
operator|.
name|ResourceFieldSchema
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|pig
operator|.
name|data
operator|.
name|DataBag
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|pig
operator|.
name|data
operator|.
name|DataByteArray
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|pig
operator|.
name|data
operator|.
name|DataType
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|pig
operator|.
name|data
operator|.
name|DefaultDataBag
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|pig
operator|.
name|data
operator|.
name|Tuple
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|pig
operator|.
name|data
operator|.
name|TupleFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|pig
operator|.
name|impl
operator|.
name|logicalLayer
operator|.
name|schema
operator|.
name|Schema
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|pig
operator|.
name|impl
operator|.
name|util
operator|.
name|UDFContext
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|pig
operator|.
name|impl
operator|.
name|util
operator|.
name|Utils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|joda
operator|.
name|time
operator|.
name|DateTime
import|;
end_import

begin_import
import|import
name|org
operator|.
name|joda
operator|.
name|time
operator|.
name|DateTimeZone
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|Logger
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|LoggerFactory
import|;
end_import

begin_class
class|class
name|PigHCatUtil
block|{
specifier|private
specifier|static
specifier|final
name|Logger
name|LOG
init|=
name|LoggerFactory
operator|.
name|getLogger
argument_list|(
name|PigHCatUtil
operator|.
name|class
argument_list|)
decl_stmt|;
specifier|static
specifier|final
name|int
name|PIG_EXCEPTION_CODE
init|=
literal|1115
decl_stmt|;
comment|// http://wiki.apache.org/pig/PigErrorHandlingFunctionalSpecification#Error_codes
specifier|private
specifier|static
specifier|final
name|String
name|DEFAULT_DB
init|=
name|MetaStoreUtils
operator|.
name|DEFAULT_DATABASE_NAME
decl_stmt|;
specifier|private
specifier|final
name|Map
argument_list|<
name|Pair
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
argument_list|,
name|Table
argument_list|>
name|hcatTableCache
init|=
operator|new
name|HashMap
argument_list|<
name|Pair
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
argument_list|,
name|Table
argument_list|>
argument_list|()
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|TupleFactory
name|tupFac
init|=
name|TupleFactory
operator|.
name|getInstance
argument_list|()
decl_stmt|;
specifier|private
specifier|static
name|boolean
name|pigHasBooleanSupport
init|=
literal|false
decl_stmt|;
comment|/**    * Determine if the current Pig version supports boolean columns. This works around a    * dependency conflict preventing HCatalog from requiring a version of Pig with boolean    * field support and should be removed once HCATALOG-466 has been resolved.    */
static|static
block|{
comment|// DETAILS:
comment|//
comment|// PIG-1429 added support for boolean fields, which shipped in 0.10.0;
comment|// this version of Pig depends on antlr 3.4.
comment|//
comment|// HCatalog depends heavily on Hive, which at this time uses antlr 3.0.1.
comment|//
comment|// antlr 3.0.1 and 3.4 are incompatible, so Pig 0.10.0 and Hive cannot be depended on in the
comment|// same project. Pig 0.8.0 did not use antlr for its parser and can coexist with Hive,
comment|// so that Pig version is depended on by HCatalog at this time.
try|try
block|{
name|Schema
name|schema
init|=
name|Utils
operator|.
name|getSchemaFromString
argument_list|(
literal|"myBooleanField: boolean"
argument_list|)
decl_stmt|;
name|pigHasBooleanSupport
operator|=
operator|(
name|schema
operator|.
name|getField
argument_list|(
literal|"myBooleanField"
argument_list|)
operator|.
name|type
operator|==
name|DataType
operator|.
name|BOOLEAN
operator|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Throwable
name|e
parameter_list|)
block|{
comment|// pass
block|}
if|if
condition|(
operator|!
name|pigHasBooleanSupport
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"This version of Pig does not support boolean fields. To enable "
operator|+
literal|"boolean-to-integer conversion, set the "
operator|+
name|HCatConstants
operator|.
name|HCAT_DATA_CONVERT_BOOLEAN_TO_INTEGER
operator|+
literal|"=true configuration parameter."
argument_list|)
expr_stmt|;
block|}
block|}
specifier|static
specifier|public
name|boolean
name|pigHasBooleanSupport
parameter_list|()
block|{
return|return
name|pigHasBooleanSupport
return|;
block|}
specifier|static
specifier|public
name|Pair
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|getDBTableNames
parameter_list|(
name|String
name|location
parameter_list|)
throws|throws
name|IOException
block|{
comment|// the location string will be of the form:
comment|//<database name>.<table name> - parse it and
comment|// communicate the information to HCatInputFormat
try|try
block|{
return|return
name|HCatUtil
operator|.
name|getDbAndTableName
argument_list|(
name|location
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|String
name|locationErrMsg
init|=
literal|"The input location in load statement "
operator|+
literal|"should be of the form "
operator|+
literal|"<databasename>.<table name> or<table name>. Got "
operator|+
name|location
decl_stmt|;
throw|throw
operator|new
name|PigException
argument_list|(
name|locationErrMsg
argument_list|,
name|PIG_EXCEPTION_CODE
argument_list|)
throw|;
block|}
block|}
specifier|static
specifier|public
name|String
name|getHCatServerUri
parameter_list|(
name|Job
name|job
parameter_list|)
block|{
return|return
name|job
operator|.
name|getConfiguration
argument_list|()
operator|.
name|get
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|METASTOREURIS
operator|.
name|varname
argument_list|)
return|;
block|}
specifier|static
specifier|public
name|String
name|getHCatServerPrincipal
parameter_list|(
name|Job
name|job
parameter_list|)
block|{
return|return
name|job
operator|.
name|getConfiguration
argument_list|()
operator|.
name|get
argument_list|(
name|HCatConstants
operator|.
name|HCAT_METASTORE_PRINCIPAL
argument_list|)
return|;
block|}
specifier|private
specifier|static
name|HiveMetaStoreClient
name|getHiveMetaClient
parameter_list|(
name|String
name|serverUri
parameter_list|,
name|String
name|serverKerberosPrincipal
parameter_list|,
name|Class
argument_list|<
name|?
argument_list|>
name|clazz
parameter_list|,
name|Job
name|job
parameter_list|)
throws|throws
name|Exception
block|{
comment|// The job configuration is passed in so the configuration will be cloned
comment|// from the pig job configuration. This is necessary for overriding
comment|// metastore configuration arguments like the metastore jdbc connection string
comment|// and password, in the case of an embedded metastore, which you get when
comment|// hive.metastore.uris = "".
name|HiveConf
name|hiveConf
init|=
operator|new
name|HiveConf
argument_list|(
name|job
operator|.
name|getConfiguration
argument_list|()
argument_list|,
name|clazz
argument_list|)
decl_stmt|;
if|if
condition|(
name|serverUri
operator|!=
literal|null
condition|)
block|{
name|hiveConf
operator|.
name|setVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|METASTOREURIS
argument_list|,
name|serverUri
operator|.
name|trim
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|serverKerberosPrincipal
operator|!=
literal|null
condition|)
block|{
name|hiveConf
operator|.
name|setBoolVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|METASTORE_USE_THRIFT_SASL
argument_list|,
literal|true
argument_list|)
expr_stmt|;
name|hiveConf
operator|.
name|setVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|METASTORE_KERBEROS_PRINCIPAL
argument_list|,
name|serverKerberosPrincipal
argument_list|)
expr_stmt|;
block|}
try|try
block|{
return|return
name|HCatUtil
operator|.
name|getHiveClient
argument_list|(
name|hiveConf
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|Exception
argument_list|(
literal|"Could not instantiate a HiveMetaStoreClient connecting to server uri:["
operator|+
name|serverUri
operator|+
literal|"]"
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
name|HCatSchema
name|getHCatSchema
parameter_list|(
name|List
argument_list|<
name|RequiredField
argument_list|>
name|fields
parameter_list|,
name|String
name|signature
parameter_list|,
name|Class
argument_list|<
name|?
argument_list|>
name|classForUDFCLookup
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|fields
operator|==
literal|null
condition|)
block|{
return|return
literal|null
return|;
block|}
name|Properties
name|props
init|=
name|UDFContext
operator|.
name|getUDFContext
argument_list|()
operator|.
name|getUDFProperties
argument_list|(
name|classForUDFCLookup
argument_list|,
operator|new
name|String
index|[]
block|{
name|signature
block|}
argument_list|)
decl_stmt|;
name|HCatSchema
name|hcatTableSchema
init|=
operator|(
name|HCatSchema
operator|)
name|props
operator|.
name|get
argument_list|(
name|HCatConstants
operator|.
name|HCAT_TABLE_SCHEMA
argument_list|)
decl_stmt|;
name|ArrayList
argument_list|<
name|HCatFieldSchema
argument_list|>
name|fcols
init|=
operator|new
name|ArrayList
argument_list|<
name|HCatFieldSchema
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|RequiredField
name|rf
range|:
name|fields
control|)
block|{
name|fcols
operator|.
name|add
argument_list|(
name|hcatTableSchema
operator|.
name|getFields
argument_list|()
operator|.
name|get
argument_list|(
name|rf
operator|.
name|getIndex
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
return|return
operator|new
name|HCatSchema
argument_list|(
name|fcols
argument_list|)
return|;
block|}
comment|/*   * The job argument is passed so that configuration overrides can be used to initialize   * the metastore configuration in the special case of an embedded metastore   * (hive.metastore.uris = "").   */
specifier|public
name|Table
name|getTable
parameter_list|(
name|String
name|location
parameter_list|,
name|String
name|hcatServerUri
parameter_list|,
name|String
name|hcatServerPrincipal
parameter_list|,
name|Job
name|job
parameter_list|)
throws|throws
name|IOException
block|{
name|Pair
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|loc_server
init|=
operator|new
name|Pair
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
argument_list|(
name|location
argument_list|,
name|hcatServerUri
argument_list|)
decl_stmt|;
name|Table
name|hcatTable
init|=
name|hcatTableCache
operator|.
name|get
argument_list|(
name|loc_server
argument_list|)
decl_stmt|;
if|if
condition|(
name|hcatTable
operator|!=
literal|null
condition|)
block|{
return|return
name|hcatTable
return|;
block|}
name|Pair
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|dbTablePair
init|=
name|PigHCatUtil
operator|.
name|getDBTableNames
argument_list|(
name|location
argument_list|)
decl_stmt|;
name|String
name|dbName
init|=
name|dbTablePair
operator|.
name|first
decl_stmt|;
name|String
name|tableName
init|=
name|dbTablePair
operator|.
name|second
decl_stmt|;
name|Table
name|table
init|=
literal|null
decl_stmt|;
name|HiveMetaStoreClient
name|client
init|=
literal|null
decl_stmt|;
try|try
block|{
name|client
operator|=
name|getHiveMetaClient
argument_list|(
name|hcatServerUri
argument_list|,
name|hcatServerPrincipal
argument_list|,
name|PigHCatUtil
operator|.
name|class
argument_list|,
name|job
argument_list|)
expr_stmt|;
name|table
operator|=
name|HCatUtil
operator|.
name|getTable
argument_list|(
name|client
argument_list|,
name|dbName
argument_list|,
name|tableName
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|NoSuchObjectException
name|nsoe
parameter_list|)
block|{
throw|throw
operator|new
name|PigException
argument_list|(
literal|"Table not found : "
operator|+
name|nsoe
operator|.
name|getMessage
argument_list|()
argument_list|,
name|PIG_EXCEPTION_CODE
argument_list|)
throw|;
comment|// prettier error messages to frontend
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
name|e
argument_list|)
throw|;
block|}
finally|finally
block|{
name|HCatUtil
operator|.
name|closeHiveClientQuietly
argument_list|(
name|client
argument_list|)
expr_stmt|;
block|}
name|hcatTableCache
operator|.
name|put
argument_list|(
name|loc_server
argument_list|,
name|table
argument_list|)
expr_stmt|;
return|return
name|table
return|;
block|}
specifier|public
specifier|static
name|ResourceSchema
name|getResourceSchema
parameter_list|(
name|HCatSchema
name|hcatSchema
parameter_list|)
throws|throws
name|IOException
block|{
name|List
argument_list|<
name|ResourceFieldSchema
argument_list|>
name|rfSchemaList
init|=
operator|new
name|ArrayList
argument_list|<
name|ResourceFieldSchema
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|HCatFieldSchema
name|hfs
range|:
name|hcatSchema
operator|.
name|getFields
argument_list|()
control|)
block|{
name|ResourceFieldSchema
name|rfSchema
decl_stmt|;
name|rfSchema
operator|=
name|getResourceSchemaFromFieldSchema
argument_list|(
name|hfs
argument_list|)
expr_stmt|;
name|rfSchemaList
operator|.
name|add
argument_list|(
name|rfSchema
argument_list|)
expr_stmt|;
block|}
name|ResourceSchema
name|rSchema
init|=
operator|new
name|ResourceSchema
argument_list|()
decl_stmt|;
name|rSchema
operator|.
name|setFields
argument_list|(
name|rfSchemaList
operator|.
name|toArray
argument_list|(
operator|new
name|ResourceFieldSchema
index|[
name|rfSchemaList
operator|.
name|size
argument_list|()
index|]
argument_list|)
argument_list|)
expr_stmt|;
return|return
name|rSchema
return|;
block|}
specifier|private
specifier|static
name|ResourceFieldSchema
name|getResourceSchemaFromFieldSchema
parameter_list|(
name|HCatFieldSchema
name|hfs
parameter_list|)
throws|throws
name|IOException
block|{
name|ResourceFieldSchema
name|rfSchema
decl_stmt|;
comment|// if we are dealing with a bag or tuple column - need to worry about subschema
if|if
condition|(
name|hfs
operator|.
name|getType
argument_list|()
operator|==
name|Type
operator|.
name|STRUCT
condition|)
block|{
name|rfSchema
operator|=
operator|new
name|ResourceFieldSchema
argument_list|()
operator|.
name|setName
argument_list|(
name|hfs
operator|.
name|getName
argument_list|()
argument_list|)
operator|.
name|setDescription
argument_list|(
name|hfs
operator|.
name|getComment
argument_list|()
argument_list|)
operator|.
name|setType
argument_list|(
name|getPigType
argument_list|(
name|hfs
argument_list|)
argument_list|)
operator|.
name|setSchema
argument_list|(
name|getTupleSubSchema
argument_list|(
name|hfs
argument_list|)
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|hfs
operator|.
name|getType
argument_list|()
operator|==
name|Type
operator|.
name|ARRAY
condition|)
block|{
name|rfSchema
operator|=
operator|new
name|ResourceFieldSchema
argument_list|()
operator|.
name|setName
argument_list|(
name|hfs
operator|.
name|getName
argument_list|()
argument_list|)
operator|.
name|setDescription
argument_list|(
name|hfs
operator|.
name|getComment
argument_list|()
argument_list|)
operator|.
name|setType
argument_list|(
name|getPigType
argument_list|(
name|hfs
argument_list|)
argument_list|)
operator|.
name|setSchema
argument_list|(
name|getBagSubSchema
argument_list|(
name|hfs
argument_list|)
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|rfSchema
operator|=
operator|new
name|ResourceFieldSchema
argument_list|()
operator|.
name|setName
argument_list|(
name|hfs
operator|.
name|getName
argument_list|()
argument_list|)
operator|.
name|setDescription
argument_list|(
name|hfs
operator|.
name|getComment
argument_list|()
argument_list|)
operator|.
name|setType
argument_list|(
name|getPigType
argument_list|(
name|hfs
argument_list|)
argument_list|)
operator|.
name|setSchema
argument_list|(
literal|null
argument_list|)
expr_stmt|;
comment|// no munging inner-schemas
block|}
return|return
name|rfSchema
return|;
block|}
specifier|protected
specifier|static
name|ResourceSchema
name|getBagSubSchema
parameter_list|(
name|HCatFieldSchema
name|hfs
parameter_list|)
throws|throws
name|IOException
block|{
comment|// there are two cases - array<Type> and array<struct<...>>
comment|// in either case the element type of the array is represented in a
comment|// tuple field schema in the bag's field schema - the second case (struct)
comment|// more naturally translates to the tuple - in the first case (array<Type>)
comment|// we simulate the tuple by putting the single field in a tuple
name|Properties
name|props
init|=
name|UDFContext
operator|.
name|getUDFContext
argument_list|()
operator|.
name|getClientSystemProps
argument_list|()
decl_stmt|;
name|String
name|innerTupleName
init|=
name|HCatConstants
operator|.
name|HCAT_PIG_INNER_TUPLE_NAME_DEFAULT
decl_stmt|;
if|if
condition|(
name|props
operator|!=
literal|null
operator|&&
name|props
operator|.
name|containsKey
argument_list|(
name|HCatConstants
operator|.
name|HCAT_PIG_INNER_TUPLE_NAME
argument_list|)
condition|)
block|{
name|innerTupleName
operator|=
name|props
operator|.
name|getProperty
argument_list|(
name|HCatConstants
operator|.
name|HCAT_PIG_INNER_TUPLE_NAME
argument_list|)
operator|.
name|replaceAll
argument_list|(
literal|"FIELDNAME"
argument_list|,
name|hfs
operator|.
name|getName
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|String
name|innerFieldName
init|=
name|HCatConstants
operator|.
name|HCAT_PIG_INNER_FIELD_NAME_DEFAULT
decl_stmt|;
if|if
condition|(
name|props
operator|!=
literal|null
operator|&&
name|props
operator|.
name|containsKey
argument_list|(
name|HCatConstants
operator|.
name|HCAT_PIG_INNER_FIELD_NAME
argument_list|)
condition|)
block|{
name|innerFieldName
operator|=
name|props
operator|.
name|getProperty
argument_list|(
name|HCatConstants
operator|.
name|HCAT_PIG_INNER_FIELD_NAME
argument_list|)
operator|.
name|replaceAll
argument_list|(
literal|"FIELDNAME"
argument_list|,
name|hfs
operator|.
name|getName
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|ResourceFieldSchema
index|[]
name|bagSubFieldSchemas
init|=
operator|new
name|ResourceFieldSchema
index|[
literal|1
index|]
decl_stmt|;
name|bagSubFieldSchemas
index|[
literal|0
index|]
operator|=
operator|new
name|ResourceFieldSchema
argument_list|()
operator|.
name|setName
argument_list|(
name|innerTupleName
argument_list|)
operator|.
name|setDescription
argument_list|(
literal|"The tuple in the bag"
argument_list|)
operator|.
name|setType
argument_list|(
name|DataType
operator|.
name|TUPLE
argument_list|)
expr_stmt|;
name|HCatFieldSchema
name|arrayElementFieldSchema
init|=
name|hfs
operator|.
name|getArrayElementSchema
argument_list|()
operator|.
name|get
argument_list|(
literal|0
argument_list|)
decl_stmt|;
if|if
condition|(
name|arrayElementFieldSchema
operator|.
name|getType
argument_list|()
operator|==
name|Type
operator|.
name|STRUCT
condition|)
block|{
name|bagSubFieldSchemas
index|[
literal|0
index|]
operator|.
name|setSchema
argument_list|(
name|getTupleSubSchema
argument_list|(
name|arrayElementFieldSchema
argument_list|)
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|arrayElementFieldSchema
operator|.
name|getType
argument_list|()
operator|==
name|Type
operator|.
name|ARRAY
condition|)
block|{
name|ResourceSchema
name|s
init|=
operator|new
name|ResourceSchema
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|ResourceFieldSchema
argument_list|>
name|lrfs
init|=
name|Arrays
operator|.
name|asList
argument_list|(
name|getResourceSchemaFromFieldSchema
argument_list|(
name|arrayElementFieldSchema
argument_list|)
argument_list|)
decl_stmt|;
name|s
operator|.
name|setFields
argument_list|(
name|lrfs
operator|.
name|toArray
argument_list|(
operator|new
name|ResourceFieldSchema
index|[
name|lrfs
operator|.
name|size
argument_list|()
index|]
argument_list|)
argument_list|)
expr_stmt|;
name|bagSubFieldSchemas
index|[
literal|0
index|]
operator|.
name|setSchema
argument_list|(
name|s
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|ResourceFieldSchema
index|[]
name|innerTupleFieldSchemas
init|=
operator|new
name|ResourceFieldSchema
index|[
literal|1
index|]
decl_stmt|;
name|innerTupleFieldSchemas
index|[
literal|0
index|]
operator|=
operator|new
name|ResourceFieldSchema
argument_list|()
operator|.
name|setName
argument_list|(
name|innerFieldName
argument_list|)
operator|.
name|setDescription
argument_list|(
literal|"The inner field in the tuple in the bag"
argument_list|)
operator|.
name|setType
argument_list|(
name|getPigType
argument_list|(
name|arrayElementFieldSchema
argument_list|)
argument_list|)
operator|.
name|setSchema
argument_list|(
literal|null
argument_list|)
expr_stmt|;
comment|// the element type is not a tuple - so no subschema
name|bagSubFieldSchemas
index|[
literal|0
index|]
operator|.
name|setSchema
argument_list|(
operator|new
name|ResourceSchema
argument_list|()
operator|.
name|setFields
argument_list|(
name|innerTupleFieldSchemas
argument_list|)
argument_list|)
expr_stmt|;
block|}
return|return
operator|new
name|ResourceSchema
argument_list|()
operator|.
name|setFields
argument_list|(
name|bagSubFieldSchemas
argument_list|)
return|;
block|}
specifier|private
specifier|static
name|ResourceSchema
name|getTupleSubSchema
parameter_list|(
name|HCatFieldSchema
name|hfs
parameter_list|)
throws|throws
name|IOException
block|{
comment|// for each struct subfield, create equivalent ResourceFieldSchema
name|ResourceSchema
name|s
init|=
operator|new
name|ResourceSchema
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|ResourceFieldSchema
argument_list|>
name|lrfs
init|=
operator|new
name|ArrayList
argument_list|<
name|ResourceFieldSchema
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|HCatFieldSchema
name|subField
range|:
name|hfs
operator|.
name|getStructSubSchema
argument_list|()
operator|.
name|getFields
argument_list|()
control|)
block|{
name|lrfs
operator|.
name|add
argument_list|(
name|getResourceSchemaFromFieldSchema
argument_list|(
name|subField
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|s
operator|.
name|setFields
argument_list|(
name|lrfs
operator|.
name|toArray
argument_list|(
operator|new
name|ResourceFieldSchema
index|[
name|lrfs
operator|.
name|size
argument_list|()
index|]
argument_list|)
argument_list|)
expr_stmt|;
return|return
name|s
return|;
block|}
comment|/**    * @param hfs the field schema of the column    * @return corresponding pig type    * @throws IOException    */
specifier|static
specifier|public
name|byte
name|getPigType
parameter_list|(
name|HCatFieldSchema
name|hfs
parameter_list|)
throws|throws
name|IOException
block|{
return|return
name|getPigType
argument_list|(
name|hfs
operator|.
name|getType
argument_list|()
argument_list|)
return|;
block|}
comment|/**    * Defines a mapping of HCatalog type to Pig type; not every mapping is exact,     * see {@link #extractPigObject(Object, org.apache.hive.hcatalog.data.schema.HCatFieldSchema)}    * See http://pig.apache.org/docs/r0.12.0/basic.html#data-types    * See {@link org.apache.hive.hcatalog.pig.HCatBaseStorer#validateSchema(org.apache.pig.impl.logicalLayer.schema.Schema.FieldSchema, org.apache.hive.hcatalog.data.schema.HCatFieldSchema, org.apache.pig.impl.logicalLayer.schema.Schema, org.apache.hive.hcatalog.data.schema.HCatSchema, int)}    * for Pig->Hive type mapping.    */
specifier|static
specifier|public
name|byte
name|getPigType
parameter_list|(
name|Type
name|type
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|type
operator|==
name|Type
operator|.
name|STRING
operator|||
name|type
operator|==
name|Type
operator|.
name|CHAR
operator|||
name|type
operator|==
name|Type
operator|.
name|VARCHAR
condition|)
block|{
comment|//CHARARRAY is unbounded so Hive->Pig is lossless
return|return
name|DataType
operator|.
name|CHARARRAY
return|;
block|}
if|if
condition|(
operator|(
name|type
operator|==
name|Type
operator|.
name|INT
operator|)
operator|||
operator|(
name|type
operator|==
name|Type
operator|.
name|SMALLINT
operator|)
operator|||
operator|(
name|type
operator|==
name|Type
operator|.
name|TINYINT
operator|)
condition|)
block|{
return|return
name|DataType
operator|.
name|INTEGER
return|;
block|}
if|if
condition|(
name|type
operator|==
name|Type
operator|.
name|ARRAY
condition|)
block|{
return|return
name|DataType
operator|.
name|BAG
return|;
block|}
if|if
condition|(
name|type
operator|==
name|Type
operator|.
name|STRUCT
condition|)
block|{
return|return
name|DataType
operator|.
name|TUPLE
return|;
block|}
if|if
condition|(
name|type
operator|==
name|Type
operator|.
name|MAP
condition|)
block|{
return|return
name|DataType
operator|.
name|MAP
return|;
block|}
if|if
condition|(
name|type
operator|==
name|Type
operator|.
name|BIGINT
condition|)
block|{
return|return
name|DataType
operator|.
name|LONG
return|;
block|}
if|if
condition|(
name|type
operator|==
name|Type
operator|.
name|FLOAT
condition|)
block|{
return|return
name|DataType
operator|.
name|FLOAT
return|;
block|}
if|if
condition|(
name|type
operator|==
name|Type
operator|.
name|DOUBLE
condition|)
block|{
return|return
name|DataType
operator|.
name|DOUBLE
return|;
block|}
if|if
condition|(
name|type
operator|==
name|Type
operator|.
name|BINARY
condition|)
block|{
return|return
name|DataType
operator|.
name|BYTEARRAY
return|;
block|}
if|if
condition|(
name|type
operator|==
name|Type
operator|.
name|BOOLEAN
operator|&&
name|pigHasBooleanSupport
condition|)
block|{
return|return
name|DataType
operator|.
name|BOOLEAN
return|;
block|}
if|if
condition|(
name|type
operator|==
name|Type
operator|.
name|DECIMAL
condition|)
block|{
comment|//Hive is more restrictive, so Hive->Pig works
return|return
name|DataType
operator|.
name|BIGDECIMAL
return|;
block|}
if|if
condition|(
name|type
operator|==
name|Type
operator|.
name|DATE
operator|||
name|type
operator|==
name|Type
operator|.
name|TIMESTAMP
condition|)
block|{
comment|//Hive Date is representable as Pig DATETIME
return|return
name|DataType
operator|.
name|DATETIME
return|;
block|}
throw|throw
operator|new
name|PigException
argument_list|(
literal|"HCatalog column type '"
operator|+
name|type
operator|.
name|toString
argument_list|()
operator|+
literal|"' is not supported in Pig as a column type"
argument_list|,
name|PIG_EXCEPTION_CODE
argument_list|)
throw|;
block|}
specifier|public
specifier|static
name|Tuple
name|transformToTuple
parameter_list|(
name|HCatRecord
name|hr
parameter_list|,
name|HCatSchema
name|hs
parameter_list|)
throws|throws
name|Exception
block|{
if|if
condition|(
name|hr
operator|==
literal|null
condition|)
block|{
return|return
literal|null
return|;
block|}
return|return
name|transformToTuple
argument_list|(
name|hr
operator|.
name|getAll
argument_list|()
argument_list|,
name|hs
argument_list|)
return|;
block|}
comment|/**    * Converts object from Hive's value system to Pig's value system    * see HCatBaseStorer#getJavaObj() for Pig->Hive conversion     * @param o object from Hive value system    * @return object in Pig value system     */
specifier|public
specifier|static
name|Object
name|extractPigObject
parameter_list|(
name|Object
name|o
parameter_list|,
name|HCatFieldSchema
name|hfs
parameter_list|)
throws|throws
name|Exception
block|{
comment|/*Note that HCatRecordSerDe.serializePrimitiveField() will be called before this, thus some     * type promotion/conversion may occur: e.g. Short to Integer.  We should refactor this so     * that it's hapenning in one place per module/product that we are integrating with.     * All Pig conversion should be done here, etc.*/
if|if
condition|(
name|o
operator|==
literal|null
condition|)
block|{
return|return
literal|null
return|;
block|}
name|Object
name|result
decl_stmt|;
name|Type
name|itemType
init|=
name|hfs
operator|.
name|getType
argument_list|()
decl_stmt|;
switch|switch
condition|(
name|itemType
condition|)
block|{
case|case
name|BINARY
case|:
name|result
operator|=
operator|new
name|DataByteArray
argument_list|(
operator|(
name|byte
index|[]
operator|)
name|o
argument_list|)
expr_stmt|;
break|break;
case|case
name|STRUCT
case|:
name|result
operator|=
name|transformToTuple
argument_list|(
operator|(
name|List
argument_list|<
name|?
argument_list|>
operator|)
name|o
argument_list|,
name|hfs
argument_list|)
expr_stmt|;
break|break;
case|case
name|ARRAY
case|:
name|result
operator|=
name|transformToBag
argument_list|(
operator|(
name|List
argument_list|<
name|?
argument_list|>
operator|)
name|o
argument_list|,
name|hfs
argument_list|)
expr_stmt|;
break|break;
case|case
name|MAP
case|:
name|result
operator|=
name|transformToPigMap
argument_list|(
operator|(
name|Map
argument_list|<
name|?
argument_list|,
name|?
argument_list|>
operator|)
name|o
argument_list|,
name|hfs
argument_list|)
expr_stmt|;
break|break;
case|case
name|DECIMAL
case|:
name|result
operator|=
operator|(
operator|(
name|HiveDecimal
operator|)
name|o
operator|)
operator|.
name|bigDecimalValue
argument_list|()
expr_stmt|;
break|break;
case|case
name|CHAR
case|:
name|result
operator|=
operator|(
operator|(
name|HiveChar
operator|)
name|o
operator|)
operator|.
name|getValue
argument_list|()
expr_stmt|;
break|break;
case|case
name|VARCHAR
case|:
name|result
operator|=
operator|(
operator|(
name|HiveVarchar
operator|)
name|o
operator|)
operator|.
name|getValue
argument_list|()
expr_stmt|;
break|break;
case|case
name|DATE
case|:
comment|/*java.sql.Date is weird.  It automatically adjusts it's millis value to be in the local TZ       * e.g. d = new java.sql.Date(System.currentMillis()).toString() so if you do this just after       * midnight in Palo Alto, you'll get yesterday's date printed out.*/
name|Date
name|d
init|=
operator|(
name|Date
operator|)
name|o
decl_stmt|;
name|result
operator|=
operator|new
name|DateTime
argument_list|(
name|d
operator|.
name|getYear
argument_list|()
operator|+
literal|1900
argument_list|,
name|d
operator|.
name|getMonth
argument_list|()
operator|+
literal|1
argument_list|,
name|d
operator|.
name|getDate
argument_list|()
argument_list|,
literal|0
argument_list|,
literal|0
argument_list|)
expr_stmt|;
comment|//uses local TZ
break|break;
case|case
name|TIMESTAMP
case|:
comment|/*DATA TRUNCATION!!!        Timestamp may have nanos; we'll strip those away and create a Joda DateTime        object in local TZ; This is arbitrary, since Hive value doesn't have any TZ notion, but        we need to set something for TZ.        Timestamp is consistently in GMT (unless you call toString() on it) so we use millis*/
name|result
operator|=
operator|new
name|DateTime
argument_list|(
operator|(
operator|(
name|Timestamp
operator|)
name|o
operator|)
operator|.
name|getTime
argument_list|()
argument_list|)
expr_stmt|;
comment|//uses local TZ
break|break;
default|default:
name|result
operator|=
name|o
expr_stmt|;
break|break;
block|}
return|return
name|result
return|;
block|}
specifier|private
specifier|static
name|Tuple
name|transformToTuple
parameter_list|(
name|List
argument_list|<
name|?
argument_list|>
name|objList
parameter_list|,
name|HCatFieldSchema
name|hfs
parameter_list|)
throws|throws
name|Exception
block|{
try|try
block|{
return|return
name|transformToTuple
argument_list|(
name|objList
argument_list|,
name|hfs
operator|.
name|getStructSubSchema
argument_list|()
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
if|if
condition|(
name|hfs
operator|.
name|getType
argument_list|()
operator|!=
name|Type
operator|.
name|STRUCT
condition|)
block|{
throw|throw
operator|new
name|Exception
argument_list|(
literal|"Expected Struct type, got "
operator|+
name|hfs
operator|.
name|getType
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
else|else
block|{
throw|throw
name|e
throw|;
block|}
block|}
block|}
specifier|private
specifier|static
name|Tuple
name|transformToTuple
parameter_list|(
name|List
argument_list|<
name|?
argument_list|>
name|objList
parameter_list|,
name|HCatSchema
name|hs
parameter_list|)
throws|throws
name|Exception
block|{
if|if
condition|(
name|objList
operator|==
literal|null
condition|)
block|{
return|return
literal|null
return|;
block|}
name|Tuple
name|t
init|=
name|tupFac
operator|.
name|newTuple
argument_list|(
name|objList
operator|.
name|size
argument_list|()
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|HCatFieldSchema
argument_list|>
name|subFields
init|=
name|hs
operator|.
name|getFields
argument_list|()
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|subFields
operator|.
name|size
argument_list|()
condition|;
name|i
operator|++
control|)
block|{
name|t
operator|.
name|set
argument_list|(
name|i
argument_list|,
name|extractPigObject
argument_list|(
name|objList
operator|.
name|get
argument_list|(
name|i
argument_list|)
argument_list|,
name|subFields
operator|.
name|get
argument_list|(
name|i
argument_list|)
argument_list|)
argument_list|)
expr_stmt|;
block|}
return|return
name|t
return|;
block|}
specifier|private
specifier|static
name|Map
argument_list|<
name|String
argument_list|,
name|Object
argument_list|>
name|transformToPigMap
parameter_list|(
name|Map
argument_list|<
name|?
argument_list|,
name|?
argument_list|>
name|map
parameter_list|,
name|HCatFieldSchema
name|hfs
parameter_list|)
throws|throws
name|Exception
block|{
if|if
condition|(
name|map
operator|==
literal|null
condition|)
block|{
return|return
literal|null
return|;
block|}
name|Map
argument_list|<
name|String
argument_list|,
name|Object
argument_list|>
name|result
init|=
operator|new
name|HashMap
argument_list|<
name|String
argument_list|,
name|Object
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|Entry
argument_list|<
name|?
argument_list|,
name|?
argument_list|>
name|entry
range|:
name|map
operator|.
name|entrySet
argument_list|()
control|)
block|{
comment|// since map key for Pig has to be Strings
name|result
operator|.
name|put
argument_list|(
name|entry
operator|.
name|getKey
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|,
name|extractPigObject
argument_list|(
name|entry
operator|.
name|getValue
argument_list|()
argument_list|,
name|hfs
operator|.
name|getMapValueSchema
argument_list|()
operator|.
name|get
argument_list|(
literal|0
argument_list|)
argument_list|)
argument_list|)
expr_stmt|;
block|}
return|return
name|result
return|;
block|}
specifier|private
specifier|static
name|DataBag
name|transformToBag
parameter_list|(
name|List
argument_list|<
name|?
argument_list|>
name|list
parameter_list|,
name|HCatFieldSchema
name|hfs
parameter_list|)
throws|throws
name|Exception
block|{
if|if
condition|(
name|list
operator|==
literal|null
condition|)
block|{
return|return
literal|null
return|;
block|}
name|HCatFieldSchema
name|elementSubFieldSchema
init|=
name|hfs
operator|.
name|getArrayElementSchema
argument_list|()
operator|.
name|getFields
argument_list|()
operator|.
name|get
argument_list|(
literal|0
argument_list|)
decl_stmt|;
name|DataBag
name|db
init|=
operator|new
name|DefaultDataBag
argument_list|()
decl_stmt|;
for|for
control|(
name|Object
name|o
range|:
name|list
control|)
block|{
name|Tuple
name|tuple
decl_stmt|;
if|if
condition|(
name|elementSubFieldSchema
operator|.
name|getType
argument_list|()
operator|==
name|Type
operator|.
name|STRUCT
condition|)
block|{
name|tuple
operator|=
name|transformToTuple
argument_list|(
operator|(
name|List
argument_list|<
name|?
argument_list|>
operator|)
name|o
argument_list|,
name|elementSubFieldSchema
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// bags always contain tuples
name|tuple
operator|=
name|tupFac
operator|.
name|newTuple
argument_list|(
name|extractPigObject
argument_list|(
name|o
argument_list|,
name|elementSubFieldSchema
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|db
operator|.
name|add
argument_list|(
name|tuple
argument_list|)
expr_stmt|;
block|}
return|return
name|db
return|;
block|}
specifier|private
specifier|static
name|void
name|validateHCatSchemaFollowsPigRules
parameter_list|(
name|HCatSchema
name|tblSchema
parameter_list|)
throws|throws
name|PigException
block|{
for|for
control|(
name|HCatFieldSchema
name|hcatField
range|:
name|tblSchema
operator|.
name|getFields
argument_list|()
control|)
block|{
name|validateHcatFieldFollowsPigRules
argument_list|(
name|hcatField
argument_list|)
expr_stmt|;
block|}
block|}
specifier|private
specifier|static
name|void
name|validateHcatFieldFollowsPigRules
parameter_list|(
name|HCatFieldSchema
name|hcatField
parameter_list|)
throws|throws
name|PigException
block|{
try|try
block|{
name|Type
name|hType
init|=
name|hcatField
operator|.
name|getType
argument_list|()
decl_stmt|;
switch|switch
condition|(
name|hType
condition|)
block|{
case|case
name|BOOLEAN
case|:
if|if
condition|(
operator|!
name|pigHasBooleanSupport
condition|)
block|{
throw|throw
operator|new
name|PigException
argument_list|(
literal|"Incompatible type found in HCat table schema: "
operator|+
name|hcatField
argument_list|,
name|PigHCatUtil
operator|.
name|PIG_EXCEPTION_CODE
argument_list|)
throw|;
block|}
break|break;
case|case
name|ARRAY
case|:
name|validateHCatSchemaFollowsPigRules
argument_list|(
name|hcatField
operator|.
name|getArrayElementSchema
argument_list|()
argument_list|)
expr_stmt|;
break|break;
case|case
name|STRUCT
case|:
name|validateHCatSchemaFollowsPigRules
argument_list|(
name|hcatField
operator|.
name|getStructSubSchema
argument_list|()
argument_list|)
expr_stmt|;
break|break;
case|case
name|MAP
case|:
comment|// key is only string
if|if
condition|(
name|hcatField
operator|.
name|getMapKeyType
argument_list|()
operator|!=
name|Type
operator|.
name|STRING
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Converting non-String key of map "
operator|+
name|hcatField
operator|.
name|getName
argument_list|()
operator|+
literal|" from "
operator|+
name|hcatField
operator|.
name|getMapKeyType
argument_list|()
operator|+
literal|" to String."
argument_list|)
expr_stmt|;
block|}
name|validateHCatSchemaFollowsPigRules
argument_list|(
name|hcatField
operator|.
name|getMapValueSchema
argument_list|()
argument_list|)
expr_stmt|;
break|break;
block|}
block|}
catch|catch
parameter_list|(
name|HCatException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|PigException
argument_list|(
literal|"Incompatible type found in hcat table schema: "
operator|+
name|hcatField
argument_list|,
name|PigHCatUtil
operator|.
name|PIG_EXCEPTION_CODE
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
specifier|static
name|void
name|validateHCatTableSchemaFollowsPigRules
parameter_list|(
name|HCatSchema
name|hcatTableSchema
parameter_list|)
throws|throws
name|IOException
block|{
name|validateHCatSchemaFollowsPigRules
argument_list|(
name|hcatTableSchema
argument_list|)
expr_stmt|;
block|}
specifier|public
specifier|static
name|void
name|getConfigFromUDFProperties
parameter_list|(
name|Properties
name|p
parameter_list|,
name|Configuration
name|config
parameter_list|,
name|String
name|propName
parameter_list|)
block|{
if|if
condition|(
name|p
operator|.
name|getProperty
argument_list|(
name|propName
argument_list|)
operator|!=
literal|null
condition|)
block|{
name|config
operator|.
name|set
argument_list|(
name|propName
argument_list|,
name|p
operator|.
name|getProperty
argument_list|(
name|propName
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
specifier|public
specifier|static
name|void
name|saveConfigIntoUDFProperties
parameter_list|(
name|Properties
name|p
parameter_list|,
name|Configuration
name|config
parameter_list|,
name|String
name|propName
parameter_list|)
block|{
if|if
condition|(
name|config
operator|.
name|get
argument_list|(
name|propName
argument_list|)
operator|!=
literal|null
condition|)
block|{
name|p
operator|.
name|setProperty
argument_list|(
name|propName
argument_list|,
name|config
operator|.
name|get
argument_list|(
name|propName
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
block|}
end_class

end_unit

