begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/*  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hcatalog
operator|.
name|mapreduce
package|;
end_package

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|LinkedHashMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Properties
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
operator|.
name|Entry
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|Log
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|LogFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configuration
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileStatus
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|PathFilter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|permission
operator|.
name|FsPermission
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|HiveMetaStoreClient
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|FieldSchema
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|MetaException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|StorageDescriptor
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Table
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|thrift
operator|.
name|DelegationTokenIdentifier
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|thrift
operator|.
name|DelegationTokenSelector
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|Text
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|Writable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|WritableComparable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|Job
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|JobContext
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|OutputCommitter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|OutputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|RecordWriter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|TaskAttemptContext
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|security
operator|.
name|AccessControlException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|security
operator|.
name|UserGroupInformation
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|security
operator|.
name|token
operator|.
name|Token
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|security
operator|.
name|token
operator|.
name|TokenIdentifier
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|security
operator|.
name|token
operator|.
name|TokenSelector
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|security
operator|.
name|token
operator|.
name|delegation
operator|.
name|AbstractDelegationTokenIdentifier
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hcatalog
operator|.
name|common
operator|.
name|ErrorType
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hcatalog
operator|.
name|common
operator|.
name|HCatConstants
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hcatalog
operator|.
name|common
operator|.
name|HCatException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hcatalog
operator|.
name|common
operator|.
name|HCatUtil
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hcatalog
operator|.
name|data
operator|.
name|HCatRecord
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hcatalog
operator|.
name|data
operator|.
name|schema
operator|.
name|HCatSchema
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|thrift
operator|.
name|TException
import|;
end_import

begin_comment
comment|/** The OutputFormat to use to write data to HCat. The key value is ignored and  * and should be given as null. The value is the HCatRecord to write.*/
end_comment

begin_class
specifier|public
class|class
name|HCatOutputFormat
extends|extends
name|HCatBaseOutputFormat
block|{
comment|//    static final private Log LOG = LogFactory.getLog(HCatOutputFormat.class);
comment|/** The directory under which data is initially written for a non partitioned table */
specifier|protected
specifier|static
specifier|final
name|String
name|TEMP_DIR_NAME
init|=
literal|"_TEMP"
decl_stmt|;
comment|/** */
specifier|protected
specifier|static
specifier|final
name|String
name|DYNTEMP_DIR_NAME
init|=
literal|"_DYN"
decl_stmt|;
specifier|private
specifier|static
name|Map
argument_list|<
name|String
argument_list|,
name|Token
argument_list|<
name|?
extends|extends
name|AbstractDelegationTokenIdentifier
argument_list|>
argument_list|>
name|tokenMap
init|=
operator|new
name|HashMap
argument_list|<
name|String
argument_list|,
name|Token
argument_list|<
name|?
extends|extends
name|AbstractDelegationTokenIdentifier
argument_list|>
argument_list|>
argument_list|()
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|PathFilter
name|hiddenFileFilter
init|=
operator|new
name|PathFilter
argument_list|()
block|{
specifier|public
name|boolean
name|accept
parameter_list|(
name|Path
name|p
parameter_list|)
block|{
name|String
name|name
init|=
name|p
operator|.
name|getName
argument_list|()
decl_stmt|;
return|return
operator|!
name|name
operator|.
name|startsWith
argument_list|(
literal|"_"
argument_list|)
operator|&&
operator|!
name|name
operator|.
name|startsWith
argument_list|(
literal|"."
argument_list|)
return|;
block|}
block|}
decl_stmt|;
specifier|private
specifier|static
name|int
name|maxDynamicPartitions
decl_stmt|;
specifier|private
specifier|static
name|boolean
name|harRequested
decl_stmt|;
comment|/**      * Set the info about the output to write for the Job. This queries the metadata server      * to find the StorageDriver to use for the table.  Throws error if partition is already published.      * @param job the job object      * @param outputInfo the table output info      * @throws IOException the exception in communicating with the metadata server      */
annotation|@
name|SuppressWarnings
argument_list|(
literal|"unchecked"
argument_list|)
specifier|public
specifier|static
name|void
name|setOutput
parameter_list|(
name|Job
name|job
parameter_list|,
name|HCatTableInfo
name|outputInfo
parameter_list|)
throws|throws
name|IOException
block|{
name|HiveMetaStoreClient
name|client
init|=
literal|null
decl_stmt|;
try|try
block|{
name|Configuration
name|conf
init|=
name|job
operator|.
name|getConfiguration
argument_list|()
decl_stmt|;
name|client
operator|=
name|createHiveClient
argument_list|(
name|outputInfo
operator|.
name|getServerUri
argument_list|()
argument_list|,
name|conf
argument_list|)
expr_stmt|;
name|Table
name|table
init|=
name|client
operator|.
name|getTable
argument_list|(
name|outputInfo
operator|.
name|getDatabaseName
argument_list|()
argument_list|,
name|outputInfo
operator|.
name|getTableName
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|table
operator|.
name|getPartitionKeysSize
argument_list|()
operator|==
literal|0
condition|)
block|{
if|if
condition|(
operator|(
name|outputInfo
operator|.
name|getPartitionValues
argument_list|()
operator|!=
literal|null
operator|)
operator|&&
operator|(
operator|!
name|outputInfo
operator|.
name|getPartitionValues
argument_list|()
operator|.
name|isEmpty
argument_list|()
operator|)
condition|)
block|{
comment|// attempt made to save partition values in non-partitioned table - throw error.
throw|throw
operator|new
name|HCatException
argument_list|(
name|ErrorType
operator|.
name|ERROR_INVALID_PARTITION_VALUES
argument_list|,
literal|"Partition values specified for non-partitioned table"
argument_list|)
throw|;
block|}
comment|// non-partitioned table
name|outputInfo
operator|.
name|setPartitionValues
argument_list|(
operator|new
name|HashMap
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
argument_list|()
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// partitioned table, we expect partition values
comment|// convert user specified map to have lower case key names
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|valueMap
init|=
operator|new
name|HashMap
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
argument_list|()
decl_stmt|;
if|if
condition|(
name|outputInfo
operator|.
name|getPartitionValues
argument_list|()
operator|!=
literal|null
condition|)
block|{
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|entry
range|:
name|outputInfo
operator|.
name|getPartitionValues
argument_list|()
operator|.
name|entrySet
argument_list|()
control|)
block|{
name|valueMap
operator|.
name|put
argument_list|(
name|entry
operator|.
name|getKey
argument_list|()
operator|.
name|toLowerCase
argument_list|()
argument_list|,
name|entry
operator|.
name|getValue
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
operator|(
name|outputInfo
operator|.
name|getPartitionValues
argument_list|()
operator|==
literal|null
operator|)
operator|||
operator|(
name|outputInfo
operator|.
name|getPartitionValues
argument_list|()
operator|.
name|size
argument_list|()
operator|<
name|table
operator|.
name|getPartitionKeysSize
argument_list|()
operator|)
condition|)
block|{
comment|// dynamic partition usecase - partition values were null, or not all were specified
comment|// need to figure out which keys are not specified.
name|List
argument_list|<
name|String
argument_list|>
name|dynamicPartitioningKeys
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
name|boolean
name|firstItem
init|=
literal|true
decl_stmt|;
for|for
control|(
name|FieldSchema
name|fs
range|:
name|table
operator|.
name|getPartitionKeys
argument_list|()
control|)
block|{
if|if
condition|(
operator|!
name|valueMap
operator|.
name|containsKey
argument_list|(
name|fs
operator|.
name|getName
argument_list|()
operator|.
name|toLowerCase
argument_list|()
argument_list|)
condition|)
block|{
name|dynamicPartitioningKeys
operator|.
name|add
argument_list|(
name|fs
operator|.
name|getName
argument_list|()
operator|.
name|toLowerCase
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
name|valueMap
operator|.
name|size
argument_list|()
operator|+
name|dynamicPartitioningKeys
operator|.
name|size
argument_list|()
operator|!=
name|table
operator|.
name|getPartitionKeysSize
argument_list|()
condition|)
block|{
comment|// If this isn't equal, then bogus key values have been inserted, error out.
throw|throw
operator|new
name|HCatException
argument_list|(
name|ErrorType
operator|.
name|ERROR_INVALID_PARTITION_VALUES
argument_list|,
literal|"Invalid partition keys specified"
argument_list|)
throw|;
block|}
name|outputInfo
operator|.
name|setDynamicPartitioningKeys
argument_list|(
name|dynamicPartitioningKeys
argument_list|)
expr_stmt|;
name|String
name|dynHash
decl_stmt|;
if|if
condition|(
operator|(
name|dynHash
operator|=
name|conf
operator|.
name|get
argument_list|(
name|HCatConstants
operator|.
name|HCAT_DYNAMIC_PTN_JOBID
argument_list|)
operator|)
operator|==
literal|null
condition|)
block|{
name|dynHash
operator|=
name|String
operator|.
name|valueOf
argument_list|(
name|Math
operator|.
name|random
argument_list|()
argument_list|)
expr_stmt|;
comment|//              LOG.info("New dynHash : ["+dynHash+"]");
comment|//            }else{
comment|//              LOG.info("Old dynHash : ["+dynHash+"]");
block|}
name|conf
operator|.
name|set
argument_list|(
name|HCatConstants
operator|.
name|HCAT_DYNAMIC_PTN_JOBID
argument_list|,
name|dynHash
argument_list|)
expr_stmt|;
block|}
name|outputInfo
operator|.
name|setPartitionValues
argument_list|(
name|valueMap
argument_list|)
expr_stmt|;
block|}
comment|//Handle duplicate publish
name|handleDuplicatePublish
argument_list|(
name|job
argument_list|,
name|outputInfo
argument_list|,
name|client
argument_list|,
name|table
argument_list|)
expr_stmt|;
name|StorageDescriptor
name|tblSD
init|=
name|table
operator|.
name|getSd
argument_list|()
decl_stmt|;
name|HCatSchema
name|tableSchema
init|=
name|HCatUtil
operator|.
name|extractSchemaFromStorageDescriptor
argument_list|(
name|tblSD
argument_list|)
decl_stmt|;
name|StorerInfo
name|storerInfo
init|=
name|InitializeInput
operator|.
name|extractStorerInfo
argument_list|(
name|tblSD
argument_list|,
name|table
operator|.
name|getParameters
argument_list|()
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|partitionCols
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|FieldSchema
name|schema
range|:
name|table
operator|.
name|getPartitionKeys
argument_list|()
control|)
block|{
name|partitionCols
operator|.
name|add
argument_list|(
name|schema
operator|.
name|getName
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|Class
argument_list|<
name|?
extends|extends
name|HCatOutputStorageDriver
argument_list|>
name|driverClass
init|=
operator|(
name|Class
argument_list|<
name|?
extends|extends
name|HCatOutputStorageDriver
argument_list|>
operator|)
name|Class
operator|.
name|forName
argument_list|(
name|storerInfo
operator|.
name|getOutputSDClass
argument_list|()
argument_list|)
decl_stmt|;
name|HCatOutputStorageDriver
name|driver
init|=
name|driverClass
operator|.
name|newInstance
argument_list|()
decl_stmt|;
name|String
name|tblLocation
init|=
name|tblSD
operator|.
name|getLocation
argument_list|()
decl_stmt|;
name|String
name|location
init|=
name|driver
operator|.
name|getOutputLocation
argument_list|(
name|job
argument_list|,
name|tblLocation
argument_list|,
name|partitionCols
argument_list|,
name|outputInfo
operator|.
name|getPartitionValues
argument_list|()
argument_list|,
name|conf
operator|.
name|get
argument_list|(
name|HCatConstants
operator|.
name|HCAT_DYNAMIC_PTN_JOBID
argument_list|)
argument_list|)
decl_stmt|;
comment|//Serialize the output info into the configuration
name|OutputJobInfo
name|jobInfo
init|=
operator|new
name|OutputJobInfo
argument_list|(
name|outputInfo
argument_list|,
name|tableSchema
argument_list|,
name|tableSchema
argument_list|,
name|storerInfo
argument_list|,
name|location
argument_list|,
name|table
argument_list|)
decl_stmt|;
name|jobInfo
operator|.
name|setHarRequested
argument_list|(
name|harRequested
argument_list|)
expr_stmt|;
name|jobInfo
operator|.
name|setMaximumDynamicPartitions
argument_list|(
name|maxDynamicPartitions
argument_list|)
expr_stmt|;
name|conf
operator|.
name|set
argument_list|(
name|HCatConstants
operator|.
name|HCAT_KEY_OUTPUT_INFO
argument_list|,
name|HCatUtil
operator|.
name|serialize
argument_list|(
name|jobInfo
argument_list|)
argument_list|)
expr_stmt|;
name|Path
name|tblPath
init|=
operator|new
name|Path
argument_list|(
name|tblLocation
argument_list|)
decl_stmt|;
comment|/*  Set the umask in conf such that files/dirs get created with table-dir          * permissions. Following three assumptions are made:          * 1. Actual files/dirs creation is done by RecordWriter of underlying          * output format. It is assumed that they use default permissions while creation.          * 2. Default Permissions = FsPermission.getDefault() = 777.          * 3. UMask is honored by underlying filesystem.          */
name|FsPermission
operator|.
name|setUMask
argument_list|(
name|conf
argument_list|,
name|FsPermission
operator|.
name|getDefault
argument_list|()
operator|.
name|applyUMask
argument_list|(
name|tblPath
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
operator|.
name|getFileStatus
argument_list|(
name|tblPath
argument_list|)
operator|.
name|getPermission
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
if|if
condition|(
name|UserGroupInformation
operator|.
name|isSecurityEnabled
argument_list|()
condition|)
block|{
name|UserGroupInformation
name|ugi
init|=
name|UserGroupInformation
operator|.
name|getCurrentUser
argument_list|()
decl_stmt|;
comment|// check if oozie has set up a hcat deleg. token - if so use it
name|TokenSelector
argument_list|<
name|?
extends|extends
name|TokenIdentifier
argument_list|>
name|tokenSelector
init|=
operator|new
name|DelegationTokenSelector
argument_list|()
decl_stmt|;
comment|// TODO: will oozie use a "service" called "oozie" - then instead of
comment|// new Text() do new Text("oozie") below - if this change is made also
comment|// remember to do:
comment|//  job.getConfiguration().set(HCAT_KEY_TOKEN_SIGNATURE, "oozie");
comment|// Also change code in HCatOutputCommitter.cleanupJob() to cancel the
comment|// token only if token.service is not "oozie" - remove the condition of
comment|// HCAT_KEY_TOKEN_SIGNATURE != null in that code.
name|Token
argument_list|<
name|?
extends|extends
name|TokenIdentifier
argument_list|>
name|token
init|=
name|tokenSelector
operator|.
name|selectToken
argument_list|(
operator|new
name|Text
argument_list|()
argument_list|,
name|ugi
operator|.
name|getTokens
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|token
operator|!=
literal|null
condition|)
block|{
name|job
operator|.
name|getCredentials
argument_list|()
operator|.
name|addToken
argument_list|(
operator|new
name|Text
argument_list|(
name|ugi
operator|.
name|getUserName
argument_list|()
argument_list|)
argument_list|,
name|token
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// we did not get token set up by oozie, let's get them ourselves here.
comment|// we essentially get a token per unique Output HCatTableInfo - this is
comment|// done because through Pig, setOutput() method is called multiple times
comment|// We want to only get the token once per unique output HCatTableInfo -
comment|// we cannot just get one token since in multi-query case (> 1 store in 1 job)
comment|// or the case when a single pig script results in> 1 jobs, the single
comment|// token will get cancelled by the output committer and the subsequent
comment|// stores will fail - by tying the token with the concatenation of
comment|// dbname, tablename and partition keyvalues of the output
comment|// TableInfo, we can have as many tokens as there are stores and the TokenSelector
comment|// will correctly pick the right tokens which the committer will use and
comment|// cancel.
name|String
name|tokenSignature
init|=
name|getTokenSignature
argument_list|(
name|outputInfo
argument_list|)
decl_stmt|;
if|if
condition|(
name|tokenMap
operator|.
name|get
argument_list|(
name|tokenSignature
argument_list|)
operator|==
literal|null
condition|)
block|{
comment|// get delegation tokens from hcat server and store them into the "job"
comment|// These will be used in the HCatOutputCommitter to publish partitions to
comment|// hcat
comment|// when the JobTracker in Hadoop MapReduce starts supporting renewal of
comment|// arbitrary tokens, the renewer should be the principal of the JobTracker
name|tokenMap
operator|.
name|put
argument_list|(
name|tokenSignature
argument_list|,
name|HCatUtil
operator|.
name|extractThriftToken
argument_list|(
name|client
operator|.
name|getDelegationToken
argument_list|(
name|ugi
operator|.
name|getUserName
argument_list|()
argument_list|)
argument_list|,
name|tokenSignature
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|String
name|jcTokenSignature
init|=
literal|"jc."
operator|+
name|tokenSignature
decl_stmt|;
if|if
condition|(
name|tokenMap
operator|.
name|get
argument_list|(
name|jcTokenSignature
argument_list|)
operator|==
literal|null
condition|)
block|{
name|tokenMap
operator|.
name|put
argument_list|(
name|jcTokenSignature
argument_list|,
name|HCatUtil
operator|.
name|getJobTrackerDelegationToken
argument_list|(
name|conf
argument_list|,
name|ugi
operator|.
name|getUserName
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|job
operator|.
name|getCredentials
argument_list|()
operator|.
name|addToken
argument_list|(
operator|new
name|Text
argument_list|(
name|ugi
operator|.
name|getUserName
argument_list|()
operator|+
name|tokenSignature
argument_list|)
argument_list|,
name|tokenMap
operator|.
name|get
argument_list|(
name|tokenSignature
argument_list|)
argument_list|)
expr_stmt|;
comment|// this will be used by the outputcommitter to pass on to the metastore client
comment|// which in turn will pass on to the TokenSelector so that it can select
comment|// the right token.
name|job
operator|.
name|getCredentials
argument_list|()
operator|.
name|addToken
argument_list|(
operator|new
name|Text
argument_list|(
name|ugi
operator|.
name|getUserName
argument_list|()
operator|+
name|jcTokenSignature
argument_list|)
argument_list|,
name|tokenMap
operator|.
name|get
argument_list|(
name|jcTokenSignature
argument_list|)
argument_list|)
expr_stmt|;
name|job
operator|.
name|getConfiguration
argument_list|()
operator|.
name|set
argument_list|(
name|HCatConstants
operator|.
name|HCAT_KEY_TOKEN_SIGNATURE
argument_list|,
name|tokenSignature
argument_list|)
expr_stmt|;
name|job
operator|.
name|getConfiguration
argument_list|()
operator|.
name|set
argument_list|(
name|HCatConstants
operator|.
name|HCAT_KEY_JOBCLIENT_TOKEN_SIGNATURE
argument_list|,
name|jcTokenSignature
argument_list|)
expr_stmt|;
name|job
operator|.
name|getConfiguration
argument_list|()
operator|.
name|set
argument_list|(
name|HCatConstants
operator|.
name|HCAT_KEY_JOBCLIENT_TOKEN_STRFORM
argument_list|,
name|tokenMap
operator|.
name|get
argument_list|(
name|jcTokenSignature
argument_list|)
operator|.
name|encodeToUrlString
argument_list|()
argument_list|)
expr_stmt|;
comment|//            LOG.info("Set hive dt["+tokenSignature+"]");
comment|//            LOG.info("Set jt dt["+jcTokenSignature+"]");
block|}
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
if|if
condition|(
name|e
operator|instanceof
name|HCatException
condition|)
block|{
throw|throw
operator|(
name|HCatException
operator|)
name|e
throw|;
block|}
else|else
block|{
throw|throw
operator|new
name|HCatException
argument_list|(
name|ErrorType
operator|.
name|ERROR_SET_OUTPUT
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
finally|finally
block|{
if|if
condition|(
name|client
operator|!=
literal|null
condition|)
block|{
name|client
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
comment|//        HCatUtil.logAllTokens(LOG,job);
block|}
block|}
comment|// a signature string to associate with a HCatTableInfo - essentially
comment|// a concatenation of dbname, tablename and partition keyvalues.
specifier|private
specifier|static
name|String
name|getTokenSignature
parameter_list|(
name|HCatTableInfo
name|outputInfo
parameter_list|)
block|{
name|StringBuilder
name|result
init|=
operator|new
name|StringBuilder
argument_list|(
literal|""
argument_list|)
decl_stmt|;
name|String
name|dbName
init|=
name|outputInfo
operator|.
name|getDatabaseName
argument_list|()
decl_stmt|;
if|if
condition|(
name|dbName
operator|!=
literal|null
condition|)
block|{
name|result
operator|.
name|append
argument_list|(
name|dbName
argument_list|)
expr_stmt|;
block|}
name|String
name|tableName
init|=
name|outputInfo
operator|.
name|getTableName
argument_list|()
decl_stmt|;
if|if
condition|(
name|tableName
operator|!=
literal|null
condition|)
block|{
name|result
operator|.
name|append
argument_list|(
literal|"+"
operator|+
name|tableName
argument_list|)
expr_stmt|;
block|}
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|partValues
init|=
name|outputInfo
operator|.
name|getPartitionValues
argument_list|()
decl_stmt|;
if|if
condition|(
name|partValues
operator|!=
literal|null
condition|)
block|{
for|for
control|(
name|Entry
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|entry
range|:
name|partValues
operator|.
name|entrySet
argument_list|()
control|)
block|{
name|result
operator|.
name|append
argument_list|(
literal|"+"
operator|+
name|entry
operator|.
name|getKey
argument_list|()
operator|+
literal|"="
operator|+
name|entry
operator|.
name|getValue
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|result
operator|.
name|toString
argument_list|()
return|;
block|}
comment|/**      * Handles duplicate publish of partition. Fails if partition already exists.      * For non partitioned tables, fails if files are present in table directory.      * For dynamic partitioned publish, does nothing - check would need to be done at recordwriter time      * @param job the job      * @param outputInfo the output info      * @param client the metastore client      * @param table the table being written to      * @throws IOException      * @throws MetaException      * @throws TException      */
specifier|private
specifier|static
name|void
name|handleDuplicatePublish
parameter_list|(
name|Job
name|job
parameter_list|,
name|HCatTableInfo
name|outputInfo
parameter_list|,
name|HiveMetaStoreClient
name|client
parameter_list|,
name|Table
name|table
parameter_list|)
throws|throws
name|IOException
throws|,
name|MetaException
throws|,
name|TException
block|{
comment|/*        * For fully specified ptn, follow strict checks for existence of partitions in metadata        * For unpartitioned tables, follow filechecks        * For partially specified tables:        *    This would then need filechecks at the start of a ptn write,        *    Doing metadata checks can get potentially very expensive (fat conf) if         *    there are a large number of partitions that match the partial specifications        */
if|if
condition|(
name|table
operator|.
name|getPartitionKeys
argument_list|()
operator|.
name|size
argument_list|()
operator|>
literal|0
condition|)
block|{
if|if
condition|(
operator|!
name|outputInfo
operator|.
name|isDynamicPartitioningUsed
argument_list|()
condition|)
block|{
name|List
argument_list|<
name|String
argument_list|>
name|partitionValues
init|=
name|HCatOutputCommitter
operator|.
name|getPartitionValueList
argument_list|(
name|table
argument_list|,
name|outputInfo
operator|.
name|getPartitionValues
argument_list|()
argument_list|)
decl_stmt|;
comment|// fully-specified partition
name|List
argument_list|<
name|String
argument_list|>
name|currentParts
init|=
name|client
operator|.
name|listPartitionNames
argument_list|(
name|outputInfo
operator|.
name|getDatabaseName
argument_list|()
argument_list|,
name|outputInfo
operator|.
name|getTableName
argument_list|()
argument_list|,
name|partitionValues
argument_list|,
operator|(
name|short
operator|)
literal|1
argument_list|)
decl_stmt|;
if|if
condition|(
name|currentParts
operator|.
name|size
argument_list|()
operator|>
literal|0
condition|)
block|{
throw|throw
operator|new
name|HCatException
argument_list|(
name|ErrorType
operator|.
name|ERROR_DUPLICATE_PARTITION
argument_list|)
throw|;
block|}
block|}
block|}
else|else
block|{
name|List
argument_list|<
name|String
argument_list|>
name|partitionValues
init|=
name|HCatOutputCommitter
operator|.
name|getPartitionValueList
argument_list|(
name|table
argument_list|,
name|outputInfo
operator|.
name|getPartitionValues
argument_list|()
argument_list|)
decl_stmt|;
comment|// non-partitioned table
name|Path
name|tablePath
init|=
operator|new
name|Path
argument_list|(
name|table
operator|.
name|getSd
argument_list|()
operator|.
name|getLocation
argument_list|()
argument_list|)
decl_stmt|;
name|FileSystem
name|fs
init|=
name|tablePath
operator|.
name|getFileSystem
argument_list|(
name|job
operator|.
name|getConfiguration
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|fs
operator|.
name|exists
argument_list|(
name|tablePath
argument_list|)
condition|)
block|{
name|FileStatus
index|[]
name|status
init|=
name|fs
operator|.
name|globStatus
argument_list|(
operator|new
name|Path
argument_list|(
name|tablePath
argument_list|,
literal|"*"
argument_list|)
argument_list|,
name|hiddenFileFilter
argument_list|)
decl_stmt|;
if|if
condition|(
name|status
operator|.
name|length
operator|>
literal|0
condition|)
block|{
throw|throw
operator|new
name|HCatException
argument_list|(
name|ErrorType
operator|.
name|ERROR_NON_EMPTY_TABLE
argument_list|,
name|table
operator|.
name|getDbName
argument_list|()
operator|+
literal|"."
operator|+
name|table
operator|.
name|getTableName
argument_list|()
argument_list|)
throw|;
block|}
block|}
block|}
block|}
comment|/**      * Set the schema for the data being written out to the partition. The      * table schema is used by default for the partition if this is not called.      * @param job the job object      * @param schema the schema for the data      */
specifier|public
specifier|static
name|void
name|setSchema
parameter_list|(
specifier|final
name|Job
name|job
parameter_list|,
specifier|final
name|HCatSchema
name|schema
parameter_list|)
throws|throws
name|IOException
block|{
name|OutputJobInfo
name|jobInfo
init|=
name|getJobInfo
argument_list|(
name|job
argument_list|)
decl_stmt|;
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|partMap
init|=
name|jobInfo
operator|.
name|getTableInfo
argument_list|()
operator|.
name|getPartitionValues
argument_list|()
decl_stmt|;
name|setPartDetails
argument_list|(
name|jobInfo
argument_list|,
name|schema
argument_list|,
name|partMap
argument_list|)
expr_stmt|;
name|job
operator|.
name|getConfiguration
argument_list|()
operator|.
name|set
argument_list|(
name|HCatConstants
operator|.
name|HCAT_KEY_OUTPUT_INFO
argument_list|,
name|HCatUtil
operator|.
name|serialize
argument_list|(
name|jobInfo
argument_list|)
argument_list|)
expr_stmt|;
block|}
comment|/**      * Get the record writer for the job. Uses the Table's default OutputStorageDriver      * to get the record writer.      * @param context the information about the current task.      * @return a RecordWriter to write the output for the job.      * @throws IOException      */
annotation|@
name|Override
specifier|public
name|RecordWriter
argument_list|<
name|WritableComparable
argument_list|<
name|?
argument_list|>
argument_list|,
name|HCatRecord
argument_list|>
name|getRecordWriter
parameter_list|(
name|TaskAttemptContext
name|context
parameter_list|)
throws|throws
name|IOException
throws|,
name|InterruptedException
block|{
name|HCatRecordWriter
name|rw
init|=
operator|new
name|HCatRecordWriter
argument_list|(
name|context
argument_list|)
decl_stmt|;
name|rw
operator|.
name|prepareForStorageDriverOutput
argument_list|(
name|context
argument_list|)
expr_stmt|;
return|return
name|rw
return|;
block|}
comment|/**      * Get the output committer for this output format. This is responsible      * for ensuring the output is committed correctly.      * @param context the task context      * @return an output committer      * @throws IOException      * @throws InterruptedException      */
annotation|@
name|Override
specifier|public
name|OutputCommitter
name|getOutputCommitter
parameter_list|(
name|TaskAttemptContext
name|context
parameter_list|)
throws|throws
name|IOException
throws|,
name|InterruptedException
block|{
name|OutputFormat
argument_list|<
name|?
super|super
name|WritableComparable
argument_list|<
name|?
argument_list|>
argument_list|,
name|?
super|super
name|Writable
argument_list|>
name|outputFormat
init|=
name|getOutputFormat
argument_list|(
name|context
argument_list|)
decl_stmt|;
return|return
operator|new
name|HCatOutputCommitter
argument_list|(
name|context
argument_list|,
name|outputFormat
operator|.
name|getOutputCommitter
argument_list|(
name|context
argument_list|)
argument_list|)
return|;
block|}
specifier|static
name|HiveMetaStoreClient
name|createHiveClient
parameter_list|(
name|String
name|url
parameter_list|,
name|Configuration
name|conf
parameter_list|)
throws|throws
name|IOException
throws|,
name|MetaException
block|{
name|HiveConf
name|hiveConf
init|=
name|getHiveConf
argument_list|(
name|url
argument_list|,
name|conf
argument_list|)
decl_stmt|;
comment|//      HCatUtil.logHiveConf(LOG, hiveConf);
return|return
operator|new
name|HiveMetaStoreClient
argument_list|(
name|hiveConf
argument_list|)
return|;
block|}
specifier|private
specifier|static
name|HiveConf
name|getHiveConf
parameter_list|(
name|String
name|url
parameter_list|,
name|Configuration
name|conf
parameter_list|)
throws|throws
name|IOException
block|{
name|HiveConf
name|hiveConf
init|=
operator|new
name|HiveConf
argument_list|(
name|HCatOutputFormat
operator|.
name|class
argument_list|)
decl_stmt|;
if|if
condition|(
name|url
operator|!=
literal|null
condition|)
block|{
comment|//User specified a thrift url
name|hiveConf
operator|.
name|setBoolean
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|METASTORE_USE_THRIFT_SASL
operator|.
name|varname
argument_list|,
literal|true
argument_list|)
expr_stmt|;
name|hiveConf
operator|.
name|set
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|METASTORE_KERBEROS_PRINCIPAL
operator|.
name|varname
argument_list|,
name|conf
operator|.
name|get
argument_list|(
name|HCatConstants
operator|.
name|HCAT_METASTORE_PRINCIPAL
argument_list|)
argument_list|)
expr_stmt|;
name|hiveConf
operator|.
name|set
argument_list|(
literal|"hive.metastore.local"
argument_list|,
literal|"false"
argument_list|)
expr_stmt|;
name|hiveConf
operator|.
name|set
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|METASTOREURIS
operator|.
name|varname
argument_list|,
name|url
argument_list|)
expr_stmt|;
if|if
condition|(
name|conf
operator|.
name|get
argument_list|(
name|HCatConstants
operator|.
name|HCAT_KEY_TOKEN_SIGNATURE
argument_list|)
operator|!=
literal|null
condition|)
block|{
name|hiveConf
operator|.
name|set
argument_list|(
literal|"hive.metastore.token.signature"
argument_list|,
name|conf
operator|.
name|get
argument_list|(
name|HCatConstants
operator|.
name|HCAT_KEY_TOKEN_SIGNATURE
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
comment|//Thrift url is null, copy the hive conf into the job conf and restore it
comment|//in the backend context
if|if
condition|(
name|conf
operator|.
name|get
argument_list|(
name|HCatConstants
operator|.
name|HCAT_KEY_HIVE_CONF
argument_list|)
operator|==
literal|null
condition|)
block|{
name|conf
operator|.
name|set
argument_list|(
name|HCatConstants
operator|.
name|HCAT_KEY_HIVE_CONF
argument_list|,
name|HCatUtil
operator|.
name|serialize
argument_list|(
name|hiveConf
operator|.
name|getAllProperties
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|//Copy configuration properties into the hive conf
name|Properties
name|properties
init|=
operator|(
name|Properties
operator|)
name|HCatUtil
operator|.
name|deserialize
argument_list|(
name|conf
operator|.
name|get
argument_list|(
name|HCatConstants
operator|.
name|HCAT_KEY_HIVE_CONF
argument_list|)
argument_list|)
decl_stmt|;
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|Object
argument_list|,
name|Object
argument_list|>
name|prop
range|:
name|properties
operator|.
name|entrySet
argument_list|()
control|)
block|{
if|if
condition|(
name|prop
operator|.
name|getValue
argument_list|()
operator|instanceof
name|String
condition|)
block|{
name|hiveConf
operator|.
name|set
argument_list|(
operator|(
name|String
operator|)
name|prop
operator|.
name|getKey
argument_list|()
argument_list|,
operator|(
name|String
operator|)
name|prop
operator|.
name|getValue
argument_list|()
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|prop
operator|.
name|getValue
argument_list|()
operator|instanceof
name|Integer
condition|)
block|{
name|hiveConf
operator|.
name|setInt
argument_list|(
operator|(
name|String
operator|)
name|prop
operator|.
name|getKey
argument_list|()
argument_list|,
operator|(
name|Integer
operator|)
name|prop
operator|.
name|getValue
argument_list|()
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|prop
operator|.
name|getValue
argument_list|()
operator|instanceof
name|Boolean
condition|)
block|{
name|hiveConf
operator|.
name|setBoolean
argument_list|(
operator|(
name|String
operator|)
name|prop
operator|.
name|getKey
argument_list|()
argument_list|,
operator|(
name|Boolean
operator|)
name|prop
operator|.
name|getValue
argument_list|()
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|prop
operator|.
name|getValue
argument_list|()
operator|instanceof
name|Long
condition|)
block|{
name|hiveConf
operator|.
name|setLong
argument_list|(
operator|(
name|String
operator|)
name|prop
operator|.
name|getKey
argument_list|()
argument_list|,
operator|(
name|Long
operator|)
name|prop
operator|.
name|getValue
argument_list|()
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|prop
operator|.
name|getValue
argument_list|()
operator|instanceof
name|Float
condition|)
block|{
name|hiveConf
operator|.
name|setFloat
argument_list|(
operator|(
name|String
operator|)
name|prop
operator|.
name|getKey
argument_list|()
argument_list|,
operator|(
name|Float
operator|)
name|prop
operator|.
name|getValue
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
comment|// figure out what the maximum number of partitions allowed is, so we can pass it on to our outputinfo
if|if
condition|(
name|HCatConstants
operator|.
name|HCAT_IS_DYNAMIC_MAX_PTN_CHECK_ENABLED
condition|)
block|{
name|maxDynamicPartitions
operator|=
name|hiveConf
operator|.
name|getIntVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|DYNAMICPARTITIONMAXPARTS
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|maxDynamicPartitions
operator|=
operator|-
literal|1
expr_stmt|;
comment|// disables bounds checking for maximum number of dynamic partitions
block|}
name|harRequested
operator|=
name|hiveConf
operator|.
name|getBoolVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEARCHIVEENABLED
argument_list|)
expr_stmt|;
return|return
name|hiveConf
return|;
block|}
comment|/**      * Any initialization of file paths, set permissions and group on freshly created files      * This is called at RecordWriter instantiation time which can be at write-time for        * a dynamic partitioning usecase      * @param context      * @throws IOException      */
specifier|public
specifier|static
name|void
name|prepareOutputLocation
parameter_list|(
name|HCatOutputStorageDriver
name|osd
parameter_list|,
name|TaskAttemptContext
name|context
parameter_list|)
throws|throws
name|IOException
block|{
name|OutputJobInfo
name|info
init|=
name|HCatBaseOutputFormat
operator|.
name|getJobInfo
argument_list|(
name|context
argument_list|)
decl_stmt|;
comment|//      Path workFile = osd.getWorkFilePath(context,info.getLocation());
name|Path
name|workFile
init|=
name|osd
operator|.
name|getWorkFilePath
argument_list|(
name|context
argument_list|,
name|context
operator|.
name|getConfiguration
argument_list|()
operator|.
name|get
argument_list|(
literal|"mapred.output.dir"
argument_list|)
argument_list|)
decl_stmt|;
name|Path
name|tblPath
init|=
operator|new
name|Path
argument_list|(
name|info
operator|.
name|getTable
argument_list|()
operator|.
name|getSd
argument_list|()
operator|.
name|getLocation
argument_list|()
argument_list|)
decl_stmt|;
name|FileSystem
name|fs
init|=
name|tblPath
operator|.
name|getFileSystem
argument_list|(
name|context
operator|.
name|getConfiguration
argument_list|()
argument_list|)
decl_stmt|;
name|FileStatus
name|tblPathStat
init|=
name|fs
operator|.
name|getFileStatus
argument_list|(
name|tblPath
argument_list|)
decl_stmt|;
comment|//      LOG.info("Attempting to set permission ["+tblPathStat.getPermission()+"] on ["+
comment|//          workFile+"], location=["+info.getLocation()+"] , mapred.locn =["+
comment|//          context.getConfiguration().get("mapred.output.dir")+"]");
comment|//
comment|//      FileStatus wFileStatus = fs.getFileStatus(workFile);
comment|//      LOG.info("Table : "+tblPathStat.getPath());
comment|//      LOG.info("Working File : "+wFileStatus.getPath());
name|fs
operator|.
name|setPermission
argument_list|(
name|workFile
argument_list|,
name|tblPathStat
operator|.
name|getPermission
argument_list|()
argument_list|)
expr_stmt|;
try|try
block|{
name|fs
operator|.
name|setOwner
argument_list|(
name|workFile
argument_list|,
literal|null
argument_list|,
name|tblPathStat
operator|.
name|getGroup
argument_list|()
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|AccessControlException
name|ace
parameter_list|)
block|{
comment|// log the messages before ignoring. Currently, logging is not built in HCat.
block|}
block|}
block|}
end_class

end_unit

