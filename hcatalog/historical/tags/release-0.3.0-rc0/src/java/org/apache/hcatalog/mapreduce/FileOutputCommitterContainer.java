begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/*  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hcatalog
operator|.
name|mapreduce
package|;
end_package

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configuration
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileStatus
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|permission
operator|.
name|FsPermission
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|FileUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|HiveMetaStoreClient
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|Warehouse
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|FieldSchema
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|InvalidOperationException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|MetaException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|StorageDescriptor
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Table
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|JobContext
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|JobStatus
operator|.
name|State
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|OutputCommitter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|TaskAttemptContext
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|TaskAttemptID
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|security
operator|.
name|AccessControlException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hcatalog
operator|.
name|common
operator|.
name|ErrorType
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hcatalog
operator|.
name|common
operator|.
name|HCatConstants
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hcatalog
operator|.
name|common
operator|.
name|HCatException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hcatalog
operator|.
name|common
operator|.
name|HCatUtil
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hcatalog
operator|.
name|data
operator|.
name|schema
operator|.
name|HCatFieldSchema
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hcatalog
operator|.
name|data
operator|.
name|schema
operator|.
name|HCatSchema
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hcatalog
operator|.
name|data
operator|.
name|schema
operator|.
name|HCatSchemaUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hcatalog
operator|.
name|har
operator|.
name|HarOutputCommitterPostProcessor
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hcatalog
operator|.
name|shims
operator|.
name|HCatHadoopShims
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|thrift
operator|.
name|TException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|net
operator|.
name|URI
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|LinkedHashMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
operator|.
name|Entry
import|;
end_import

begin_comment
comment|/**  * Part of the FileOutput*Container classes  * See {@link FileOutputFormatContainer} for more information  */
end_comment

begin_class
class|class
name|FileOutputCommitterContainer
extends|extends
name|OutputCommitterContainer
block|{
specifier|private
specifier|final
name|boolean
name|dynamicPartitioningUsed
decl_stmt|;
specifier|private
name|boolean
name|partitionsDiscovered
decl_stmt|;
specifier|private
name|Map
argument_list|<
name|String
argument_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
argument_list|>
name|partitionsDiscoveredByPath
decl_stmt|;
specifier|private
name|Map
argument_list|<
name|String
argument_list|,
name|HCatOutputStorageDriver
argument_list|>
name|storageDriversDiscoveredByPath
decl_stmt|;
name|HarOutputCommitterPostProcessor
name|harProcessor
init|=
operator|new
name|HarOutputCommitterPostProcessor
argument_list|()
decl_stmt|;
specifier|private
name|String
name|ptnRootLocation
init|=
literal|null
decl_stmt|;
specifier|private
name|OutputJobInfo
name|jobInfo
init|=
literal|null
decl_stmt|;
comment|/**      * @param context current JobContext      * @param baseCommitter OutputCommitter to contain      * @throws IOException      */
specifier|public
name|FileOutputCommitterContainer
parameter_list|(
name|JobContext
name|context
parameter_list|,
name|OutputCommitter
name|baseCommitter
parameter_list|)
throws|throws
name|IOException
block|{
name|super
argument_list|(
name|context
argument_list|,
name|baseCommitter
argument_list|)
expr_stmt|;
name|jobInfo
operator|=
name|HCatOutputFormat
operator|.
name|getJobInfo
argument_list|(
name|context
argument_list|)
expr_stmt|;
name|dynamicPartitioningUsed
operator|=
name|jobInfo
operator|.
name|isDynamicPartitioningUsed
argument_list|()
expr_stmt|;
name|this
operator|.
name|partitionsDiscovered
operator|=
operator|!
name|dynamicPartitioningUsed
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|abortTask
parameter_list|(
name|TaskAttemptContext
name|context
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
operator|!
name|dynamicPartitioningUsed
condition|)
block|{
name|getBaseOutputCommitter
argument_list|()
operator|.
name|abortTask
argument_list|(
name|context
argument_list|)
expr_stmt|;
block|}
block|}
annotation|@
name|Override
specifier|public
name|void
name|commitTask
parameter_list|(
name|TaskAttemptContext
name|context
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
operator|!
name|dynamicPartitioningUsed
condition|)
block|{
name|getBaseOutputCommitter
argument_list|()
operator|.
name|commitTask
argument_list|(
name|context
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// called explicitly through FileRecordWriterContainer.close() if dynamic
block|}
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|needsTaskCommit
parameter_list|(
name|TaskAttemptContext
name|context
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
operator|!
name|dynamicPartitioningUsed
condition|)
block|{
return|return
name|getBaseOutputCommitter
argument_list|()
operator|.
name|needsTaskCommit
argument_list|(
name|context
argument_list|)
return|;
block|}
else|else
block|{
comment|// called explicitly through FileRecordWriterContainer.close() if dynamic - return false by default
return|return
literal|false
return|;
block|}
block|}
annotation|@
name|Override
specifier|public
name|void
name|setupJob
parameter_list|(
name|JobContext
name|context
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|getBaseOutputCommitter
argument_list|()
operator|!=
literal|null
operator|&&
operator|!
name|dynamicPartitioningUsed
condition|)
block|{
name|getBaseOutputCommitter
argument_list|()
operator|.
name|setupJob
argument_list|(
name|context
argument_list|)
expr_stmt|;
block|}
comment|// in dynamic usecase, called through FileRecordWriterContainer
block|}
annotation|@
name|Override
specifier|public
name|void
name|setupTask
parameter_list|(
name|TaskAttemptContext
name|context
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
operator|!
name|dynamicPartitioningUsed
condition|)
block|{
name|getBaseOutputCommitter
argument_list|()
operator|.
name|setupTask
argument_list|(
name|context
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// called explicitly through FileRecordWriterContainer.write() if dynamic
block|}
block|}
annotation|@
name|Override
specifier|public
name|void
name|abortJob
parameter_list|(
name|JobContext
name|jobContext
parameter_list|,
name|State
name|state
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|dynamicPartitioningUsed
condition|)
block|{
name|discoverPartitions
argument_list|(
name|jobContext
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|getBaseOutputCommitter
argument_list|()
operator|!=
literal|null
operator|&&
operator|!
name|dynamicPartitioningUsed
condition|)
block|{
name|getBaseOutputCommitter
argument_list|()
operator|.
name|abortJob
argument_list|(
name|jobContext
argument_list|,
name|state
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|dynamicPartitioningUsed
condition|)
block|{
for|for
control|(
name|HCatOutputStorageDriver
name|baseOsd
range|:
name|storageDriversDiscoveredByPath
operator|.
name|values
argument_list|()
control|)
block|{
try|try
block|{
name|baseOsd
operator|.
name|abortOutputCommitterJob
argument_list|(
name|HCatHadoopShims
operator|.
name|Instance
operator|.
name|get
argument_list|()
operator|.
name|createTaskAttemptContext
argument_list|(
name|jobContext
operator|.
name|getConfiguration
argument_list|()
argument_list|,
name|TaskAttemptID
operator|.
name|forName
argument_list|(
name|ptnRootLocation
argument_list|)
argument_list|)
argument_list|,
name|state
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
block|}
name|OutputJobInfo
name|jobInfo
init|=
name|HCatOutputFormat
operator|.
name|getJobInfo
argument_list|(
name|jobContext
argument_list|)
decl_stmt|;
try|try
block|{
name|HiveMetaStoreClient
name|client
init|=
name|HCatOutputFormat
operator|.
name|createHiveClient
argument_list|(
name|jobInfo
operator|.
name|getServerUri
argument_list|()
argument_list|,
name|jobContext
operator|.
name|getConfiguration
argument_list|()
argument_list|)
decl_stmt|;
comment|// cancel the deleg. tokens that were acquired for this job now that
comment|// we are done - we should cancel if the tokens were acquired by
comment|// HCatOutputFormat and not if they were supplied by Oozie. In the latter
comment|// case the HCAT_KEY_TOKEN_SIGNATURE property in the conf will not be set
name|String
name|tokenStrForm
init|=
name|client
operator|.
name|getTokenStrForm
argument_list|()
decl_stmt|;
if|if
condition|(
name|tokenStrForm
operator|!=
literal|null
operator|&&
name|jobContext
operator|.
name|getConfiguration
argument_list|()
operator|.
name|get
argument_list|(
name|HCatConstants
operator|.
name|HCAT_KEY_TOKEN_SIGNATURE
argument_list|)
operator|!=
literal|null
condition|)
block|{
name|client
operator|.
name|cancelDelegationToken
argument_list|(
name|tokenStrForm
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|harProcessor
operator|.
name|isEnabled
argument_list|()
condition|)
block|{
name|String
name|jcTokenStrForm
init|=
name|jobContext
operator|.
name|getConfiguration
argument_list|()
operator|.
name|get
argument_list|(
name|HCatConstants
operator|.
name|HCAT_KEY_JOBCLIENT_TOKEN_STRFORM
argument_list|)
decl_stmt|;
name|String
name|jcTokenSignature
init|=
name|jobContext
operator|.
name|getConfiguration
argument_list|()
operator|.
name|get
argument_list|(
name|HCatConstants
operator|.
name|HCAT_KEY_JOBCLIENT_TOKEN_SIGNATURE
argument_list|)
decl_stmt|;
if|if
condition|(
name|jcTokenStrForm
operator|!=
literal|null
operator|&&
name|jcTokenSignature
operator|!=
literal|null
condition|)
block|{
name|HCatUtil
operator|.
name|cancelJobTrackerDelegationToken
argument_list|(
name|tokenStrForm
argument_list|,
name|jcTokenSignature
argument_list|)
expr_stmt|;
block|}
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
if|if
condition|(
name|e
operator|instanceof
name|HCatException
condition|)
block|{
throw|throw
operator|(
name|HCatException
operator|)
name|e
throw|;
block|}
else|else
block|{
throw|throw
operator|new
name|HCatException
argument_list|(
name|ErrorType
operator|.
name|ERROR_PUBLISHING_PARTITION
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
name|Path
name|src
decl_stmt|;
if|if
condition|(
name|dynamicPartitioningUsed
condition|)
block|{
name|src
operator|=
operator|new
name|Path
argument_list|(
name|getPartitionRootLocation
argument_list|(
name|jobInfo
operator|.
name|getLocation
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|,
name|jobInfo
operator|.
name|getTableInfo
argument_list|()
operator|.
name|getTable
argument_list|()
operator|.
name|getPartitionKeysSize
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|src
operator|=
operator|new
name|Path
argument_list|(
name|jobInfo
operator|.
name|getLocation
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|FileSystem
name|fs
init|=
name|src
operator|.
name|getFileSystem
argument_list|(
name|jobContext
operator|.
name|getConfiguration
argument_list|()
argument_list|)
decl_stmt|;
comment|//      LOG.warn("abortJob about to delete ["+src.toString() +"]");
name|fs
operator|.
name|delete
argument_list|(
name|src
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
specifier|public
specifier|static
specifier|final
name|String
name|SUCCEEDED_FILE_NAME
init|=
literal|"_SUCCESS"
decl_stmt|;
specifier|static
specifier|final
name|String
name|SUCCESSFUL_JOB_OUTPUT_DIR_MARKER
init|=
literal|"mapreduce.fileoutputcommitter.marksuccessfuljobs"
decl_stmt|;
specifier|private
specifier|static
name|boolean
name|getOutputDirMarking
parameter_list|(
name|Configuration
name|conf
parameter_list|)
block|{
return|return
name|conf
operator|.
name|getBoolean
argument_list|(
name|SUCCESSFUL_JOB_OUTPUT_DIR_MARKER
argument_list|,
literal|false
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|commitJob
parameter_list|(
name|JobContext
name|jobContext
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|dynamicPartitioningUsed
condition|)
block|{
name|discoverPartitions
argument_list|(
name|jobContext
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|getBaseOutputCommitter
argument_list|()
operator|!=
literal|null
operator|&&
operator|!
name|dynamicPartitioningUsed
condition|)
block|{
name|getBaseOutputCommitter
argument_list|()
operator|.
name|commitJob
argument_list|(
name|jobContext
argument_list|)
expr_stmt|;
block|}
comment|// create _SUCCESS FILE if so requested.
name|OutputJobInfo
name|jobInfo
init|=
name|HCatOutputFormat
operator|.
name|getJobInfo
argument_list|(
name|jobContext
argument_list|)
decl_stmt|;
if|if
condition|(
name|getOutputDirMarking
argument_list|(
name|jobContext
operator|.
name|getConfiguration
argument_list|()
argument_list|)
condition|)
block|{
name|Path
name|outputPath
init|=
operator|new
name|Path
argument_list|(
name|jobInfo
operator|.
name|getLocation
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|outputPath
operator|!=
literal|null
condition|)
block|{
name|FileSystem
name|fileSys
init|=
name|outputPath
operator|.
name|getFileSystem
argument_list|(
name|jobContext
operator|.
name|getConfiguration
argument_list|()
argument_list|)
decl_stmt|;
comment|// create a file in the folder to mark it
if|if
condition|(
name|fileSys
operator|.
name|exists
argument_list|(
name|outputPath
argument_list|)
condition|)
block|{
name|Path
name|filePath
init|=
operator|new
name|Path
argument_list|(
name|outputPath
argument_list|,
name|SUCCEEDED_FILE_NAME
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|fileSys
operator|.
name|exists
argument_list|(
name|filePath
argument_list|)
condition|)
block|{
comment|// may have been created by baseCommitter.commitJob()
name|fileSys
operator|.
name|create
argument_list|(
name|filePath
argument_list|)
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
block|}
block|}
name|cleanupJob
argument_list|(
name|jobContext
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|cleanupJob
parameter_list|(
name|JobContext
name|context
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|dynamicPartitioningUsed
condition|)
block|{
name|discoverPartitions
argument_list|(
name|context
argument_list|)
expr_stmt|;
block|}
name|OutputJobInfo
name|jobInfo
init|=
name|HCatOutputFormat
operator|.
name|getJobInfo
argument_list|(
name|context
argument_list|)
decl_stmt|;
name|Configuration
name|conf
init|=
name|context
operator|.
name|getConfiguration
argument_list|()
decl_stmt|;
name|Table
name|table
init|=
name|jobInfo
operator|.
name|getTableInfo
argument_list|()
operator|.
name|getTable
argument_list|()
decl_stmt|;
name|Path
name|tblPath
init|=
operator|new
name|Path
argument_list|(
name|table
operator|.
name|getSd
argument_list|()
operator|.
name|getLocation
argument_list|()
argument_list|)
decl_stmt|;
name|FileSystem
name|fs
init|=
name|tblPath
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
if|if
condition|(
name|table
operator|.
name|getPartitionKeys
argument_list|()
operator|.
name|size
argument_list|()
operator|==
literal|0
condition|)
block|{
comment|//non partitioned table
if|if
condition|(
name|getBaseOutputCommitter
argument_list|()
operator|!=
literal|null
operator|&&
operator|!
name|dynamicPartitioningUsed
condition|)
block|{
name|getBaseOutputCommitter
argument_list|()
operator|.
name|cleanupJob
argument_list|(
name|context
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|dynamicPartitioningUsed
condition|)
block|{
for|for
control|(
name|HCatOutputStorageDriver
name|baseOsd
range|:
name|storageDriversDiscoveredByPath
operator|.
name|values
argument_list|()
control|)
block|{
try|try
block|{
name|baseOsd
operator|.
name|cleanupOutputCommitterJob
argument_list|(
name|HCatHadoopShims
operator|.
name|Instance
operator|.
name|get
argument_list|()
operator|.
name|createTaskAttemptContext
argument_list|(
name|context
operator|.
name|getConfiguration
argument_list|()
argument_list|,
name|TaskAttemptID
operator|.
name|forName
argument_list|(
name|ptnRootLocation
argument_list|)
argument_list|)
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
block|}
comment|//Move data from temp directory the actual table directory
comment|//No metastore operation required.
name|Path
name|src
init|=
operator|new
name|Path
argument_list|(
name|jobInfo
operator|.
name|getLocation
argument_list|()
argument_list|)
decl_stmt|;
name|moveTaskOutputs
argument_list|(
name|fs
argument_list|,
name|src
argument_list|,
name|src
argument_list|,
name|tblPath
argument_list|,
literal|false
argument_list|)
expr_stmt|;
name|fs
operator|.
name|delete
argument_list|(
name|src
argument_list|,
literal|true
argument_list|)
expr_stmt|;
return|return;
block|}
name|HiveMetaStoreClient
name|client
init|=
literal|null
decl_stmt|;
name|HCatTableInfo
name|tableInfo
init|=
name|jobInfo
operator|.
name|getTableInfo
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|Partition
argument_list|>
name|partitionsAdded
init|=
operator|new
name|ArrayList
argument_list|<
name|Partition
argument_list|>
argument_list|()
decl_stmt|;
try|try
block|{
name|client
operator|=
name|HCatOutputFormat
operator|.
name|createHiveClient
argument_list|(
name|jobInfo
operator|.
name|getServerUri
argument_list|()
argument_list|,
name|conf
argument_list|)
expr_stmt|;
name|StorerInfo
name|storer
init|=
name|InitializeInput
operator|.
name|extractStorerInfo
argument_list|(
name|table
operator|.
name|getSd
argument_list|()
argument_list|,
name|table
operator|.
name|getParameters
argument_list|()
argument_list|)
decl_stmt|;
name|updateTableSchema
argument_list|(
name|client
argument_list|,
name|table
argument_list|,
name|jobInfo
operator|.
name|getOutputSchema
argument_list|()
argument_list|)
expr_stmt|;
name|FileStatus
name|tblStat
init|=
name|fs
operator|.
name|getFileStatus
argument_list|(
name|tblPath
argument_list|)
decl_stmt|;
name|String
name|grpName
init|=
name|tblStat
operator|.
name|getGroup
argument_list|()
decl_stmt|;
name|FsPermission
name|perms
init|=
name|tblStat
operator|.
name|getPermission
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|Partition
argument_list|>
name|partitionsToAdd
init|=
operator|new
name|ArrayList
argument_list|<
name|Partition
argument_list|>
argument_list|()
decl_stmt|;
if|if
condition|(
operator|!
name|dynamicPartitioningUsed
condition|)
block|{
name|partitionsToAdd
operator|.
name|add
argument_list|(
name|constructPartition
argument_list|(
name|context
argument_list|,
name|tblPath
operator|.
name|toString
argument_list|()
argument_list|,
name|jobInfo
operator|.
name|getPartitionValues
argument_list|()
argument_list|,
name|jobInfo
operator|.
name|getOutputSchema
argument_list|()
argument_list|,
name|getStorerParameterMap
argument_list|(
name|storer
argument_list|)
argument_list|,
name|table
argument_list|,
name|fs
argument_list|,
name|grpName
argument_list|,
name|perms
argument_list|)
argument_list|)
expr_stmt|;
block|}
else|else
block|{
for|for
control|(
name|Entry
argument_list|<
name|String
argument_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
argument_list|>
name|entry
range|:
name|partitionsDiscoveredByPath
operator|.
name|entrySet
argument_list|()
control|)
block|{
name|partitionsToAdd
operator|.
name|add
argument_list|(
name|constructPartition
argument_list|(
name|context
argument_list|,
name|getPartitionRootLocation
argument_list|(
name|entry
operator|.
name|getKey
argument_list|()
argument_list|,
name|entry
operator|.
name|getValue
argument_list|()
operator|.
name|size
argument_list|()
argument_list|)
argument_list|,
name|entry
operator|.
name|getValue
argument_list|()
argument_list|,
name|jobInfo
operator|.
name|getOutputSchema
argument_list|()
argument_list|,
name|getStorerParameterMap
argument_list|(
name|storer
argument_list|)
argument_list|,
name|table
argument_list|,
name|fs
argument_list|,
name|grpName
argument_list|,
name|perms
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
comment|//Publish the new partition(s)
if|if
condition|(
name|dynamicPartitioningUsed
operator|&&
name|harProcessor
operator|.
name|isEnabled
argument_list|()
operator|&&
operator|(
operator|!
name|partitionsToAdd
operator|.
name|isEmpty
argument_list|()
operator|)
condition|)
block|{
name|Path
name|src
init|=
operator|new
name|Path
argument_list|(
name|ptnRootLocation
argument_list|)
decl_stmt|;
comment|// check here for each dir we're copying out, to see if it already exists, error out if so
name|moveTaskOutputs
argument_list|(
name|fs
argument_list|,
name|src
argument_list|,
name|src
argument_list|,
name|tblPath
argument_list|,
literal|true
argument_list|)
expr_stmt|;
name|moveTaskOutputs
argument_list|(
name|fs
argument_list|,
name|src
argument_list|,
name|src
argument_list|,
name|tblPath
argument_list|,
literal|false
argument_list|)
expr_stmt|;
name|fs
operator|.
name|delete
argument_list|(
name|src
argument_list|,
literal|true
argument_list|)
expr_stmt|;
comment|//          for (Partition partition : partitionsToAdd){
comment|//            partitionsAdded.add(client.add_partition(partition));
comment|//            // currently following add_partition instead of add_partitions because latter isn't
comment|//            // all-or-nothing and we want to be able to roll back partitions we added if need be.
comment|//          }
try|try
block|{
name|client
operator|.
name|add_partitions
argument_list|(
name|partitionsToAdd
argument_list|)
expr_stmt|;
name|partitionsAdded
operator|=
name|partitionsToAdd
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
comment|// There was an error adding partitions : rollback fs copy and rethrow
for|for
control|(
name|Partition
name|p
range|:
name|partitionsToAdd
control|)
block|{
name|Path
name|ptnPath
init|=
operator|new
name|Path
argument_list|(
name|harProcessor
operator|.
name|getParentFSPath
argument_list|(
operator|new
name|Path
argument_list|(
name|p
operator|.
name|getSd
argument_list|()
operator|.
name|getLocation
argument_list|()
argument_list|)
argument_list|)
argument_list|)
decl_stmt|;
if|if
condition|(
name|fs
operator|.
name|exists
argument_list|(
name|ptnPath
argument_list|)
condition|)
block|{
name|fs
operator|.
name|delete
argument_list|(
name|ptnPath
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
block|}
throw|throw
name|e
throw|;
block|}
block|}
else|else
block|{
comment|// no harProcessor, regular operation
comment|// No duplicate partition publish case to worry about because we'll
comment|// get a AlreadyExistsException here if so, and appropriately rollback
name|client
operator|.
name|add_partitions
argument_list|(
name|partitionsToAdd
argument_list|)
expr_stmt|;
name|partitionsAdded
operator|=
name|partitionsToAdd
expr_stmt|;
if|if
condition|(
name|dynamicPartitioningUsed
operator|&&
operator|(
name|partitionsAdded
operator|.
name|size
argument_list|()
operator|>
literal|0
operator|)
condition|)
block|{
name|Path
name|src
init|=
operator|new
name|Path
argument_list|(
name|ptnRootLocation
argument_list|)
decl_stmt|;
name|moveTaskOutputs
argument_list|(
name|fs
argument_list|,
name|src
argument_list|,
name|src
argument_list|,
name|tblPath
argument_list|,
literal|false
argument_list|)
expr_stmt|;
name|fs
operator|.
name|delete
argument_list|(
name|src
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
name|getBaseOutputCommitter
argument_list|()
operator|!=
literal|null
operator|&&
operator|!
name|dynamicPartitioningUsed
condition|)
block|{
name|getBaseOutputCommitter
argument_list|()
operator|.
name|cleanupJob
argument_list|(
name|context
argument_list|)
expr_stmt|;
block|}
comment|//Cancel HCat and JobTracker tokens
comment|// cancel the deleg. tokens that were acquired for this job now that
comment|// we are done - we should cancel if the tokens were acquired by
comment|// HCatOutputFormat and not if they were supplied by Oozie. In the latter
comment|// case the HCAT_KEY_TOKEN_SIGNATURE property in the conf will not be set
name|String
name|tokenStrForm
init|=
name|client
operator|.
name|getTokenStrForm
argument_list|()
decl_stmt|;
if|if
condition|(
name|tokenStrForm
operator|!=
literal|null
operator|&&
name|context
operator|.
name|getConfiguration
argument_list|()
operator|.
name|get
argument_list|(
name|HCatConstants
operator|.
name|HCAT_KEY_TOKEN_SIGNATURE
argument_list|)
operator|!=
literal|null
condition|)
block|{
name|client
operator|.
name|cancelDelegationToken
argument_list|(
name|tokenStrForm
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|harProcessor
operator|.
name|isEnabled
argument_list|()
condition|)
block|{
name|String
name|jcTokenStrForm
init|=
name|context
operator|.
name|getConfiguration
argument_list|()
operator|.
name|get
argument_list|(
name|HCatConstants
operator|.
name|HCAT_KEY_JOBCLIENT_TOKEN_STRFORM
argument_list|)
decl_stmt|;
name|String
name|jcTokenSignature
init|=
name|context
operator|.
name|getConfiguration
argument_list|()
operator|.
name|get
argument_list|(
name|HCatConstants
operator|.
name|HCAT_KEY_JOBCLIENT_TOKEN_SIGNATURE
argument_list|)
decl_stmt|;
if|if
condition|(
name|jcTokenStrForm
operator|!=
literal|null
operator|&&
name|jcTokenSignature
operator|!=
literal|null
condition|)
block|{
name|HCatUtil
operator|.
name|cancelJobTrackerDelegationToken
argument_list|(
name|tokenStrForm
argument_list|,
name|jcTokenSignature
argument_list|)
expr_stmt|;
block|}
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
if|if
condition|(
name|partitionsAdded
operator|.
name|size
argument_list|()
operator|>
literal|0
condition|)
block|{
try|try
block|{
comment|//baseCommitter.cleanupJob failed, try to clean up the metastore
for|for
control|(
name|Partition
name|p
range|:
name|partitionsAdded
control|)
block|{
name|client
operator|.
name|dropPartition
argument_list|(
name|tableInfo
operator|.
name|getDatabaseName
argument_list|()
argument_list|,
name|tableInfo
operator|.
name|getTableName
argument_list|()
argument_list|,
name|p
operator|.
name|getValues
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|te
parameter_list|)
block|{
comment|//Keep cause as the original exception
throw|throw
operator|new
name|HCatException
argument_list|(
name|ErrorType
operator|.
name|ERROR_PUBLISHING_PARTITION
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
if|if
condition|(
name|e
operator|instanceof
name|HCatException
condition|)
block|{
throw|throw
operator|(
name|HCatException
operator|)
name|e
throw|;
block|}
else|else
block|{
throw|throw
operator|new
name|HCatException
argument_list|(
name|ErrorType
operator|.
name|ERROR_PUBLISHING_PARTITION
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
finally|finally
block|{
if|if
condition|(
name|client
operator|!=
literal|null
condition|)
block|{
name|client
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
block|}
specifier|private
name|String
name|getPartitionRootLocation
parameter_list|(
name|String
name|ptnLocn
parameter_list|,
name|int
name|numPtnKeys
parameter_list|)
block|{
if|if
condition|(
name|ptnRootLocation
operator|==
literal|null
condition|)
block|{
comment|// we only need to calculate it once, it'll be the same for other partitions in this job.
name|Path
name|ptnRoot
init|=
operator|new
name|Path
argument_list|(
name|ptnLocn
argument_list|)
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|numPtnKeys
condition|;
name|i
operator|++
control|)
block|{
comment|//          LOG.info("Getting parent of "+ptnRoot.getName());
name|ptnRoot
operator|=
name|ptnRoot
operator|.
name|getParent
argument_list|()
expr_stmt|;
block|}
name|ptnRootLocation
operator|=
name|ptnRoot
operator|.
name|toString
argument_list|()
expr_stmt|;
block|}
comment|//      LOG.info("Returning final parent : "+ptnRootLocation);
return|return
name|ptnRootLocation
return|;
block|}
comment|/**      * Generate partition metadata object to be used to add to metadata.      * @param partLocnRoot The table-equivalent location root of the partition      *                       (temporary dir if dynamic partition, table dir if static)      * @param partKVs The keyvalue pairs that form the partition      * @param outputSchema The output schema for the partition      * @param params The parameters to store inside the partition      * @param table The Table metadata object under which this Partition will reside      * @param fs FileSystem object to operate on the underlying filesystem      * @param grpName Group name that owns the table dir      * @param perms FsPermission that's the default permission of the table dir.      * @return Constructed Partition metadata object      * @throws java.io.IOException      */
specifier|private
name|Partition
name|constructPartition
parameter_list|(
name|JobContext
name|context
parameter_list|,
name|String
name|partLocnRoot
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|partKVs
parameter_list|,
name|HCatSchema
name|outputSchema
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|params
parameter_list|,
name|Table
name|table
parameter_list|,
name|FileSystem
name|fs
parameter_list|,
name|String
name|grpName
parameter_list|,
name|FsPermission
name|perms
parameter_list|)
throws|throws
name|IOException
block|{
name|StorageDescriptor
name|tblSD
init|=
name|table
operator|.
name|getSd
argument_list|()
decl_stmt|;
name|Partition
name|partition
init|=
operator|new
name|Partition
argument_list|()
decl_stmt|;
name|partition
operator|.
name|setDbName
argument_list|(
name|table
operator|.
name|getDbName
argument_list|()
argument_list|)
expr_stmt|;
name|partition
operator|.
name|setTableName
argument_list|(
name|table
operator|.
name|getTableName
argument_list|()
argument_list|)
expr_stmt|;
name|partition
operator|.
name|setSd
argument_list|(
operator|new
name|StorageDescriptor
argument_list|(
name|tblSD
argument_list|)
argument_list|)
expr_stmt|;
name|List
argument_list|<
name|FieldSchema
argument_list|>
name|fields
init|=
operator|new
name|ArrayList
argument_list|<
name|FieldSchema
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|HCatFieldSchema
name|fieldSchema
range|:
name|outputSchema
operator|.
name|getFields
argument_list|()
control|)
block|{
name|fields
operator|.
name|add
argument_list|(
name|HCatSchemaUtils
operator|.
name|getFieldSchema
argument_list|(
name|fieldSchema
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|partition
operator|.
name|getSd
argument_list|()
operator|.
name|setCols
argument_list|(
name|fields
argument_list|)
expr_stmt|;
name|partition
operator|.
name|setValues
argument_list|(
name|FileOutputFormatContainer
operator|.
name|getPartitionValueList
argument_list|(
name|table
argument_list|,
name|partKVs
argument_list|)
argument_list|)
expr_stmt|;
name|partition
operator|.
name|setParameters
argument_list|(
name|params
argument_list|)
expr_stmt|;
comment|// Sets permissions and group name on partition dirs.
name|Path
name|partPath
init|=
operator|new
name|Path
argument_list|(
name|partLocnRoot
argument_list|)
decl_stmt|;
for|for
control|(
name|FieldSchema
name|partKey
range|:
name|table
operator|.
name|getPartitionKeys
argument_list|()
control|)
block|{
name|partPath
operator|=
name|constructPartialPartPath
argument_list|(
name|partPath
argument_list|,
name|partKey
operator|.
name|getName
argument_list|()
operator|.
name|toLowerCase
argument_list|()
argument_list|,
name|partKVs
argument_list|)
expr_stmt|;
comment|//        LOG.info("Setting perms for "+partPath.toString());
name|fs
operator|.
name|setPermission
argument_list|(
name|partPath
argument_list|,
name|perms
argument_list|)
expr_stmt|;
try|try
block|{
name|fs
operator|.
name|setOwner
argument_list|(
name|partPath
argument_list|,
literal|null
argument_list|,
name|grpName
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|AccessControlException
name|ace
parameter_list|)
block|{
comment|// log the messages before ignoring. Currently, logging is not built in Hcatalog.
comment|//          LOG.warn(ace);
block|}
block|}
if|if
condition|(
name|dynamicPartitioningUsed
condition|)
block|{
name|String
name|dynamicPartitionDestination
init|=
name|getFinalDynamicPartitionDestination
argument_list|(
name|table
argument_list|,
name|partKVs
argument_list|)
decl_stmt|;
if|if
condition|(
name|harProcessor
operator|.
name|isEnabled
argument_list|()
condition|)
block|{
name|harProcessor
operator|.
name|exec
argument_list|(
name|context
argument_list|,
name|partition
argument_list|,
name|partPath
argument_list|)
expr_stmt|;
name|partition
operator|.
name|getSd
argument_list|()
operator|.
name|setLocation
argument_list|(
name|harProcessor
operator|.
name|getProcessedLocation
argument_list|(
operator|new
name|Path
argument_list|(
name|dynamicPartitionDestination
argument_list|)
argument_list|)
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|partition
operator|.
name|getSd
argument_list|()
operator|.
name|setLocation
argument_list|(
name|dynamicPartitionDestination
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
name|partition
operator|.
name|getSd
argument_list|()
operator|.
name|setLocation
argument_list|(
name|partPath
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
return|return
name|partition
return|;
block|}
specifier|private
name|String
name|getFinalDynamicPartitionDestination
parameter_list|(
name|Table
name|table
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|partKVs
parameter_list|)
block|{
comment|// file:///tmp/hcat_junit_warehouse/employee/_DYN0.7770480401313761/emp_country=IN/emp_state=KA  ->
comment|// file:///tmp/hcat_junit_warehouse/employee/emp_country=IN/emp_state=KA
name|Path
name|partPath
init|=
operator|new
name|Path
argument_list|(
name|table
operator|.
name|getSd
argument_list|()
operator|.
name|getLocation
argument_list|()
argument_list|)
decl_stmt|;
for|for
control|(
name|FieldSchema
name|partKey
range|:
name|table
operator|.
name|getPartitionKeys
argument_list|()
control|)
block|{
name|partPath
operator|=
name|constructPartialPartPath
argument_list|(
name|partPath
argument_list|,
name|partKey
operator|.
name|getName
argument_list|()
operator|.
name|toLowerCase
argument_list|()
argument_list|,
name|partKVs
argument_list|)
expr_stmt|;
block|}
return|return
name|partPath
operator|.
name|toString
argument_list|()
return|;
block|}
specifier|private
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|getStorerParameterMap
parameter_list|(
name|StorerInfo
name|storer
parameter_list|)
block|{
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|params
init|=
operator|new
name|HashMap
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
argument_list|()
decl_stmt|;
name|params
operator|.
name|put
argument_list|(
name|HCatConstants
operator|.
name|HCAT_ISD_CLASS
argument_list|,
name|storer
operator|.
name|getInputSDClass
argument_list|()
argument_list|)
expr_stmt|;
name|params
operator|.
name|put
argument_list|(
name|HCatConstants
operator|.
name|HCAT_OSD_CLASS
argument_list|,
name|storer
operator|.
name|getOutputSDClass
argument_list|()
argument_list|)
expr_stmt|;
comment|//Copy table level hcat.* keys to the partition
for|for
control|(
name|Entry
argument_list|<
name|Object
argument_list|,
name|Object
argument_list|>
name|entry
range|:
name|storer
operator|.
name|getProperties
argument_list|()
operator|.
name|entrySet
argument_list|()
control|)
block|{
name|params
operator|.
name|put
argument_list|(
name|entry
operator|.
name|getKey
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|,
name|entry
operator|.
name|getValue
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
return|return
name|params
return|;
block|}
specifier|private
name|Path
name|constructPartialPartPath
parameter_list|(
name|Path
name|partialPath
parameter_list|,
name|String
name|partKey
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|partKVs
parameter_list|)
block|{
name|StringBuilder
name|sb
init|=
operator|new
name|StringBuilder
argument_list|(
name|FileUtils
operator|.
name|escapePathName
argument_list|(
name|partKey
argument_list|)
argument_list|)
decl_stmt|;
name|sb
operator|.
name|append
argument_list|(
literal|"="
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
name|FileUtils
operator|.
name|escapePathName
argument_list|(
name|partKVs
operator|.
name|get
argument_list|(
name|partKey
argument_list|)
argument_list|)
argument_list|)
expr_stmt|;
return|return
operator|new
name|Path
argument_list|(
name|partialPath
argument_list|,
name|sb
operator|.
name|toString
argument_list|()
argument_list|)
return|;
block|}
comment|/**      * Update table schema, adding new columns as added for the partition.      * @param client the client      * @param table the table      * @param partitionSchema the schema of the partition      * @throws java.io.IOException Signals that an I/O exception has occurred.      * @throws org.apache.hadoop.hive.metastore.api.InvalidOperationException the invalid operation exception      * @throws org.apache.hadoop.hive.metastore.api.MetaException the meta exception      * @throws org.apache.thrift.TException the t exception      */
specifier|private
name|void
name|updateTableSchema
parameter_list|(
name|HiveMetaStoreClient
name|client
parameter_list|,
name|Table
name|table
parameter_list|,
name|HCatSchema
name|partitionSchema
parameter_list|)
throws|throws
name|IOException
throws|,
name|InvalidOperationException
throws|,
name|MetaException
throws|,
name|TException
block|{
name|List
argument_list|<
name|FieldSchema
argument_list|>
name|newColumns
init|=
name|HCatUtil
operator|.
name|validatePartitionSchema
argument_list|(
name|table
argument_list|,
name|partitionSchema
argument_list|)
decl_stmt|;
if|if
condition|(
name|newColumns
operator|.
name|size
argument_list|()
operator|!=
literal|0
condition|)
block|{
name|List
argument_list|<
name|FieldSchema
argument_list|>
name|tableColumns
init|=
operator|new
name|ArrayList
argument_list|<
name|FieldSchema
argument_list|>
argument_list|(
name|table
operator|.
name|getSd
argument_list|()
operator|.
name|getCols
argument_list|()
argument_list|)
decl_stmt|;
name|tableColumns
operator|.
name|addAll
argument_list|(
name|newColumns
argument_list|)
expr_stmt|;
comment|//Update table schema to add the newly added columns
name|table
operator|.
name|getSd
argument_list|()
operator|.
name|setCols
argument_list|(
name|tableColumns
argument_list|)
expr_stmt|;
name|client
operator|.
name|alter_table
argument_list|(
name|table
operator|.
name|getDbName
argument_list|()
argument_list|,
name|table
operator|.
name|getTableName
argument_list|()
argument_list|,
name|table
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**      * Move all of the files from the temp directory to the final location      * @param fs the output file system      * @param file the file to move      * @param src the source directory      * @param dest the target directory      * @param dryRun - a flag that simply tests if this move would succeed or not based      *                 on whether other files exist where we're trying to copy      * @throws java.io.IOException      */
specifier|private
name|void
name|moveTaskOutputs
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|Path
name|file
parameter_list|,
name|Path
name|src
parameter_list|,
name|Path
name|dest
parameter_list|,
name|boolean
name|dryRun
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|fs
operator|.
name|isFile
argument_list|(
name|file
argument_list|)
condition|)
block|{
name|Path
name|finalOutputPath
init|=
name|getFinalPath
argument_list|(
name|file
argument_list|,
name|src
argument_list|,
name|dest
argument_list|)
decl_stmt|;
if|if
condition|(
name|dryRun
condition|)
block|{
comment|//        LOG.info("Testing if moving ["+file+"] to ["+finalOutputPath+"] would cause a problem");
if|if
condition|(
name|fs
operator|.
name|exists
argument_list|(
name|finalOutputPath
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|HCatException
argument_list|(
name|ErrorType
operator|.
name|ERROR_MOVE_FAILED
argument_list|,
literal|"Data already exists in "
operator|+
name|finalOutputPath
operator|+
literal|", duplicate publish possible."
argument_list|)
throw|;
block|}
block|}
else|else
block|{
comment|//        LOG.info("Moving ["+file+"] to ["+finalOutputPath+"]");
if|if
condition|(
operator|!
name|fs
operator|.
name|rename
argument_list|(
name|file
argument_list|,
name|finalOutputPath
argument_list|)
condition|)
block|{
if|if
condition|(
operator|!
name|fs
operator|.
name|delete
argument_list|(
name|finalOutputPath
argument_list|,
literal|true
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|HCatException
argument_list|(
name|ErrorType
operator|.
name|ERROR_MOVE_FAILED
argument_list|,
literal|"Failed to delete existing path "
operator|+
name|finalOutputPath
argument_list|)
throw|;
block|}
if|if
condition|(
operator|!
name|fs
operator|.
name|rename
argument_list|(
name|file
argument_list|,
name|finalOutputPath
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|HCatException
argument_list|(
name|ErrorType
operator|.
name|ERROR_MOVE_FAILED
argument_list|,
literal|"Failed to move output to "
operator|+
name|dest
argument_list|)
throw|;
block|}
block|}
block|}
block|}
elseif|else
if|if
condition|(
name|fs
operator|.
name|getFileStatus
argument_list|(
name|file
argument_list|)
operator|.
name|isDir
argument_list|()
condition|)
block|{
name|FileStatus
index|[]
name|paths
init|=
name|fs
operator|.
name|listStatus
argument_list|(
name|file
argument_list|)
decl_stmt|;
name|Path
name|finalOutputPath
init|=
name|getFinalPath
argument_list|(
name|file
argument_list|,
name|src
argument_list|,
name|dest
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|dryRun
condition|)
block|{
name|fs
operator|.
name|mkdirs
argument_list|(
name|finalOutputPath
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|paths
operator|!=
literal|null
condition|)
block|{
for|for
control|(
name|FileStatus
name|path
range|:
name|paths
control|)
block|{
name|moveTaskOutputs
argument_list|(
name|fs
argument_list|,
name|path
operator|.
name|getPath
argument_list|()
argument_list|,
name|src
argument_list|,
name|dest
argument_list|,
name|dryRun
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
comment|/**      * Find the final name of a given output file, given the output directory      * and the work directory.      * @param file the file to move      * @param src the source directory      * @param dest the target directory      * @return the final path for the specific output file      * @throws java.io.IOException      */
specifier|private
name|Path
name|getFinalPath
parameter_list|(
name|Path
name|file
parameter_list|,
name|Path
name|src
parameter_list|,
name|Path
name|dest
parameter_list|)
throws|throws
name|IOException
block|{
name|URI
name|taskOutputUri
init|=
name|file
operator|.
name|toUri
argument_list|()
decl_stmt|;
name|URI
name|relativePath
init|=
name|src
operator|.
name|toUri
argument_list|()
operator|.
name|relativize
argument_list|(
name|taskOutputUri
argument_list|)
decl_stmt|;
if|if
condition|(
name|taskOutputUri
operator|==
name|relativePath
condition|)
block|{
throw|throw
operator|new
name|HCatException
argument_list|(
name|ErrorType
operator|.
name|ERROR_MOVE_FAILED
argument_list|,
literal|"Can not get the relative path: base = "
operator|+
name|src
operator|+
literal|" child = "
operator|+
name|file
argument_list|)
throw|;
block|}
if|if
condition|(
name|relativePath
operator|.
name|getPath
argument_list|()
operator|.
name|length
argument_list|()
operator|>
literal|0
condition|)
block|{
return|return
operator|new
name|Path
argument_list|(
name|dest
argument_list|,
name|relativePath
operator|.
name|getPath
argument_list|()
argument_list|)
return|;
block|}
else|else
block|{
return|return
name|dest
return|;
block|}
block|}
comment|/**      * Run to discover dynamic partitions available      */
specifier|private
name|void
name|discoverPartitions
parameter_list|(
name|JobContext
name|context
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
operator|!
name|partitionsDiscovered
condition|)
block|{
comment|//      LOG.info("discover ptns called");
name|OutputJobInfo
name|jobInfo
init|=
name|HCatOutputFormat
operator|.
name|getJobInfo
argument_list|(
name|context
argument_list|)
decl_stmt|;
name|harProcessor
operator|.
name|setEnabled
argument_list|(
name|jobInfo
operator|.
name|getHarRequested
argument_list|()
argument_list|)
expr_stmt|;
name|List
argument_list|<
name|Integer
argument_list|>
name|dynamicPartCols
init|=
name|jobInfo
operator|.
name|getPosOfDynPartCols
argument_list|()
decl_stmt|;
name|int
name|maxDynamicPartitions
init|=
name|jobInfo
operator|.
name|getMaxDynamicPartitions
argument_list|()
decl_stmt|;
name|Path
name|loadPath
init|=
operator|new
name|Path
argument_list|(
name|jobInfo
operator|.
name|getLocation
argument_list|()
argument_list|)
decl_stmt|;
name|FileSystem
name|fs
init|=
name|loadPath
operator|.
name|getFileSystem
argument_list|(
name|context
operator|.
name|getConfiguration
argument_list|()
argument_list|)
decl_stmt|;
comment|// construct a path pattern (e.g., /*/*) to find all dynamically generated paths
name|String
name|dynPathSpec
init|=
name|loadPath
operator|.
name|toUri
argument_list|()
operator|.
name|getPath
argument_list|()
decl_stmt|;
name|dynPathSpec
operator|=
name|dynPathSpec
operator|.
name|replaceAll
argument_list|(
literal|"__HIVE_DEFAULT_PARTITION__"
argument_list|,
literal|"*"
argument_list|)
expr_stmt|;
comment|// TODO : replace this with a param pull from HiveConf
comment|//      LOG.info("Searching for "+dynPathSpec);
name|Path
name|pathPattern
init|=
operator|new
name|Path
argument_list|(
name|loadPath
argument_list|,
name|dynPathSpec
argument_list|)
decl_stmt|;
name|FileStatus
index|[]
name|status
init|=
name|fs
operator|.
name|globStatus
argument_list|(
name|pathPattern
argument_list|)
decl_stmt|;
name|partitionsDiscoveredByPath
operator|=
operator|new
name|LinkedHashMap
argument_list|<
name|String
argument_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
argument_list|>
argument_list|()
expr_stmt|;
name|storageDriversDiscoveredByPath
operator|=
operator|new
name|LinkedHashMap
argument_list|<
name|String
argument_list|,
name|HCatOutputStorageDriver
argument_list|>
argument_list|()
expr_stmt|;
if|if
condition|(
name|status
operator|.
name|length
operator|==
literal|0
condition|)
block|{
comment|//        LOG.warn("No partition found genereated by dynamic partitioning in ["
comment|//            +loadPath+"] with depth["+jobInfo.getTable().getPartitionKeysSize()
comment|//            +"], dynSpec["+dynPathSpec+"]");
block|}
else|else
block|{
if|if
condition|(
operator|(
name|maxDynamicPartitions
operator|!=
operator|-
literal|1
operator|)
operator|&&
operator|(
name|status
operator|.
name|length
operator|>
name|maxDynamicPartitions
operator|)
condition|)
block|{
name|this
operator|.
name|partitionsDiscovered
operator|=
literal|true
expr_stmt|;
throw|throw
operator|new
name|HCatException
argument_list|(
name|ErrorType
operator|.
name|ERROR_TOO_MANY_DYNAMIC_PTNS
argument_list|,
literal|"Number of dynamic partitions being created "
operator|+
literal|"exceeds configured max allowable partitions["
operator|+
name|maxDynamicPartitions
operator|+
literal|"], increase parameter ["
operator|+
name|HiveConf
operator|.
name|ConfVars
operator|.
name|DYNAMICPARTITIONMAXPARTS
operator|.
name|varname
operator|+
literal|"] if needed."
argument_list|)
throw|;
block|}
for|for
control|(
name|FileStatus
name|st
range|:
name|status
control|)
block|{
name|LinkedHashMap
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|fullPartSpec
init|=
operator|new
name|LinkedHashMap
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
argument_list|()
decl_stmt|;
name|Warehouse
operator|.
name|makeSpecFromName
argument_list|(
name|fullPartSpec
argument_list|,
name|st
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
name|partitionsDiscoveredByPath
operator|.
name|put
argument_list|(
name|st
operator|.
name|getPath
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|,
name|fullPartSpec
argument_list|)
expr_stmt|;
name|storageDriversDiscoveredByPath
operator|.
name|put
argument_list|(
name|st
operator|.
name|getPath
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|,
name|HCatOutputFormat
operator|.
name|getOutputDriverInstance
argument_list|(
name|context
argument_list|,
name|jobInfo
argument_list|,
name|fullPartSpec
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
comment|//      for (Entry<String,Map<String,String>> spec : partitionsDiscoveredByPath.entrySet()){
comment|//        LOG.info("Partition "+ spec.getKey());
comment|//        for (Entry<String,String> e : spec.getValue().entrySet()){
comment|//          LOG.info(e.getKey() + "=>" +e.getValue());
comment|//        }
comment|//      }
name|this
operator|.
name|partitionsDiscovered
operator|=
literal|true
expr_stmt|;
block|}
block|}
block|}
end_class

end_unit

