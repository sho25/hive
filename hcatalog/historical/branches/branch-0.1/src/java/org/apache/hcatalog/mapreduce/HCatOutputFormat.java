begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/*  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hcatalog
operator|.
name|mapreduce
package|;
end_package

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Properties
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
operator|.
name|Entry
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configuration
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileStatus
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|PathFilter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|permission
operator|.
name|FsPermission
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|HiveMetaStoreClient
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|FieldSchema
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|MetaException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|StorageDescriptor
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Table
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|thrift
operator|.
name|DelegationTokenIdentifier
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|thrift
operator|.
name|DelegationTokenSelector
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|Text
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|Writable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|WritableComparable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|Job
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|JobContext
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|OutputCommitter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|OutputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|RecordWriter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|TaskAttemptContext
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|security
operator|.
name|AccessControlException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|security
operator|.
name|UserGroupInformation
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|security
operator|.
name|token
operator|.
name|Token
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|security
operator|.
name|token
operator|.
name|TokenIdentifier
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|security
operator|.
name|token
operator|.
name|TokenSelector
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hcatalog
operator|.
name|common
operator|.
name|ErrorType
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hcatalog
operator|.
name|common
operator|.
name|HCatConstants
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hcatalog
operator|.
name|common
operator|.
name|HCatException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hcatalog
operator|.
name|common
operator|.
name|HCatUtil
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hcatalog
operator|.
name|data
operator|.
name|HCatRecord
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hcatalog
operator|.
name|data
operator|.
name|schema
operator|.
name|HCatSchema
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|thrift
operator|.
name|TException
import|;
end_import

begin_comment
comment|/** The OutputFormat to use to write data to Howl. The key value is ignored and  * and should be given as null. The value is the HowlRecord to write.*/
end_comment

begin_class
specifier|public
class|class
name|HCatOutputFormat
extends|extends
name|OutputFormat
argument_list|<
name|WritableComparable
argument_list|<
name|?
argument_list|>
argument_list|,
name|HCatRecord
argument_list|>
block|{
comment|/** The directory under which data is initially written for a non partitioned table */
specifier|protected
specifier|static
specifier|final
name|String
name|TEMP_DIR_NAME
init|=
literal|"_TEMP"
decl_stmt|;
specifier|private
specifier|static
name|Map
argument_list|<
name|String
argument_list|,
name|Token
argument_list|<
name|DelegationTokenIdentifier
argument_list|>
argument_list|>
name|tokenMap
init|=
operator|new
name|HashMap
argument_list|<
name|String
argument_list|,
name|Token
argument_list|<
name|DelegationTokenIdentifier
argument_list|>
argument_list|>
argument_list|()
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|PathFilter
name|hiddenFileFilter
init|=
operator|new
name|PathFilter
argument_list|()
block|{
specifier|public
name|boolean
name|accept
parameter_list|(
name|Path
name|p
parameter_list|)
block|{
name|String
name|name
init|=
name|p
operator|.
name|getName
argument_list|()
decl_stmt|;
return|return
operator|!
name|name
operator|.
name|startsWith
argument_list|(
literal|"_"
argument_list|)
operator|&&
operator|!
name|name
operator|.
name|startsWith
argument_list|(
literal|"."
argument_list|)
return|;
block|}
block|}
decl_stmt|;
comment|/**      * Set the info about the output to write for the Job. This queries the metadata server      * to find the StorageDriver to use for the table.  Throws error if partition is already published.      * @param job the job object      * @param outputInfo the table output info      * @throws IOException the exception in communicating with the metadata server      */
annotation|@
name|SuppressWarnings
argument_list|(
literal|"unchecked"
argument_list|)
specifier|public
specifier|static
name|void
name|setOutput
parameter_list|(
name|Job
name|job
parameter_list|,
name|HCatTableInfo
name|outputInfo
parameter_list|)
throws|throws
name|IOException
block|{
name|HiveMetaStoreClient
name|client
init|=
literal|null
decl_stmt|;
try|try
block|{
name|Configuration
name|conf
init|=
name|job
operator|.
name|getConfiguration
argument_list|()
decl_stmt|;
name|client
operator|=
name|createHiveClient
argument_list|(
name|outputInfo
operator|.
name|getServerUri
argument_list|()
argument_list|,
name|conf
argument_list|)
expr_stmt|;
name|Table
name|table
init|=
name|client
operator|.
name|getTable
argument_list|(
name|outputInfo
operator|.
name|getDatabaseName
argument_list|()
argument_list|,
name|outputInfo
operator|.
name|getTableName
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|outputInfo
operator|.
name|getPartitionValues
argument_list|()
operator|==
literal|null
condition|)
block|{
name|outputInfo
operator|.
name|setPartitionValues
argument_list|(
operator|new
name|HashMap
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
argument_list|()
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|//Convert user specified map to have lower case key names
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|valueMap
init|=
operator|new
name|HashMap
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|entry
range|:
name|outputInfo
operator|.
name|getPartitionValues
argument_list|()
operator|.
name|entrySet
argument_list|()
control|)
block|{
name|valueMap
operator|.
name|put
argument_list|(
name|entry
operator|.
name|getKey
argument_list|()
operator|.
name|toLowerCase
argument_list|()
argument_list|,
name|entry
operator|.
name|getValue
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|outputInfo
operator|.
name|setPartitionValues
argument_list|(
name|valueMap
argument_list|)
expr_stmt|;
block|}
comment|//Handle duplicate publish
name|handleDuplicatePublish
argument_list|(
name|job
argument_list|,
name|outputInfo
argument_list|,
name|client
argument_list|,
name|table
argument_list|)
expr_stmt|;
name|StorageDescriptor
name|tblSD
init|=
name|table
operator|.
name|getSd
argument_list|()
decl_stmt|;
name|HCatSchema
name|tableSchema
init|=
name|HCatUtil
operator|.
name|extractSchemaFromStorageDescriptor
argument_list|(
name|tblSD
argument_list|)
decl_stmt|;
name|StorerInfo
name|storerInfo
init|=
name|InitializeInput
operator|.
name|extractStorerInfo
argument_list|(
name|tblSD
argument_list|,
name|table
operator|.
name|getParameters
argument_list|()
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|partitionCols
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|FieldSchema
name|schema
range|:
name|table
operator|.
name|getPartitionKeys
argument_list|()
control|)
block|{
name|partitionCols
operator|.
name|add
argument_list|(
name|schema
operator|.
name|getName
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|Class
argument_list|<
name|?
extends|extends
name|HCatOutputStorageDriver
argument_list|>
name|driverClass
init|=
operator|(
name|Class
argument_list|<
name|?
extends|extends
name|HCatOutputStorageDriver
argument_list|>
operator|)
name|Class
operator|.
name|forName
argument_list|(
name|storerInfo
operator|.
name|getOutputSDClass
argument_list|()
argument_list|)
decl_stmt|;
name|HCatOutputStorageDriver
name|driver
init|=
name|driverClass
operator|.
name|newInstance
argument_list|()
decl_stmt|;
name|String
name|tblLocation
init|=
name|tblSD
operator|.
name|getLocation
argument_list|()
decl_stmt|;
name|String
name|location
init|=
name|driver
operator|.
name|getOutputLocation
argument_list|(
name|job
argument_list|,
name|tblLocation
argument_list|,
name|partitionCols
argument_list|,
name|outputInfo
operator|.
name|getPartitionValues
argument_list|()
argument_list|)
decl_stmt|;
comment|//Serialize the output info into the configuration
name|OutputJobInfo
name|jobInfo
init|=
operator|new
name|OutputJobInfo
argument_list|(
name|outputInfo
argument_list|,
name|tableSchema
argument_list|,
name|tableSchema
argument_list|,
name|storerInfo
argument_list|,
name|location
argument_list|,
name|table
argument_list|)
decl_stmt|;
name|conf
operator|.
name|set
argument_list|(
name|HCatConstants
operator|.
name|HCAT_KEY_OUTPUT_INFO
argument_list|,
name|HCatUtil
operator|.
name|serialize
argument_list|(
name|jobInfo
argument_list|)
argument_list|)
expr_stmt|;
name|Path
name|tblPath
init|=
operator|new
name|Path
argument_list|(
name|tblLocation
argument_list|)
decl_stmt|;
comment|/*  Set the umask in conf such that files/dirs get created with table-dir          * permissions. Following three assumptions are made:          * 1. Actual files/dirs creation is done by RecordWriter of underlying          * output format. It is assumed that they use default permissions while creation.          * 2. Default Permissions = FsPermission.getDefault() = 777.          * 3. UMask is honored by underlying filesystem.          */
name|FsPermission
operator|.
name|setUMask
argument_list|(
name|conf
argument_list|,
name|FsPermission
operator|.
name|getDefault
argument_list|()
operator|.
name|applyUMask
argument_list|(
name|tblPath
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
operator|.
name|getFileStatus
argument_list|(
name|tblPath
argument_list|)
operator|.
name|getPermission
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
if|if
condition|(
name|UserGroupInformation
operator|.
name|isSecurityEnabled
argument_list|()
condition|)
block|{
name|UserGroupInformation
name|ugi
init|=
name|UserGroupInformation
operator|.
name|getCurrentUser
argument_list|()
decl_stmt|;
comment|// check if oozie has set up a howl deleg. token - if so use it
name|TokenSelector
argument_list|<
name|?
extends|extends
name|TokenIdentifier
argument_list|>
name|tokenSelector
init|=
operator|new
name|DelegationTokenSelector
argument_list|()
decl_stmt|;
comment|// TODO: will oozie use a "service" called "oozie" - then instead of
comment|// new Text() do new Text("oozie") below - if this change is made also
comment|// remember to do:
comment|//  job.getConfiguration().set(HCAT_KEY_TOKEN_SIGNATURE, "oozie");
comment|// Also change code in HowlOutputCommitter.cleanupJob() to cancel the
comment|// token only if token.service is not "oozie" - remove the condition of
comment|// HCAT_KEY_TOKEN_SIGNATURE != null in that code.
name|Token
argument_list|<
name|?
extends|extends
name|TokenIdentifier
argument_list|>
name|token
init|=
name|tokenSelector
operator|.
name|selectToken
argument_list|(
operator|new
name|Text
argument_list|()
argument_list|,
name|ugi
operator|.
name|getTokens
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|token
operator|!=
literal|null
condition|)
block|{
name|job
operator|.
name|getCredentials
argument_list|()
operator|.
name|addToken
argument_list|(
operator|new
name|Text
argument_list|(
name|ugi
operator|.
name|getUserName
argument_list|()
argument_list|)
argument_list|,
name|token
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// we did not get token set up by oozie, let's get them ourselves here.
comment|// we essentially get a token per unique Output HowlTableInfo - this is
comment|// done because through Pig, setOutput() method is called multiple times
comment|// We want to only get the token once per unique output HowlTableInfo -
comment|// we cannot just get one token since in multi-query case (> 1 store in 1 job)
comment|// or the case when a single pig script results in> 1 jobs, the single
comment|// token will get cancelled by the output committer and the subsequent
comment|// stores will fail - by tying the token with the concatenation of
comment|// dbname, tablename and partition keyvalues of the output
comment|// TableInfo, we can have as many tokens as there are stores and the TokenSelector
comment|// will correctly pick the right tokens which the committer will use and
comment|// cancel.
name|String
name|tokenSignature
init|=
name|getTokenSignature
argument_list|(
name|outputInfo
argument_list|)
decl_stmt|;
if|if
condition|(
name|tokenMap
operator|.
name|get
argument_list|(
name|tokenSignature
argument_list|)
operator|==
literal|null
condition|)
block|{
comment|// get delegation tokens from howl server and store them into the "job"
comment|// These will be used in the HowlOutputCommitter to publish partitions to
comment|// howl
name|String
name|tokenStrForm
init|=
name|client
operator|.
name|getDelegationTokenWithSignature
argument_list|(
name|ugi
operator|.
name|getUserName
argument_list|()
argument_list|,
name|tokenSignature
argument_list|)
decl_stmt|;
name|Token
argument_list|<
name|DelegationTokenIdentifier
argument_list|>
name|t
init|=
operator|new
name|Token
argument_list|<
name|DelegationTokenIdentifier
argument_list|>
argument_list|()
decl_stmt|;
name|t
operator|.
name|decodeFromUrlString
argument_list|(
name|tokenStrForm
argument_list|)
expr_stmt|;
name|tokenMap
operator|.
name|put
argument_list|(
name|tokenSignature
argument_list|,
name|t
argument_list|)
expr_stmt|;
block|}
name|job
operator|.
name|getCredentials
argument_list|()
operator|.
name|addToken
argument_list|(
operator|new
name|Text
argument_list|(
name|ugi
operator|.
name|getUserName
argument_list|()
operator|+
name|tokenSignature
argument_list|)
argument_list|,
name|tokenMap
operator|.
name|get
argument_list|(
name|tokenSignature
argument_list|)
argument_list|)
expr_stmt|;
comment|// this will be used by the outputcommitter to pass on to the metastore client
comment|// which in turn will pass on to the TokenSelector so that it can select
comment|// the right token.
name|job
operator|.
name|getConfiguration
argument_list|()
operator|.
name|set
argument_list|(
name|HCatConstants
operator|.
name|HCAT_KEY_TOKEN_SIGNATURE
argument_list|,
name|tokenSignature
argument_list|)
expr_stmt|;
block|}
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
if|if
condition|(
name|e
operator|instanceof
name|HCatException
condition|)
block|{
throw|throw
operator|(
name|HCatException
operator|)
name|e
throw|;
block|}
else|else
block|{
throw|throw
operator|new
name|HCatException
argument_list|(
name|ErrorType
operator|.
name|ERROR_SET_OUTPUT
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
finally|finally
block|{
if|if
condition|(
name|client
operator|!=
literal|null
condition|)
block|{
name|client
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
block|}
comment|// a signature string to associate with a HowlTableInfo - essentially
comment|// a concatenation of dbname, tablename and partition keyvalues.
specifier|private
specifier|static
name|String
name|getTokenSignature
parameter_list|(
name|HCatTableInfo
name|outputInfo
parameter_list|)
block|{
name|StringBuilder
name|result
init|=
operator|new
name|StringBuilder
argument_list|(
literal|""
argument_list|)
decl_stmt|;
name|String
name|dbName
init|=
name|outputInfo
operator|.
name|getDatabaseName
argument_list|()
decl_stmt|;
if|if
condition|(
name|dbName
operator|!=
literal|null
condition|)
block|{
name|result
operator|.
name|append
argument_list|(
name|dbName
argument_list|)
expr_stmt|;
block|}
name|String
name|tableName
init|=
name|outputInfo
operator|.
name|getTableName
argument_list|()
decl_stmt|;
if|if
condition|(
name|tableName
operator|!=
literal|null
condition|)
block|{
name|result
operator|.
name|append
argument_list|(
literal|"+"
operator|+
name|tableName
argument_list|)
expr_stmt|;
block|}
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|partValues
init|=
name|outputInfo
operator|.
name|getPartitionValues
argument_list|()
decl_stmt|;
if|if
condition|(
name|partValues
operator|!=
literal|null
condition|)
block|{
for|for
control|(
name|Entry
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|entry
range|:
name|partValues
operator|.
name|entrySet
argument_list|()
control|)
block|{
name|result
operator|.
name|append
argument_list|(
literal|"+"
operator|+
name|entry
operator|.
name|getKey
argument_list|()
operator|+
literal|"="
operator|+
name|entry
operator|.
name|getValue
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|result
operator|.
name|toString
argument_list|()
return|;
block|}
comment|/**      * Handles duplicate publish of partition. Fails if partition already exists.      * For non partitioned tables, fails if files are present in table directory.      * @param job the job      * @param outputInfo the output info      * @param client the metastore client      * @param table the table being written to      * @throws IOException      * @throws MetaException      * @throws TException      */
specifier|private
specifier|static
name|void
name|handleDuplicatePublish
parameter_list|(
name|Job
name|job
parameter_list|,
name|HCatTableInfo
name|outputInfo
parameter_list|,
name|HiveMetaStoreClient
name|client
parameter_list|,
name|Table
name|table
parameter_list|)
throws|throws
name|IOException
throws|,
name|MetaException
throws|,
name|TException
block|{
name|List
argument_list|<
name|String
argument_list|>
name|partitionValues
init|=
name|HCatOutputCommitter
operator|.
name|getPartitionValueList
argument_list|(
name|table
argument_list|,
name|outputInfo
operator|.
name|getPartitionValues
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|table
operator|.
name|getPartitionKeys
argument_list|()
operator|.
name|size
argument_list|()
operator|>
literal|0
condition|)
block|{
comment|//For partitioned table, fail if partition is already present
name|List
argument_list|<
name|String
argument_list|>
name|currentParts
init|=
name|client
operator|.
name|listPartitionNames
argument_list|(
name|outputInfo
operator|.
name|getDatabaseName
argument_list|()
argument_list|,
name|outputInfo
operator|.
name|getTableName
argument_list|()
argument_list|,
name|partitionValues
argument_list|,
operator|(
name|short
operator|)
literal|1
argument_list|)
decl_stmt|;
if|if
condition|(
name|currentParts
operator|.
name|size
argument_list|()
operator|>
literal|0
condition|)
block|{
throw|throw
operator|new
name|HCatException
argument_list|(
name|ErrorType
operator|.
name|ERROR_DUPLICATE_PARTITION
argument_list|)
throw|;
block|}
block|}
else|else
block|{
name|Path
name|tablePath
init|=
operator|new
name|Path
argument_list|(
name|table
operator|.
name|getSd
argument_list|()
operator|.
name|getLocation
argument_list|()
argument_list|)
decl_stmt|;
name|FileSystem
name|fs
init|=
name|tablePath
operator|.
name|getFileSystem
argument_list|(
name|job
operator|.
name|getConfiguration
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|fs
operator|.
name|exists
argument_list|(
name|tablePath
argument_list|)
condition|)
block|{
name|FileStatus
index|[]
name|status
init|=
name|fs
operator|.
name|globStatus
argument_list|(
operator|new
name|Path
argument_list|(
name|tablePath
argument_list|,
literal|"*"
argument_list|)
argument_list|,
name|hiddenFileFilter
argument_list|)
decl_stmt|;
if|if
condition|(
name|status
operator|.
name|length
operator|>
literal|0
condition|)
block|{
throw|throw
operator|new
name|HCatException
argument_list|(
name|ErrorType
operator|.
name|ERROR_NON_EMPTY_TABLE
argument_list|,
name|table
operator|.
name|getDbName
argument_list|()
operator|+
literal|"."
operator|+
name|table
operator|.
name|getTableName
argument_list|()
argument_list|)
throw|;
block|}
block|}
block|}
block|}
comment|/**      * Set the schema for the data being written out to the partition. The      * table schema is used by default for the partition if this is not called.      * @param job the job object      * @param schema the schema for the data      */
specifier|public
specifier|static
name|void
name|setSchema
parameter_list|(
specifier|final
name|Job
name|job
parameter_list|,
specifier|final
name|HCatSchema
name|schema
parameter_list|)
throws|throws
name|IOException
block|{
name|OutputJobInfo
name|jobInfo
init|=
name|getJobInfo
argument_list|(
name|job
argument_list|)
decl_stmt|;
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|partMap
init|=
name|jobInfo
operator|.
name|getTableInfo
argument_list|()
operator|.
name|getPartitionValues
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|Integer
argument_list|>
name|posOfPartCols
init|=
operator|new
name|ArrayList
argument_list|<
name|Integer
argument_list|>
argument_list|()
decl_stmt|;
comment|// If partition columns occur in data, we want to remove them.
comment|// So, find out positions of partition columns in schema provided by user.
comment|// We also need to update the output Schema with these deletions.
comment|// Note that, output storage drivers never sees partition columns in data
comment|// or schema.
name|HCatSchema
name|schemaWithoutParts
init|=
operator|new
name|HCatSchema
argument_list|(
name|schema
operator|.
name|getFields
argument_list|()
argument_list|)
decl_stmt|;
for|for
control|(
name|String
name|partKey
range|:
name|partMap
operator|.
name|keySet
argument_list|()
control|)
block|{
name|Integer
name|idx
decl_stmt|;
if|if
condition|(
operator|(
name|idx
operator|=
name|schema
operator|.
name|getPosition
argument_list|(
name|partKey
argument_list|)
operator|)
operator|!=
literal|null
condition|)
block|{
name|posOfPartCols
operator|.
name|add
argument_list|(
name|idx
argument_list|)
expr_stmt|;
name|schemaWithoutParts
operator|.
name|remove
argument_list|(
name|schema
operator|.
name|get
argument_list|(
name|partKey
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
name|HCatUtil
operator|.
name|validatePartitionSchema
argument_list|(
name|jobInfo
operator|.
name|getTable
argument_list|()
argument_list|,
name|schemaWithoutParts
argument_list|)
expr_stmt|;
name|jobInfo
operator|.
name|setPosOfPartCols
argument_list|(
name|posOfPartCols
argument_list|)
expr_stmt|;
name|jobInfo
operator|.
name|setOutputSchema
argument_list|(
name|schemaWithoutParts
argument_list|)
expr_stmt|;
name|job
operator|.
name|getConfiguration
argument_list|()
operator|.
name|set
argument_list|(
name|HCatConstants
operator|.
name|HCAT_KEY_OUTPUT_INFO
argument_list|,
name|HCatUtil
operator|.
name|serialize
argument_list|(
name|jobInfo
argument_list|)
argument_list|)
expr_stmt|;
block|}
comment|/**      * Gets the table schema for the table specified in the HowlOutputFormat.setOutput call      * on the specified job context.      * @param context the context      * @return the table schema      * @throws IOException if HowlOutputFromat.setOutput has not been called for the passed context      */
specifier|public
specifier|static
name|HCatSchema
name|getTableSchema
parameter_list|(
name|JobContext
name|context
parameter_list|)
throws|throws
name|IOException
block|{
name|OutputJobInfo
name|jobInfo
init|=
name|getJobInfo
argument_list|(
name|context
argument_list|)
decl_stmt|;
return|return
name|jobInfo
operator|.
name|getTableSchema
argument_list|()
return|;
block|}
comment|/**      * Get the record writer for the job. Uses the Table's default OutputStorageDriver      * to get the record writer.      * @param context the information about the current task.      * @return a RecordWriter to write the output for the job.      * @throws IOException      */
annotation|@
name|Override
specifier|public
name|RecordWriter
argument_list|<
name|WritableComparable
argument_list|<
name|?
argument_list|>
argument_list|,
name|HCatRecord
argument_list|>
name|getRecordWriter
parameter_list|(
name|TaskAttemptContext
name|context
parameter_list|)
throws|throws
name|IOException
throws|,
name|InterruptedException
block|{
comment|// First create the RW.
name|HCatRecordWriter
name|rw
init|=
operator|new
name|HCatRecordWriter
argument_list|(
name|context
argument_list|)
decl_stmt|;
comment|// Now set permissions and group on freshly created files.
name|OutputJobInfo
name|info
init|=
name|getJobInfo
argument_list|(
name|context
argument_list|)
decl_stmt|;
name|Path
name|workFile
init|=
name|rw
operator|.
name|getStorageDriver
argument_list|()
operator|.
name|getWorkFilePath
argument_list|(
name|context
argument_list|,
name|info
operator|.
name|getLocation
argument_list|()
argument_list|)
decl_stmt|;
name|Path
name|tblPath
init|=
operator|new
name|Path
argument_list|(
name|info
operator|.
name|getTable
argument_list|()
operator|.
name|getSd
argument_list|()
operator|.
name|getLocation
argument_list|()
argument_list|)
decl_stmt|;
name|FileSystem
name|fs
init|=
name|tblPath
operator|.
name|getFileSystem
argument_list|(
name|context
operator|.
name|getConfiguration
argument_list|()
argument_list|)
decl_stmt|;
name|FileStatus
name|tblPathStat
init|=
name|fs
operator|.
name|getFileStatus
argument_list|(
name|tblPath
argument_list|)
decl_stmt|;
name|fs
operator|.
name|setPermission
argument_list|(
name|workFile
argument_list|,
name|tblPathStat
operator|.
name|getPermission
argument_list|()
argument_list|)
expr_stmt|;
try|try
block|{
name|fs
operator|.
name|setOwner
argument_list|(
name|workFile
argument_list|,
literal|null
argument_list|,
name|tblPathStat
operator|.
name|getGroup
argument_list|()
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|AccessControlException
name|ace
parameter_list|)
block|{
comment|// log the messages before ignoring. Currently, logging is not built in Howl.
block|}
return|return
name|rw
return|;
block|}
comment|/**      * Check for validity of the output-specification for the job.      * @param context information about the job      * @throws IOException when output should not be attempted      */
annotation|@
name|Override
specifier|public
name|void
name|checkOutputSpecs
parameter_list|(
name|JobContext
name|context
parameter_list|)
throws|throws
name|IOException
throws|,
name|InterruptedException
block|{
name|OutputFormat
argument_list|<
name|?
super|super
name|WritableComparable
argument_list|<
name|?
argument_list|>
argument_list|,
name|?
super|super
name|Writable
argument_list|>
name|outputFormat
init|=
name|getOutputFormat
argument_list|(
name|context
argument_list|)
decl_stmt|;
name|outputFormat
operator|.
name|checkOutputSpecs
argument_list|(
name|context
argument_list|)
expr_stmt|;
block|}
comment|/**      * Get the output committer for this output format. This is responsible      * for ensuring the output is committed correctly.      * @param context the task context      * @return an output committer      * @throws IOException      * @throws InterruptedException      */
annotation|@
name|Override
specifier|public
name|OutputCommitter
name|getOutputCommitter
parameter_list|(
name|TaskAttemptContext
name|context
parameter_list|)
throws|throws
name|IOException
throws|,
name|InterruptedException
block|{
name|OutputFormat
argument_list|<
name|?
super|super
name|WritableComparable
argument_list|<
name|?
argument_list|>
argument_list|,
name|?
super|super
name|Writable
argument_list|>
name|outputFormat
init|=
name|getOutputFormat
argument_list|(
name|context
argument_list|)
decl_stmt|;
return|return
operator|new
name|HCatOutputCommitter
argument_list|(
name|outputFormat
operator|.
name|getOutputCommitter
argument_list|(
name|context
argument_list|)
argument_list|)
return|;
block|}
comment|/**      * Gets the output format instance.      * @param context the job context      * @return the output format instance      * @throws IOException      */
specifier|private
name|OutputFormat
argument_list|<
name|?
super|super
name|WritableComparable
argument_list|<
name|?
argument_list|>
argument_list|,
name|?
super|super
name|Writable
argument_list|>
name|getOutputFormat
parameter_list|(
name|JobContext
name|context
parameter_list|)
throws|throws
name|IOException
block|{
name|OutputJobInfo
name|jobInfo
init|=
name|getJobInfo
argument_list|(
name|context
argument_list|)
decl_stmt|;
name|HCatOutputStorageDriver
name|driver
init|=
name|getOutputDriverInstance
argument_list|(
name|context
argument_list|,
name|jobInfo
argument_list|)
decl_stmt|;
name|OutputFormat
argument_list|<
name|?
super|super
name|WritableComparable
argument_list|<
name|?
argument_list|>
argument_list|,
name|?
super|super
name|Writable
argument_list|>
name|outputFormat
init|=
name|driver
operator|.
name|getOutputFormat
argument_list|()
decl_stmt|;
return|return
name|outputFormat
return|;
block|}
comment|/**      * Gets the HowlOuputJobInfo object by reading the Configuration and deserializing      * the string. If JobInfo is not present in the configuration, throws an      * exception since that means HowlOutputFormat.setOutput has not been called.      * @param jobContext the job context      * @return the OutputJobInfo object      * @throws IOException the IO exception      */
specifier|static
name|OutputJobInfo
name|getJobInfo
parameter_list|(
name|JobContext
name|jobContext
parameter_list|)
throws|throws
name|IOException
block|{
name|String
name|jobString
init|=
name|jobContext
operator|.
name|getConfiguration
argument_list|()
operator|.
name|get
argument_list|(
name|HCatConstants
operator|.
name|HCAT_KEY_OUTPUT_INFO
argument_list|)
decl_stmt|;
if|if
condition|(
name|jobString
operator|==
literal|null
condition|)
block|{
throw|throw
operator|new
name|HCatException
argument_list|(
name|ErrorType
operator|.
name|ERROR_NOT_INITIALIZED
argument_list|)
throw|;
block|}
return|return
operator|(
name|OutputJobInfo
operator|)
name|HCatUtil
operator|.
name|deserialize
argument_list|(
name|jobString
argument_list|)
return|;
block|}
comment|/**      * Gets the output storage driver instance.      * @param jobContext the job context      * @param jobInfo the output job info      * @return the output driver instance      * @throws IOException      */
annotation|@
name|SuppressWarnings
argument_list|(
literal|"unchecked"
argument_list|)
specifier|static
name|HCatOutputStorageDriver
name|getOutputDriverInstance
parameter_list|(
name|JobContext
name|jobContext
parameter_list|,
name|OutputJobInfo
name|jobInfo
parameter_list|)
throws|throws
name|IOException
block|{
try|try
block|{
name|Class
argument_list|<
name|?
extends|extends
name|HCatOutputStorageDriver
argument_list|>
name|driverClass
init|=
operator|(
name|Class
argument_list|<
name|?
extends|extends
name|HCatOutputStorageDriver
argument_list|>
operator|)
name|Class
operator|.
name|forName
argument_list|(
name|jobInfo
operator|.
name|getStorerInfo
argument_list|()
operator|.
name|getOutputSDClass
argument_list|()
argument_list|)
decl_stmt|;
name|HCatOutputStorageDriver
name|driver
init|=
name|driverClass
operator|.
name|newInstance
argument_list|()
decl_stmt|;
comment|//Initialize the storage driver
name|driver
operator|.
name|setSchema
argument_list|(
name|jobContext
argument_list|,
name|jobInfo
operator|.
name|getOutputSchema
argument_list|()
argument_list|)
expr_stmt|;
name|driver
operator|.
name|setPartitionValues
argument_list|(
name|jobContext
argument_list|,
name|jobInfo
operator|.
name|getTableInfo
argument_list|()
operator|.
name|getPartitionValues
argument_list|()
argument_list|)
expr_stmt|;
name|driver
operator|.
name|setOutputPath
argument_list|(
name|jobContext
argument_list|,
name|jobInfo
operator|.
name|getLocation
argument_list|()
argument_list|)
expr_stmt|;
name|driver
operator|.
name|initialize
argument_list|(
name|jobContext
argument_list|,
name|jobInfo
operator|.
name|getStorerInfo
argument_list|()
operator|.
name|getProperties
argument_list|()
argument_list|)
expr_stmt|;
return|return
name|driver
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HCatException
argument_list|(
name|ErrorType
operator|.
name|ERROR_INIT_STORAGE_DRIVER
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
specifier|static
name|HiveMetaStoreClient
name|createHiveClient
parameter_list|(
name|String
name|url
parameter_list|,
name|Configuration
name|conf
parameter_list|)
throws|throws
name|IOException
throws|,
name|MetaException
block|{
name|HiveConf
name|hiveConf
init|=
operator|new
name|HiveConf
argument_list|(
name|HCatOutputFormat
operator|.
name|class
argument_list|)
decl_stmt|;
if|if
condition|(
name|url
operator|!=
literal|null
condition|)
block|{
comment|//User specified a thrift url
name|hiveConf
operator|.
name|setBoolean
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|METASTORE_USE_THRIFT_SASL
operator|.
name|varname
argument_list|,
literal|true
argument_list|)
expr_stmt|;
name|hiveConf
operator|.
name|set
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|METASTORE_KERBEROS_PRINCIPAL
operator|.
name|varname
argument_list|,
name|conf
operator|.
name|get
argument_list|(
name|HCatConstants
operator|.
name|HCAT_METASTORE_PRINCIPAL
argument_list|)
argument_list|)
expr_stmt|;
name|hiveConf
operator|.
name|set
argument_list|(
literal|"hive.metastore.local"
argument_list|,
literal|"false"
argument_list|)
expr_stmt|;
name|hiveConf
operator|.
name|set
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|METASTOREURIS
operator|.
name|varname
argument_list|,
name|url
argument_list|)
expr_stmt|;
if|if
condition|(
name|conf
operator|.
name|get
argument_list|(
name|HCatConstants
operator|.
name|HCAT_KEY_TOKEN_SIGNATURE
argument_list|)
operator|!=
literal|null
condition|)
block|{
name|hiveConf
operator|.
name|set
argument_list|(
literal|"hive.metastore.token.signature"
argument_list|,
name|conf
operator|.
name|get
argument_list|(
name|HCatConstants
operator|.
name|HCAT_KEY_TOKEN_SIGNATURE
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
comment|//Thrift url is null, copy the hive conf into the job conf and restore it
comment|//in the backend context
if|if
condition|(
name|conf
operator|.
name|get
argument_list|(
name|HCatConstants
operator|.
name|HCAT_KEY_HIVE_CONF
argument_list|)
operator|==
literal|null
condition|)
block|{
name|conf
operator|.
name|set
argument_list|(
name|HCatConstants
operator|.
name|HCAT_KEY_HIVE_CONF
argument_list|,
name|HCatUtil
operator|.
name|serialize
argument_list|(
name|hiveConf
operator|.
name|getAllProperties
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|//Copy configuration properties into the hive conf
name|Properties
name|properties
init|=
operator|(
name|Properties
operator|)
name|HCatUtil
operator|.
name|deserialize
argument_list|(
name|conf
operator|.
name|get
argument_list|(
name|HCatConstants
operator|.
name|HCAT_KEY_HIVE_CONF
argument_list|)
argument_list|)
decl_stmt|;
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|Object
argument_list|,
name|Object
argument_list|>
name|prop
range|:
name|properties
operator|.
name|entrySet
argument_list|()
control|)
block|{
if|if
condition|(
name|prop
operator|.
name|getValue
argument_list|()
operator|instanceof
name|String
condition|)
block|{
name|hiveConf
operator|.
name|set
argument_list|(
operator|(
name|String
operator|)
name|prop
operator|.
name|getKey
argument_list|()
argument_list|,
operator|(
name|String
operator|)
name|prop
operator|.
name|getValue
argument_list|()
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|prop
operator|.
name|getValue
argument_list|()
operator|instanceof
name|Integer
condition|)
block|{
name|hiveConf
operator|.
name|setInt
argument_list|(
operator|(
name|String
operator|)
name|prop
operator|.
name|getKey
argument_list|()
argument_list|,
operator|(
name|Integer
operator|)
name|prop
operator|.
name|getValue
argument_list|()
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|prop
operator|.
name|getValue
argument_list|()
operator|instanceof
name|Boolean
condition|)
block|{
name|hiveConf
operator|.
name|setBoolean
argument_list|(
operator|(
name|String
operator|)
name|prop
operator|.
name|getKey
argument_list|()
argument_list|,
operator|(
name|Boolean
operator|)
name|prop
operator|.
name|getValue
argument_list|()
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|prop
operator|.
name|getValue
argument_list|()
operator|instanceof
name|Long
condition|)
block|{
name|hiveConf
operator|.
name|setLong
argument_list|(
operator|(
name|String
operator|)
name|prop
operator|.
name|getKey
argument_list|()
argument_list|,
operator|(
name|Long
operator|)
name|prop
operator|.
name|getValue
argument_list|()
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|prop
operator|.
name|getValue
argument_list|()
operator|instanceof
name|Float
condition|)
block|{
name|hiveConf
operator|.
name|setFloat
argument_list|(
operator|(
name|String
operator|)
name|prop
operator|.
name|getKey
argument_list|()
argument_list|,
operator|(
name|Float
operator|)
name|prop
operator|.
name|getValue
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
return|return
operator|new
name|HiveMetaStoreClient
argument_list|(
name|hiveConf
argument_list|)
return|;
block|}
block|}
end_class

end_unit

