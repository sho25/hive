<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<META http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta content="Apache Forrest" name="Generator">
<meta name="Forrest-version" content="0.9">
<meta name="Forrest-skin-name" content="pelt">
<title>Input and Output Interfaces</title>
<link type="text/css" href="skin/basic.css" rel="stylesheet">
<link media="screen" type="text/css" href="skin/screen.css" rel="stylesheet">
<link media="print" type="text/css" href="skin/print.css" rel="stylesheet">
<link type="text/css" href="skin/profile.css" rel="stylesheet">
<script src="skin/getBlank.js" language="javascript" type="text/javascript"></script><script src="skin/getMenu.js" language="javascript" type="text/javascript"></script><script src="skin/fontsize.js" language="javascript" type="text/javascript"></script>
<link rel="shortcut icon" href="">
</head>
<body onload="init()">
<script type="text/javascript">ndeSetTextSize();</script>
<div id="top">
<!--+
    |breadtrail
    +-->
<div class="breadtrail">
<script src="skin/breadcrumbs.js" language="JavaScript" type="text/javascript"></script>
</div>
<!--+
    |header
    +-->
<div class="header">
<!--+
    |start group logo
    +-->
<div class="grouplogo">
<a href=""><img class="logoImage" alt="HCatalog" src="images/hcat.jpg" title=""></a>
</div>
<!--+
    |end group logo
    +-->
<!--+
    |start Project Logo
    +-->
<div class="projectlogoA1">
<a href=""><img class="logoImage" alt="HCatalog" src="images/hcat-box.jpg" title="A table abstraction on top of data for use with java MapReduce programs, Pig scripts and Hive queryies."></a>
</div>
<!--+
    |end Project Logo
    +-->
<!--+
    |start Tabs
    +-->
<ul id="tabs">
<li class="current">
<a class="selected" href="index.html">HCatalog 0.4.0 Documentation</a>
</li>
</ul>
<!--+
    |end Tabs
    +-->
</div>
</div>
<div id="main">
<div id="publishedStrip">
<!--+
    |start Subtabs
    +-->
<div id="level2tabs"></div>
<!--+
    |end Endtabs
    +-->
<script type="text/javascript"><!--
document.write("Last Published: " + document.lastModified);
//  --></script>
</div>
<!--+
    |breadtrail
    +-->
<div class="breadtrail">

             &nbsp;
           </div>
<!--+
    |start Menu, mainarea
    +-->
<!--+
    |start Menu
    +-->
<div id="menu">
<div onclick="SwitchMenu('menu_selected_1.1', 'skin/')" id="menu_selected_1.1Title" class="menutitle" style="background-image: url('skin/images/chapter_open.gif');">HCatalog</div>
<div id="menu_selected_1.1" class="selectedmenuitemgroup" style="display: block;">
<div class="menuitem">
<a href="index.html">Overview</a>
</div>
<div class="menuitem">
<a href="install.html">Installation From Tarball</a>
</div>
<div class="menuitem">
<a href="loadstore.html">Load &amp; Store Interfaces</a>
</div>
<div class="menupage">
<div class="menupagetitle">Input &amp; Output Interfaces </div>
</div>
<div class="menuitem">
<a href="cli.html">Command Line Interface </a>
</div>
<div class="menuitem">
<a href="supportedformats.html">Storage Formats</a>
</div>
<div class="menuitem">
<a href="dynpartition.html">Dynamic Partitioning</a>
</div>
<div class="menuitem">
<a href="notification.html">Notification</a>
</div>
<div class="menuitem">
<a href="api/index.html">API Docs</a>
</div>
</div>
<div id="credit"></div>
<div id="roundbottom">
<img style="display: none" class="corner" height="15" width="15" alt="" src="skin/images/rc-b-l-15-1body-2menu-3menu.png"></div>
<!--+
  |alternative credits
  +-->
<div id="credit2"></div>
</div>
<!--+
    |end Menu
    +-->
<!--+
    |start content
    +-->
<div id="content">
<div title="Portable Document Format" class="pdflink">
<a class="dida" href="inputoutput.pdf"><img alt="PDF -icon" src="skin/images/pdfdoc.gif" class="skin"><br>
        PDF</a>
</div>
<h1>Input and Output Interfaces</h1>
<div id="front-matter">
<div id="minitoc-area">
<ul class="minitoc">
<li>
<a href="#Set+Up">Set Up</a>
</li>
<li>
<a href="#HCatInputFormat">HCatInputFormat</a>
<ul class="minitoc">
<li>
<a href="#API">API</a>
</li>
</ul>
</li>
<li>
<a href="#HCatOutputFormat">HCatOutputFormat</a>
<ul class="minitoc">
<li>
<a href="#API-N1005C">API</a>
</li>
</ul>
</li>
<li>
<a href="#Examples">Examples</a>
</li>
</ul>
</div>
</div>

 <!-- ==================================================================== --> 
  
<a name="Set+Up"></a>
<h2 class="h3">Set Up</h2>
<div class="section">
<p>No HCatalog-specific setup is required for the HCatInputFormat and HCatOutputFormat interfaces.</p>
<p></p>
</div>

<!-- ==================================================================== -->

<a name="HCatInputFormat"></a>
<h2 class="h3">HCatInputFormat</h2>
<div class="section">
<p>The HCatInputFormat is used with MapReduce jobs to read data from HCatalog managed tables.</p>
<p>HCatInputFormat exposes a Hadoop 0.20 MapReduce API for reading data as if it had been published to a table.</p>
<a name="API"></a>
<h3 class="h4">API</h3>
<p>The API exposed by HCatInputFormat is shown below.</p>
<p>To use HCatInputFormat to read data, first instantiate as <span class="codefrag">InputJobInfo</span> with the necessary information from the table being read 
	and then call setInput with the <span class="codefrag">InputJobInfo</span>.</p>
<p>You can use the <span class="codefrag">setOutputSchema</span> method to include a projection schema, to
specify specific output fields. If a schema is not specified all the columns in the table
will be returned.</p>
<p>You can use the <span class="codefrag">getTableSchema</span> methods to determine the table schema for a specified input table.</p>
<pre class="code">
  /**
   * Set the input to use for the Job. This queries the metadata server with
   * the specified partition predicates, gets the matching partitions, puts
   * the information in the conf object. The inputInfo object is updated with
   * information needed in the client context
   * @param job the job object
   * @param inputJobInfo the input info for table to read
   * @throws IOException the exception in communicating with the metadata server
   */
  public static void setInput(Job job,
      InputJobInfo inputJobInfo) throws IOException;

  /**
   * Set the schema for the HCatRecord data returned by HCatInputFormat.
   * @param job the job object
   * @param hcatSchema the schema to use as the consolidated schema
   */
  public static void setOutputSchema(Job job,HCatSchema hcatSchema) 
    throws IOException;

  /**
   * Gets the HCatTable schema for the table specified in the HCatInputFormat.setInput call
   * on the specified job context. This information is available only after HCatInputFormat.setInput
   * has been called for a JobContext.
   * @param context the context
   * @return the table schema
   * @throws IOException if HCatInputFormat.setInput has not been called 
   *                     for the current context
   */
  public static HCatSchema getTableSchema(JobContext context) 
    throws IOException;	

</pre>
</div>    
 
 
<!-- ==================================================================== -->      

<a name="HCatOutputFormat"></a>
<h2 class="h3">HCatOutputFormat</h2>
<div class="section">
<p>HCatOutputFormat is used with MapReduce jobs to write data to HCatalog managed tables.</p>
<p>HCatOutputFormat exposes a Hadoop 20 MapReduce API for writing data to a table.
    When a MapReduce job uses HCatOutputFormat to write output, the default OutputFormat configured for the table is used and the new partition is published to the table after the job completes. </p>
<a name="API-N1005C"></a>
<h3 class="h4">API</h3>
<p>The API exposed by HCatOutputFormat is shown below.</p>
<p>The first call on the HCatOutputFormat must be <span class="codefrag">setOutput</span>; any other call will throw an exception saying the output format is not initialized. The schema for the data being written out is specified by the <span class="codefrag">setSchema </span> method. You must call this method, providing the schema of data you are writing. If your data has same schema as table schema, you can use HCatOutputFormat.getTableSchema() to get the table schema and then pass that along to setSchema(). </p>
<pre class="code">
    /**
     * Set the info about the output to write for the Job. This queries the metadata server
     * to find the StorageDriver to use for the table.  Throws error if partition is already published.
     * @param job the job object
     * @param outputJobInfo the table output info
     * @throws IOException the exception in communicating with the metadata server
     */
    @SuppressWarnings("unchecked")
    public static void setOutput(Job job, OutputJobInfo outputJobInfo) throws IOException;

    /**
     * Set the schema for the data being written out to the partition. The
     * table schema is used by default for the partition if this is not called.
     * @param job the job object
     * @param schema the schema for the data
     */
    public static void setSchema(final Job job, final HCatSchema schema) throws IOException;

  /**
   * Gets the table schema for the table specified in the HCatOutputFormat.setOutput call
   * on the specified job context.
   * @param context the context
   * @return the table schema
   * @throws IOException if HCatOutputFromat.setOutput has not been called for the passed context
   */
  public static HCatSchema getTableSchema(JobContext context) throws IOException;

</pre>
</div>


<a name="Examples"></a>
<h2 class="h3">Examples</h2>
<div class="section">
<p>
<strong>Running MapReduce with HCatalog</strong>
</p>
<p>
Your MapReduce program will need to know where the thrift server to connect to is.  The
easiest way to do this is pass it as an argument to your Java program. You will need to
pass the Hive and HCatalog jars MapReduce as well, via the -libjars argument.</p>
<pre class="code">
export HADOOP_HOME=&lt;path_to_hadoop_install&gt;
export HCAT_HOME=&lt;path_to_hcat_install&gt;
export LIB_JARS=$HCAT_HOME/share/hcatalog/hcatalog-0.4.0.jar,
$HIVE_HOME/lib/hive-metastore-0.9.0.jar,
$HIVE_HOME/lib/libthrift-0.7.0.jar,
$HIVE_HOME/lib/hive-exec-0.9.0.jar,
$HIVE_HOME/lib/libfb303-0.7.0.jar,
$HIVE_HOME/lib/jdo2-api-2.3-ec.jar,
$HIVE_HOME/lib/slf4j-api-1.6.1.jar

export HADOOP_CLASSPATH=$HCAT_HOME/share/hcatalog/hcatalog-0.4.0.jar:
$HIVE_HOME/lib/hive-metastore-0.9.0.jar:
$HIVE_HOME/lib/libthrift-0.7.0.jar:
$HIVE_HOME/lib/hive-exec-0.9.0.jar:
$HIVE_HOME/lib/libfb303-0.7.0.jar:
$HIVE_HOME/lib/jdo2-api-2.3-ec.jar:
$HIVE_HOME/conf:$HADOOP_HOME/conf:
$HIVE_HOME/lib/slf4j-api-1.6.1.jar

$HADOOP_HOME/bin/hadoop --config $HADOOP_HOME/conf jar &lt;path_to_jar&gt;
&lt;main_class&gt; -libjars $LIB_JARS &lt;program_arguments&gt;
</pre>
<p>
<strong>Authentication</strong>
</p>
<table class="ForrestTable" cellspacing="1" cellpadding="4">
	
<tr>
	
<td colspan="1" rowspan="1">
<p>If a failure results in a message like "2010-11-03 16:17:28,225 WARN hive.metastore ... - Unable to connect metastore with URI thrift://..." in /tmp/&lt;username&gt;/hive.log, then make sure you have run "kinit &lt;username&gt;@FOO.COM" to get a Kerberos ticket and to be able to authenticate to the HCatalog server. </p>
</td>
	
</tr>

</table>
<p>
<strong>Read Example</strong>
</p>
<p>
The following very simple MapReduce program reads data from one table which it assumes to have an integer in the
second column, and counts how many different values it sees.   That is, is does the
equivalent of <span class="codefrag">select col1, count(*) from $table group by col1;</span>.
</p>
<pre class="code">
public class GroupByAge extends Configured implements Tool {

    public static class Map extends
            Mapper&lt;WritableComparable, HCatRecord, IntWritable, IntWritable&gt; {

        int age;

        @Override
        protected void map(
                WritableComparable key,
                HCatRecord value,
                org.apache.hadoop.mapreduce.Mapper&lt;WritableComparable, HCatRecord, 
                        IntWritable, IntWritable&gt;.Context context)
                throws IOException, InterruptedException {
            age = (Integer) value.get(1);
            context.write(new IntWritable(age), new IntWritable(1));
        }
    }

    public static class Reduce extends Reducer&lt;IntWritable, IntWritable,
    WritableComparable, HCatRecord&gt; {


      @Override 
      protected void reduce(
              IntWritable key,
              java.lang.Iterable&lt;IntWritable&gt; values, 
              org.apache.hadoop.mapreduce.Reducer&lt;IntWritable, IntWritable,
                      WritableComparable, HCatRecord&gt;.Context context)
              throws IOException, InterruptedException {
          int sum = 0;
          Iterator&lt;IntWritable&gt; iter = values.iterator();
          while (iter.hasNext()) {
              sum++;
              iter.next();
          }
          HCatRecord record = new DefaultHCatRecord(2);
          record.set(0, key.get());
          record.set(1, sum);

          context.write(null, record);
        }
    }

    public int run(String[] args) throws Exception {
        Configuration conf = getConf();
        args = new GenericOptionsParser(conf, args).getRemainingArgs();

        String inputTableName = args[0];
        String outputTableName = args[1];
        String dbName = null;

        Job job = new Job(conf, "GroupByAge");
        HCatInputFormat.setInput(job, InputJobInfo.create(dbName,
                inputTableName, null));
        // initialize HCatOutputFormat

        job.setInputFormatClass(HCatInputFormat.class);
        job.setJarByClass(GroupByAge.class);
        job.setMapperClass(Map.class);
        job.setReducerClass(Reduce.class);
        job.setMapOutputKeyClass(IntWritable.class);
        job.setMapOutputValueClass(IntWritable.class);
        job.setOutputKeyClass(WritableComparable.class);
        job.setOutputValueClass(DefaultHCatRecord.class);
        HCatOutputFormat.setOutput(job, OutputJobInfo.create(dbName,
                outputTableName, null));
        HCatSchema s = HCatOutputFormat.getTableSchema(job);
        System.err.println("INFO: output schema explicitly set for writing:"
                + s);
        HCatOutputFormat.setSchema(job, s);
        job.setOutputFormatClass(HCatOutputFormat.class);
        return (job.waitForCompletion(true) ? 0 : 1);
    }

    public static void main(String[] args) throws Exception {
        int exitCode = ToolRunner.run(new GroupByAge(), args);
        System.exit(exitCode);
    }
}
</pre>
<p>Notice a number of important points about this program:</p>
<ol>

<li>The implementation of Map takes HCatRecord as an input and the implementation of Reduce produces it as an output.</li>

<li>This example program assumes the schema of the input, but it could also retrieve the schema via
HCatOutputFormat.getOutputSchema() and retrieve fields based on the results of that call.</li>

<li>The input descriptor for the table to be read is created by calling InputJobInfo.create.  It requires the database name,
table name, and partition filter.  In this example the partition filter is null, so all partitions of the table
will be read.</li>

<li>The output descriptor for the table to be written is created by calling OutputJobInfo.create.  It requires the
database name, the table name, and a Map of partition keys and values that describe the partition being written.
In this example it is assumed the table is unpartitioned, so this Map is null.</li>

</ol>
<p>To scan just selected partitions of a table, a filter describing the desired partitions can be passed to
InputJobInfo.create.  To scan a single filter, the filter string should look like: "datestamp=20120401" where
datestamp is the partition column name and 20120401 is the value you want to read.</p>
<p>
<strong>Filter Operators</strong>
</p>
<p>A filter can contain the operators 'and', 'or', 'like', '()', '=', '&lt;&gt;' (not equal), '&lt;', '&gt;', '&lt;='
and '&gt;='.  For example: </p>
<ul>

<li>
<span class="codefrag">datestamp &gt; "20110924"</span>
</li>

<li>
<span class="codefrag">datestamp &lt; "20110925</span>
</li>

<li>
<span class="codefrag">datestamp &lt;= "20110925" and datestamp &gt;= "20110924"</span>
</li>

</ul>
<p>
<strong>Scan Filter</strong>
</p>
<p>Assume for example you have a web_logs table that is partitioned by the column datestamp.  You could select one partition of the table by changing</p>
<pre class="code">
HCatInputFormat.setInput(job, InputJobInfo.create(dbName, inputTableName, null));
</pre>
<p>
to
</p>
<pre class="code">
HCatInputFormat.setInput(job,
    InputJobInfo.create(dbName, inputTableName, "datestamp=\"20110924\""));
  </pre>
<p>
This filter must reference only partition columns.  Values from other columns will cause the job to fail.</p>
<p>
<strong>Write Filter</strong>
</p>
<p>
To write to a single partition you can change the above example to have a Map of key value pairs that describe all
of the partition keys and values for that partition.  In our example web_logs table, there is only one partition
column (datestamp), so our Map will have only one entry.  Change </p>
<pre class="code">
HCatOutputFormat.setOutput(job, OutputJobInfo.create(dbName, outputTableName, null));
</pre>
<p>to </p>
<pre class="code">
Map partitions = new HashMap&lt;String, String&gt;(1);
partitions.put("datestamp", "20110924");
HCatOutputFormat.setOutput(job, OutputJobInfo.create(dbName, outputTableName, partitions));
</pre>
<p>To write multiple partitions simultaneously you can leave the Map null, but all of the partitioning columns must be present in the data you are writing.
</p>
</div>


  
</div>
<!--+
    |end content
    +-->
<div class="clearboth">&nbsp;</div>
</div>
<div id="footer">
<!--+
    |start bottomstrip
    +-->
<div class="lastmodified">
<script type="text/javascript"><!--
document.write("Last Published: " + document.lastModified);
//  --></script>
</div>
<div class="copyright">
        Copyright &copy;
         2011-2012 <a href="http://www.apache.org/licenses/">The Apache Software Foundation</a>
</div>
<!--+
    |end bottomstrip
    +-->
</div>
</body>
</html>
