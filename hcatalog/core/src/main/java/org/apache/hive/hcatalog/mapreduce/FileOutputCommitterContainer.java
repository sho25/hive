begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/**  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing,  * software distributed under the License is distributed on an  * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY  * KIND, either express or implied.  See the License for the  * specific language governing permissions and limitations  * under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hive
operator|.
name|hcatalog
operator|.
name|mapreduce
package|;
end_package

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|net
operator|.
name|URI
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|LinkedHashMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
operator|.
name|Entry
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|lang
operator|.
name|StringUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configuration
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileStatus
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|permission
operator|.
name|FsPermission
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|FileUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|StatsSetupConst
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|IMetaStoreClient
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|Warehouse
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|FieldSchema
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|InvalidOperationException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|MetaException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|NoSuchObjectException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|StorageDescriptor
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|HiveStorageHandler
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|Table
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|shims
operator|.
name|ShimLoader
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|JobConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|JobContext
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|JobStatus
operator|.
name|State
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|TaskAttemptContext
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hive
operator|.
name|hcatalog
operator|.
name|common
operator|.
name|ErrorType
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hive
operator|.
name|hcatalog
operator|.
name|common
operator|.
name|HCatConstants
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hive
operator|.
name|hcatalog
operator|.
name|common
operator|.
name|HCatException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hive
operator|.
name|hcatalog
operator|.
name|common
operator|.
name|HCatUtil
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hive
operator|.
name|hcatalog
operator|.
name|data
operator|.
name|schema
operator|.
name|HCatFieldSchema
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hive
operator|.
name|hcatalog
operator|.
name|data
operator|.
name|schema
operator|.
name|HCatSchema
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hive
operator|.
name|hcatalog
operator|.
name|data
operator|.
name|schema
operator|.
name|HCatSchemaUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hive
operator|.
name|hcatalog
operator|.
name|har
operator|.
name|HarOutputCommitterPostProcessor
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|thrift
operator|.
name|TException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|Logger
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|LoggerFactory
import|;
end_import

begin_comment
comment|/**  * Part of the FileOutput*Container classes  * See {@link FileOutputFormatContainer} for more information  */
end_comment

begin_class
class|class
name|FileOutputCommitterContainer
extends|extends
name|OutputCommitterContainer
block|{
specifier|private
specifier|static
specifier|final
name|String
name|TEMP_DIR_NAME
init|=
literal|"_temporary"
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|String
name|LOGS_DIR_NAME
init|=
literal|"_logs"
decl_stmt|;
specifier|static
specifier|final
name|String
name|DYNTEMP_DIR_NAME
init|=
literal|"_DYN"
decl_stmt|;
specifier|static
specifier|final
name|String
name|SCRATCH_DIR_NAME
init|=
literal|"_SCRATCH"
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|String
name|APPEND_SUFFIX
init|=
literal|"_a_"
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|int
name|APPEND_COUNTER_WARN_THRESHOLD
init|=
literal|1000
decl_stmt|;
specifier|private
specifier|final
name|int
name|maxAppendAttempts
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|Logger
name|LOG
init|=
name|LoggerFactory
operator|.
name|getLogger
argument_list|(
name|FileOutputCommitterContainer
operator|.
name|class
argument_list|)
decl_stmt|;
specifier|private
specifier|final
name|boolean
name|dynamicPartitioningUsed
decl_stmt|;
specifier|private
name|boolean
name|partitionsDiscovered
decl_stmt|;
specifier|private
specifier|final
name|boolean
name|customDynamicLocationUsed
decl_stmt|;
specifier|private
name|Map
argument_list|<
name|String
argument_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
argument_list|>
name|partitionsDiscoveredByPath
decl_stmt|;
specifier|private
name|Map
argument_list|<
name|String
argument_list|,
name|JobContext
argument_list|>
name|contextDiscoveredByPath
decl_stmt|;
specifier|private
specifier|final
name|HiveStorageHandler
name|cachedStorageHandler
decl_stmt|;
name|HarOutputCommitterPostProcessor
name|harProcessor
init|=
operator|new
name|HarOutputCommitterPostProcessor
argument_list|()
decl_stmt|;
specifier|private
name|String
name|ptnRootLocation
init|=
literal|null
decl_stmt|;
specifier|private
name|OutputJobInfo
name|jobInfo
init|=
literal|null
decl_stmt|;
comment|/**    * @param context current JobContext    * @param baseCommitter OutputCommitter to contain    * @throws IOException    */
specifier|public
name|FileOutputCommitterContainer
parameter_list|(
name|JobContext
name|context
parameter_list|,
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|OutputCommitter
name|baseCommitter
parameter_list|)
throws|throws
name|IOException
block|{
name|super
argument_list|(
name|context
argument_list|,
name|baseCommitter
argument_list|)
expr_stmt|;
name|jobInfo
operator|=
name|HCatOutputFormat
operator|.
name|getJobInfo
argument_list|(
name|context
operator|.
name|getConfiguration
argument_list|()
argument_list|)
expr_stmt|;
name|dynamicPartitioningUsed
operator|=
name|jobInfo
operator|.
name|isDynamicPartitioningUsed
argument_list|()
expr_stmt|;
name|this
operator|.
name|partitionsDiscovered
operator|=
operator|!
name|dynamicPartitioningUsed
expr_stmt|;
name|cachedStorageHandler
operator|=
name|HCatUtil
operator|.
name|getStorageHandler
argument_list|(
name|context
operator|.
name|getConfiguration
argument_list|()
argument_list|,
name|jobInfo
operator|.
name|getTableInfo
argument_list|()
operator|.
name|getStorerInfo
argument_list|()
argument_list|)
expr_stmt|;
name|Table
name|table
init|=
operator|new
name|Table
argument_list|(
name|jobInfo
operator|.
name|getTableInfo
argument_list|()
operator|.
name|getTable
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|dynamicPartitioningUsed
operator|&&
name|Boolean
operator|.
name|parseBoolean
argument_list|(
operator|(
name|String
operator|)
name|table
operator|.
name|getProperty
argument_list|(
literal|"EXTERNAL"
argument_list|)
argument_list|)
operator|&&
name|jobInfo
operator|.
name|getCustomDynamicPath
argument_list|()
operator|!=
literal|null
operator|&&
name|jobInfo
operator|.
name|getCustomDynamicPath
argument_list|()
operator|.
name|length
argument_list|()
operator|>
literal|0
condition|)
block|{
name|customDynamicLocationUsed
operator|=
literal|true
expr_stmt|;
block|}
else|else
block|{
name|customDynamicLocationUsed
operator|=
literal|false
expr_stmt|;
block|}
name|this
operator|.
name|maxAppendAttempts
operator|=
name|context
operator|.
name|getConfiguration
argument_list|()
operator|.
name|getInt
argument_list|(
name|HCatConstants
operator|.
name|HCAT_APPEND_LIMIT
argument_list|,
name|APPEND_COUNTER_WARN_THRESHOLD
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|abortTask
parameter_list|(
name|TaskAttemptContext
name|context
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
operator|!
name|dynamicPartitioningUsed
condition|)
block|{
name|FileOutputFormatContainer
operator|.
name|setWorkOutputPath
argument_list|(
name|context
argument_list|)
expr_stmt|;
name|getBaseOutputCommitter
argument_list|()
operator|.
name|abortTask
argument_list|(
name|HCatMapRedUtil
operator|.
name|createTaskAttemptContext
argument_list|(
name|context
argument_list|)
argument_list|)
expr_stmt|;
block|}
else|else
block|{
try|try
block|{
name|TaskCommitContextRegistry
operator|.
name|getInstance
argument_list|()
operator|.
name|abortTask
argument_list|(
name|context
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
name|TaskCommitContextRegistry
operator|.
name|getInstance
argument_list|()
operator|.
name|discardCleanupFor
argument_list|(
name|context
argument_list|)
expr_stmt|;
block|}
block|}
block|}
annotation|@
name|Override
specifier|public
name|void
name|commitTask
parameter_list|(
name|TaskAttemptContext
name|context
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
operator|!
name|dynamicPartitioningUsed
condition|)
block|{
comment|//See HCATALOG-499
name|FileOutputFormatContainer
operator|.
name|setWorkOutputPath
argument_list|(
name|context
argument_list|)
expr_stmt|;
name|getBaseOutputCommitter
argument_list|()
operator|.
name|commitTask
argument_list|(
name|HCatMapRedUtil
operator|.
name|createTaskAttemptContext
argument_list|(
name|context
argument_list|)
argument_list|)
expr_stmt|;
block|}
else|else
block|{
try|try
block|{
name|TaskCommitContextRegistry
operator|.
name|getInstance
argument_list|()
operator|.
name|commitTask
argument_list|(
name|context
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
name|TaskCommitContextRegistry
operator|.
name|getInstance
argument_list|()
operator|.
name|discardCleanupFor
argument_list|(
name|context
argument_list|)
expr_stmt|;
block|}
block|}
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|needsTaskCommit
parameter_list|(
name|TaskAttemptContext
name|context
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
operator|!
name|dynamicPartitioningUsed
condition|)
block|{
name|FileOutputFormatContainer
operator|.
name|setWorkOutputPath
argument_list|(
name|context
argument_list|)
expr_stmt|;
return|return
name|getBaseOutputCommitter
argument_list|()
operator|.
name|needsTaskCommit
argument_list|(
name|HCatMapRedUtil
operator|.
name|createTaskAttemptContext
argument_list|(
name|context
argument_list|)
argument_list|)
return|;
block|}
else|else
block|{
comment|// called explicitly through FileRecordWriterContainer.close() if dynamic - return false by default
return|return
literal|true
return|;
block|}
block|}
annotation|@
name|Override
specifier|public
name|void
name|setupJob
parameter_list|(
name|JobContext
name|context
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|getBaseOutputCommitter
argument_list|()
operator|!=
literal|null
operator|&&
operator|!
name|dynamicPartitioningUsed
condition|)
block|{
name|getBaseOutputCommitter
argument_list|()
operator|.
name|setupJob
argument_list|(
name|HCatMapRedUtil
operator|.
name|createJobContext
argument_list|(
name|context
argument_list|)
argument_list|)
expr_stmt|;
block|}
comment|// in dynamic usecase, called through FileRecordWriterContainer
block|}
annotation|@
name|Override
specifier|public
name|void
name|setupTask
parameter_list|(
name|TaskAttemptContext
name|context
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
operator|!
name|dynamicPartitioningUsed
condition|)
block|{
name|getBaseOutputCommitter
argument_list|()
operator|.
name|setupTask
argument_list|(
name|HCatMapRedUtil
operator|.
name|createTaskAttemptContext
argument_list|(
name|context
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
annotation|@
name|Override
specifier|public
name|void
name|abortJob
parameter_list|(
name|JobContext
name|jobContext
parameter_list|,
name|State
name|state
parameter_list|)
throws|throws
name|IOException
block|{
try|try
block|{
if|if
condition|(
name|dynamicPartitioningUsed
condition|)
block|{
name|discoverPartitions
argument_list|(
name|jobContext
argument_list|)
expr_stmt|;
block|}
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|JobContext
name|mapRedJobContext
init|=
name|HCatMapRedUtil
operator|.
name|createJobContext
argument_list|(
name|jobContext
argument_list|)
decl_stmt|;
if|if
condition|(
name|getBaseOutputCommitter
argument_list|()
operator|!=
literal|null
operator|&&
operator|!
name|dynamicPartitioningUsed
condition|)
block|{
name|getBaseOutputCommitter
argument_list|()
operator|.
name|abortJob
argument_list|(
name|mapRedJobContext
argument_list|,
name|state
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|dynamicPartitioningUsed
condition|)
block|{
for|for
control|(
name|JobContext
name|currContext
range|:
name|contextDiscoveredByPath
operator|.
name|values
argument_list|()
control|)
block|{
try|try
block|{
operator|new
name|JobConf
argument_list|(
name|currContext
operator|.
name|getConfiguration
argument_list|()
argument_list|)
operator|.
name|getOutputCommitter
argument_list|()
operator|.
name|abortJob
argument_list|(
name|currContext
argument_list|,
name|state
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
block|}
name|Path
name|src
decl_stmt|;
name|OutputJobInfo
name|jobInfo
init|=
name|HCatOutputFormat
operator|.
name|getJobInfo
argument_list|(
name|jobContext
operator|.
name|getConfiguration
argument_list|()
argument_list|)
decl_stmt|;
name|Path
name|tblPath
init|=
operator|new
name|Path
argument_list|(
name|jobInfo
operator|.
name|getTableInfo
argument_list|()
operator|.
name|getTableLocation
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|dynamicPartitioningUsed
condition|)
block|{
if|if
condition|(
operator|!
name|customDynamicLocationUsed
condition|)
block|{
name|src
operator|=
operator|new
name|Path
argument_list|(
name|getPartitionRootLocation
argument_list|(
name|jobInfo
operator|.
name|getLocation
argument_list|()
argument_list|,
name|jobInfo
operator|.
name|getTableInfo
argument_list|()
operator|.
name|getTable
argument_list|()
operator|.
name|getPartitionKeysSize
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|src
operator|=
operator|new
name|Path
argument_list|(
name|getCustomPartitionRootLocation
argument_list|(
name|jobInfo
argument_list|,
name|jobContext
operator|.
name|getConfiguration
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
name|src
operator|=
operator|new
name|Path
argument_list|(
name|jobInfo
operator|.
name|getLocation
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|FileSystem
name|fs
init|=
name|src
operator|.
name|getFileSystem
argument_list|(
name|jobContext
operator|.
name|getConfiguration
argument_list|()
argument_list|)
decl_stmt|;
comment|// Note fs.delete will fail on Windows. The reason is in OutputCommitter,
comment|// Hadoop is still writing to _logs/history. On Linux, OS don't care file is still
comment|// open and remove the directory anyway, but on Windows, OS refuse to remove a
comment|// directory containing open files. So on Windows, we will leave output directory
comment|// behind when job fail. User needs to remove the output directory manually
name|LOG
operator|.
name|info
argument_list|(
literal|"Job failed. Try cleaning up temporary directory [{}]."
argument_list|,
name|src
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|src
operator|.
name|equals
argument_list|(
name|tblPath
argument_list|)
condition|)
block|{
name|fs
operator|.
name|delete
argument_list|(
name|src
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
block|}
finally|finally
block|{
name|cancelDelegationTokens
argument_list|(
name|jobContext
argument_list|)
expr_stmt|;
block|}
block|}
specifier|public
specifier|static
specifier|final
name|String
name|SUCCEEDED_FILE_NAME
init|=
literal|"_SUCCESS"
decl_stmt|;
specifier|static
specifier|final
name|String
name|SUCCESSFUL_JOB_OUTPUT_DIR_MARKER
init|=
literal|"mapreduce.fileoutputcommitter.marksuccessfuljobs"
decl_stmt|;
specifier|private
specifier|static
name|boolean
name|getOutputDirMarking
parameter_list|(
name|Configuration
name|conf
parameter_list|)
block|{
return|return
name|conf
operator|.
name|getBoolean
argument_list|(
name|SUCCESSFUL_JOB_OUTPUT_DIR_MARKER
argument_list|,
literal|false
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|commitJob
parameter_list|(
name|JobContext
name|jobContext
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|dynamicPartitioningUsed
condition|)
block|{
name|discoverPartitions
argument_list|(
name|jobContext
argument_list|)
expr_stmt|;
comment|// Commit each partition so it gets moved out of the job work
comment|// dir
for|for
control|(
name|JobContext
name|context
range|:
name|contextDiscoveredByPath
operator|.
name|values
argument_list|()
control|)
block|{
operator|new
name|JobConf
argument_list|(
name|context
operator|.
name|getConfiguration
argument_list|()
argument_list|)
operator|.
name|getOutputCommitter
argument_list|()
operator|.
name|commitJob
argument_list|(
name|context
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
name|getBaseOutputCommitter
argument_list|()
operator|!=
literal|null
operator|&&
operator|!
name|dynamicPartitioningUsed
condition|)
block|{
name|getBaseOutputCommitter
argument_list|()
operator|.
name|commitJob
argument_list|(
name|HCatMapRedUtil
operator|.
name|createJobContext
argument_list|(
name|jobContext
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|registerPartitions
argument_list|(
name|jobContext
argument_list|)
expr_stmt|;
comment|// create _SUCCESS FILE if so requested.
name|OutputJobInfo
name|jobInfo
init|=
name|HCatOutputFormat
operator|.
name|getJobInfo
argument_list|(
name|jobContext
operator|.
name|getConfiguration
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|getOutputDirMarking
argument_list|(
name|jobContext
operator|.
name|getConfiguration
argument_list|()
argument_list|)
condition|)
block|{
name|Path
name|outputPath
init|=
operator|new
name|Path
argument_list|(
name|jobInfo
operator|.
name|getLocation
argument_list|()
argument_list|)
decl_stmt|;
name|FileSystem
name|fileSys
init|=
name|outputPath
operator|.
name|getFileSystem
argument_list|(
name|jobContext
operator|.
name|getConfiguration
argument_list|()
argument_list|)
decl_stmt|;
comment|// create a file in the folder to mark it
if|if
condition|(
name|fileSys
operator|.
name|exists
argument_list|(
name|outputPath
argument_list|)
condition|)
block|{
name|Path
name|filePath
init|=
operator|new
name|Path
argument_list|(
name|outputPath
argument_list|,
name|SUCCEEDED_FILE_NAME
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|fileSys
operator|.
name|exists
argument_list|(
name|filePath
argument_list|)
condition|)
block|{
comment|// may have been
comment|// created by
comment|// baseCommitter.commitJob()
name|fileSys
operator|.
name|create
argument_list|(
name|filePath
argument_list|)
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
block|}
comment|// Commit has succeeded (since no exceptions have been thrown.)
comment|// Safe to cancel delegation tokens now.
name|cancelDelegationTokens
argument_list|(
name|jobContext
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|cleanupJob
parameter_list|(
name|JobContext
name|context
parameter_list|)
throws|throws
name|IOException
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"The method cleanupJob is deprecated and should not be called."
argument_list|)
throw|;
block|}
specifier|private
name|String
name|getCustomPartitionRootLocation
parameter_list|(
name|OutputJobInfo
name|jobInfo
parameter_list|,
name|Configuration
name|conf
parameter_list|)
block|{
if|if
condition|(
name|ptnRootLocation
operator|==
literal|null
condition|)
block|{
comment|// we only need to calculate it once, it'll be the same for other partitions in this job.
name|String
name|parentPath
init|=
name|jobInfo
operator|.
name|getTableInfo
argument_list|()
operator|.
name|getTableLocation
argument_list|()
decl_stmt|;
if|if
condition|(
name|jobInfo
operator|.
name|getCustomDynamicRoot
argument_list|()
operator|!=
literal|null
operator|&&
name|jobInfo
operator|.
name|getCustomDynamicRoot
argument_list|()
operator|.
name|length
argument_list|()
operator|>
literal|0
condition|)
block|{
name|parentPath
operator|=
operator|new
name|Path
argument_list|(
name|parentPath
argument_list|,
name|jobInfo
operator|.
name|getCustomDynamicRoot
argument_list|()
argument_list|)
operator|.
name|toString
argument_list|()
expr_stmt|;
block|}
name|Path
name|ptnRoot
init|=
operator|new
name|Path
argument_list|(
name|parentPath
argument_list|,
name|DYNTEMP_DIR_NAME
operator|+
name|conf
operator|.
name|get
argument_list|(
name|HCatConstants
operator|.
name|HCAT_DYNAMIC_PTN_JOBID
argument_list|)
argument_list|)
decl_stmt|;
name|ptnRootLocation
operator|=
name|ptnRoot
operator|.
name|toString
argument_list|()
expr_stmt|;
block|}
return|return
name|ptnRootLocation
return|;
block|}
specifier|private
name|String
name|getPartitionRootLocation
parameter_list|(
name|String
name|ptnLocn
parameter_list|,
name|int
name|numPtnKeys
parameter_list|)
block|{
if|if
condition|(
name|customDynamicLocationUsed
condition|)
block|{
return|return
literal|null
return|;
block|}
if|if
condition|(
name|ptnRootLocation
operator|==
literal|null
condition|)
block|{
comment|// we only need to calculate it once, it'll be the same for other partitions in this job.
name|Path
name|ptnRoot
init|=
operator|new
name|Path
argument_list|(
name|ptnLocn
argument_list|)
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|numPtnKeys
condition|;
name|i
operator|++
control|)
block|{
comment|//          LOG.info("Getting parent of "+ptnRoot.getName());
name|ptnRoot
operator|=
name|ptnRoot
operator|.
name|getParent
argument_list|()
expr_stmt|;
block|}
name|ptnRootLocation
operator|=
name|ptnRoot
operator|.
name|toString
argument_list|()
expr_stmt|;
block|}
comment|//      LOG.info("Returning final parent : "+ptnRootLocation);
return|return
name|ptnRootLocation
return|;
block|}
comment|/**    * Generate partition metadata object to be used to add to metadata.    * @param context The job context.    * @param jobInfo The OutputJobInfo.    * @param partLocnRoot The table-equivalent location root of the partition    *                       (temporary dir if dynamic partition, table dir if static)    * @param dynPartPath The path of dynamic partition which is created    * @param partKVs The keyvalue pairs that form the partition    * @param outputSchema The output schema for the partition    * @param params The parameters to store inside the partition    * @param table The Table metadata object under which this Partition will reside    * @param fs FileSystem object to operate on the underlying filesystem    * @param grpName Group name that owns the table dir    * @param perms FsPermission that's the default permission of the table dir.    * @return Constructed Partition metadata object    * @throws java.io.IOException    */
specifier|private
name|Partition
name|constructPartition
parameter_list|(
name|JobContext
name|context
parameter_list|,
name|OutputJobInfo
name|jobInfo
parameter_list|,
name|String
name|partLocnRoot
parameter_list|,
name|String
name|dynPartPath
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|partKVs
parameter_list|,
name|HCatSchema
name|outputSchema
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|params
parameter_list|,
name|Table
name|table
parameter_list|,
name|FileSystem
name|fs
parameter_list|,
name|String
name|grpName
parameter_list|,
name|FsPermission
name|perms
parameter_list|)
throws|throws
name|IOException
block|{
name|Partition
name|partition
init|=
operator|new
name|Partition
argument_list|()
decl_stmt|;
name|partition
operator|.
name|setDbName
argument_list|(
name|table
operator|.
name|getDbName
argument_list|()
argument_list|)
expr_stmt|;
name|partition
operator|.
name|setTableName
argument_list|(
name|table
operator|.
name|getTableName
argument_list|()
argument_list|)
expr_stmt|;
name|partition
operator|.
name|setSd
argument_list|(
operator|new
name|StorageDescriptor
argument_list|(
name|table
operator|.
name|getTTable
argument_list|()
operator|.
name|getSd
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
name|List
argument_list|<
name|FieldSchema
argument_list|>
name|fields
init|=
operator|new
name|ArrayList
argument_list|<
name|FieldSchema
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|HCatFieldSchema
name|fieldSchema
range|:
name|outputSchema
operator|.
name|getFields
argument_list|()
control|)
block|{
name|fields
operator|.
name|add
argument_list|(
name|HCatSchemaUtils
operator|.
name|getFieldSchema
argument_list|(
name|fieldSchema
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|partition
operator|.
name|getSd
argument_list|()
operator|.
name|setCols
argument_list|(
name|fields
argument_list|)
expr_stmt|;
name|partition
operator|.
name|setValues
argument_list|(
name|FileOutputFormatContainer
operator|.
name|getPartitionValueList
argument_list|(
name|table
argument_list|,
name|partKVs
argument_list|)
argument_list|)
expr_stmt|;
name|partition
operator|.
name|setParameters
argument_list|(
name|params
argument_list|)
expr_stmt|;
comment|// Sets permissions and group name on partition dirs and files.
name|Path
name|partPath
decl_stmt|;
if|if
condition|(
name|customDynamicLocationUsed
condition|)
block|{
name|partPath
operator|=
operator|new
name|Path
argument_list|(
name|dynPartPath
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
operator|!
name|dynamicPartitioningUsed
operator|&&
name|Boolean
operator|.
name|parseBoolean
argument_list|(
operator|(
name|String
operator|)
name|table
operator|.
name|getProperty
argument_list|(
literal|"EXTERNAL"
argument_list|)
argument_list|)
operator|&&
name|jobInfo
operator|.
name|getLocation
argument_list|()
operator|!=
literal|null
operator|&&
name|jobInfo
operator|.
name|getLocation
argument_list|()
operator|.
name|length
argument_list|()
operator|>
literal|0
condition|)
block|{
comment|// Now, we need to de-scratchify this location - i.e., get rid of any
comment|// _SCRATCH[\d].?[\d]+ from the location.
name|String
name|jobLocation
init|=
name|jobInfo
operator|.
name|getLocation
argument_list|()
decl_stmt|;
name|String
name|finalLocn
init|=
name|jobLocation
operator|.
name|replaceAll
argument_list|(
name|Path
operator|.
name|SEPARATOR
operator|+
name|SCRATCH_DIR_NAME
operator|+
literal|"\\d\\.?\\d+"
argument_list|,
literal|""
argument_list|)
decl_stmt|;
name|partPath
operator|=
operator|new
name|Path
argument_list|(
name|finalLocn
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|partPath
operator|=
operator|new
name|Path
argument_list|(
name|partLocnRoot
argument_list|)
expr_stmt|;
name|int
name|i
init|=
literal|0
decl_stmt|;
for|for
control|(
name|FieldSchema
name|partKey
range|:
name|table
operator|.
name|getPartitionKeys
argument_list|()
control|)
block|{
if|if
condition|(
name|i
operator|++
operator|!=
literal|0
condition|)
block|{
name|fs
operator|.
name|mkdirs
argument_list|(
name|partPath
argument_list|)
expr_stmt|;
comment|// Attempt to make the path in case it does not exist before we check
name|applyGroupAndPerms
argument_list|(
name|fs
argument_list|,
name|partPath
argument_list|,
name|perms
argument_list|,
name|grpName
argument_list|,
literal|false
argument_list|)
expr_stmt|;
block|}
name|partPath
operator|=
name|constructPartialPartPath
argument_list|(
name|partPath
argument_list|,
name|partKey
operator|.
name|getName
argument_list|()
operator|.
name|toLowerCase
argument_list|()
argument_list|,
name|partKVs
argument_list|)
expr_stmt|;
block|}
block|}
comment|// Apply the group and permissions to the leaf partition and files.
comment|// Need not bother in case of HDFS as permission is taken care of by setting UMask
name|fs
operator|.
name|mkdirs
argument_list|(
name|partPath
argument_list|)
expr_stmt|;
comment|// Attempt to make the path in case it does not exist before we check
if|if
condition|(
operator|!
name|ShimLoader
operator|.
name|getHadoopShims
argument_list|()
operator|.
name|getHCatShim
argument_list|()
operator|.
name|isFileInHDFS
argument_list|(
name|fs
argument_list|,
name|partPath
argument_list|)
condition|)
block|{
name|applyGroupAndPerms
argument_list|(
name|fs
argument_list|,
name|partPath
argument_list|,
name|perms
argument_list|,
name|grpName
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
comment|// Set the location in the StorageDescriptor
if|if
condition|(
name|dynamicPartitioningUsed
condition|)
block|{
name|String
name|dynamicPartitionDestination
init|=
name|getFinalDynamicPartitionDestination
argument_list|(
name|table
argument_list|,
name|partKVs
argument_list|,
name|jobInfo
argument_list|)
decl_stmt|;
if|if
condition|(
name|harProcessor
operator|.
name|isEnabled
argument_list|()
condition|)
block|{
name|harProcessor
operator|.
name|exec
argument_list|(
name|context
argument_list|,
name|partition
argument_list|,
name|partPath
argument_list|)
expr_stmt|;
name|partition
operator|.
name|getSd
argument_list|()
operator|.
name|setLocation
argument_list|(
name|harProcessor
operator|.
name|getProcessedLocation
argument_list|(
operator|new
name|Path
argument_list|(
name|dynamicPartitionDestination
argument_list|)
argument_list|)
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|partition
operator|.
name|getSd
argument_list|()
operator|.
name|setLocation
argument_list|(
name|dynamicPartitionDestination
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
name|partition
operator|.
name|getSd
argument_list|()
operator|.
name|setLocation
argument_list|(
name|partPath
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
return|return
name|partition
return|;
block|}
specifier|private
name|void
name|applyGroupAndPerms
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|Path
name|dir
parameter_list|,
name|FsPermission
name|permission
parameter_list|,
name|String
name|group
parameter_list|,
name|boolean
name|recursive
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"applyGroupAndPerms : "
operator|+
name|dir
operator|+
literal|" perms: "
operator|+
name|permission
operator|+
literal|" group: "
operator|+
name|group
operator|+
literal|" recursive: "
operator|+
name|recursive
argument_list|)
expr_stmt|;
block|}
name|fs
operator|.
name|setPermission
argument_list|(
name|dir
argument_list|,
name|permission
argument_list|)
expr_stmt|;
if|if
condition|(
name|recursive
condition|)
block|{
for|for
control|(
name|FileStatus
name|fileStatus
range|:
name|fs
operator|.
name|listStatus
argument_list|(
name|dir
argument_list|)
control|)
block|{
if|if
condition|(
name|fileStatus
operator|.
name|isDir
argument_list|()
condition|)
block|{
name|applyGroupAndPerms
argument_list|(
name|fs
argument_list|,
name|fileStatus
operator|.
name|getPath
argument_list|()
argument_list|,
name|permission
argument_list|,
name|group
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|fs
operator|.
name|setPermission
argument_list|(
name|fileStatus
operator|.
name|getPath
argument_list|()
argument_list|,
name|permission
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
specifier|private
name|String
name|getFinalDynamicPartitionDestination
parameter_list|(
name|Table
name|table
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|partKVs
parameter_list|,
name|OutputJobInfo
name|jobInfo
parameter_list|)
block|{
name|Path
name|partPath
init|=
operator|new
name|Path
argument_list|(
name|table
operator|.
name|getTTable
argument_list|()
operator|.
name|getSd
argument_list|()
operator|.
name|getLocation
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|customDynamicLocationUsed
condition|)
block|{
comment|// file:///tmp/hcat_junit_warehouse/employee/_DYN0.7770480401313761/emp_country=IN/emp_state=KA  ->
comment|// file:///tmp/hcat_junit_warehouse/employee/emp_country=IN/emp_state=KA
for|for
control|(
name|FieldSchema
name|partKey
range|:
name|table
operator|.
name|getPartitionKeys
argument_list|()
control|)
block|{
name|partPath
operator|=
name|constructPartialPartPath
argument_list|(
name|partPath
argument_list|,
name|partKey
operator|.
name|getName
argument_list|()
operator|.
name|toLowerCase
argument_list|()
argument_list|,
name|partKVs
argument_list|)
expr_stmt|;
block|}
return|return
name|partPath
operator|.
name|toString
argument_list|()
return|;
block|}
else|else
block|{
comment|// if custom root specified, update the parent path
if|if
condition|(
name|jobInfo
operator|.
name|getCustomDynamicRoot
argument_list|()
operator|!=
literal|null
operator|&&
name|jobInfo
operator|.
name|getCustomDynamicRoot
argument_list|()
operator|.
name|length
argument_list|()
operator|>
literal|0
condition|)
block|{
name|partPath
operator|=
operator|new
name|Path
argument_list|(
name|partPath
argument_list|,
name|jobInfo
operator|.
name|getCustomDynamicRoot
argument_list|()
argument_list|)
expr_stmt|;
block|}
return|return
operator|new
name|Path
argument_list|(
name|partPath
argument_list|,
name|HCatFileUtil
operator|.
name|resolveCustomPath
argument_list|(
name|jobInfo
argument_list|,
name|partKVs
argument_list|,
literal|false
argument_list|)
argument_list|)
operator|.
name|toString
argument_list|()
return|;
block|}
block|}
specifier|private
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|getStorerParameterMap
parameter_list|(
name|StorerInfo
name|storer
parameter_list|)
block|{
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|params
init|=
operator|new
name|HashMap
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
argument_list|()
decl_stmt|;
comment|//Copy table level hcat.* keys to the partition
for|for
control|(
name|Entry
argument_list|<
name|Object
argument_list|,
name|Object
argument_list|>
name|entry
range|:
name|storer
operator|.
name|getProperties
argument_list|()
operator|.
name|entrySet
argument_list|()
control|)
block|{
if|if
condition|(
operator|!
name|entry
operator|.
name|getKey
argument_list|()
operator|.
name|toString
argument_list|()
operator|.
name|equals
argument_list|(
name|StatsSetupConst
operator|.
name|COLUMN_STATS_ACCURATE
argument_list|)
condition|)
block|{
name|params
operator|.
name|put
argument_list|(
name|entry
operator|.
name|getKey
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|,
name|entry
operator|.
name|getValue
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|params
return|;
block|}
specifier|private
name|Path
name|constructPartialPartPath
parameter_list|(
name|Path
name|partialPath
parameter_list|,
name|String
name|partKey
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|partKVs
parameter_list|)
block|{
name|StringBuilder
name|sb
init|=
operator|new
name|StringBuilder
argument_list|(
name|FileUtils
operator|.
name|escapePathName
argument_list|(
name|partKey
argument_list|)
argument_list|)
decl_stmt|;
name|sb
operator|.
name|append
argument_list|(
literal|"="
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
name|FileUtils
operator|.
name|escapePathName
argument_list|(
name|partKVs
operator|.
name|get
argument_list|(
name|partKey
argument_list|)
argument_list|)
argument_list|)
expr_stmt|;
return|return
operator|new
name|Path
argument_list|(
name|partialPath
argument_list|,
name|sb
operator|.
name|toString
argument_list|()
argument_list|)
return|;
block|}
comment|/**    * Update table schema, adding new columns as added for the partition.    * @param client the client    * @param table the table    * @param partitionSchema the schema of the partition    * @throws java.io.IOException Signals that an I/O exception has occurred.    * @throws org.apache.hadoop.hive.metastore.api.InvalidOperationException the invalid operation exception    * @throws org.apache.hadoop.hive.metastore.api.MetaException the meta exception    * @throws org.apache.thrift.TException the t exception    */
specifier|private
name|void
name|updateTableSchema
parameter_list|(
name|IMetaStoreClient
name|client
parameter_list|,
name|Table
name|table
parameter_list|,
name|HCatSchema
name|partitionSchema
parameter_list|)
throws|throws
name|IOException
throws|,
name|InvalidOperationException
throws|,
name|MetaException
throws|,
name|TException
block|{
name|List
argument_list|<
name|FieldSchema
argument_list|>
name|newColumns
init|=
name|HCatUtil
operator|.
name|validatePartitionSchema
argument_list|(
name|table
argument_list|,
name|partitionSchema
argument_list|)
decl_stmt|;
if|if
condition|(
name|newColumns
operator|.
name|size
argument_list|()
operator|!=
literal|0
condition|)
block|{
name|List
argument_list|<
name|FieldSchema
argument_list|>
name|tableColumns
init|=
operator|new
name|ArrayList
argument_list|<
name|FieldSchema
argument_list|>
argument_list|(
name|table
operator|.
name|getTTable
argument_list|()
operator|.
name|getSd
argument_list|()
operator|.
name|getCols
argument_list|()
argument_list|)
decl_stmt|;
name|tableColumns
operator|.
name|addAll
argument_list|(
name|newColumns
argument_list|)
expr_stmt|;
comment|//Update table schema to add the newly added columns
name|table
operator|.
name|getTTable
argument_list|()
operator|.
name|getSd
argument_list|()
operator|.
name|setCols
argument_list|(
name|tableColumns
argument_list|)
expr_stmt|;
name|client
operator|.
name|alter_table
argument_list|(
name|table
operator|.
name|getDbName
argument_list|()
argument_list|,
name|table
operator|.
name|getTableName
argument_list|()
argument_list|,
name|table
operator|.
name|getTTable
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * Move all of the files from the temp directory to the final location    * @param fs the output file system    * @param file the file to move    * @param srcDir the source directory    * @param destDir the target directory    * @param dryRun - a flag that simply tests if this move would succeed or not based    *                 on whether other files exist where we're trying to copy    * @throws java.io.IOException    */
specifier|private
name|void
name|moveTaskOutputs
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|Path
name|file
parameter_list|,
name|Path
name|srcDir
parameter_list|,
name|Path
name|destDir
parameter_list|,
specifier|final
name|boolean
name|dryRun
parameter_list|,
name|boolean
name|immutable
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"moveTaskOutputs "
operator|+
name|file
operator|+
literal|" from: "
operator|+
name|srcDir
operator|+
literal|" to: "
operator|+
name|destDir
operator|+
literal|" dry: "
operator|+
name|dryRun
operator|+
literal|" immutable: "
operator|+
name|immutable
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|dynamicPartitioningUsed
condition|)
block|{
name|immutable
operator|=
literal|true
expr_stmt|;
comment|// Making sure we treat dynamic partitioning jobs as if they were immutable.
block|}
if|if
condition|(
name|file
operator|.
name|getName
argument_list|()
operator|.
name|equals
argument_list|(
name|TEMP_DIR_NAME
argument_list|)
operator|||
name|file
operator|.
name|getName
argument_list|()
operator|.
name|equals
argument_list|(
name|LOGS_DIR_NAME
argument_list|)
operator|||
name|file
operator|.
name|getName
argument_list|()
operator|.
name|equals
argument_list|(
name|SUCCEEDED_FILE_NAME
argument_list|)
condition|)
block|{
return|return;
block|}
specifier|final
name|Path
name|finalOutputPath
init|=
name|getFinalPath
argument_list|(
name|fs
argument_list|,
name|file
argument_list|,
name|srcDir
argument_list|,
name|destDir
argument_list|,
name|immutable
argument_list|)
decl_stmt|;
name|FileStatus
name|fileStatus
init|=
name|fs
operator|.
name|getFileStatus
argument_list|(
name|file
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|fileStatus
operator|.
name|isDir
argument_list|()
condition|)
block|{
if|if
condition|(
name|dryRun
condition|)
block|{
if|if
condition|(
name|immutable
condition|)
block|{
comment|// Dryrun checks are meaningless for mutable table - we should always succeed
comment|// unless there is a runtime IOException.
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Testing if moving file: ["
operator|+
name|file
operator|+
literal|"] to ["
operator|+
name|finalOutputPath
operator|+
literal|"] would cause a problem"
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|fs
operator|.
name|exists
argument_list|(
name|finalOutputPath
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|HCatException
argument_list|(
name|ErrorType
operator|.
name|ERROR_MOVE_FAILED
argument_list|,
literal|"Data already exists in "
operator|+
name|finalOutputPath
operator|+
literal|", duplicate publish not possible."
argument_list|)
throw|;
block|}
block|}
block|}
else|else
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Moving file: [ "
operator|+
name|file
operator|+
literal|"] to ["
operator|+
name|finalOutputPath
operator|+
literal|"]"
argument_list|)
expr_stmt|;
block|}
comment|// Make sure the parent directory exists.  It is not an error
comment|// to recreate an existing directory
name|fs
operator|.
name|mkdirs
argument_list|(
name|finalOutputPath
operator|.
name|getParent
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|fs
operator|.
name|rename
argument_list|(
name|file
argument_list|,
name|finalOutputPath
argument_list|)
condition|)
block|{
if|if
condition|(
operator|!
name|fs
operator|.
name|delete
argument_list|(
name|finalOutputPath
argument_list|,
literal|true
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|HCatException
argument_list|(
name|ErrorType
operator|.
name|ERROR_MOVE_FAILED
argument_list|,
literal|"Failed to delete existing path "
operator|+
name|finalOutputPath
argument_list|)
throw|;
block|}
if|if
condition|(
operator|!
name|fs
operator|.
name|rename
argument_list|(
name|file
argument_list|,
name|finalOutputPath
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|HCatException
argument_list|(
name|ErrorType
operator|.
name|ERROR_MOVE_FAILED
argument_list|,
literal|"Failed to move output to "
operator|+
name|finalOutputPath
argument_list|)
throw|;
block|}
block|}
block|}
block|}
else|else
block|{
name|FileStatus
index|[]
name|children
init|=
name|fs
operator|.
name|listStatus
argument_list|(
name|file
argument_list|)
decl_stmt|;
name|FileStatus
name|firstChild
init|=
literal|null
decl_stmt|;
if|if
condition|(
name|children
operator|!=
literal|null
condition|)
block|{
name|int
name|index
init|=
literal|0
decl_stmt|;
while|while
condition|(
name|index
operator|<
name|children
operator|.
name|length
condition|)
block|{
if|if
condition|(
operator|!
name|children
index|[
name|index
index|]
operator|.
name|getPath
argument_list|()
operator|.
name|getName
argument_list|()
operator|.
name|equals
argument_list|(
name|TEMP_DIR_NAME
argument_list|)
operator|&&
operator|!
name|children
index|[
name|index
index|]
operator|.
name|getPath
argument_list|()
operator|.
name|getName
argument_list|()
operator|.
name|equals
argument_list|(
name|LOGS_DIR_NAME
argument_list|)
operator|&&
operator|!
name|children
index|[
name|index
index|]
operator|.
name|getPath
argument_list|()
operator|.
name|getName
argument_list|()
operator|.
name|equals
argument_list|(
name|SUCCEEDED_FILE_NAME
argument_list|)
condition|)
block|{
name|firstChild
operator|=
name|children
index|[
name|index
index|]
expr_stmt|;
break|break;
block|}
name|index
operator|++
expr_stmt|;
block|}
block|}
if|if
condition|(
name|firstChild
operator|!=
literal|null
operator|&&
name|firstChild
operator|.
name|isDir
argument_list|()
condition|)
block|{
comment|// If the first child is directory, then rest would be directory too according to HCatalog dir structure
comment|// recurse in that case
for|for
control|(
name|FileStatus
name|child
range|:
name|children
control|)
block|{
name|moveTaskOutputs
argument_list|(
name|fs
argument_list|,
name|child
operator|.
name|getPath
argument_list|()
argument_list|,
name|srcDir
argument_list|,
name|destDir
argument_list|,
name|dryRun
argument_list|,
name|immutable
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
if|if
condition|(
operator|!
name|dryRun
condition|)
block|{
if|if
condition|(
name|dynamicPartitioningUsed
condition|)
block|{
comment|// Optimization: if the first child is file, we have reached the leaf directory, move the parent directory itself
comment|// instead of moving each file under the directory. See HCATALOG-538
comment|// Note for future Append implementation : This optimization is another reason dynamic
comment|// partitioning is currently incompatible with append on mutable tables.
specifier|final
name|Path
name|parentDir
init|=
name|finalOutputPath
operator|.
name|getParent
argument_list|()
decl_stmt|;
comment|// Create the directory
name|Path
name|placeholder
init|=
operator|new
name|Path
argument_list|(
name|parentDir
argument_list|,
literal|"_placeholder"
operator|+
name|String
operator|.
name|valueOf
argument_list|(
name|Math
operator|.
name|random
argument_list|()
argument_list|)
argument_list|)
decl_stmt|;
if|if
condition|(
name|fs
operator|.
name|mkdirs
argument_list|(
name|parentDir
argument_list|)
condition|)
block|{
comment|// It is weird but we need a placeholder,
comment|// otherwise rename cannot move file to the right place
name|fs
operator|.
name|create
argument_list|(
name|placeholder
argument_list|)
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Moving directory: "
operator|+
name|file
operator|+
literal|" to "
operator|+
name|parentDir
argument_list|)
expr_stmt|;
block|}
comment|// If custom dynamic location provided, need to rename to final output path
name|Path
name|dstPath
init|=
operator|!
name|customDynamicLocationUsed
condition|?
name|parentDir
else|:
name|finalOutputPath
decl_stmt|;
if|if
condition|(
operator|!
name|fs
operator|.
name|rename
argument_list|(
name|file
argument_list|,
name|dstPath
argument_list|)
condition|)
block|{
specifier|final
name|String
name|msg
init|=
literal|"Failed to move file: "
operator|+
name|file
operator|+
literal|" to "
operator|+
name|dstPath
decl_stmt|;
name|LOG
operator|.
name|error
argument_list|(
name|msg
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HCatException
argument_list|(
name|ErrorType
operator|.
name|ERROR_MOVE_FAILED
argument_list|,
name|msg
argument_list|)
throw|;
block|}
name|fs
operator|.
name|delete
argument_list|(
name|placeholder
argument_list|,
literal|false
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// In case of no partition we have to move each file
for|for
control|(
name|FileStatus
name|child
range|:
name|children
control|)
block|{
name|moveTaskOutputs
argument_list|(
name|fs
argument_list|,
name|child
operator|.
name|getPath
argument_list|()
argument_list|,
name|srcDir
argument_list|,
name|destDir
argument_list|,
name|dryRun
argument_list|,
name|immutable
argument_list|)
expr_stmt|;
block|}
block|}
block|}
else|else
block|{
if|if
condition|(
name|immutable
operator|&&
name|fs
operator|.
name|exists
argument_list|(
name|finalOutputPath
argument_list|)
operator|&&
operator|!
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|utils
operator|.
name|FileUtils
operator|.
name|isDirEmpty
argument_list|(
name|fs
argument_list|,
name|finalOutputPath
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|HCatException
argument_list|(
name|ErrorType
operator|.
name|ERROR_DUPLICATE_PARTITION
argument_list|,
literal|"Data already exists in "
operator|+
name|finalOutputPath
operator|+
literal|", duplicate publish not possible."
argument_list|)
throw|;
block|}
block|}
block|}
block|}
block|}
comment|/**    * Find the final name of a given output file, given the output directory    * and the work directory. If immutable, attempt to create file of name    * _aN till we find an item that does not exist.    * @param file the file to move    * @param src the source directory    * @param dest the target directory    * @return the final path for the specific output file    * @throws java.io.IOException    */
specifier|private
name|Path
name|getFinalPath
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|Path
name|file
parameter_list|,
name|Path
name|src
parameter_list|,
name|Path
name|dest
parameter_list|,
specifier|final
name|boolean
name|immutable
parameter_list|)
throws|throws
name|IOException
block|{
name|URI
name|taskOutputUri
init|=
name|file
operator|.
name|toUri
argument_list|()
decl_stmt|;
name|URI
name|relativePath
init|=
name|src
operator|.
name|toUri
argument_list|()
operator|.
name|relativize
argument_list|(
name|taskOutputUri
argument_list|)
decl_stmt|;
if|if
condition|(
name|taskOutputUri
operator|==
name|relativePath
condition|)
block|{
throw|throw
operator|new
name|HCatException
argument_list|(
name|ErrorType
operator|.
name|ERROR_MOVE_FAILED
argument_list|,
literal|"Can not get the relative path: base = "
operator|+
name|src
operator|+
literal|" child = "
operator|+
name|file
argument_list|)
throw|;
block|}
if|if
condition|(
name|relativePath
operator|.
name|getPath
argument_list|()
operator|.
name|length
argument_list|()
operator|>
literal|0
condition|)
block|{
name|Path
name|itemDest
init|=
operator|new
name|Path
argument_list|(
name|dest
argument_list|,
name|relativePath
operator|.
name|getPath
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|immutable
condition|)
block|{
name|String
name|name
init|=
name|relativePath
operator|.
name|getPath
argument_list|()
decl_stmt|;
name|String
name|filetype
decl_stmt|;
name|int
name|index
init|=
name|name
operator|.
name|lastIndexOf
argument_list|(
literal|'.'
argument_list|)
decl_stmt|;
if|if
condition|(
name|index
operator|>=
literal|0
condition|)
block|{
name|filetype
operator|=
name|name
operator|.
name|substring
argument_list|(
name|index
argument_list|)
expr_stmt|;
name|name
operator|=
name|name
operator|.
name|substring
argument_list|(
literal|0
argument_list|,
name|index
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|filetype
operator|=
literal|""
expr_stmt|;
block|}
comment|// Attempt to find maxAppendAttempts possible alternatives to a filename by
comment|// appending _a_N and seeing if that destination also clashes. If we're
comment|// still clashing after that, give up.
name|int
name|counter
init|=
literal|1
decl_stmt|;
for|for
control|(
init|;
name|fs
operator|.
name|exists
argument_list|(
name|itemDest
argument_list|)
operator|&&
name|counter
operator|<
name|maxAppendAttempts
condition|;
name|counter
operator|++
control|)
block|{
name|itemDest
operator|=
operator|new
name|Path
argument_list|(
name|dest
argument_list|,
name|name
operator|+
operator|(
name|APPEND_SUFFIX
operator|+
name|counter
operator|)
operator|+
name|filetype
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|counter
operator|==
name|maxAppendAttempts
condition|)
block|{
throw|throw
operator|new
name|HCatException
argument_list|(
name|ErrorType
operator|.
name|ERROR_MOVE_FAILED
argument_list|,
literal|"Could not find a unique destination path for move: file = "
operator|+
name|file
operator|+
literal|" , src = "
operator|+
name|src
operator|+
literal|", dest = "
operator|+
name|dest
argument_list|)
throw|;
block|}
elseif|else
if|if
condition|(
name|counter
operator|>
name|APPEND_COUNTER_WARN_THRESHOLD
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Append job used filename clash counter ["
operator|+
name|counter
operator|+
literal|"] which is greater than warning limit ["
operator|+
name|APPEND_COUNTER_WARN_THRESHOLD
operator|+
literal|"]. Please compact this table so that performance is not impacted."
operator|+
literal|" Please see HIVE-9381 for details."
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"FinalPath(file:"
operator|+
name|file
operator|+
literal|":"
operator|+
name|src
operator|+
literal|"->"
operator|+
name|dest
operator|+
literal|"="
operator|+
name|itemDest
argument_list|)
expr_stmt|;
block|}
return|return
name|itemDest
return|;
block|}
else|else
block|{
return|return
name|dest
return|;
block|}
block|}
comment|/**    * Run to discover dynamic partitions available    */
specifier|private
name|void
name|discoverPartitions
parameter_list|(
name|JobContext
name|context
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
operator|!
name|partitionsDiscovered
condition|)
block|{
comment|//      LOG.info("discover ptns called");
name|OutputJobInfo
name|jobInfo
init|=
name|HCatOutputFormat
operator|.
name|getJobInfo
argument_list|(
name|context
operator|.
name|getConfiguration
argument_list|()
argument_list|)
decl_stmt|;
name|harProcessor
operator|.
name|setEnabled
argument_list|(
name|jobInfo
operator|.
name|getHarRequested
argument_list|()
argument_list|)
expr_stmt|;
name|List
argument_list|<
name|Integer
argument_list|>
name|dynamicPartCols
init|=
name|jobInfo
operator|.
name|getPosOfDynPartCols
argument_list|()
decl_stmt|;
name|int
name|maxDynamicPartitions
init|=
name|jobInfo
operator|.
name|getMaxDynamicPartitions
argument_list|()
decl_stmt|;
name|Path
name|loadPath
init|=
operator|new
name|Path
argument_list|(
name|jobInfo
operator|.
name|getLocation
argument_list|()
argument_list|)
decl_stmt|;
name|FileSystem
name|fs
init|=
name|loadPath
operator|.
name|getFileSystem
argument_list|(
name|context
operator|.
name|getConfiguration
argument_list|()
argument_list|)
decl_stmt|;
comment|// construct a path pattern (e.g., /*/*) to find all dynamically generated paths
name|String
name|dynPathSpec
init|=
name|loadPath
operator|.
name|toUri
argument_list|()
operator|.
name|getPath
argument_list|()
decl_stmt|;
name|dynPathSpec
operator|=
name|dynPathSpec
operator|.
name|replaceAll
argument_list|(
literal|"__HIVE_DEFAULT_PARTITION__"
argument_list|,
literal|"*"
argument_list|)
expr_stmt|;
comment|//      LOG.info("Searching for "+dynPathSpec);
name|Path
name|pathPattern
init|=
operator|new
name|Path
argument_list|(
name|dynPathSpec
argument_list|)
decl_stmt|;
name|FileStatus
index|[]
name|status
init|=
name|fs
operator|.
name|globStatus
argument_list|(
name|pathPattern
argument_list|,
name|FileUtils
operator|.
name|HIDDEN_FILES_PATH_FILTER
argument_list|)
decl_stmt|;
name|partitionsDiscoveredByPath
operator|=
operator|new
name|LinkedHashMap
argument_list|<
name|String
argument_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
argument_list|>
argument_list|()
expr_stmt|;
name|contextDiscoveredByPath
operator|=
operator|new
name|LinkedHashMap
argument_list|<
name|String
argument_list|,
name|JobContext
argument_list|>
argument_list|()
expr_stmt|;
if|if
condition|(
name|status
operator|.
name|length
operator|==
literal|0
condition|)
block|{
comment|//        LOG.warn("No partition found genereated by dynamic partitioning in ["
comment|//            +loadPath+"] with depth["+jobInfo.getTable().getPartitionKeysSize()
comment|//            +"], dynSpec["+dynPathSpec+"]");
block|}
else|else
block|{
if|if
condition|(
operator|(
name|maxDynamicPartitions
operator|!=
operator|-
literal|1
operator|)
operator|&&
operator|(
name|status
operator|.
name|length
operator|>
name|maxDynamicPartitions
operator|)
condition|)
block|{
name|this
operator|.
name|partitionsDiscovered
operator|=
literal|true
expr_stmt|;
throw|throw
operator|new
name|HCatException
argument_list|(
name|ErrorType
operator|.
name|ERROR_TOO_MANY_DYNAMIC_PTNS
argument_list|,
literal|"Number of dynamic partitions being created "
operator|+
literal|"exceeds configured max allowable partitions["
operator|+
name|maxDynamicPartitions
operator|+
literal|"], increase parameter ["
operator|+
name|HiveConf
operator|.
name|ConfVars
operator|.
name|DYNAMICPARTITIONMAXPARTS
operator|.
name|varname
operator|+
literal|"] if needed."
argument_list|)
throw|;
block|}
for|for
control|(
name|FileStatus
name|st
range|:
name|status
control|)
block|{
name|LinkedHashMap
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|fullPartSpec
init|=
operator|new
name|LinkedHashMap
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
argument_list|()
decl_stmt|;
if|if
condition|(
operator|!
name|customDynamicLocationUsed
condition|)
block|{
name|Warehouse
operator|.
name|makeSpecFromName
argument_list|(
name|fullPartSpec
argument_list|,
name|st
operator|.
name|getPath
argument_list|()
argument_list|,
literal|null
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|HCatFileUtil
operator|.
name|getPartKeyValuesForCustomLocation
argument_list|(
name|fullPartSpec
argument_list|,
name|jobInfo
argument_list|,
name|st
operator|.
name|getPath
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|partitionsDiscoveredByPath
operator|.
name|put
argument_list|(
name|st
operator|.
name|getPath
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|,
name|fullPartSpec
argument_list|)
expr_stmt|;
name|JobConf
name|jobConf
init|=
operator|(
name|JobConf
operator|)
name|context
operator|.
name|getConfiguration
argument_list|()
decl_stmt|;
name|JobContext
name|currContext
init|=
name|HCatMapRedUtil
operator|.
name|createJobContext
argument_list|(
name|jobConf
argument_list|,
name|context
operator|.
name|getJobID
argument_list|()
argument_list|,
name|InternalUtil
operator|.
name|createReporter
argument_list|(
name|HCatMapRedUtil
operator|.
name|createTaskAttemptContext
argument_list|(
name|jobConf
argument_list|,
name|ShimLoader
operator|.
name|getHadoopShims
argument_list|()
operator|.
name|getHCatShim
argument_list|()
operator|.
name|createTaskAttemptID
argument_list|()
argument_list|)
argument_list|)
argument_list|)
decl_stmt|;
name|HCatOutputFormat
operator|.
name|configureOutputStorageHandler
argument_list|(
name|currContext
argument_list|,
name|jobInfo
argument_list|,
name|fullPartSpec
argument_list|)
expr_stmt|;
name|contextDiscoveredByPath
operator|.
name|put
argument_list|(
name|st
operator|.
name|getPath
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|,
name|currContext
argument_list|)
expr_stmt|;
block|}
block|}
comment|//      for (Entry<String,Map<String,String>> spec : partitionsDiscoveredByPath.entrySet()){
comment|//        LOG.info("Partition "+ spec.getKey());
comment|//        for (Entry<String,String> e : spec.getValue().entrySet()){
comment|//          LOG.info(e.getKey() + "=>" +e.getValue());
comment|//        }
comment|//      }
name|this
operator|.
name|partitionsDiscovered
operator|=
literal|true
expr_stmt|;
block|}
block|}
specifier|private
name|void
name|registerPartitions
parameter_list|(
name|JobContext
name|context
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|dynamicPartitioningUsed
condition|)
block|{
name|discoverPartitions
argument_list|(
name|context
argument_list|)
expr_stmt|;
block|}
name|OutputJobInfo
name|jobInfo
init|=
name|HCatOutputFormat
operator|.
name|getJobInfo
argument_list|(
name|context
operator|.
name|getConfiguration
argument_list|()
argument_list|)
decl_stmt|;
name|Configuration
name|conf
init|=
name|context
operator|.
name|getConfiguration
argument_list|()
decl_stmt|;
name|Table
name|table
init|=
operator|new
name|Table
argument_list|(
name|jobInfo
operator|.
name|getTableInfo
argument_list|()
operator|.
name|getTable
argument_list|()
argument_list|)
decl_stmt|;
name|Path
name|tblPath
init|=
operator|new
name|Path
argument_list|(
name|table
operator|.
name|getTTable
argument_list|()
operator|.
name|getSd
argument_list|()
operator|.
name|getLocation
argument_list|()
argument_list|)
decl_stmt|;
name|FileSystem
name|fs
init|=
name|tblPath
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|IMetaStoreClient
name|client
init|=
literal|null
decl_stmt|;
name|HCatTableInfo
name|tableInfo
init|=
name|jobInfo
operator|.
name|getTableInfo
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|Partition
argument_list|>
name|partitionsAdded
init|=
operator|new
name|ArrayList
argument_list|<
name|Partition
argument_list|>
argument_list|()
decl_stmt|;
try|try
block|{
name|HiveConf
name|hiveConf
init|=
name|HCatUtil
operator|.
name|getHiveConf
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|client
operator|=
name|HCatUtil
operator|.
name|getHiveMetastoreClient
argument_list|(
name|hiveConf
argument_list|)
expr_stmt|;
if|if
condition|(
name|table
operator|.
name|getPartitionKeys
argument_list|()
operator|.
name|size
argument_list|()
operator|==
literal|0
condition|)
block|{
comment|// Move data from temp directory the actual table directory
comment|// No metastore operation required.
name|Path
name|src
init|=
operator|new
name|Path
argument_list|(
name|jobInfo
operator|.
name|getLocation
argument_list|()
argument_list|)
decl_stmt|;
name|moveTaskOutputs
argument_list|(
name|fs
argument_list|,
name|src
argument_list|,
name|src
argument_list|,
name|tblPath
argument_list|,
literal|false
argument_list|,
name|table
operator|.
name|isImmutable
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|src
operator|.
name|equals
argument_list|(
name|tblPath
argument_list|)
condition|)
block|{
name|fs
operator|.
name|delete
argument_list|(
name|src
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|table
operator|.
name|getParameters
argument_list|()
operator|!=
literal|null
operator|&&
name|table
operator|.
name|getParameters
argument_list|()
operator|.
name|containsKey
argument_list|(
name|StatsSetupConst
operator|.
name|COLUMN_STATS_ACCURATE
argument_list|)
condition|)
block|{
name|table
operator|.
name|getParameters
argument_list|()
operator|.
name|remove
argument_list|(
name|StatsSetupConst
operator|.
name|COLUMN_STATS_ACCURATE
argument_list|)
expr_stmt|;
name|client
operator|.
name|alter_table
argument_list|(
name|table
operator|.
name|getDbName
argument_list|()
argument_list|,
name|table
operator|.
name|getTableName
argument_list|()
argument_list|,
name|table
operator|.
name|getTTable
argument_list|()
argument_list|)
expr_stmt|;
block|}
return|return;
block|}
name|StorerInfo
name|storer
init|=
name|InternalUtil
operator|.
name|extractStorerInfo
argument_list|(
name|table
operator|.
name|getTTable
argument_list|()
operator|.
name|getSd
argument_list|()
argument_list|,
name|table
operator|.
name|getParameters
argument_list|()
argument_list|)
decl_stmt|;
name|FileStatus
name|tblStat
init|=
name|fs
operator|.
name|getFileStatus
argument_list|(
name|tblPath
argument_list|)
decl_stmt|;
name|String
name|grpName
init|=
name|tblStat
operator|.
name|getGroup
argument_list|()
decl_stmt|;
name|FsPermission
name|perms
init|=
name|tblStat
operator|.
name|getPermission
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|Partition
argument_list|>
name|partitionsToAdd
init|=
operator|new
name|ArrayList
argument_list|<
name|Partition
argument_list|>
argument_list|()
decl_stmt|;
if|if
condition|(
operator|!
name|dynamicPartitioningUsed
condition|)
block|{
name|partitionsToAdd
operator|.
name|add
argument_list|(
name|constructPartition
argument_list|(
name|context
argument_list|,
name|jobInfo
argument_list|,
name|tblPath
operator|.
name|toString
argument_list|()
argument_list|,
literal|null
argument_list|,
name|jobInfo
operator|.
name|getPartitionValues
argument_list|()
argument_list|,
name|jobInfo
operator|.
name|getOutputSchema
argument_list|()
argument_list|,
name|getStorerParameterMap
argument_list|(
name|storer
argument_list|)
argument_list|,
name|table
argument_list|,
name|fs
argument_list|,
name|grpName
argument_list|,
name|perms
argument_list|)
argument_list|)
expr_stmt|;
block|}
else|else
block|{
for|for
control|(
name|Entry
argument_list|<
name|String
argument_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
argument_list|>
name|entry
range|:
name|partitionsDiscoveredByPath
operator|.
name|entrySet
argument_list|()
control|)
block|{
name|partitionsToAdd
operator|.
name|add
argument_list|(
name|constructPartition
argument_list|(
name|context
argument_list|,
name|jobInfo
argument_list|,
name|getPartitionRootLocation
argument_list|(
name|entry
operator|.
name|getKey
argument_list|()
argument_list|,
name|entry
operator|.
name|getValue
argument_list|()
operator|.
name|size
argument_list|()
argument_list|)
argument_list|,
name|entry
operator|.
name|getKey
argument_list|()
argument_list|,
name|entry
operator|.
name|getValue
argument_list|()
argument_list|,
name|jobInfo
operator|.
name|getOutputSchema
argument_list|()
argument_list|,
name|getStorerParameterMap
argument_list|(
name|storer
argument_list|)
argument_list|,
name|table
argument_list|,
name|fs
argument_list|,
name|grpName
argument_list|,
name|perms
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
name|ArrayList
argument_list|<
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
argument_list|>
name|ptnInfos
init|=
operator|new
name|ArrayList
argument_list|<
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|Partition
name|ptn
range|:
name|partitionsToAdd
control|)
block|{
name|ptnInfos
operator|.
name|add
argument_list|(
name|InternalUtil
operator|.
name|createPtnKeyValueMap
argument_list|(
operator|new
name|Table
argument_list|(
name|tableInfo
operator|.
name|getTable
argument_list|()
argument_list|)
argument_list|,
name|ptn
argument_list|)
argument_list|)
expr_stmt|;
block|}
comment|/**        * Dynamic partitioning& Append incompatibility note:        *        * Currently, we do not support mixing dynamic partitioning and append in the        * same job. One reason is that we need exhaustive testing of corner cases        * for that, and a second reason is the behaviour of add_partitions. To support        * dynamic partitioning with append, we'd have to have a add_partitions_if_not_exist        * call, rather than an add_partitions call. Thus far, we've tried to keep the        * implementation of append jobtype-agnostic, but here, in code, we assume that        * a table is considered immutable if dynamic partitioning is enabled on the job.        *        * This does not mean that we can check before the job begins that this is going        * to be a dynamic partition job on an immutable table and thus fail the job, since        * it is quite possible to have a dynamic partitioning job run on an unpopulated        * immutable table. It simply means that at the end of the job, as far as copying        * in data is concerned, we will pretend that the table is immutable irrespective        * of what table.isImmutable() tells us.        */
comment|//Publish the new partition(s)
if|if
condition|(
name|dynamicPartitioningUsed
operator|&&
name|harProcessor
operator|.
name|isEnabled
argument_list|()
operator|&&
operator|(
operator|!
name|partitionsToAdd
operator|.
name|isEmpty
argument_list|()
operator|)
condition|)
block|{
if|if
condition|(
operator|!
name|customDynamicLocationUsed
condition|)
block|{
name|Path
name|src
init|=
operator|new
name|Path
argument_list|(
name|ptnRootLocation
argument_list|)
decl_stmt|;
comment|// check here for each dir we're copying out, to see if it
comment|// already exists, error out if so.
comment|// Also, treat dyn-writes as writes to immutable tables.
name|moveTaskOutputs
argument_list|(
name|fs
argument_list|,
name|src
argument_list|,
name|src
argument_list|,
name|tblPath
argument_list|,
literal|true
argument_list|,
literal|true
argument_list|)
expr_stmt|;
comment|// dryRun = true, immutable = true
name|moveTaskOutputs
argument_list|(
name|fs
argument_list|,
name|src
argument_list|,
name|src
argument_list|,
name|tblPath
argument_list|,
literal|false
argument_list|,
literal|true
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|src
operator|.
name|equals
argument_list|(
name|tblPath
argument_list|)
condition|)
block|{
name|fs
operator|.
name|delete
argument_list|(
name|src
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
name|moveCustomLocationTaskOutputs
argument_list|(
name|fs
argument_list|,
name|table
argument_list|,
name|hiveConf
argument_list|)
expr_stmt|;
block|}
try|try
block|{
name|updateTableSchema
argument_list|(
name|client
argument_list|,
name|table
argument_list|,
name|jobInfo
operator|.
name|getOutputSchema
argument_list|()
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"HAR is being used. The table {} has new partitions {}."
argument_list|,
name|table
operator|.
name|getTableName
argument_list|()
argument_list|,
name|ptnInfos
argument_list|)
expr_stmt|;
name|client
operator|.
name|add_partitions
argument_list|(
name|partitionsToAdd
argument_list|)
expr_stmt|;
name|partitionsAdded
operator|=
name|partitionsToAdd
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
comment|// There was an error adding partitions : rollback fs copy and rethrow
for|for
control|(
name|Partition
name|p
range|:
name|partitionsToAdd
control|)
block|{
name|Path
name|ptnPath
init|=
operator|new
name|Path
argument_list|(
name|harProcessor
operator|.
name|getParentFSPath
argument_list|(
operator|new
name|Path
argument_list|(
name|p
operator|.
name|getSd
argument_list|()
operator|.
name|getLocation
argument_list|()
argument_list|)
argument_list|)
argument_list|)
decl_stmt|;
if|if
condition|(
name|fs
operator|.
name|exists
argument_list|(
name|ptnPath
argument_list|)
condition|)
block|{
name|fs
operator|.
name|delete
argument_list|(
name|ptnPath
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
block|}
throw|throw
name|e
throw|;
block|}
block|}
else|else
block|{
comment|// no harProcessor, regular operation
name|updateTableSchema
argument_list|(
name|client
argument_list|,
name|table
argument_list|,
name|jobInfo
operator|.
name|getOutputSchema
argument_list|()
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"HAR not is not being used. The table {} has new partitions {}."
argument_list|,
name|table
operator|.
name|getTableName
argument_list|()
argument_list|,
name|ptnInfos
argument_list|)
expr_stmt|;
if|if
condition|(
name|partitionsToAdd
operator|.
name|size
argument_list|()
operator|>
literal|0
condition|)
block|{
if|if
condition|(
operator|!
name|dynamicPartitioningUsed
condition|)
block|{
comment|// regular single-partition write into a partitioned table.
comment|//Move data from temp directory the actual table directory
if|if
condition|(
name|partitionsToAdd
operator|.
name|size
argument_list|()
operator|>
literal|1
condition|)
block|{
throw|throw
operator|new
name|HCatException
argument_list|(
name|ErrorType
operator|.
name|ERROR_PUBLISHING_PARTITION
argument_list|,
literal|"More than one partition to publish in non-dynamic partitioning job"
argument_list|)
throw|;
block|}
name|Partition
name|p
init|=
name|partitionsToAdd
operator|.
name|get
argument_list|(
literal|0
argument_list|)
decl_stmt|;
name|Path
name|src
init|=
operator|new
name|Path
argument_list|(
name|jobInfo
operator|.
name|getLocation
argument_list|()
argument_list|)
decl_stmt|;
name|Path
name|dest
init|=
operator|new
name|Path
argument_list|(
name|p
operator|.
name|getSd
argument_list|()
operator|.
name|getLocation
argument_list|()
argument_list|)
decl_stmt|;
name|moveTaskOutputs
argument_list|(
name|fs
argument_list|,
name|src
argument_list|,
name|src
argument_list|,
name|dest
argument_list|,
literal|true
argument_list|,
name|table
operator|.
name|isImmutable
argument_list|()
argument_list|)
expr_stmt|;
name|moveTaskOutputs
argument_list|(
name|fs
argument_list|,
name|src
argument_list|,
name|src
argument_list|,
name|dest
argument_list|,
literal|false
argument_list|,
name|table
operator|.
name|isImmutable
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|src
operator|.
name|equals
argument_list|(
name|dest
argument_list|)
condition|)
block|{
if|if
condition|(
name|src
operator|.
name|toString
argument_list|()
operator|.
name|matches
argument_list|(
literal|".*"
operator|+
name|Path
operator|.
name|SEPARATOR
operator|+
name|SCRATCH_DIR_NAME
operator|+
literal|"\\d\\.?\\d+.*"
argument_list|)
condition|)
block|{
comment|// src is scratch directory, need to trim the part key value pairs from path
name|String
name|diff
init|=
name|StringUtils
operator|.
name|difference
argument_list|(
name|src
operator|.
name|toString
argument_list|()
argument_list|,
name|dest
operator|.
name|toString
argument_list|()
argument_list|)
decl_stmt|;
name|fs
operator|.
name|delete
argument_list|(
operator|new
name|Path
argument_list|(
name|StringUtils
operator|.
name|substringBefore
argument_list|(
name|src
operator|.
name|toString
argument_list|()
argument_list|,
name|diff
argument_list|)
argument_list|)
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|fs
operator|.
name|delete
argument_list|(
name|src
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
block|}
comment|// Now, we check if the partition already exists. If not, we go ahead.
comment|// If so, we error out if immutable, and if mutable, check that the partition's IF
comment|// matches our current job's IF (table's IF) to check for compatibility. If compatible, we
comment|// ignore and do not add. If incompatible, we error out again.
name|boolean
name|publishRequired
init|=
literal|false
decl_stmt|;
try|try
block|{
name|Partition
name|existingP
init|=
name|client
operator|.
name|getPartition
argument_list|(
name|p
operator|.
name|getDbName
argument_list|()
argument_list|,
name|p
operator|.
name|getTableName
argument_list|()
argument_list|,
name|p
operator|.
name|getValues
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|existingP
operator|!=
literal|null
condition|)
block|{
if|if
condition|(
name|table
operator|.
name|isImmutable
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|HCatException
argument_list|(
name|ErrorType
operator|.
name|ERROR_DUPLICATE_PARTITION
argument_list|,
literal|"Attempted duplicate partition publish on to immutable table"
argument_list|)
throw|;
block|}
else|else
block|{
if|if
condition|(
operator|!
name|existingP
operator|.
name|getSd
argument_list|()
operator|.
name|getInputFormat
argument_list|()
operator|.
name|equals
argument_list|(
name|table
operator|.
name|getInputFormatClass
argument_list|()
operator|.
name|getName
argument_list|()
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|HCatException
argument_list|(
name|ErrorType
operator|.
name|ERROR_PUBLISHING_PARTITION
argument_list|,
literal|"Attempted partition append, where old partition format was "
operator|+
name|existingP
operator|.
name|getSd
argument_list|()
operator|.
name|getInputFormat
argument_list|()
operator|+
literal|" and table format was "
operator|+
name|table
operator|.
name|getInputFormatClass
argument_list|()
operator|.
name|getName
argument_list|()
argument_list|)
throw|;
block|}
block|}
block|}
else|else
block|{
name|publishRequired
operator|=
literal|true
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|NoSuchObjectException
name|e
parameter_list|)
block|{
comment|// All good, no such partition exists, move on.
name|publishRequired
operator|=
literal|true
expr_stmt|;
block|}
if|if
condition|(
name|publishRequired
condition|)
block|{
name|client
operator|.
name|add_partitions
argument_list|(
name|partitionsToAdd
argument_list|)
expr_stmt|;
name|partitionsAdded
operator|=
name|partitionsToAdd
expr_stmt|;
block|}
block|}
else|else
block|{
comment|// Dynamic partitioning usecase
if|if
condition|(
operator|!
name|customDynamicLocationUsed
condition|)
block|{
name|Path
name|src
init|=
operator|new
name|Path
argument_list|(
name|ptnRootLocation
argument_list|)
decl_stmt|;
name|moveTaskOutputs
argument_list|(
name|fs
argument_list|,
name|src
argument_list|,
name|src
argument_list|,
name|tblPath
argument_list|,
literal|true
argument_list|,
literal|true
argument_list|)
expr_stmt|;
comment|// dryRun = true, immutable = true
name|moveTaskOutputs
argument_list|(
name|fs
argument_list|,
name|src
argument_list|,
name|src
argument_list|,
name|tblPath
argument_list|,
literal|false
argument_list|,
literal|true
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|src
operator|.
name|equals
argument_list|(
name|tblPath
argument_list|)
condition|)
block|{
name|fs
operator|.
name|delete
argument_list|(
name|src
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
name|moveCustomLocationTaskOutputs
argument_list|(
name|fs
argument_list|,
name|table
argument_list|,
name|hiveConf
argument_list|)
expr_stmt|;
block|}
name|client
operator|.
name|add_partitions
argument_list|(
name|partitionsToAdd
argument_list|)
expr_stmt|;
name|partitionsAdded
operator|=
name|partitionsToAdd
expr_stmt|;
block|}
block|}
comment|// Set permissions appropriately for each of the partitions we just created
comment|// so as to have their permissions mimic the table permissions
for|for
control|(
name|Partition
name|p
range|:
name|partitionsAdded
control|)
block|{
name|applyGroupAndPerms
argument_list|(
name|fs
argument_list|,
operator|new
name|Path
argument_list|(
name|p
operator|.
name|getSd
argument_list|()
operator|.
name|getLocation
argument_list|()
argument_list|)
argument_list|,
name|tblStat
operator|.
name|getPermission
argument_list|()
argument_list|,
name|tblStat
operator|.
name|getGroup
argument_list|()
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
if|if
condition|(
name|partitionsAdded
operator|.
name|size
argument_list|()
operator|>
literal|0
condition|)
block|{
try|try
block|{
comment|// baseCommitter.cleanupJob failed, try to clean up the
comment|// metastore
for|for
control|(
name|Partition
name|p
range|:
name|partitionsAdded
control|)
block|{
name|client
operator|.
name|dropPartition
argument_list|(
name|tableInfo
operator|.
name|getDatabaseName
argument_list|()
argument_list|,
name|tableInfo
operator|.
name|getTableName
argument_list|()
argument_list|,
name|p
operator|.
name|getValues
argument_list|()
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|te
parameter_list|)
block|{
comment|// Keep cause as the original exception
throw|throw
operator|new
name|HCatException
argument_list|(
name|ErrorType
operator|.
name|ERROR_PUBLISHING_PARTITION
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
if|if
condition|(
name|e
operator|instanceof
name|HCatException
condition|)
block|{
throw|throw
operator|(
name|HCatException
operator|)
name|e
throw|;
block|}
else|else
block|{
throw|throw
operator|new
name|HCatException
argument_list|(
name|ErrorType
operator|.
name|ERROR_PUBLISHING_PARTITION
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
finally|finally
block|{
name|HCatUtil
operator|.
name|closeHiveClientQuietly
argument_list|(
name|client
argument_list|)
expr_stmt|;
block|}
block|}
specifier|private
name|void
name|moveCustomLocationTaskOutputs
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|Table
name|table
parameter_list|,
name|Configuration
name|conf
parameter_list|)
throws|throws
name|IOException
block|{
comment|// in case of custom dynamic partitions, we can't just move the sub-tree of partition root
comment|// directory since the partitions location contain regex pattern. We need to first find the
comment|// final destination of each partition and move its output.
for|for
control|(
name|Entry
argument_list|<
name|String
argument_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
argument_list|>
name|entry
range|:
name|partitionsDiscoveredByPath
operator|.
name|entrySet
argument_list|()
control|)
block|{
name|Path
name|src
init|=
operator|new
name|Path
argument_list|(
name|entry
operator|.
name|getKey
argument_list|()
argument_list|)
decl_stmt|;
name|Path
name|destPath
init|=
operator|new
name|Path
argument_list|(
name|getFinalDynamicPartitionDestination
argument_list|(
name|table
argument_list|,
name|entry
operator|.
name|getValue
argument_list|()
argument_list|,
name|jobInfo
argument_list|)
argument_list|)
decl_stmt|;
name|moveTaskOutputs
argument_list|(
name|fs
argument_list|,
name|src
argument_list|,
name|src
argument_list|,
name|destPath
argument_list|,
literal|true
argument_list|,
literal|true
argument_list|)
expr_stmt|;
comment|// dryRun = true, immutable = true
name|moveTaskOutputs
argument_list|(
name|fs
argument_list|,
name|src
argument_list|,
name|src
argument_list|,
name|destPath
argument_list|,
literal|false
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
comment|// delete the parent temp directory of all custom dynamic partitions
name|Path
name|parentPath
init|=
operator|new
name|Path
argument_list|(
name|getCustomPartitionRootLocation
argument_list|(
name|jobInfo
argument_list|,
name|conf
argument_list|)
argument_list|)
decl_stmt|;
if|if
condition|(
name|fs
operator|.
name|exists
argument_list|(
name|parentPath
argument_list|)
condition|)
block|{
name|fs
operator|.
name|delete
argument_list|(
name|parentPath
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
block|}
specifier|private
name|void
name|cancelDelegationTokens
parameter_list|(
name|JobContext
name|context
parameter_list|)
throws|throws
name|IOException
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Cancelling delegation token for the job."
argument_list|)
expr_stmt|;
name|IMetaStoreClient
name|client
init|=
literal|null
decl_stmt|;
try|try
block|{
name|HiveConf
name|hiveConf
init|=
name|HCatUtil
operator|.
name|getHiveConf
argument_list|(
name|context
operator|.
name|getConfiguration
argument_list|()
argument_list|)
decl_stmt|;
name|client
operator|=
name|HCatUtil
operator|.
name|getHiveMetastoreClient
argument_list|(
name|hiveConf
argument_list|)
expr_stmt|;
comment|// cancel the deleg. tokens that were acquired for this job now that
comment|// we are done - we should cancel if the tokens were acquired by
comment|// HCatOutputFormat and not if they were supplied by Oozie.
comment|// In the latter case the HCAT_KEY_TOKEN_SIGNATURE property in
comment|// the conf will not be set
name|String
name|tokenStrForm
init|=
name|client
operator|.
name|getTokenStrForm
argument_list|()
decl_stmt|;
name|String
name|hCatKeyTokenSignature
init|=
name|context
operator|.
name|getConfiguration
argument_list|()
operator|.
name|get
argument_list|(
name|HCatConstants
operator|.
name|HCAT_KEY_TOKEN_SIGNATURE
argument_list|)
decl_stmt|;
if|if
condition|(
name|tokenStrForm
operator|!=
literal|null
operator|&&
name|hCatKeyTokenSignature
operator|!=
literal|null
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"FileOutputCommitterContainer::cancelDelegationTokens(): "
operator|+
literal|"Cancelling token fetched for HCAT_KEY_TOKEN_SIGNATURE == ("
operator|+
name|hCatKeyTokenSignature
operator|+
literal|")."
argument_list|)
expr_stmt|;
name|client
operator|.
name|cancelDelegationToken
argument_list|(
name|tokenStrForm
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"FileOutputCommitterContainer::cancelDelegationTokens(): "
operator|+
literal|"Could not find tokenStrForm, or HCAT_KEY_TOKEN_SIGNATURE. Skipping token cancellation."
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|MetaException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"MetaException while cancelling delegation token."
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|TException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"TException while cancelling delegation token."
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
name|HCatUtil
operator|.
name|closeHiveClientQuietly
argument_list|(
name|client
argument_list|)
expr_stmt|;
block|}
block|}
block|}
end_class

end_unit

