begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/*  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hive
operator|.
name|streaming
package|;
end_package

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|InputStream
import|;
end_import

begin_import
import|import
name|java
operator|.
name|net
operator|.
name|InetAddress
import|;
end_import

begin_import
import|import
name|java
operator|.
name|net
operator|.
name|UnknownHostException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashSet
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Set
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|UUID
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|atomic
operator|.
name|AtomicBoolean
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|classification
operator|.
name|InterfaceStability
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FSDataOutputStream
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|StreamCapabilities
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|BlobStorageUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|HiveMetaStoreUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|IMetaStoreClient
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|Warehouse
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|AlreadyExistsException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|MetaException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|TxnToWriteId
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|conf
operator|.
name|MetastoreConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|AcidUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|lockmgr
operator|.
name|DbTxnManager
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|Hive
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|HiveException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|Table
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|Utilities
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|security
operator|.
name|UserGroupInformation
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hive
operator|.
name|common
operator|.
name|util
operator|.
name|ShutdownHookManager
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|thrift
operator|.
name|TException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|Logger
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|LoggerFactory
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|annotations
operator|.
name|VisibleForTesting
import|;
end_import

begin_comment
comment|/**  * Streaming connection implementation for hive. To create a streaming connection, use the builder API  * to create record writer first followed by the connection itself. Once connection is created, clients can  * begin a transaction, keep writing using the connection, commit the transaction and close connection when done.  * To bind to the correct metastore, HiveConf object has to be created from hive-site.xml or HIVE_CONF_DIR.  * If hive conf is manually created, metastore uri has to be set correctly. If hive conf object is not specified,  * "thrift://localhost:9083" will be used as default.  *<br><br>  * NOTE: The streaming connection APIs and record writer APIs are not thread-safe. Streaming connection creation,  * begin/commit/abort transactions, write and close has to be called in the same thread. If close() or  * abortTransaction() has to be triggered from a separate thread it has to be co-ordinated via external variables or  * synchronization mechanism  *<br><br>  * Example usage:  *<pre>{@code  * // create delimited record writer whose schema exactly matches table schema  * StrictDelimitedInputWriter writer = StrictDelimitedInputWriter.newBuilder()  *                                      .withFieldDelimiter(',')  *                                      .build();  * // create and open streaming connection (default.src table has to exist already)  * StreamingConnection connection = HiveStreamingConnection.newBuilder()  *                                    .withDatabase("default")  *                                    .withTable("src")  *                                    .withAgentInfo("nifi-agent")  *                                    .withRecordWriter(writer)  *                                    .withHiveConf(hiveConf)  *                                    .connect();  * // begin a transaction, write records and commit 1st transaction  * connection.beginTransaction();  * connection.write("key1,val1".getBytes());  * connection.write("key2,val2".getBytes());  * connection.commitTransaction();  * // begin another transaction, write more records and commit 2nd transaction  * connection.beginTransaction();  * connection.write("key3,val3".getBytes());  * connection.write("key4,val4".getBytes());  * connection.commitTransaction();  * // close the streaming connection  * connection.close();  * }  *</pre>  */
end_comment

begin_class
specifier|public
class|class
name|HiveStreamingConnection
implements|implements
name|StreamingConnection
block|{
specifier|private
specifier|static
specifier|final
name|Logger
name|LOG
init|=
name|LoggerFactory
operator|.
name|getLogger
argument_list|(
name|HiveStreamingConnection
operator|.
name|class
operator|.
name|getName
argument_list|()
argument_list|)
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|String
name|DEFAULT_METASTORE_URI
init|=
literal|"thrift://localhost:9083"
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|int
name|DEFAULT_TRANSACTION_BATCH_SIZE
init|=
literal|1
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|boolean
name|DEFAULT_STREAMING_OPTIMIZATIONS_ENABLED
init|=
literal|true
decl_stmt|;
specifier|public
enum|enum
name|TxnState
block|{
name|INACTIVE
argument_list|(
literal|"I"
argument_list|)
block|,
name|OPEN
argument_list|(
literal|"O"
argument_list|)
block|,
name|COMMITTED
argument_list|(
literal|"C"
argument_list|)
block|,
name|ABORTED
argument_list|(
literal|"A"
argument_list|)
block|,
name|PREPARED_FOR_COMMIT
argument_list|(
literal|"P"
argument_list|)
block|;
specifier|private
specifier|final
name|String
name|code
decl_stmt|;
name|TxnState
parameter_list|(
name|String
name|code
parameter_list|)
block|{
name|this
operator|.
name|code
operator|=
name|code
expr_stmt|;
block|}
specifier|public
name|String
name|toString
parameter_list|()
block|{
return|return
name|code
return|;
block|}
block|}
comment|// fields populated from builder
specifier|private
name|String
name|database
decl_stmt|;
specifier|private
name|String
name|table
decl_stmt|;
specifier|private
name|List
argument_list|<
name|String
argument_list|>
name|staticPartitionValues
decl_stmt|;
specifier|private
name|String
name|agentInfo
decl_stmt|;
specifier|private
name|int
name|transactionBatchSize
decl_stmt|;
specifier|private
name|RecordWriter
name|recordWriter
decl_stmt|;
specifier|private
name|StreamingTransaction
name|currentTransactionBatch
decl_stmt|;
specifier|private
name|HiveConf
name|conf
decl_stmt|;
specifier|private
name|boolean
name|streamingOptimizations
decl_stmt|;
specifier|private
name|AtomicBoolean
name|isConnectionClosed
init|=
operator|new
name|AtomicBoolean
argument_list|(
literal|false
argument_list|)
decl_stmt|;
comment|// internal fields
specifier|private
name|boolean
name|isPartitionedTable
decl_stmt|;
specifier|private
name|IMetaStoreClient
name|msClient
decl_stmt|;
specifier|private
name|IMetaStoreClient
name|heartbeatMSClient
decl_stmt|;
specifier|private
specifier|final
name|String
name|username
decl_stmt|;
specifier|private
specifier|final
name|boolean
name|secureMode
decl_stmt|;
specifier|private
name|Table
name|tableObject
init|=
literal|null
decl_stmt|;
specifier|private
name|String
name|metastoreUri
decl_stmt|;
specifier|private
name|ConnectionStats
name|connectionStats
decl_stmt|;
specifier|private
specifier|final
name|Long
name|writeId
decl_stmt|;
specifier|private
specifier|final
name|Integer
name|statementId
decl_stmt|;
specifier|private
name|boolean
name|manageTransactions
decl_stmt|;
specifier|private
name|int
name|countTransactions
init|=
literal|0
decl_stmt|;
specifier|private
name|Set
argument_list|<
name|String
argument_list|>
name|partitions
decl_stmt|;
specifier|private
name|Map
argument_list|<
name|String
argument_list|,
name|WriteDirInfo
argument_list|>
name|writePaths
decl_stmt|;
specifier|private
name|Runnable
name|onShutdownRunner
decl_stmt|;
specifier|private
name|HiveStreamingConnection
parameter_list|(
name|Builder
name|builder
parameter_list|)
throws|throws
name|StreamingException
block|{
name|this
operator|.
name|database
operator|=
name|builder
operator|.
name|database
operator|.
name|toLowerCase
argument_list|()
expr_stmt|;
name|this
operator|.
name|table
operator|=
name|builder
operator|.
name|table
operator|.
name|toLowerCase
argument_list|()
expr_stmt|;
name|this
operator|.
name|staticPartitionValues
operator|=
name|builder
operator|.
name|staticPartitionValues
expr_stmt|;
name|this
operator|.
name|conf
operator|=
name|builder
operator|.
name|hiveConf
expr_stmt|;
name|this
operator|.
name|agentInfo
operator|=
name|builder
operator|.
name|agentInfo
expr_stmt|;
name|this
operator|.
name|streamingOptimizations
operator|=
name|builder
operator|.
name|streamingOptimizations
expr_stmt|;
name|this
operator|.
name|writeId
operator|=
name|builder
operator|.
name|writeId
expr_stmt|;
name|this
operator|.
name|statementId
operator|=
name|builder
operator|.
name|statementId
expr_stmt|;
name|this
operator|.
name|tableObject
operator|=
name|builder
operator|.
name|tableObject
expr_stmt|;
name|this
operator|.
name|setPartitionedTable
argument_list|(
name|builder
operator|.
name|isPartitioned
argument_list|)
expr_stmt|;
name|this
operator|.
name|manageTransactions
operator|=
name|builder
operator|.
name|manageTransactions
expr_stmt|;
name|this
operator|.
name|writePaths
operator|=
operator|new
name|HashMap
argument_list|<>
argument_list|()
expr_stmt|;
name|UserGroupInformation
name|loggedInUser
init|=
literal|null
decl_stmt|;
try|try
block|{
name|loggedInUser
operator|=
name|UserGroupInformation
operator|.
name|getLoginUser
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Unable to get logged in user via UGI. err: {}"
argument_list|,
name|e
operator|.
name|getMessage
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|loggedInUser
operator|==
literal|null
condition|)
block|{
name|this
operator|.
name|username
operator|=
name|System
operator|.
name|getProperty
argument_list|(
literal|"user.name"
argument_list|)
expr_stmt|;
name|this
operator|.
name|secureMode
operator|=
literal|false
expr_stmt|;
block|}
else|else
block|{
name|this
operator|.
name|username
operator|=
name|loggedInUser
operator|.
name|getShortUserName
argument_list|()
expr_stmt|;
name|this
operator|.
name|secureMode
operator|=
name|loggedInUser
operator|.
name|hasKerberosCredentials
argument_list|()
expr_stmt|;
block|}
name|this
operator|.
name|transactionBatchSize
operator|=
name|builder
operator|.
name|transactionBatchSize
expr_stmt|;
name|this
operator|.
name|recordWriter
operator|=
name|builder
operator|.
name|recordWriter
expr_stmt|;
name|this
operator|.
name|connectionStats
operator|=
operator|new
name|ConnectionStats
argument_list|()
expr_stmt|;
if|if
condition|(
name|agentInfo
operator|==
literal|null
condition|)
block|{
try|try
block|{
name|agentInfo
operator|=
name|username
operator|+
literal|":"
operator|+
name|InetAddress
operator|.
name|getLocalHost
argument_list|()
operator|.
name|getHostName
argument_list|()
operator|+
literal|":"
operator|+
name|Thread
operator|.
name|currentThread
argument_list|()
operator|.
name|getName
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|UnknownHostException
name|e
parameter_list|)
block|{
comment|// ignore and use UUID instead
name|this
operator|.
name|agentInfo
operator|=
name|UUID
operator|.
name|randomUUID
argument_list|()
operator|.
name|toString
argument_list|()
expr_stmt|;
block|}
block|}
if|if
condition|(
name|conf
operator|==
literal|null
condition|)
block|{
name|conf
operator|=
name|createHiveConf
argument_list|(
name|this
operator|.
name|getClass
argument_list|()
argument_list|,
name|DEFAULT_METASTORE_URI
argument_list|)
expr_stmt|;
block|}
name|overrideConfSettings
argument_list|(
name|conf
argument_list|)
expr_stmt|;
if|if
condition|(
name|manageTransactions
condition|)
block|{
name|this
operator|.
name|metastoreUri
operator|=
name|conf
operator|.
name|get
argument_list|(
name|MetastoreConf
operator|.
name|ConfVars
operator|.
name|THRIFT_URIS
operator|.
name|getHiveName
argument_list|()
argument_list|)
expr_stmt|;
name|this
operator|.
name|msClient
operator|=
name|getMetaStoreClient
argument_list|(
name|conf
argument_list|,
name|metastoreUri
argument_list|,
name|secureMode
argument_list|,
literal|"streaming-connection"
argument_list|)
expr_stmt|;
comment|// We use a separate metastore client for heartbeat calls to ensure heartbeat RPC calls are
comment|// isolated from the other transaction related RPC calls.
name|this
operator|.
name|heartbeatMSClient
operator|=
name|getMetaStoreClient
argument_list|(
name|conf
argument_list|,
name|metastoreUri
argument_list|,
name|secureMode
argument_list|,
literal|"streaming-connection-heartbeat"
argument_list|)
expr_stmt|;
name|validateTable
argument_list|()
expr_stmt|;
block|}
name|LOG
operator|.
name|info
argument_list|(
literal|"STREAMING CONNECTION INFO: {}"
argument_list|,
name|toConnectionInfoString
argument_list|()
argument_list|)
expr_stmt|;
block|}
specifier|public
specifier|static
name|Builder
name|newBuilder
parameter_list|()
block|{
return|return
operator|new
name|Builder
argument_list|()
return|;
block|}
specifier|public
specifier|static
class|class
name|Builder
block|{
specifier|private
name|String
name|database
decl_stmt|;
specifier|private
name|String
name|table
decl_stmt|;
specifier|private
name|List
argument_list|<
name|String
argument_list|>
name|staticPartitionValues
decl_stmt|;
specifier|private
name|String
name|agentInfo
decl_stmt|;
specifier|private
name|HiveConf
name|hiveConf
decl_stmt|;
specifier|private
name|int
name|transactionBatchSize
init|=
name|DEFAULT_TRANSACTION_BATCH_SIZE
decl_stmt|;
specifier|private
name|boolean
name|streamingOptimizations
init|=
name|DEFAULT_STREAMING_OPTIMIZATIONS_ENABLED
decl_stmt|;
specifier|private
name|RecordWriter
name|recordWriter
decl_stmt|;
specifier|private
name|long
name|writeId
init|=
operator|-
literal|1
decl_stmt|;
specifier|private
name|int
name|statementId
init|=
operator|-
literal|1
decl_stmt|;
specifier|private
name|boolean
name|manageTransactions
init|=
literal|true
decl_stmt|;
specifier|private
name|Table
name|tableObject
decl_stmt|;
specifier|private
name|boolean
name|isPartitioned
decl_stmt|;
comment|/**      * Specify database to use for streaming connection.      *      * @param database - db name      * @return - builder      */
specifier|public
name|Builder
name|withDatabase
parameter_list|(
specifier|final
name|String
name|database
parameter_list|)
block|{
name|this
operator|.
name|database
operator|=
name|database
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**      * Specify table to use for streaming connection.      *      * @param table - table name      * @return - builder      */
specifier|public
name|Builder
name|withTable
parameter_list|(
specifier|final
name|String
name|table
parameter_list|)
block|{
name|this
operator|.
name|table
operator|=
name|table
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**      * Specify the name of partition to use for streaming connection.      *      * @param staticPartitionValues - static partition values      * @return - builder      */
specifier|public
name|Builder
name|withStaticPartitionValues
parameter_list|(
specifier|final
name|List
argument_list|<
name|String
argument_list|>
name|staticPartitionValues
parameter_list|)
block|{
name|this
operator|.
name|staticPartitionValues
operator|=
name|staticPartitionValues
operator|==
literal|null
condition|?
literal|null
else|:
operator|new
name|ArrayList
argument_list|<>
argument_list|(
name|staticPartitionValues
argument_list|)
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**      * Specify agent info to use for streaming connection.      *      * @param agentInfo - agent info      * @return - builder      */
specifier|public
name|Builder
name|withAgentInfo
parameter_list|(
specifier|final
name|String
name|agentInfo
parameter_list|)
block|{
name|this
operator|.
name|agentInfo
operator|=
name|agentInfo
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**      * Specify hive configuration object to use for streaming connection.      * Generate this object by point to already existing hive-site.xml or HIVE_CONF_DIR.      * Make sure if metastore URI has been set correctly else thrift://localhost:9083 will be      * used as default.      *      * @param hiveConf - hive conf object      * @return - builder      */
specifier|public
name|Builder
name|withHiveConf
parameter_list|(
specifier|final
name|HiveConf
name|hiveConf
parameter_list|)
block|{
name|this
operator|.
name|hiveConf
operator|=
name|hiveConf
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**      * Transaction batch size to use (default value is 10). This is expert level configuration.      * For every transaction batch a delta directory will be created which will impact      * when compaction will trigger.      * NOTE: This is evolving API and is subject to change/might not be honored in future releases.      *      * @param transactionBatchSize - transaction batch size      * @return - builder      */
annotation|@
name|InterfaceStability
operator|.
name|Evolving
specifier|public
name|Builder
name|withTransactionBatchSize
parameter_list|(
specifier|final
name|int
name|transactionBatchSize
parameter_list|)
block|{
name|this
operator|.
name|transactionBatchSize
operator|=
name|transactionBatchSize
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**      * Whether to enable streaming optimizations. This is expert level configurations.      * Disabling streaming optimizations will have significant impact to performance and memory consumption.      *      * @param enable - flag to enable or not      * @return - builder      */
specifier|public
name|Builder
name|withStreamingOptimizations
parameter_list|(
specifier|final
name|boolean
name|enable
parameter_list|)
block|{
name|this
operator|.
name|streamingOptimizations
operator|=
name|enable
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**      * Record writer to use for writing records to destination table.      *      * @param recordWriter - record writer      * @return - builder      */
specifier|public
name|Builder
name|withRecordWriter
parameter_list|(
specifier|final
name|RecordWriter
name|recordWriter
parameter_list|)
block|{
name|this
operator|.
name|recordWriter
operator|=
name|recordWriter
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**      * Specify this parameter if we want the current connection      * to join an ongoing transaction without having to query      * the metastore to create it.      * @param writeId write id      * @return builder      */
specifier|public
name|Builder
name|withWriteId
parameter_list|(
specifier|final
name|long
name|writeId
parameter_list|)
block|{
name|this
operator|.
name|writeId
operator|=
name|writeId
expr_stmt|;
name|manageTransactions
operator|=
literal|false
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**      * Specify this parameter to set an statement id in the writer.      * This really only makes sense to be specified when a writeId is      * provided as well      * @param statementId statement id      * @return builder      */
specifier|public
name|Builder
name|withStatementId
parameter_list|(
specifier|final
name|int
name|statementId
parameter_list|)
block|{
name|this
operator|.
name|statementId
operator|=
name|statementId
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**      * Specify the table object since sometimes no connections      * to the metastore will be opened.      * @param table table object.      * @return builder      */
specifier|public
name|Builder
name|withTableObject
parameter_list|(
name|Table
name|table
parameter_list|)
block|{
name|this
operator|.
name|tableObject
operator|=
name|table
expr_stmt|;
name|this
operator|.
name|isPartitioned
operator|=
name|tableObject
operator|.
name|getPartitionKeys
argument_list|()
operator|!=
literal|null
operator|&&
operator|!
name|tableObject
operator|.
name|getPartitionKeys
argument_list|()
operator|.
name|isEmpty
argument_list|()
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**      * Returning a streaming connection to hive.      *      * @return - hive streaming connection      */
specifier|public
name|HiveStreamingConnection
name|connect
parameter_list|()
throws|throws
name|StreamingException
block|{
if|if
condition|(
name|database
operator|==
literal|null
condition|)
block|{
throw|throw
operator|new
name|StreamingException
argument_list|(
literal|"Database cannot be null for streaming connection"
argument_list|)
throw|;
block|}
if|if
condition|(
name|table
operator|==
literal|null
condition|)
block|{
if|if
condition|(
name|tableObject
operator|==
literal|null
condition|)
block|{
throw|throw
operator|new
name|StreamingException
argument_list|(
literal|"Table and table object cannot be "
operator|+
literal|"null for streaming connection"
argument_list|)
throw|;
block|}
else|else
block|{
name|table
operator|=
name|tableObject
operator|.
name|getTableName
argument_list|()
expr_stmt|;
block|}
block|}
if|if
condition|(
name|tableObject
operator|!=
literal|null
operator|&&
operator|!
name|tableObject
operator|.
name|getTableName
argument_list|()
operator|.
name|equals
argument_list|(
name|table
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|StreamingException
argument_list|(
literal|"Table must match tableObject table name"
argument_list|)
throw|;
block|}
if|if
condition|(
name|recordWriter
operator|==
literal|null
condition|)
block|{
throw|throw
operator|new
name|StreamingException
argument_list|(
literal|"Record writer cannot be null for streaming connection"
argument_list|)
throw|;
block|}
if|if
condition|(
operator|(
name|writeId
operator|!=
operator|-
literal|1
operator|&&
name|tableObject
operator|==
literal|null
operator|)
operator|||
operator|(
name|writeId
operator|==
operator|-
literal|1
operator|&&
name|tableObject
operator|!=
literal|null
operator|)
condition|)
block|{
throw|throw
operator|new
name|StreamingException
argument_list|(
literal|"If writeId is set, tableObject "
operator|+
literal|"must be set as well and vice versa"
argument_list|)
throw|;
block|}
name|HiveStreamingConnection
name|streamingConnection
init|=
operator|new
name|HiveStreamingConnection
argument_list|(
name|this
argument_list|)
decl_stmt|;
name|streamingConnection
operator|.
name|onShutdownRunner
operator|=
name|streamingConnection
operator|::
name|close
expr_stmt|;
comment|// assigning higher priority than FileSystem shutdown hook so that streaming connection gets closed first before
comment|// filesystem close (to avoid ClosedChannelException)
name|ShutdownHookManager
operator|.
name|addShutdownHook
argument_list|(
name|streamingConnection
operator|.
name|onShutdownRunner
argument_list|,
name|FileSystem
operator|.
name|SHUTDOWN_HOOK_PRIORITY
operator|+
literal|1
argument_list|)
expr_stmt|;
name|Thread
operator|.
name|setDefaultUncaughtExceptionHandler
argument_list|(
parameter_list|(
name|t
parameter_list|,
name|e
parameter_list|)
lambda|->
name|streamingConnection
operator|.
name|close
argument_list|()
argument_list|)
expr_stmt|;
return|return
name|streamingConnection
return|;
block|}
block|}
specifier|private
name|void
name|setPartitionedTable
parameter_list|(
name|Boolean
name|isPartitionedTable
parameter_list|)
block|{
name|this
operator|.
name|isPartitionedTable
operator|=
name|isPartitionedTable
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|String
name|toString
parameter_list|()
block|{
return|return
literal|"{ metaStoreUri: "
operator|+
name|metastoreUri
operator|+
literal|", database: "
operator|+
name|database
operator|+
literal|", table: "
operator|+
name|table
operator|+
literal|" }"
return|;
block|}
specifier|private
name|String
name|toConnectionInfoString
parameter_list|()
block|{
return|return
literal|"{ metastore-uri: "
operator|+
name|metastoreUri
operator|+
literal|", "
operator|+
literal|"database: "
operator|+
name|database
operator|+
literal|", "
operator|+
literal|"table: "
operator|+
name|table
operator|+
literal|", "
operator|+
literal|"partitioned-table: "
operator|+
name|isPartitionedTable
argument_list|()
operator|+
literal|", "
operator|+
literal|"dynamic-partitioning: "
operator|+
name|isDynamicPartitioning
argument_list|()
operator|+
literal|", "
operator|+
literal|"username: "
operator|+
name|username
operator|+
literal|", "
operator|+
literal|"secure-mode: "
operator|+
name|secureMode
operator|+
literal|", "
operator|+
literal|"record-writer: "
operator|+
name|recordWriter
operator|.
name|getClass
argument_list|()
operator|.
name|getSimpleName
argument_list|()
operator|+
literal|", "
operator|+
literal|"agent-info: "
operator|+
name|agentInfo
operator|+
literal|", "
operator|+
literal|"writeId: "
operator|+
name|writeId
operator|+
literal|", "
operator|+
literal|"statementId: "
operator|+
name|statementId
operator|+
literal|" }"
return|;
block|}
annotation|@
name|VisibleForTesting
name|String
name|toTransactionString
parameter_list|()
block|{
return|return
name|currentTransactionBatch
operator|==
literal|null
condition|?
literal|""
else|:
name|currentTransactionBatch
operator|.
name|toString
argument_list|()
return|;
block|}
annotation|@
name|Override
specifier|public
name|PartitionInfo
name|createPartitionIfNotExists
parameter_list|(
specifier|final
name|List
argument_list|<
name|String
argument_list|>
name|partitionValues
parameter_list|)
throws|throws
name|StreamingException
block|{
name|String
name|partLocation
init|=
literal|null
decl_stmt|;
name|String
name|partName
init|=
literal|null
decl_stmt|;
name|boolean
name|exists
init|=
literal|false
decl_stmt|;
try|try
block|{
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|partSpec
init|=
name|Warehouse
operator|.
name|makeSpecFromValues
argument_list|(
name|tableObject
operator|.
name|getPartitionKeys
argument_list|()
argument_list|,
name|partitionValues
argument_list|)
decl_stmt|;
name|Path
name|location
init|=
operator|new
name|Path
argument_list|(
name|tableObject
operator|.
name|getDataLocation
argument_list|()
argument_list|,
name|Warehouse
operator|.
name|makePartPath
argument_list|(
name|partSpec
argument_list|)
argument_list|)
decl_stmt|;
name|location
operator|=
operator|new
name|Path
argument_list|(
name|Utilities
operator|.
name|getQualifiedPath
argument_list|(
name|conf
argument_list|,
name|location
argument_list|)
argument_list|)
expr_stmt|;
name|partLocation
operator|=
name|location
operator|.
name|toString
argument_list|()
expr_stmt|;
name|partName
operator|=
name|Warehouse
operator|.
name|makePartName
argument_list|(
name|tableObject
operator|.
name|getPartitionKeys
argument_list|()
argument_list|,
name|partitionValues
argument_list|)
expr_stmt|;
name|Partition
name|partition
init|=
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|Partition
operator|.
name|createMetaPartitionObject
argument_list|(
name|tableObject
argument_list|,
name|partSpec
argument_list|,
name|location
argument_list|)
decl_stmt|;
if|if
condition|(
name|getMSC
argument_list|()
operator|==
literal|null
condition|)
block|{
comment|// We assume it doesn't exist if we can't check it
comment|// so the driver will decide
return|return
operator|new
name|PartitionInfo
argument_list|(
name|partName
argument_list|,
name|partLocation
argument_list|,
literal|false
argument_list|)
return|;
block|}
name|getMSC
argument_list|()
operator|.
name|add_partition
argument_list|(
name|partition
argument_list|)
expr_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Created partition {} for table {}"
argument_list|,
name|partName
argument_list|,
name|tableObject
operator|.
name|getFullyQualifiedName
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|AlreadyExistsException
name|e
parameter_list|)
block|{
name|exists
operator|=
literal|true
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|HiveException
decl||
name|TException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|StreamingException
argument_list|(
literal|"Unable to creation partition for values: "
operator|+
name|partitionValues
operator|+
literal|" connection: "
operator|+
name|toConnectionInfoString
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
return|return
operator|new
name|PartitionInfo
argument_list|(
name|partName
argument_list|,
name|partLocation
argument_list|,
name|exists
argument_list|)
return|;
block|}
comment|/**    * Returns the file that would be used to store rows under this.    * parameters    * @param partitionValues partition values    * @param bucketId bucket id    * @param minWriteId min write Id    * @param maxWriteId max write Id    * @param statementId statement Id    * @return the location of the file.    * @throws StreamingException when the path is not found    */
annotation|@
name|Override
specifier|public
name|Path
name|getDeltaFileLocation
parameter_list|(
name|List
argument_list|<
name|String
argument_list|>
name|partitionValues
parameter_list|,
name|Integer
name|bucketId
parameter_list|,
name|Long
name|minWriteId
parameter_list|,
name|Long
name|maxWriteId
parameter_list|,
name|Integer
name|statementId
parameter_list|)
throws|throws
name|StreamingException
block|{
return|return
name|recordWriter
operator|.
name|getDeltaFileLocation
argument_list|(
name|partitionValues
argument_list|,
name|bucketId
argument_list|,
name|minWriteId
argument_list|,
name|maxWriteId
argument_list|,
name|statementId
argument_list|,
name|tableObject
argument_list|)
return|;
block|}
name|IMetaStoreClient
name|getMSC
parameter_list|()
block|{
name|connectionStats
operator|.
name|incrementMetastoreCalls
argument_list|()
expr_stmt|;
return|return
name|msClient
return|;
block|}
name|IMetaStoreClient
name|getHeatbeatMSC
parameter_list|()
block|{
name|connectionStats
operator|.
name|incrementMetastoreCalls
argument_list|()
expr_stmt|;
return|return
name|heartbeatMSClient
return|;
block|}
specifier|private
name|void
name|validateTable
parameter_list|()
throws|throws
name|InvalidTable
throws|,
name|ConnectionError
block|{
try|try
block|{
name|tableObject
operator|=
operator|new
name|Table
argument_list|(
name|getMSC
argument_list|()
operator|.
name|getTable
argument_list|(
name|database
argument_list|,
name|table
argument_list|)
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Unable to validate the table for connection: "
operator|+
name|toConnectionInfoString
argument_list|()
argument_list|,
name|e
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|InvalidTable
argument_list|(
name|database
argument_list|,
name|table
argument_list|,
name|e
argument_list|)
throw|;
block|}
comment|// 1 - check that the table is Acid
if|if
condition|(
operator|!
name|AcidUtils
operator|.
name|isFullAcidTable
argument_list|(
name|tableObject
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"HiveEndPoint "
operator|+
name|this
operator|+
literal|" must use an acid table"
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|InvalidTable
argument_list|(
name|database
argument_list|,
name|table
argument_list|,
literal|"is not an Acid table"
argument_list|)
throw|;
block|}
if|if
condition|(
name|tableObject
operator|.
name|getPartitionKeys
argument_list|()
operator|!=
literal|null
operator|&&
operator|!
name|tableObject
operator|.
name|getPartitionKeys
argument_list|()
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|setPartitionedTable
argument_list|(
literal|true
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|setPartitionedTable
argument_list|(
literal|false
argument_list|)
expr_stmt|;
block|}
comment|// partition values are specified on non-partitioned table
if|if
condition|(
operator|!
name|isPartitionedTable
argument_list|()
operator|&&
operator|(
name|staticPartitionValues
operator|!=
literal|null
operator|&&
operator|!
name|staticPartitionValues
operator|.
name|isEmpty
argument_list|()
operator|)
condition|)
block|{
comment|// Invalid if table is not partitioned, but endPoint's partitionVals is not empty
name|String
name|errMsg
init|=
name|this
operator|.
name|toString
argument_list|()
operator|+
literal|" specifies partitions for un-partitioned table"
decl_stmt|;
name|LOG
operator|.
name|error
argument_list|(
name|errMsg
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|ConnectionError
argument_list|(
name|errMsg
argument_list|)
throw|;
block|}
comment|// batch size is only used for managed transactions, not for unmanaged single transactions
if|if
condition|(
name|transactionBatchSize
operator|>
literal|1
condition|)
block|{
try|try
init|(
name|FileSystem
name|fs
init|=
name|tableObject
operator|.
name|getDataLocation
argument_list|()
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
init|)
block|{
if|if
condition|(
name|BlobStorageUtils
operator|.
name|isBlobStorageFileSystem
argument_list|(
name|conf
argument_list|,
name|fs
argument_list|)
condition|)
block|{
comment|// currently not all filesystems implement StreamCapabilities, while FSDataOutputStream does
name|Path
name|path
init|=
operator|new
name|Path
argument_list|(
literal|"/tmp"
argument_list|,
literal|"_tmp_stream_verify_"
operator|+
name|UUID
operator|.
name|randomUUID
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|)
decl_stmt|;
try|try
init|(
name|FSDataOutputStream
name|out
init|=
name|fs
operator|.
name|create
argument_list|(
name|path
argument_list|,
literal|false
argument_list|)
init|)
block|{
if|if
condition|(
operator|!
name|out
operator|.
name|hasCapability
argument_list|(
name|StreamCapabilities
operator|.
name|HFLUSH
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|ConnectionError
argument_list|(
literal|"The backing filesystem only supports transaction batch sizes of 1, but "
operator|+
name|transactionBatchSize
operator|+
literal|" was requested."
argument_list|)
throw|;
block|}
name|fs
operator|.
name|deleteOnExit
argument_list|(
name|path
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|ConnectionError
argument_list|(
literal|"Could not create path for database"
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|ConnectionError
argument_list|(
literal|"Could not retrieve FileSystem of table"
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
block|}
specifier|private
name|void
name|beginNextTransaction
parameter_list|()
throws|throws
name|StreamingException
block|{
if|if
condition|(
name|currentTransactionBatch
operator|==
literal|null
condition|)
block|{
name|currentTransactionBatch
operator|=
name|createNewTransactionBatch
argument_list|()
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Opened new transaction batch {}"
argument_list|,
name|currentTransactionBatch
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|currentTransactionBatch
operator|.
name|isClosed
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|StreamingException
argument_list|(
literal|"Cannot begin next transaction on a closed streaming connection"
argument_list|)
throw|;
block|}
if|if
condition|(
name|currentTransactionBatch
operator|.
name|remainingTransactions
argument_list|()
operator|==
literal|0
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Transaction batch {} is done. Rolling over to next transaction batch."
argument_list|,
name|currentTransactionBatch
argument_list|)
expr_stmt|;
name|closeCurrentTransactionBatch
argument_list|()
expr_stmt|;
name|currentTransactionBatch
operator|=
name|createNewTransactionBatch
argument_list|()
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Rolled over to new transaction batch {}"
argument_list|,
name|currentTransactionBatch
argument_list|)
expr_stmt|;
block|}
name|currentTransactionBatch
operator|.
name|beginNextTransaction
argument_list|()
expr_stmt|;
block|}
specifier|private
name|StreamingTransaction
name|createNewTransactionBatch
parameter_list|()
throws|throws
name|StreamingException
block|{
name|countTransactions
operator|++
expr_stmt|;
if|if
condition|(
name|manageTransactions
condition|)
block|{
return|return
operator|new
name|TransactionBatch
argument_list|(
name|this
argument_list|)
return|;
block|}
else|else
block|{
if|if
condition|(
name|countTransactions
operator|>
literal|1
condition|)
block|{
throw|throw
operator|new
name|StreamingException
argument_list|(
literal|"If a writeId is passed for the "
operator|+
literal|"construction of HiveStreaming only one transaction batch"
operator|+
literal|" can be done"
argument_list|)
throw|;
block|}
return|return
operator|new
name|UnManagedSingleTransaction
argument_list|(
name|this
argument_list|)
return|;
block|}
block|}
specifier|private
name|void
name|checkClosedState
parameter_list|()
throws|throws
name|StreamingException
block|{
if|if
condition|(
name|isConnectionClosed
operator|.
name|get
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|StreamingException
argument_list|(
literal|"Streaming connection is closed already."
argument_list|)
throw|;
block|}
block|}
specifier|private
name|void
name|checkState
parameter_list|()
throws|throws
name|StreamingException
block|{
name|checkClosedState
argument_list|()
expr_stmt|;
if|if
condition|(
name|currentTransactionBatch
operator|==
literal|null
condition|)
block|{
throw|throw
operator|new
name|StreamingException
argument_list|(
literal|"Transaction batch is null. Missing beginTransaction?"
argument_list|)
throw|;
block|}
if|if
condition|(
name|currentTransactionBatch
operator|.
name|getCurrentTransactionState
argument_list|()
operator|!=
name|TxnState
operator|.
name|OPEN
condition|)
block|{
throw|throw
operator|new
name|StreamingException
argument_list|(
literal|"Transaction state is not OPEN. Missing beginTransaction?"
argument_list|)
throw|;
block|}
block|}
specifier|private
name|void
name|closeCurrentTransactionBatch
parameter_list|()
throws|throws
name|StreamingException
block|{
name|currentTransactionBatch
operator|.
name|close
argument_list|()
expr_stmt|;
name|writePaths
operator|.
name|clear
argument_list|()
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|beginTransaction
parameter_list|()
throws|throws
name|StreamingException
block|{
name|checkClosedState
argument_list|()
expr_stmt|;
name|partitions
operator|=
operator|new
name|HashSet
argument_list|<>
argument_list|()
expr_stmt|;
name|beginNextTransaction
argument_list|()
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|commitTransaction
parameter_list|()
throws|throws
name|StreamingException
block|{
name|commitTransaction
argument_list|(
literal|null
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|commitTransaction
parameter_list|(
name|Set
argument_list|<
name|String
argument_list|>
name|partitions
parameter_list|)
throws|throws
name|StreamingException
block|{
name|commitTransaction
argument_list|(
name|partitions
argument_list|,
literal|null
argument_list|,
literal|null
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|commitTransaction
parameter_list|(
name|Set
argument_list|<
name|String
argument_list|>
name|partitions
parameter_list|,
name|String
name|key
parameter_list|,
name|String
name|value
parameter_list|)
throws|throws
name|StreamingException
block|{
name|checkState
argument_list|()
expr_stmt|;
name|Set
argument_list|<
name|String
argument_list|>
name|createdPartitions
init|=
operator|new
name|HashSet
argument_list|<>
argument_list|()
decl_stmt|;
if|if
condition|(
name|partitions
operator|!=
literal|null
condition|)
block|{
for|for
control|(
name|String
name|partition
range|:
name|partitions
control|)
block|{
try|try
block|{
name|PartitionInfo
name|info
init|=
name|createPartitionIfNotExists
argument_list|(
name|Warehouse
operator|.
name|getPartValuesFromPartName
argument_list|(
name|partition
argument_list|)
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|info
operator|.
name|isExists
argument_list|()
condition|)
block|{
name|createdPartitions
operator|.
name|add
argument_list|(
name|partition
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|MetaException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|StreamingException
argument_list|(
literal|"Partition "
operator|+
name|partition
operator|+
literal|" is invalid."
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
name|connectionStats
operator|.
name|incrementTotalPartitions
argument_list|(
name|partitions
operator|.
name|size
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|currentTransactionBatch
operator|.
name|commit
argument_list|(
name|createdPartitions
argument_list|,
name|key
argument_list|,
name|value
argument_list|)
expr_stmt|;
name|this
operator|.
name|partitions
operator|.
name|addAll
argument_list|(
name|currentTransactionBatch
operator|.
name|getPartitions
argument_list|()
argument_list|)
expr_stmt|;
name|connectionStats
operator|.
name|incrementCreatedPartitions
argument_list|(
name|createdPartitions
operator|.
name|size
argument_list|()
argument_list|)
expr_stmt|;
name|connectionStats
operator|.
name|incrementCommittedTransactions
argument_list|()
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|abortTransaction
parameter_list|()
throws|throws
name|StreamingException
block|{
name|checkState
argument_list|()
expr_stmt|;
name|currentTransactionBatch
operator|.
name|abort
argument_list|()
expr_stmt|;
name|connectionStats
operator|.
name|incrementAbortedTransactions
argument_list|()
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|write
parameter_list|(
specifier|final
name|byte
index|[]
name|record
parameter_list|)
throws|throws
name|StreamingException
block|{
name|checkState
argument_list|()
expr_stmt|;
name|currentTransactionBatch
operator|.
name|write
argument_list|(
name|record
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|write
parameter_list|(
specifier|final
name|InputStream
name|inputStream
parameter_list|)
throws|throws
name|StreamingException
block|{
name|checkState
argument_list|()
expr_stmt|;
name|currentTransactionBatch
operator|.
name|write
argument_list|(
name|inputStream
argument_list|)
expr_stmt|;
block|}
comment|/**    * Close connection    */
annotation|@
name|Override
specifier|public
name|void
name|close
parameter_list|()
block|{
if|if
condition|(
name|isConnectionClosed
operator|.
name|get
argument_list|()
condition|)
block|{
return|return;
block|}
name|isConnectionClosed
operator|.
name|set
argument_list|(
literal|true
argument_list|)
expr_stmt|;
try|try
block|{
if|if
condition|(
name|currentTransactionBatch
operator|!=
literal|null
condition|)
block|{
name|closeCurrentTransactionBatch
argument_list|()
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|StreamingException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Unable to close current transaction batch: "
operator|+
name|currentTransactionBatch
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
if|if
condition|(
name|manageTransactions
condition|)
block|{
name|getMSC
argument_list|()
operator|.
name|close
argument_list|()
expr_stmt|;
name|getHeatbeatMSC
argument_list|()
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
comment|//remove shutdown hook entry added while creating this connection via HiveStreamingConnection.Builder#connect()
if|if
condition|(
operator|!
name|ShutdownHookManager
operator|.
name|isShutdownInProgress
argument_list|()
condition|)
block|{
name|ShutdownHookManager
operator|.
name|removeShutdownHook
argument_list|(
name|this
operator|.
name|onShutdownRunner
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
name|LOG
operator|.
name|isInfoEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Closed streaming connection. Agent: {} Stats: {}"
argument_list|,
name|getAgentInfo
argument_list|()
argument_list|,
name|getConnectionStats
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
annotation|@
name|Override
specifier|public
name|ConnectionStats
name|getConnectionStats
parameter_list|()
block|{
return|return
name|connectionStats
return|;
block|}
specifier|private
specifier|static
name|IMetaStoreClient
name|getMetaStoreClient
parameter_list|(
name|HiveConf
name|conf
parameter_list|,
name|String
name|metastoreUri
parameter_list|,
name|boolean
name|secureMode
parameter_list|,
name|String
name|owner
parameter_list|)
throws|throws
name|ConnectionError
block|{
if|if
condition|(
name|metastoreUri
operator|!=
literal|null
condition|)
block|{
name|conf
operator|.
name|set
argument_list|(
name|MetastoreConf
operator|.
name|ConfVars
operator|.
name|THRIFT_URIS
operator|.
name|getHiveName
argument_list|()
argument_list|,
name|metastoreUri
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|secureMode
condition|)
block|{
name|conf
operator|.
name|setBoolean
argument_list|(
name|MetastoreConf
operator|.
name|ConfVars
operator|.
name|USE_THRIFT_SASL
operator|.
name|getHiveName
argument_list|()
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
try|try
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Creating metastore client for {}"
argument_list|,
name|owner
argument_list|)
expr_stmt|;
return|return
name|HiveMetaStoreUtils
operator|.
name|getHiveMetastoreClient
argument_list|(
name|conf
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|MetaException
decl||
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|ConnectionError
argument_list|(
literal|"Error connecting to Hive Metastore URI: "
operator|+
name|metastoreUri
operator|+
literal|". "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
specifier|private
specifier|static
class|class
name|WriteDirInfo
block|{
name|List
argument_list|<
name|String
argument_list|>
name|partitionVals
decl_stmt|;
name|Path
name|writeDir
decl_stmt|;
name|WriteDirInfo
parameter_list|(
name|List
argument_list|<
name|String
argument_list|>
name|partitionVals
parameter_list|,
name|Path
name|writeDir
parameter_list|)
block|{
name|this
operator|.
name|partitionVals
operator|=
name|partitionVals
expr_stmt|;
name|this
operator|.
name|writeDir
operator|=
name|writeDir
expr_stmt|;
block|}
name|List
argument_list|<
name|String
argument_list|>
name|getPartitionVals
parameter_list|()
block|{
return|return
name|this
operator|.
name|partitionVals
return|;
block|}
name|Path
name|getWriteDir
parameter_list|()
block|{
return|return
name|this
operator|.
name|writeDir
return|;
block|}
block|}
annotation|@
name|Override
specifier|public
name|void
name|addWriteDirectoryInfo
parameter_list|(
name|List
argument_list|<
name|String
argument_list|>
name|partitionValues
parameter_list|,
name|Path
name|writeDir
parameter_list|)
block|{
name|String
name|key
init|=
operator|(
name|partitionValues
operator|==
literal|null
operator|)
condition|?
name|tableObject
operator|.
name|getFullyQualifiedName
argument_list|()
else|:
name|partitionValues
operator|.
name|toString
argument_list|()
decl_stmt|;
if|if
condition|(
name|writePaths
operator|.
name|containsKey
argument_list|(
name|key
argument_list|)
condition|)
block|{
comment|// This method is invoked once per bucket file within delta directory. So, same partition or
comment|// table entry shall exist already. But the written delta directory should remain same for all
comment|// bucket files.
name|WriteDirInfo
name|dirInfo
init|=
name|writePaths
operator|.
name|get
argument_list|(
name|key
argument_list|)
decl_stmt|;
assert|assert
operator|(
name|dirInfo
operator|.
name|getWriteDir
argument_list|()
operator|.
name|equals
argument_list|(
name|writeDir
argument_list|)
operator|)
assert|;
block|}
else|else
block|{
name|writePaths
operator|.
name|put
argument_list|(
name|key
argument_list|,
operator|new
name|WriteDirInfo
argument_list|(
name|partitionValues
argument_list|,
name|writeDir
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * Add Write notification events if it is enabled.    * @throws StreamingException File operation errors or HMS errors.    */
annotation|@
name|Override
specifier|public
name|void
name|addWriteNotificationEvents
parameter_list|()
throws|throws
name|StreamingException
block|{
if|if
condition|(
operator|!
name|conf
operator|.
name|getBoolVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|FIRE_EVENTS_FOR_DML
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Write notification log is ignored as dml event logging is disabled."
argument_list|)
expr_stmt|;
return|return;
block|}
try|try
block|{
comment|// Traverse the write paths for the current streaming connection and add one write notification
comment|// event per table or partitions.
comment|// For non-partitioned table, there will be only one entry in writePath and corresponding
comment|// partitionVals is null.
name|Long
name|currentTxnId
init|=
name|getCurrentTxnId
argument_list|()
decl_stmt|;
name|Long
name|currentWriteId
init|=
name|getCurrentWriteId
argument_list|()
decl_stmt|;
for|for
control|(
name|WriteDirInfo
name|writeInfo
range|:
name|writePaths
operator|.
name|values
argument_list|()
control|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"TxnId: "
operator|+
name|currentTxnId
operator|+
literal|", WriteId: "
operator|+
name|currentWriteId
operator|+
literal|" - Logging write event for the files in path "
operator|+
name|writeInfo
operator|.
name|getWriteDir
argument_list|()
argument_list|)
expr_stmt|;
comment|// List the new files added inside the write path (delta directory).
name|FileSystem
name|fs
init|=
name|tableObject
operator|.
name|getDataLocation
argument_list|()
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|Path
argument_list|>
name|newFiles
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|()
decl_stmt|;
name|Hive
operator|.
name|listFilesInsideAcidDirectory
argument_list|(
name|writeInfo
operator|.
name|getWriteDir
argument_list|()
argument_list|,
name|fs
argument_list|,
name|newFiles
argument_list|)
expr_stmt|;
comment|// If no files are added by this streaming writes, then no need to log write notification event.
if|if
condition|(
name|newFiles
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"TxnId: "
operator|+
name|currentTxnId
operator|+
literal|", WriteId: "
operator|+
name|currentWriteId
operator|+
literal|" - Skipping empty path "
operator|+
name|writeInfo
operator|.
name|getWriteDir
argument_list|()
argument_list|)
expr_stmt|;
continue|continue;
block|}
comment|// Add write notification events into HMS table.
name|Hive
operator|.
name|addWriteNotificationLog
argument_list|(
name|conf
argument_list|,
name|tableObject
argument_list|,
name|writeInfo
operator|.
name|getPartitionVals
argument_list|()
argument_list|,
name|currentTxnId
argument_list|,
name|currentWriteId
argument_list|,
name|newFiles
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|IOException
decl||
name|TException
decl||
name|HiveException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|StreamingException
argument_list|(
literal|"Failed to log write notification events."
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
annotation|@
name|VisibleForTesting
name|TxnState
name|getCurrentTransactionState
parameter_list|()
block|{
return|return
name|currentTransactionBatch
operator|.
name|getCurrentTransactionState
argument_list|()
return|;
block|}
annotation|@
name|VisibleForTesting
name|int
name|remainingTransactions
parameter_list|()
block|{
return|return
name|currentTransactionBatch
operator|.
name|remainingTransactions
argument_list|()
return|;
block|}
annotation|@
name|VisibleForTesting
name|Long
name|getCurrentTxnId
parameter_list|()
block|{
return|return
name|currentTransactionBatch
operator|.
name|getCurrentTxnId
argument_list|()
return|;
block|}
specifier|private
name|HiveConf
name|createHiveConf
parameter_list|(
name|Class
argument_list|<
name|?
argument_list|>
name|clazz
parameter_list|,
name|String
name|metaStoreUri
parameter_list|)
block|{
name|HiveConf
name|conf
init|=
operator|new
name|HiveConf
argument_list|(
name|clazz
argument_list|)
decl_stmt|;
if|if
condition|(
name|metaStoreUri
operator|!=
literal|null
condition|)
block|{
name|conf
operator|.
name|set
argument_list|(
name|MetastoreConf
operator|.
name|ConfVars
operator|.
name|THRIFT_URIS
operator|.
name|getHiveName
argument_list|()
argument_list|,
name|metaStoreUri
argument_list|)
expr_stmt|;
block|}
return|return
name|conf
return|;
block|}
specifier|private
name|void
name|overrideConfSettings
parameter_list|(
name|HiveConf
name|conf
parameter_list|)
block|{
name|setHiveConf
argument_list|(
name|conf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_TXN_MANAGER
argument_list|,
name|DbTxnManager
operator|.
name|class
operator|.
name|getName
argument_list|()
argument_list|)
expr_stmt|;
name|setHiveConf
argument_list|(
name|conf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_SUPPORT_CONCURRENCY
argument_list|,
literal|true
argument_list|)
expr_stmt|;
name|setHiveConf
argument_list|(
name|conf
argument_list|,
name|MetastoreConf
operator|.
name|ConfVars
operator|.
name|EXECUTE_SET_UGI
operator|.
name|getHiveName
argument_list|()
argument_list|)
expr_stmt|;
name|setHiveConf
argument_list|(
name|conf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|DYNAMICPARTITIONINGMODE
argument_list|,
literal|"nonstrict"
argument_list|)
expr_stmt|;
if|if
condition|(
name|streamingOptimizations
condition|)
block|{
name|setHiveConf
argument_list|(
name|conf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_ORC_DELTA_STREAMING_OPTIMIZATIONS_ENABLED
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
comment|// since same thread creates metastore client for streaming connection thread and heartbeat thread we explicitly
comment|// disable metastore client cache
name|setHiveConf
argument_list|(
name|conf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|METASTORE_CLIENT_CACHE_ENABLED
argument_list|,
literal|false
argument_list|)
expr_stmt|;
block|}
specifier|private
specifier|static
name|void
name|setHiveConf
parameter_list|(
name|HiveConf
name|conf
parameter_list|,
name|HiveConf
operator|.
name|ConfVars
name|var
parameter_list|,
name|String
name|value
parameter_list|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Overriding HiveConf setting : "
operator|+
name|var
operator|+
literal|" = "
operator|+
name|value
argument_list|)
expr_stmt|;
block|}
name|conf
operator|.
name|setVar
argument_list|(
name|var
argument_list|,
name|value
argument_list|)
expr_stmt|;
block|}
specifier|private
specifier|static
name|void
name|setHiveConf
parameter_list|(
name|HiveConf
name|conf
parameter_list|,
name|HiveConf
operator|.
name|ConfVars
name|var
parameter_list|,
name|boolean
name|value
parameter_list|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Overriding HiveConf setting : "
operator|+
name|var
operator|+
literal|" = "
operator|+
name|value
argument_list|)
expr_stmt|;
block|}
name|conf
operator|.
name|setBoolVar
argument_list|(
name|var
argument_list|,
name|value
argument_list|)
expr_stmt|;
block|}
specifier|private
specifier|static
name|void
name|setHiveConf
parameter_list|(
name|HiveConf
name|conf
parameter_list|,
name|String
name|var
parameter_list|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Overriding HiveConf setting : "
operator|+
name|var
operator|+
literal|" = "
operator|+
literal|true
argument_list|)
expr_stmt|;
block|}
name|conf
operator|.
name|setBoolean
argument_list|(
name|var
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
specifier|public
name|List
argument_list|<
name|TxnToWriteId
argument_list|>
name|getTxnToWriteIds
parameter_list|()
block|{
if|if
condition|(
name|currentTransactionBatch
operator|!=
literal|null
condition|)
block|{
return|return
name|currentTransactionBatch
operator|.
name|getTxnToWriteIds
argument_list|()
return|;
block|}
return|return
literal|null
return|;
block|}
annotation|@
name|Override
specifier|public
name|HiveConf
name|getHiveConf
parameter_list|()
block|{
return|return
name|conf
return|;
block|}
annotation|@
name|Override
specifier|public
name|String
name|getMetastoreUri
parameter_list|()
block|{
return|return
name|metastoreUri
return|;
block|}
annotation|@
name|Override
specifier|public
name|Table
name|getTable
parameter_list|()
block|{
return|return
name|tableObject
return|;
block|}
annotation|@
name|Override
specifier|public
name|List
argument_list|<
name|String
argument_list|>
name|getStaticPartitionValues
parameter_list|()
block|{
return|return
name|staticPartitionValues
return|;
block|}
annotation|@
name|Override
specifier|public
name|String
name|getAgentInfo
parameter_list|()
block|{
return|return
name|agentInfo
return|;
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|isPartitionedTable
parameter_list|()
block|{
return|return
name|isPartitionedTable
return|;
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|isDynamicPartitioning
parameter_list|()
block|{
return|return
name|isPartitionedTable
argument_list|()
operator|&&
operator|(
name|staticPartitionValues
operator|==
literal|null
operator|||
name|staticPartitionValues
operator|.
name|isEmpty
argument_list|()
operator|)
return|;
block|}
annotation|@
name|Override
specifier|public
name|Set
argument_list|<
name|String
argument_list|>
name|getPartitions
parameter_list|()
block|{
return|return
name|partitions
return|;
block|}
specifier|public
name|String
name|getUsername
parameter_list|()
block|{
return|return
name|username
return|;
block|}
specifier|public
name|String
name|getDatabase
parameter_list|()
block|{
return|return
name|database
return|;
block|}
specifier|public
name|RecordWriter
name|getRecordWriter
parameter_list|()
block|{
return|return
name|recordWriter
return|;
block|}
specifier|public
name|int
name|getTransactionBatchSize
parameter_list|()
block|{
return|return
name|transactionBatchSize
return|;
block|}
specifier|public
name|HiveConf
name|getConf
parameter_list|()
block|{
return|return
name|conf
return|;
block|}
specifier|public
name|Long
name|getWriteId
parameter_list|()
block|{
return|return
name|writeId
return|;
block|}
specifier|public
name|Integer
name|getStatementId
parameter_list|()
block|{
return|return
name|statementId
return|;
block|}
specifier|public
name|Long
name|getCurrentWriteId
parameter_list|()
block|{
return|return
name|currentTransactionBatch
operator|.
name|getCurrentWriteId
argument_list|()
return|;
block|}
block|}
end_class

end_unit

