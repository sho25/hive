begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/**  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|shims
package|;
end_package

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|DataInput
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|DataOutput
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|security
operator|.
name|PrivilegedExceptionAction
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|javax
operator|.
name|security
operator|.
name|auth
operator|.
name|login
operator|.
name|LoginException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|Log
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|LogFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configuration
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileStatus
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|PathFilter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|Text
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|ClusterStatus
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|InputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|InputSplit
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|JobConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|RecordReader
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|Reporter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|RunningJob
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|TaskCompletionEvent
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|Job
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|JobContext
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|TaskAttemptContext
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|security
operator|.
name|UserGroupInformation
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|Progressable
import|;
end_import

begin_comment
comment|/**  * In order to be compatible with multiple versions of Hadoop, all parts  * of the Hadoop interface that are not cross-version compatible are  * encapsulated in an implementation of this class. Users should use  * the ShimLoader class as a factory to obtain an implementation of  * HadoopShims corresponding to the version of Hadoop currently on the  * classpath.  */
end_comment

begin_interface
specifier|public
interface|interface
name|HadoopShims
block|{
specifier|static
specifier|final
name|Log
name|LOG
init|=
name|LogFactory
operator|.
name|getLog
argument_list|(
name|HadoopShims
operator|.
name|class
argument_list|)
decl_stmt|;
comment|/**    * Return true if the current version of Hadoop uses the JobShell for    * command line interpretation.    */
name|boolean
name|usesJobShell
parameter_list|()
function_decl|;
comment|/**    * Return true if the job has not switched to RUNNING state yet    * and is still in PREP state    */
name|boolean
name|isJobPreparing
parameter_list|(
name|RunningJob
name|job
parameter_list|)
throws|throws
name|IOException
function_decl|;
comment|/**    * Calls fs.deleteOnExit(path) if such a function exists.    *    * @return true if the call was successful    */
name|boolean
name|fileSystemDeleteOnExit
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|Path
name|path
parameter_list|)
throws|throws
name|IOException
function_decl|;
comment|/**    * Calls fmt.validateInput(conf) if such a function exists.    */
name|void
name|inputFormatValidateInput
parameter_list|(
name|InputFormat
name|fmt
parameter_list|,
name|JobConf
name|conf
parameter_list|)
throws|throws
name|IOException
function_decl|;
comment|/**    * If JobClient.getCommandLineConfig exists, sets the given    * property/value pair in that Configuration object.    *    * This applies for Hadoop 0.17 through 0.19    */
name|void
name|setTmpFiles
parameter_list|(
name|String
name|prop
parameter_list|,
name|String
name|files
parameter_list|)
function_decl|;
comment|/**    * return the last access time of the given file.    * @param file    * @return last access time. -1 if not supported.    */
name|long
name|getAccessTime
parameter_list|(
name|FileStatus
name|file
parameter_list|)
function_decl|;
comment|/**    * Returns a shim to wrap MiniDFSCluster. This is necessary since this class    * was moved from org.apache.hadoop.dfs to org.apache.hadoop.hdfs    */
name|MiniDFSShim
name|getMiniDfs
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|int
name|numDataNodes
parameter_list|,
name|boolean
name|format
parameter_list|,
name|String
index|[]
name|racks
parameter_list|)
throws|throws
name|IOException
function_decl|;
comment|/**    * Shim around the functions in MiniDFSCluster that Hive uses.    */
specifier|public
interface|interface
name|MiniDFSShim
block|{
name|FileSystem
name|getFileSystem
parameter_list|()
throws|throws
name|IOException
function_decl|;
name|void
name|shutdown
parameter_list|()
throws|throws
name|IOException
function_decl|;
block|}
comment|/**    * We define this function here to make the code compatible between    * hadoop 0.17 and hadoop 0.20.    *    * Hive binary that compiled Text.compareTo(Text) with hadoop 0.20 won't    * work with hadoop 0.17 because in hadoop 0.20, Text.compareTo(Text) is    * implemented in org.apache.hadoop.io.BinaryComparable, and Java compiler    * references that class, which is not available in hadoop 0.17.    */
name|int
name|compareText
parameter_list|(
name|Text
name|a
parameter_list|,
name|Text
name|b
parameter_list|)
function_decl|;
name|CombineFileInputFormatShim
name|getCombineFileInputFormat
parameter_list|()
function_decl|;
name|String
name|getInputFormatClassName
parameter_list|()
function_decl|;
comment|/**    * Wrapper for Configuration.setFloat, which was not introduced    * until 0.20.    */
name|void
name|setFloatConf
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|String
name|varName
parameter_list|,
name|float
name|val
parameter_list|)
function_decl|;
comment|/**    * getTaskJobIDs returns an array of String with two elements. The first    * element is a string representing the task id and the second is a string    * representing the job id. This is necessary as TaskID and TaskAttemptID    * are not supported in Haddop 0.17    */
name|String
index|[]
name|getTaskJobIDs
parameter_list|(
name|TaskCompletionEvent
name|t
parameter_list|)
function_decl|;
name|int
name|createHadoopArchive
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|Path
name|parentDir
parameter_list|,
name|Path
name|destDir
parameter_list|,
name|String
name|archiveName
parameter_list|)
throws|throws
name|Exception
function_decl|;
comment|/**    * Hive uses side effect files exclusively for it's output. It also manages    * the setup/cleanup/commit of output from the hive client. As a result it does    * not need support for the same inside the MR framework    *    * This routine sets the appropriate options to set the output format and any    * options related to bypass setup/cleanup/commit support in the MR framework    */
name|void
name|setNullOutputFormat
parameter_list|(
name|JobConf
name|conf
parameter_list|)
function_decl|;
comment|/**    * Get the UGI that the given job configuration will run as.    *    * In secure versions of Hadoop, this simply returns the current    * access control context's user, ignoring the configuration.    */
specifier|public
name|UserGroupInformation
name|getUGIForConf
parameter_list|(
name|Configuration
name|conf
parameter_list|)
throws|throws
name|LoginException
throws|,
name|IOException
function_decl|;
comment|/**    * Used by metastore server to perform requested rpc in client context.    * @param ugi    * @param pvea    * @throws IOException    * @throws InterruptedException    */
specifier|public
name|void
name|doAs
parameter_list|(
name|UserGroupInformation
name|ugi
parameter_list|,
name|PrivilegedExceptionAction
argument_list|<
name|Void
argument_list|>
name|pvea
parameter_list|)
throws|throws
name|IOException
throws|,
name|InterruptedException
function_decl|;
comment|/**    * Used by metastore server to creates UGI object for a remote user.    * @param userName remote User Name    * @param groupNames group names associated with remote user name    * @return UGI created for the remote user.    */
specifier|public
name|UserGroupInformation
name|createRemoteUser
parameter_list|(
name|String
name|userName
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|groupNames
parameter_list|)
function_decl|;
comment|/**    * Get the short name corresponding to the subject in the passed UGI    *    * In secure versions of Hadoop, this returns the short name (after    * undergoing the translation in the kerberos name rule mapping).    * In unsecure versions of Hadoop, this returns the name of the subject    */
specifier|public
name|String
name|getShortUserName
parameter_list|(
name|UserGroupInformation
name|ugi
parameter_list|)
function_decl|;
comment|/**    * Return true if the Shim is based on Hadoop Security APIs.    */
specifier|public
name|boolean
name|isSecureShimImpl
parameter_list|()
function_decl|;
comment|/**    * Get the string form of the token given a token signature.    * The signature is used as the value of the "service" field in the token for lookup.    * Ref: AbstractDelegationTokenSelector in Hadoop. If there exists such a token    * in the token cache (credential store) of the job, the lookup returns that.    * This is relevant only when running against a "secure" hadoop release    * The method gets hold of the tokens if they are set up by hadoop - this should    * happen on the map/reduce tasks if the client added the tokens into hadoop's    * credential store in the front end during job submission. The method will    * select the hive delegation token among the set of tokens and return the string    * form of it    * @param tokenSignature    * @return the string form of the token found    * @throws IOException    */
name|String
name|getTokenStrForm
parameter_list|(
name|String
name|tokenSignature
parameter_list|)
throws|throws
name|IOException
function_decl|;
enum|enum
name|JobTrackerState
block|{
name|INITIALIZING
block|,
name|RUNNING
block|}
empty_stmt|;
comment|/**    * Convert the ClusterStatus to its Thrift equivalent: JobTrackerState.    * See MAPREDUCE-2455 for why this is a part of the shim.    * @param clusterStatus    * @return the matching JobTrackerState    * @throws Exception if no equivalent JobTrackerState exists    */
specifier|public
name|JobTrackerState
name|getJobTrackerState
parameter_list|(
name|ClusterStatus
name|clusterStatus
parameter_list|)
throws|throws
name|Exception
function_decl|;
specifier|public
name|TaskAttemptContext
name|newTaskAttemptContext
parameter_list|(
name|Configuration
name|conf
parameter_list|,
specifier|final
name|Progressable
name|progressable
parameter_list|)
function_decl|;
specifier|public
name|JobContext
name|newJobContext
parameter_list|(
name|Job
name|job
parameter_list|)
function_decl|;
comment|/**    * InputSplitShim.    *    */
specifier|public
interface|interface
name|InputSplitShim
extends|extends
name|InputSplit
block|{
name|JobConf
name|getJob
parameter_list|()
function_decl|;
name|long
name|getLength
parameter_list|()
function_decl|;
comment|/** Returns an array containing the startoffsets of the files in the split. */
name|long
index|[]
name|getStartOffsets
parameter_list|()
function_decl|;
comment|/** Returns an array containing the lengths of the files in the split. */
name|long
index|[]
name|getLengths
parameter_list|()
function_decl|;
comment|/** Returns the start offset of the i<sup>th</sup> Path. */
name|long
name|getOffset
parameter_list|(
name|int
name|i
parameter_list|)
function_decl|;
comment|/** Returns the length of the i<sup>th</sup> Path. */
name|long
name|getLength
parameter_list|(
name|int
name|i
parameter_list|)
function_decl|;
comment|/** Returns the number of Paths in the split. */
name|int
name|getNumPaths
parameter_list|()
function_decl|;
comment|/** Returns the i<sup>th</sup> Path. */
name|Path
name|getPath
parameter_list|(
name|int
name|i
parameter_list|)
function_decl|;
comment|/** Returns all the Paths in the split. */
name|Path
index|[]
name|getPaths
parameter_list|()
function_decl|;
comment|/** Returns all the Paths where this input-split resides. */
name|String
index|[]
name|getLocations
parameter_list|()
throws|throws
name|IOException
function_decl|;
name|void
name|shrinkSplit
parameter_list|(
name|long
name|length
parameter_list|)
function_decl|;
name|String
name|toString
parameter_list|()
function_decl|;
name|void
name|readFields
parameter_list|(
name|DataInput
name|in
parameter_list|)
throws|throws
name|IOException
function_decl|;
name|void
name|write
parameter_list|(
name|DataOutput
name|out
parameter_list|)
throws|throws
name|IOException
function_decl|;
block|}
comment|/**    * CombineFileInputFormatShim.    *    * @param<K>    * @param<V>    */
interface|interface
name|CombineFileInputFormatShim
parameter_list|<
name|K
parameter_list|,
name|V
parameter_list|>
block|{
name|Path
index|[]
name|getInputPathsShim
parameter_list|(
name|JobConf
name|conf
parameter_list|)
function_decl|;
name|void
name|createPool
parameter_list|(
name|JobConf
name|conf
parameter_list|,
name|PathFilter
modifier|...
name|filters
parameter_list|)
function_decl|;
name|InputSplitShim
index|[]
name|getSplits
parameter_list|(
name|JobConf
name|job
parameter_list|,
name|int
name|numSplits
parameter_list|)
throws|throws
name|IOException
function_decl|;
name|InputSplitShim
name|getInputSplitShim
parameter_list|()
throws|throws
name|IOException
function_decl|;
name|RecordReader
name|getRecordReader
parameter_list|(
name|JobConf
name|job
parameter_list|,
name|InputSplitShim
name|split
parameter_list|,
name|Reporter
name|reporter
parameter_list|,
name|Class
argument_list|<
name|RecordReader
argument_list|<
name|K
argument_list|,
name|V
argument_list|>
argument_list|>
name|rrClass
parameter_list|)
throws|throws
name|IOException
function_decl|;
block|}
block|}
end_interface

end_unit

