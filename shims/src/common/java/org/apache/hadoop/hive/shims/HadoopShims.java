begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/**  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|shims
package|;
end_package

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|DataInput
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|DataOutput
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configuration
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileStatus
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|PathFilter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|Text
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|InputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|InputSplit
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|JobConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|RecordReader
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|Reporter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|RunningJob
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|TaskCompletionEvent
import|;
end_import

begin_comment
comment|/**  * In order to be compatible with multiple versions of Hadoop, all parts  * of the Hadoop interface that are not cross-version compatible are  * encapsulated in an implementation of this class. Users should use  * the ShimLoader class as a factory to obtain an implementation of  * HadoopShims corresponding to the version of Hadoop currently on the  * classpath.  */
end_comment

begin_interface
specifier|public
interface|interface
name|HadoopShims
block|{
comment|/**    * Return true if the current version of Hadoop uses the JobShell for    * command line interpretation.    */
name|boolean
name|usesJobShell
parameter_list|()
function_decl|;
comment|/**    * Return true if the job has not switched to RUNNING state yet    * and is still in PREP state    */
name|boolean
name|isJobPreparing
parameter_list|(
name|RunningJob
name|job
parameter_list|)
throws|throws
name|IOException
function_decl|;
comment|/**    * Calls fs.deleteOnExit(path) if such a function exists.    *    * @return true if the call was successful    */
name|boolean
name|fileSystemDeleteOnExit
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|Path
name|path
parameter_list|)
throws|throws
name|IOException
function_decl|;
comment|/**    * Calls fmt.validateInput(conf) if such a function exists.    */
name|void
name|inputFormatValidateInput
parameter_list|(
name|InputFormat
name|fmt
parameter_list|,
name|JobConf
name|conf
parameter_list|)
throws|throws
name|IOException
function_decl|;
comment|/**    * If JobClient.getCommandLineConfig exists, sets the given    * property/value pair in that Configuration object.    *    * This applies for Hadoop 0.17 through 0.19    */
name|void
name|setTmpFiles
parameter_list|(
name|String
name|prop
parameter_list|,
name|String
name|files
parameter_list|)
function_decl|;
comment|/**    * return the last access time of the given file.    * @param file    * @return last access time. -1 if not supported.    */
name|long
name|getAccessTime
parameter_list|(
name|FileStatus
name|file
parameter_list|)
function_decl|;
comment|/**    * Returns a shim to wrap MiniDFSCluster. This is necessary since this class    * was moved from org.apache.hadoop.dfs to org.apache.hadoop.hdfs    */
name|MiniDFSShim
name|getMiniDfs
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|int
name|numDataNodes
parameter_list|,
name|boolean
name|format
parameter_list|,
name|String
index|[]
name|racks
parameter_list|)
throws|throws
name|IOException
function_decl|;
comment|/**    * Shim around the functions in MiniDFSCluster that Hive uses.    */
specifier|public
interface|interface
name|MiniDFSShim
block|{
name|FileSystem
name|getFileSystem
parameter_list|()
throws|throws
name|IOException
function_decl|;
name|void
name|shutdown
parameter_list|()
throws|throws
name|IOException
function_decl|;
block|}
comment|/**    * We define this function here to make the code compatible between    * hadoop 0.17 and hadoop 0.20.    *    * Hive binary that compiled Text.compareTo(Text) with hadoop 0.20 won't    * work with hadoop 0.17 because in hadoop 0.20, Text.compareTo(Text) is    * implemented in org.apache.hadoop.io.BinaryComparable, and Java compiler    * references that class, which is not available in hadoop 0.17.    */
name|int
name|compareText
parameter_list|(
name|Text
name|a
parameter_list|,
name|Text
name|b
parameter_list|)
function_decl|;
name|CombineFileInputFormatShim
name|getCombineFileInputFormat
parameter_list|()
function_decl|;
name|String
name|getInputFormatClassName
parameter_list|()
function_decl|;
comment|/**    * Wrapper for Configuration.setFloat, which was not introduced    * until 0.20.    */
name|void
name|setFloatConf
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|String
name|varName
parameter_list|,
name|float
name|val
parameter_list|)
function_decl|;
comment|/**    * getTaskJobIDs returns an array of String with two elements. The first    * element is a string representing the task id and the second is a string    * representing the job id. This is necessary as TaskID and TaskAttemptID    * are not supported in Haddop 0.17    */
name|String
index|[]
name|getTaskJobIDs
parameter_list|(
name|TaskCompletionEvent
name|t
parameter_list|)
function_decl|;
comment|/**    * Hive uses side effect files exclusively for it's output. It also manages    * the setup/cleanup/commit of output from the hive client. As a result it does    * not need support for the same inside the MR framework    *    * This routine sets the appropriate options to set the output format and any    * options related to bypass setup/cleanup/commit support in the MR framework    */
name|void
name|setNullOutputFormat
parameter_list|(
name|JobConf
name|conf
parameter_list|)
function_decl|;
comment|/**    * InputSplitShim.    *    */
specifier|public
interface|interface
name|InputSplitShim
extends|extends
name|InputSplit
block|{
name|JobConf
name|getJob
parameter_list|()
function_decl|;
name|long
name|getLength
parameter_list|()
function_decl|;
comment|/** Returns an array containing the startoffsets of the files in the split. */
name|long
index|[]
name|getStartOffsets
parameter_list|()
function_decl|;
comment|/** Returns an array containing the lengths of the files in the split. */
name|long
index|[]
name|getLengths
parameter_list|()
function_decl|;
comment|/** Returns the start offset of the i<sup>th</sup> Path. */
name|long
name|getOffset
parameter_list|(
name|int
name|i
parameter_list|)
function_decl|;
comment|/** Returns the length of the i<sup>th</sup> Path. */
name|long
name|getLength
parameter_list|(
name|int
name|i
parameter_list|)
function_decl|;
comment|/** Returns the number of Paths in the split. */
name|int
name|getNumPaths
parameter_list|()
function_decl|;
comment|/** Returns the i<sup>th</sup> Path. */
name|Path
name|getPath
parameter_list|(
name|int
name|i
parameter_list|)
function_decl|;
comment|/** Returns all the Paths in the split. */
name|Path
index|[]
name|getPaths
parameter_list|()
function_decl|;
comment|/** Returns all the Paths where this input-split resides. */
name|String
index|[]
name|getLocations
parameter_list|()
throws|throws
name|IOException
function_decl|;
name|String
name|toString
parameter_list|()
function_decl|;
name|void
name|readFields
parameter_list|(
name|DataInput
name|in
parameter_list|)
throws|throws
name|IOException
function_decl|;
name|void
name|write
parameter_list|(
name|DataOutput
name|out
parameter_list|)
throws|throws
name|IOException
function_decl|;
block|}
comment|/**    * CombineFileInputFormatShim.    *    * @param<K>    * @param<V>    */
interface|interface
name|CombineFileInputFormatShim
parameter_list|<
name|K
parameter_list|,
name|V
parameter_list|>
block|{
name|Path
index|[]
name|getInputPathsShim
parameter_list|(
name|JobConf
name|conf
parameter_list|)
function_decl|;
name|void
name|createPool
parameter_list|(
name|JobConf
name|conf
parameter_list|,
name|PathFilter
modifier|...
name|filters
parameter_list|)
function_decl|;
name|InputSplitShim
index|[]
name|getSplits
parameter_list|(
name|JobConf
name|job
parameter_list|,
name|int
name|numSplits
parameter_list|)
throws|throws
name|IOException
function_decl|;
name|InputSplitShim
name|getInputSplitShim
parameter_list|()
throws|throws
name|IOException
function_decl|;
name|RecordReader
name|getRecordReader
parameter_list|(
name|JobConf
name|job
parameter_list|,
name|InputSplitShim
name|split
parameter_list|,
name|Reporter
name|reporter
parameter_list|,
name|Class
argument_list|<
name|RecordReader
argument_list|<
name|K
argument_list|,
name|V
argument_list|>
argument_list|>
name|rrClass
parameter_list|)
throws|throws
name|IOException
function_decl|;
block|}
block|}
end_interface

end_unit

