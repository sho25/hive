begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/**  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
package|;
end_package

begin_import
import|import
name|java
operator|.
name|net
operator|.
name|URI
import|;
end_import

begin_import
import|import
name|java
operator|.
name|net
operator|.
name|URISyntaxException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Iterator
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|LinkedHashMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|LinkedList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|Log
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|LogFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|Warehouse
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|FieldSchema
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|MetaException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|Hive
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|HiveException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|Partition
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|Table
import|;
end_import

begin_comment
comment|/**  * ArchiveUtils.  *  */
end_comment

begin_class
annotation|@
name|SuppressWarnings
argument_list|(
literal|"nls"
argument_list|)
specifier|public
specifier|final
class|class
name|ArchiveUtils
block|{
specifier|private
specifier|static
specifier|final
name|Log
name|LOG
init|=
name|LogFactory
operator|.
name|getLog
argument_list|(
name|ArchiveUtils
operator|.
name|class
operator|.
name|getName
argument_list|()
argument_list|)
decl_stmt|;
specifier|public
specifier|static
name|String
name|ARCHIVING_LEVEL
init|=
literal|"archiving_level"
decl_stmt|;
comment|/**    * PartSpecInfo keeps fields and values extracted from partial partition info    * which is prefix of the full info.    */
specifier|public
specifier|static
class|class
name|PartSpecInfo
block|{
specifier|public
name|List
argument_list|<
name|FieldSchema
argument_list|>
name|fields
decl_stmt|;
specifier|public
name|List
argument_list|<
name|String
argument_list|>
name|values
decl_stmt|;
specifier|private
name|PartSpecInfo
parameter_list|(
name|List
argument_list|<
name|FieldSchema
argument_list|>
name|fields
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|values
parameter_list|)
block|{
name|this
operator|.
name|fields
operator|=
name|fields
expr_stmt|;
name|this
operator|.
name|values
operator|=
name|values
expr_stmt|;
block|}
comment|/**      * Extract partial prefix specification from table and key-value map      *      * @param tbl table in which partition is      * @param partSpec specification of partition      * @return extracted specification      */
specifier|static
specifier|public
name|PartSpecInfo
name|create
parameter_list|(
name|Table
name|tbl
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|partSpec
parameter_list|)
throws|throws
name|HiveException
block|{
comment|// we have to check if we receive prefix of partition keys so in table
comment|// scheme like table/ds=2011-01-02/hr=13/
comment|// ARCHIVE PARTITION (ds='2011-01-02') will work and
comment|// ARCHIVE PARTITION(hr='13') won't
name|List
argument_list|<
name|FieldSchema
argument_list|>
name|prefixFields
init|=
operator|new
name|ArrayList
argument_list|<
name|FieldSchema
argument_list|>
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|prefixValues
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|FieldSchema
argument_list|>
name|partCols
init|=
name|tbl
operator|.
name|getPartCols
argument_list|()
decl_stmt|;
name|Iterator
argument_list|<
name|String
argument_list|>
name|itrPsKeys
init|=
name|partSpec
operator|.
name|keySet
argument_list|()
operator|.
name|iterator
argument_list|()
decl_stmt|;
for|for
control|(
name|FieldSchema
name|fs
range|:
name|partCols
control|)
block|{
if|if
condition|(
operator|!
name|itrPsKeys
operator|.
name|hasNext
argument_list|()
condition|)
block|{
break|break;
block|}
if|if
condition|(
operator|!
name|itrPsKeys
operator|.
name|next
argument_list|()
operator|.
name|toLowerCase
argument_list|()
operator|.
name|equals
argument_list|(
name|fs
operator|.
name|getName
argument_list|()
operator|.
name|toLowerCase
argument_list|()
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Invalid partition specifiation: "
operator|+
name|partSpec
argument_list|)
throw|;
block|}
name|prefixFields
operator|.
name|add
argument_list|(
name|fs
argument_list|)
expr_stmt|;
name|prefixValues
operator|.
name|add
argument_list|(
name|partSpec
operator|.
name|get
argument_list|(
name|fs
operator|.
name|getName
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
return|return
operator|new
name|PartSpecInfo
argument_list|(
name|prefixFields
argument_list|,
name|prefixValues
argument_list|)
return|;
block|}
comment|/**      * Creates path where partitions matching prefix should lie in filesystem      * @param tbl table in which partition is      * @return expected location of partitions matching prefix in filesystem      */
specifier|public
name|Path
name|createPath
parameter_list|(
name|Table
name|tbl
parameter_list|)
throws|throws
name|HiveException
block|{
name|String
name|prefixSubdir
decl_stmt|;
try|try
block|{
name|prefixSubdir
operator|=
name|Warehouse
operator|.
name|makePartName
argument_list|(
name|fields
argument_list|,
name|values
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|MetaException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Unable to get partitions directories prefix"
argument_list|,
name|e
argument_list|)
throw|;
block|}
name|URI
name|tableDir
init|=
name|tbl
operator|.
name|getDataLocation
argument_list|()
decl_stmt|;
if|if
condition|(
name|tableDir
operator|==
literal|null
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Table has no location set"
argument_list|)
throw|;
block|}
return|return
operator|new
name|Path
argument_list|(
name|tableDir
operator|.
name|toString
argument_list|()
argument_list|,
name|prefixSubdir
argument_list|)
return|;
block|}
comment|/**      * Generates name for prefix partial partition specification.      */
specifier|public
name|String
name|getName
parameter_list|()
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|Warehouse
operator|.
name|makePartName
argument_list|(
name|fields
argument_list|,
name|values
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|MetaException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Unable to create partial name"
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
block|}
comment|/**    * HarPathHelper helps to create har:/ URIs for locations inside of archive.    */
specifier|public
specifier|static
class|class
name|HarPathHelper
block|{
name|boolean
name|parentSettable
decl_stmt|;
specifier|private
specifier|final
name|URI
name|base
decl_stmt|,
name|originalBase
decl_stmt|;
comment|/**      * Creates helper for archive.      * @param archive absolute location of archive in underlying filesystem      * @param originalBase directory for which Hadoop archive was created      */
specifier|public
name|HarPathHelper
parameter_list|(
name|HiveConf
name|hconf
parameter_list|,
name|URI
name|archive
parameter_list|,
name|URI
name|originalBase
parameter_list|)
throws|throws
name|HiveException
block|{
name|parentSettable
operator|=
name|hconf
operator|.
name|getBoolVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEHARPARENTDIRSETTABLE
argument_list|)
expr_stmt|;
name|this
operator|.
name|originalBase
operator|=
name|addSlash
argument_list|(
name|originalBase
argument_list|)
expr_stmt|;
name|String
name|parentHost
init|=
name|archive
operator|.
name|getHost
argument_list|()
decl_stmt|;
name|String
name|harHost
init|=
literal|null
decl_stmt|;
if|if
condition|(
name|parentHost
operator|==
literal|null
condition|)
block|{
name|harHost
operator|=
name|archive
operator|.
name|getScheme
argument_list|()
expr_stmt|;
block|}
else|else
block|{
name|harHost
operator|=
name|archive
operator|.
name|getScheme
argument_list|()
operator|+
literal|"-"
operator|+
name|parentHost
expr_stmt|;
block|}
comment|// have to make sure there's slash after .har, otherwise resolve doesn't work
name|String
name|path
init|=
name|addSlash
argument_list|(
name|archive
operator|.
name|getPath
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|path
operator|.
name|endsWith
argument_list|(
literal|".har/"
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"HAR archive path must end with .har"
argument_list|)
throw|;
block|}
comment|// harUri is used to access the partition's files, which are in the archive
comment|// The format of the RI is something like:
comment|// har://underlyingfsscheme-host:port/archivepath
try|try
block|{
name|base
operator|=
operator|new
name|URI
argument_list|(
literal|"har"
argument_list|,
name|archive
operator|.
name|getUserInfo
argument_list|()
argument_list|,
name|harHost
argument_list|,
name|archive
operator|.
name|getPort
argument_list|()
argument_list|,
name|path
argument_list|,
name|archive
operator|.
name|getQuery
argument_list|()
argument_list|,
name|archive
operator|.
name|getFragment
argument_list|()
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|URISyntaxException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Couldn't create har URI from archive URI"
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**      * Creates har URI for file/directory that was put there when creating HAR.      *      *  With older versions of Hadoop, archiving a directory would produce      *  the same directory structure, reflecting absoulute paths.      *  If you created myArchive.har of /tmp/myDir the files in /tmp/myDir      *  will be located under myArchive.har/tmp/myDir/*      *      *  With newer versions, the parent directory can be specified. Assuming      *  the parent directory was set to /tmp/myDir when creating the archive,      *  the files can be found under myArchive.har/*      *      *  This is why originalBase is argument - with new versions we can      *  relativize URI, in older we keep absolute one.      *      * @param original file/directory path      * @return absolute HAR uri      */
specifier|public
name|URI
name|getHarUri
parameter_list|(
name|URI
name|original
parameter_list|)
throws|throws
name|HiveException
block|{
name|URI
name|relative
init|=
literal|null
decl_stmt|;
if|if
condition|(
operator|!
name|parentSettable
condition|)
block|{
name|String
name|dirInArchive
init|=
name|original
operator|.
name|getPath
argument_list|()
decl_stmt|;
if|if
condition|(
name|dirInArchive
operator|.
name|length
argument_list|()
operator|>
literal|1
operator|&&
name|dirInArchive
operator|.
name|charAt
argument_list|(
literal|0
argument_list|)
operator|==
literal|'/'
condition|)
block|{
name|dirInArchive
operator|=
name|dirInArchive
operator|.
name|substring
argument_list|(
literal|1
argument_list|)
expr_stmt|;
block|}
try|try
block|{
name|relative
operator|=
operator|new
name|URI
argument_list|(
literal|null
argument_list|,
literal|null
argument_list|,
name|dirInArchive
argument_list|,
literal|null
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|URISyntaxException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Couldn't create har URI for location"
argument_list|)
throw|;
block|}
comment|// relative URI with path only
block|}
else|else
block|{
name|relative
operator|=
name|originalBase
operator|.
name|relativize
argument_list|(
name|original
argument_list|)
expr_stmt|;
if|if
condition|(
name|relative
operator|.
name|isAbsolute
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Unable to relativize URI"
argument_list|)
throw|;
block|}
block|}
return|return
name|base
operator|.
name|resolve
argument_list|(
name|relative
argument_list|)
return|;
block|}
block|}
specifier|public
specifier|static
name|String
name|addSlash
parameter_list|(
name|String
name|s
parameter_list|)
block|{
return|return
name|s
operator|.
name|endsWith
argument_list|(
literal|"/"
argument_list|)
condition|?
name|s
else|:
name|s
operator|+
literal|"/"
return|;
block|}
comment|/**    * Makes sure, that URI points to directory by adding slash to it.    * Useful in relativizing URIs.    */
specifier|public
specifier|static
name|URI
name|addSlash
parameter_list|(
name|URI
name|u
parameter_list|)
throws|throws
name|HiveException
block|{
if|if
condition|(
name|u
operator|.
name|getPath
argument_list|()
operator|.
name|endsWith
argument_list|(
literal|"/"
argument_list|)
condition|)
block|{
return|return
name|u
return|;
block|}
else|else
block|{
try|try
block|{
return|return
operator|new
name|URI
argument_list|(
name|u
operator|.
name|getScheme
argument_list|()
argument_list|,
name|u
operator|.
name|getAuthority
argument_list|()
argument_list|,
name|u
operator|.
name|getPath
argument_list|()
operator|+
literal|"/"
argument_list|,
name|u
operator|.
name|getQuery
argument_list|()
argument_list|,
name|u
operator|.
name|getFragment
argument_list|()
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|URISyntaxException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Couldn't append slash to a URI"
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
block|}
comment|/**    * Determines whether a partition has been archived    *    * @param p    * @return is it archived?    */
specifier|public
specifier|static
name|boolean
name|isArchived
parameter_list|(
name|Partition
name|p
parameter_list|)
block|{
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|params
init|=
name|p
operator|.
name|getParameters
argument_list|()
decl_stmt|;
if|if
condition|(
literal|"true"
operator|.
name|equalsIgnoreCase
argument_list|(
name|params
operator|.
name|get
argument_list|(
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Constants
operator|.
name|IS_ARCHIVED
argument_list|)
argument_list|)
condition|)
block|{
return|return
literal|true
return|;
block|}
else|else
block|{
return|return
literal|false
return|;
block|}
block|}
comment|/**    * Returns archiving level, which is how many fields were set in partial    * specification ARCHIVE was run for    */
specifier|public
specifier|static
name|int
name|getArchivingLevel
parameter_list|(
name|Partition
name|p
parameter_list|)
throws|throws
name|HiveException
block|{
if|if
condition|(
operator|!
name|isArchived
argument_list|(
name|p
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Getting level of unarchived partition"
argument_list|)
throw|;
block|}
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|params
init|=
name|p
operator|.
name|getParameters
argument_list|()
decl_stmt|;
name|String
name|lv
init|=
name|params
operator|.
name|get
argument_list|(
name|ArchiveUtils
operator|.
name|ARCHIVING_LEVEL
argument_list|)
decl_stmt|;
if|if
condition|(
name|lv
operator|!=
literal|null
condition|)
block|{
return|return
name|Integer
operator|.
name|parseInt
argument_list|(
name|lv
argument_list|)
return|;
block|}
else|else
block|{
comment|// partitions archived before introducing multiple archiving
return|return
name|p
operator|.
name|getValues
argument_list|()
operator|.
name|size
argument_list|()
return|;
block|}
block|}
comment|/**    * Get a prefix of the given parition's string representation. The sencond    * argument, level, is used for the prefix length. For example, partition    * (ds='2010-01-01', hr='00', min='00'), level 1 will reture 'ds=2010-01-01',    * and level 2 will return 'ds=2010-01-01/hr=00'.    *    * @param p    *          partition object    * @param level    *          level for prefix depth    * @return prefix of partition's string representation    * @throws HiveException    */
specifier|public
specifier|static
name|String
name|getPartialName
parameter_list|(
name|Partition
name|p
parameter_list|,
name|int
name|level
parameter_list|)
throws|throws
name|HiveException
block|{
name|List
argument_list|<
name|FieldSchema
argument_list|>
name|ffields
init|=
name|p
operator|.
name|getTable
argument_list|()
operator|.
name|getPartCols
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|FieldSchema
argument_list|>
name|fields
init|=
operator|new
name|ArrayList
argument_list|<
name|FieldSchema
argument_list|>
argument_list|(
name|level
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|fvalues
init|=
name|p
operator|.
name|getValues
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|values
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|(
name|level
argument_list|)
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|level
condition|;
name|i
operator|++
control|)
block|{
name|FieldSchema
name|fs
init|=
name|ffields
operator|.
name|get
argument_list|(
name|i
argument_list|)
decl_stmt|;
name|String
name|s
init|=
name|fvalues
operator|.
name|get
argument_list|(
name|i
argument_list|)
decl_stmt|;
name|fields
operator|.
name|add
argument_list|(
name|fs
argument_list|)
expr_stmt|;
name|values
operator|.
name|add
argument_list|(
name|s
argument_list|)
expr_stmt|;
block|}
try|try
block|{
return|return
name|Warehouse
operator|.
name|makePartName
argument_list|(
name|fields
argument_list|,
name|values
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|MetaException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Wasn't able to generate name"
operator|+
literal|" for partial specification"
argument_list|)
throw|;
block|}
block|}
comment|/**    * Determines if one can insert into partition(s), or there's a conflict with    * archive. It can be because partition is itself archived or it is to be    * created inside existing archive. The second case is when partition doesn't    * exist yet, but it would be inside of an archive if it existed. This one is    * quite tricky to check, we need to find at least one partition inside of    * the parent directory. If it is archived and archiving level tells that    * the archival was done of directory partition is in it means we cannot    * insert; otherwise we can.    * This method works both for full specifications and partial ones - in second    * case it checks if any partition that could possibly match such    * specification is inside archive.    *    * @param db - Hive object    * @param tbl - table where partition is    * @param partSpec - partition specification with possible nulls in case of    * dynamic partiton inserts    * @return null if partition can be inserted, string with colliding archive    * name when it can't    * @throws HiveException    */
specifier|public
specifier|static
name|String
name|conflictingArchiveNameOrNull
parameter_list|(
name|Hive
name|db
parameter_list|,
name|Table
name|tbl
parameter_list|,
name|LinkedHashMap
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|partSpec
parameter_list|)
throws|throws
name|HiveException
block|{
name|List
argument_list|<
name|FieldSchema
argument_list|>
name|partKeys
init|=
name|tbl
operator|.
name|getPartitionKeys
argument_list|()
decl_stmt|;
name|int
name|partSpecLevel
init|=
literal|0
decl_stmt|;
for|for
control|(
name|FieldSchema
name|partKey
range|:
name|partKeys
control|)
block|{
if|if
condition|(
operator|!
name|partSpec
operator|.
name|containsKey
argument_list|(
name|partKey
operator|.
name|getName
argument_list|()
argument_list|)
condition|)
block|{
break|break;
block|}
name|partSpecLevel
operator|++
expr_stmt|;
block|}
if|if
condition|(
name|partSpecLevel
operator|!=
name|partSpec
operator|.
name|size
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"partspec "
operator|+
name|partSpec
operator|+
literal|" is wrong for table "
operator|+
name|tbl
operator|.
name|getTableName
argument_list|()
argument_list|)
throw|;
block|}
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|spec
init|=
operator|new
name|HashMap
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
argument_list|(
name|partSpec
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|reversedKeys
init|=
operator|new
name|LinkedList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|FieldSchema
name|fs
range|:
name|tbl
operator|.
name|getPartCols
argument_list|()
control|)
block|{
if|if
condition|(
name|spec
operator|.
name|containsKey
argument_list|(
name|fs
operator|.
name|getName
argument_list|()
argument_list|)
condition|)
block|{
name|reversedKeys
operator|.
name|add
argument_list|(
literal|0
argument_list|,
name|fs
operator|.
name|getName
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
for|for
control|(
name|String
name|rk
range|:
name|reversedKeys
control|)
block|{
name|List
argument_list|<
name|Partition
argument_list|>
name|parts
init|=
name|db
operator|.
name|getPartitions
argument_list|(
name|tbl
argument_list|,
name|spec
argument_list|,
operator|(
name|short
operator|)
literal|1
argument_list|)
decl_stmt|;
if|if
condition|(
name|parts
operator|.
name|size
argument_list|()
operator|!=
literal|0
condition|)
block|{
name|Partition
name|p
init|=
name|parts
operator|.
name|get
argument_list|(
literal|0
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|isArchived
argument_list|(
name|p
argument_list|)
condition|)
block|{
comment|// if archiving was done at this or at upper level, every matched
comment|// partition would be archived, so it not being archived means
comment|// no archiving was done neither at this nor at upper level
return|return
literal|null
return|;
block|}
elseif|else
if|if
condition|(
name|getArchivingLevel
argument_list|(
name|p
argument_list|)
operator|>
name|spec
operator|.
name|size
argument_list|()
condition|)
block|{
comment|// if archiving was done at this or at upper level its level
comment|// would be lesser or equal to specification size
comment|// it is not, which means no archiving at this or upper level
return|return
literal|null
return|;
block|}
else|else
block|{
return|return
name|getPartialName
argument_list|(
name|p
argument_list|,
name|getArchivingLevel
argument_list|(
name|p
argument_list|)
argument_list|)
return|;
block|}
block|}
name|spec
operator|.
name|remove
argument_list|(
name|rk
argument_list|)
expr_stmt|;
block|}
return|return
literal|null
return|;
block|}
block|}
end_class

end_unit

