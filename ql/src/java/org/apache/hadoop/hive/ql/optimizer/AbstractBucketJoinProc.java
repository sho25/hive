begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/**  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.You may obtain a copy of the License at  *  * http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|optimizer
package|;
end_package

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|net
operator|.
name|URI
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Arrays
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Collections
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Iterator
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|LinkedHashMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
operator|.
name|Entry
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Stack
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|Log
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|LogFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileStatus
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|ErrorMsg
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|MapJoinOperator
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|Operator
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|TableScanOperator
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|lib
operator|.
name|Node
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|lib
operator|.
name|NodeProcessor
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|lib
operator|.
name|NodeProcessorCtx
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|HiveException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|Partition
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|Table
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|optimizer
operator|.
name|ppr
operator|.
name|PartitionPruner
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|parse
operator|.
name|ParseContext
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|parse
operator|.
name|PrunedPartitionList
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|parse
operator|.
name|QB
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|parse
operator|.
name|QBJoinTree
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|parse
operator|.
name|SemanticException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|parse
operator|.
name|TableAccessAnalyzer
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|ExprNodeColumnDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|ExprNodeDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|MapJoinDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|OperatorDesc
import|;
end_import

begin_comment
comment|/**  * this transformation does bucket map join optimization.  */
end_comment

begin_class
specifier|abstract
specifier|public
class|class
name|AbstractBucketJoinProc
implements|implements
name|NodeProcessor
block|{
specifier|private
specifier|static
specifier|final
name|Log
name|LOG
init|=
name|LogFactory
operator|.
name|getLog
argument_list|(
name|AbstractBucketJoinProc
operator|.
name|class
operator|.
name|getName
argument_list|()
argument_list|)
decl_stmt|;
specifier|protected
name|ParseContext
name|pGraphContext
decl_stmt|;
specifier|public
name|AbstractBucketJoinProc
parameter_list|(
name|ParseContext
name|pGraphContext
parameter_list|)
block|{
name|this
operator|.
name|pGraphContext
operator|=
name|pGraphContext
expr_stmt|;
block|}
specifier|public
name|AbstractBucketJoinProc
parameter_list|()
block|{   }
annotation|@
name|Override
specifier|abstract
specifier|public
name|Object
name|process
parameter_list|(
name|Node
name|nd
parameter_list|,
name|Stack
argument_list|<
name|Node
argument_list|>
name|stack
parameter_list|,
name|NodeProcessorCtx
name|procCtx
parameter_list|,
name|Object
modifier|...
name|nodeOutputs
parameter_list|)
throws|throws
name|SemanticException
function_decl|;
specifier|private
specifier|static
name|List
argument_list|<
name|String
argument_list|>
name|getBucketFilePathsOfPartition
parameter_list|(
name|URI
name|location
parameter_list|,
name|ParseContext
name|pGraphContext
parameter_list|)
throws|throws
name|SemanticException
block|{
name|List
argument_list|<
name|String
argument_list|>
name|fileNames
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
try|try
block|{
name|FileSystem
name|fs
init|=
name|FileSystem
operator|.
name|get
argument_list|(
name|location
argument_list|,
name|pGraphContext
operator|.
name|getConf
argument_list|()
argument_list|)
decl_stmt|;
name|FileStatus
index|[]
name|files
init|=
name|fs
operator|.
name|listStatus
argument_list|(
operator|new
name|Path
argument_list|(
name|location
operator|.
name|toString
argument_list|()
argument_list|)
argument_list|)
decl_stmt|;
if|if
condition|(
name|files
operator|!=
literal|null
condition|)
block|{
for|for
control|(
name|FileStatus
name|file
range|:
name|files
control|)
block|{
name|fileNames
operator|.
name|add
argument_list|(
name|file
operator|.
name|getPath
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|SemanticException
argument_list|(
name|e
argument_list|)
throw|;
block|}
return|return
name|fileNames
return|;
block|}
comment|// This function checks whether all bucketing columns are also in join keys and are in same order
specifier|private
name|boolean
name|checkBucketColumns
parameter_list|(
name|List
argument_list|<
name|String
argument_list|>
name|bucketColumns
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|joinKeys
parameter_list|,
name|Integer
index|[]
name|joinKeyOrders
parameter_list|)
block|{
if|if
condition|(
name|joinKeys
operator|==
literal|null
operator|||
name|bucketColumns
operator|==
literal|null
operator|||
name|bucketColumns
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
return|return
literal|false
return|;
block|}
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|joinKeys
operator|.
name|size
argument_list|()
condition|;
name|i
operator|++
control|)
block|{
name|int
name|index
init|=
name|bucketColumns
operator|.
name|indexOf
argument_list|(
name|joinKeys
operator|.
name|get
argument_list|(
name|i
argument_list|)
argument_list|)
decl_stmt|;
if|if
condition|(
name|joinKeyOrders
index|[
name|i
index|]
operator|!=
literal|null
operator|&&
name|joinKeyOrders
index|[
name|i
index|]
operator|!=
name|index
condition|)
block|{
return|return
literal|false
return|;
block|}
name|joinKeyOrders
index|[
name|i
index|]
operator|=
name|index
expr_stmt|;
block|}
comment|// Check if the join columns contains all bucket columns.
comment|// If a table is bucketized on column B, but the join key is A and B,
comment|// it is easy to see joining on different buckets yield empty results.
return|return
name|joinKeys
operator|.
name|containsAll
argument_list|(
name|bucketColumns
argument_list|)
return|;
block|}
specifier|private
name|boolean
name|checkNumberOfBucketsAgainstBigTable
parameter_list|(
name|Map
argument_list|<
name|String
argument_list|,
name|List
argument_list|<
name|Integer
argument_list|>
argument_list|>
name|tblAliasToNumberOfBucketsInEachPartition
parameter_list|,
name|int
name|numberOfBucketsInPartitionOfBigTable
parameter_list|)
block|{
for|for
control|(
name|List
argument_list|<
name|Integer
argument_list|>
name|bucketNums
range|:
name|tblAliasToNumberOfBucketsInEachPartition
operator|.
name|values
argument_list|()
control|)
block|{
for|for
control|(
name|int
name|nxt
range|:
name|bucketNums
control|)
block|{
name|boolean
name|ok
init|=
operator|(
name|nxt
operator|>=
name|numberOfBucketsInPartitionOfBigTable
operator|)
condition|?
name|nxt
operator|%
name|numberOfBucketsInPartitionOfBigTable
operator|==
literal|0
else|:
name|numberOfBucketsInPartitionOfBigTable
operator|%
name|nxt
operator|==
literal|0
decl_stmt|;
if|if
condition|(
operator|!
name|ok
condition|)
block|{
return|return
literal|false
return|;
block|}
block|}
block|}
return|return
literal|true
return|;
block|}
specifier|protected
name|boolean
name|canConvertMapJoinToBucketMapJoin
parameter_list|(
name|MapJoinOperator
name|mapJoinOp
parameter_list|,
name|ParseContext
name|pGraphContext
parameter_list|,
name|BucketJoinProcCtx
name|context
parameter_list|)
throws|throws
name|SemanticException
block|{
name|QBJoinTree
name|joinCtx
init|=
name|this
operator|.
name|pGraphContext
operator|.
name|getMapJoinContext
argument_list|()
operator|.
name|get
argument_list|(
name|mapJoinOp
argument_list|)
decl_stmt|;
if|if
condition|(
name|joinCtx
operator|==
literal|null
condition|)
block|{
return|return
literal|false
return|;
block|}
name|List
argument_list|<
name|String
argument_list|>
name|joinAliases
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
name|String
index|[]
name|srcs
init|=
name|joinCtx
operator|.
name|getBaseSrc
argument_list|()
decl_stmt|;
name|String
index|[]
name|left
init|=
name|joinCtx
operator|.
name|getLeftAliases
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|mapAlias
init|=
name|joinCtx
operator|.
name|getMapAliases
argument_list|()
decl_stmt|;
name|String
name|baseBigAlias
init|=
literal|null
decl_stmt|;
for|for
control|(
name|String
name|s
range|:
name|left
control|)
block|{
if|if
condition|(
name|s
operator|!=
literal|null
condition|)
block|{
name|String
name|subQueryAlias
init|=
name|QB
operator|.
name|getAppendedAliasFromId
argument_list|(
name|joinCtx
operator|.
name|getId
argument_list|()
argument_list|,
name|s
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|joinAliases
operator|.
name|contains
argument_list|(
name|subQueryAlias
argument_list|)
condition|)
block|{
name|joinAliases
operator|.
name|add
argument_list|(
name|subQueryAlias
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|mapAlias
operator|.
name|contains
argument_list|(
name|s
argument_list|)
condition|)
block|{
name|baseBigAlias
operator|=
name|subQueryAlias
expr_stmt|;
block|}
block|}
block|}
block|}
for|for
control|(
name|String
name|s
range|:
name|srcs
control|)
block|{
if|if
condition|(
name|s
operator|!=
literal|null
condition|)
block|{
name|String
name|subQueryAlias
init|=
name|QB
operator|.
name|getAppendedAliasFromId
argument_list|(
name|joinCtx
operator|.
name|getId
argument_list|()
argument_list|,
name|s
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|joinAliases
operator|.
name|contains
argument_list|(
name|subQueryAlias
argument_list|)
condition|)
block|{
name|joinAliases
operator|.
name|add
argument_list|(
name|subQueryAlias
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|mapAlias
operator|.
name|contains
argument_list|(
name|s
argument_list|)
condition|)
block|{
name|baseBigAlias
operator|=
name|subQueryAlias
expr_stmt|;
block|}
block|}
block|}
block|}
name|Map
argument_list|<
name|Byte
argument_list|,
name|List
argument_list|<
name|ExprNodeDesc
argument_list|>
argument_list|>
name|keysMap
init|=
name|mapJoinOp
operator|.
name|getConf
argument_list|()
operator|.
name|getKeys
argument_list|()
decl_stmt|;
return|return
name|checkConvertBucketMapJoin
argument_list|(
name|pGraphContext
argument_list|,
name|context
argument_list|,
name|joinCtx
argument_list|,
name|keysMap
argument_list|,
name|baseBigAlias
argument_list|,
name|joinAliases
argument_list|)
return|;
block|}
comment|/*    * Can this mapjoin be converted to a bucketed mapjoin ?    * The following checks are performed:    * a. The join columns contains all the bucket columns.    * b. The join keys are not transformed in the sub-query.    * c. All partitions contain the expected number of files (number of buckets).    * d. The number of buckets in the big table can be divided by no of buckets in small tables.    */
specifier|protected
name|boolean
name|checkConvertBucketMapJoin
parameter_list|(
name|ParseContext
name|pGraphContext
parameter_list|,
name|BucketJoinProcCtx
name|context
parameter_list|,
name|QBJoinTree
name|joinCtx
parameter_list|,
name|Map
argument_list|<
name|Byte
argument_list|,
name|List
argument_list|<
name|ExprNodeDesc
argument_list|>
argument_list|>
name|keysMap
parameter_list|,
name|String
name|baseBigAlias
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|joinAliases
parameter_list|)
throws|throws
name|SemanticException
block|{
name|LinkedHashMap
argument_list|<
name|String
argument_list|,
name|List
argument_list|<
name|Integer
argument_list|>
argument_list|>
name|tblAliasToNumberOfBucketsInEachPartition
init|=
operator|new
name|LinkedHashMap
argument_list|<
name|String
argument_list|,
name|List
argument_list|<
name|Integer
argument_list|>
argument_list|>
argument_list|()
decl_stmt|;
name|LinkedHashMap
argument_list|<
name|String
argument_list|,
name|List
argument_list|<
name|List
argument_list|<
name|String
argument_list|>
argument_list|>
argument_list|>
name|tblAliasToBucketedFilePathsInEachPartition
init|=
operator|new
name|LinkedHashMap
argument_list|<
name|String
argument_list|,
name|List
argument_list|<
name|List
argument_list|<
name|String
argument_list|>
argument_list|>
argument_list|>
argument_list|()
decl_stmt|;
name|HashMap
argument_list|<
name|String
argument_list|,
name|Operator
argument_list|<
name|?
extends|extends
name|OperatorDesc
argument_list|>
argument_list|>
name|topOps
init|=
name|pGraphContext
operator|.
name|getTopOps
argument_list|()
decl_stmt|;
name|Map
argument_list|<
name|TableScanOperator
argument_list|,
name|Table
argument_list|>
name|topToTable
init|=
name|pGraphContext
operator|.
name|getTopToTable
argument_list|()
decl_stmt|;
comment|// (partition to bucket file names) and (partition to bucket number) for
comment|// the big table;
name|LinkedHashMap
argument_list|<
name|Partition
argument_list|,
name|List
argument_list|<
name|String
argument_list|>
argument_list|>
name|bigTblPartsToBucketFileNames
init|=
operator|new
name|LinkedHashMap
argument_list|<
name|Partition
argument_list|,
name|List
argument_list|<
name|String
argument_list|>
argument_list|>
argument_list|()
decl_stmt|;
name|LinkedHashMap
argument_list|<
name|Partition
argument_list|,
name|Integer
argument_list|>
name|bigTblPartsToBucketNumber
init|=
operator|new
name|LinkedHashMap
argument_list|<
name|Partition
argument_list|,
name|Integer
argument_list|>
argument_list|()
decl_stmt|;
name|Integer
index|[]
name|joinKeyOrder
init|=
literal|null
decl_stmt|;
comment|// accessing order of join cols to bucket cols, should be same
name|boolean
name|bigTablePartitioned
init|=
literal|true
decl_stmt|;
for|for
control|(
name|int
name|index
init|=
literal|0
init|;
name|index
operator|<
name|joinAliases
operator|.
name|size
argument_list|()
condition|;
name|index
operator|++
control|)
block|{
name|String
name|alias
init|=
name|joinAliases
operator|.
name|get
argument_list|(
name|index
argument_list|)
decl_stmt|;
name|Operator
argument_list|<
name|?
extends|extends
name|OperatorDesc
argument_list|>
name|topOp
init|=
name|joinCtx
operator|.
name|getAliasToOpInfo
argument_list|()
operator|.
name|get
argument_list|(
name|alias
argument_list|)
decl_stmt|;
comment|// The alias may not be present in case of a sub-query
if|if
condition|(
name|topOp
operator|==
literal|null
condition|)
block|{
return|return
literal|false
return|;
block|}
name|List
argument_list|<
name|String
argument_list|>
name|keys
init|=
name|toColumns
argument_list|(
name|keysMap
operator|.
name|get
argument_list|(
operator|(
name|byte
operator|)
name|index
argument_list|)
argument_list|)
decl_stmt|;
if|if
condition|(
name|keys
operator|==
literal|null
operator|||
name|keys
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
return|return
literal|false
return|;
block|}
name|int
name|oldKeySize
init|=
name|keys
operator|.
name|size
argument_list|()
decl_stmt|;
name|TableScanOperator
name|tso
init|=
name|TableAccessAnalyzer
operator|.
name|genRootTableScan
argument_list|(
name|topOp
argument_list|,
name|keys
argument_list|)
decl_stmt|;
if|if
condition|(
name|tso
operator|==
literal|null
condition|)
block|{
comment|// We cannot get to root TableScan operator, likely because there is a join or group-by
comment|// between topOp and root TableScan operator. We don't handle that case, and simply return
return|return
literal|false
return|;
block|}
comment|// For nested sub-queries, the alias mapping is not maintained in QB currently.
if|if
condition|(
name|topOps
operator|.
name|containsValue
argument_list|(
name|tso
argument_list|)
condition|)
block|{
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|String
argument_list|,
name|Operator
argument_list|<
name|?
extends|extends
name|OperatorDesc
argument_list|>
argument_list|>
name|topOpEntry
range|:
name|topOps
operator|.
name|entrySet
argument_list|()
control|)
block|{
if|if
condition|(
name|topOpEntry
operator|.
name|getValue
argument_list|()
operator|==
name|tso
condition|)
block|{
name|String
name|newAlias
init|=
name|topOpEntry
operator|.
name|getKey
argument_list|()
decl_stmt|;
name|joinAliases
operator|.
name|set
argument_list|(
name|index
argument_list|,
name|newAlias
argument_list|)
expr_stmt|;
if|if
condition|(
name|baseBigAlias
operator|.
name|equals
argument_list|(
name|alias
argument_list|)
condition|)
block|{
name|baseBigAlias
operator|=
name|newAlias
expr_stmt|;
block|}
name|alias
operator|=
name|newAlias
expr_stmt|;
break|break;
block|}
block|}
block|}
else|else
block|{
comment|// Ideally, this should never happen, and this should be an assert.
return|return
literal|false
return|;
block|}
comment|// The join keys cannot be transformed in the sub-query currently.
comment|// TableAccessAnalyzer.genRootTableScan will only return the base table scan
comment|// if the join keys are constants or a column. Even a simple cast of the join keys
comment|// will result in a null table scan operator. In case of constant join keys, they would
comment|// be removed, and the size before and after the genRootTableScan will be different.
if|if
condition|(
name|keys
operator|.
name|size
argument_list|()
operator|!=
name|oldKeySize
condition|)
block|{
return|return
literal|false
return|;
block|}
if|if
condition|(
name|joinKeyOrder
operator|==
literal|null
condition|)
block|{
name|joinKeyOrder
operator|=
operator|new
name|Integer
index|[
name|keys
operator|.
name|size
argument_list|()
index|]
expr_stmt|;
block|}
name|Table
name|tbl
init|=
name|topToTable
operator|.
name|get
argument_list|(
name|tso
argument_list|)
decl_stmt|;
if|if
condition|(
name|tbl
operator|.
name|isPartitioned
argument_list|()
condition|)
block|{
name|PrunedPartitionList
name|prunedParts
decl_stmt|;
try|try
block|{
name|prunedParts
operator|=
name|pGraphContext
operator|.
name|getOpToPartList
argument_list|()
operator|.
name|get
argument_list|(
name|tso
argument_list|)
expr_stmt|;
if|if
condition|(
name|prunedParts
operator|==
literal|null
condition|)
block|{
name|prunedParts
operator|=
name|PartitionPruner
operator|.
name|prune
argument_list|(
name|tbl
argument_list|,
name|pGraphContext
operator|.
name|getOpToPartPruner
argument_list|()
operator|.
name|get
argument_list|(
name|tso
argument_list|)
argument_list|,
name|pGraphContext
operator|.
name|getConf
argument_list|()
argument_list|,
name|alias
argument_list|,
name|pGraphContext
operator|.
name|getPrunedPartitions
argument_list|()
argument_list|)
expr_stmt|;
name|pGraphContext
operator|.
name|getOpToPartList
argument_list|()
operator|.
name|put
argument_list|(
name|tso
argument_list|,
name|prunedParts
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|HiveException
name|e
parameter_list|)
block|{
comment|// Has to use full name to make sure it does not conflict with
comment|// org.apache.commons.lang.StringUtils
name|LOG
operator|.
name|error
argument_list|(
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|SemanticException
argument_list|(
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
name|List
argument_list|<
name|Partition
argument_list|>
name|partitions
init|=
name|prunedParts
operator|.
name|getNotDeniedPartns
argument_list|()
decl_stmt|;
comment|// construct a mapping of (Partition->bucket file names) and (Partition -> bucket number)
if|if
condition|(
name|partitions
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
if|if
condition|(
operator|!
name|alias
operator|.
name|equals
argument_list|(
name|baseBigAlias
argument_list|)
condition|)
block|{
name|tblAliasToNumberOfBucketsInEachPartition
operator|.
name|put
argument_list|(
name|alias
argument_list|,
name|Arrays
operator|.
expr|<
name|Integer
operator|>
name|asList
argument_list|()
argument_list|)
expr_stmt|;
name|tblAliasToBucketedFilePathsInEachPartition
operator|.
name|put
argument_list|(
name|alias
argument_list|,
operator|new
name|ArrayList
argument_list|<
name|List
argument_list|<
name|String
argument_list|>
argument_list|>
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
name|List
argument_list|<
name|Integer
argument_list|>
name|buckets
init|=
operator|new
name|ArrayList
argument_list|<
name|Integer
argument_list|>
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|List
argument_list|<
name|String
argument_list|>
argument_list|>
name|files
init|=
operator|new
name|ArrayList
argument_list|<
name|List
argument_list|<
name|String
argument_list|>
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|Partition
name|p
range|:
name|partitions
control|)
block|{
if|if
condition|(
operator|!
name|checkBucketColumns
argument_list|(
name|p
operator|.
name|getBucketCols
argument_list|()
argument_list|,
name|keys
argument_list|,
name|joinKeyOrder
argument_list|)
condition|)
block|{
return|return
literal|false
return|;
block|}
name|List
argument_list|<
name|String
argument_list|>
name|fileNames
init|=
name|getBucketFilePathsOfPartition
argument_list|(
name|p
operator|.
name|getDataLocation
argument_list|()
argument_list|,
name|pGraphContext
argument_list|)
decl_stmt|;
comment|// The number of files for the table should be same as number of buckets.
name|int
name|bucketCount
init|=
name|p
operator|.
name|getBucketCount
argument_list|()
decl_stmt|;
if|if
condition|(
name|fileNames
operator|.
name|size
argument_list|()
operator|!=
literal|0
operator|&&
name|fileNames
operator|.
name|size
argument_list|()
operator|!=
name|bucketCount
condition|)
block|{
name|String
name|msg
init|=
literal|"The number of buckets for table "
operator|+
name|tbl
operator|.
name|getTableName
argument_list|()
operator|+
literal|" partition "
operator|+
name|p
operator|.
name|getName
argument_list|()
operator|+
literal|" is "
operator|+
name|p
operator|.
name|getBucketCount
argument_list|()
operator|+
literal|", whereas the number of files is "
operator|+
name|fileNames
operator|.
name|size
argument_list|()
decl_stmt|;
throw|throw
operator|new
name|SemanticException
argument_list|(
name|ErrorMsg
operator|.
name|BUCKETED_TABLE_METADATA_INCORRECT
operator|.
name|getMsg
argument_list|(
name|msg
argument_list|)
argument_list|)
throw|;
block|}
if|if
condition|(
name|alias
operator|.
name|equals
argument_list|(
name|baseBigAlias
argument_list|)
condition|)
block|{
name|bigTblPartsToBucketFileNames
operator|.
name|put
argument_list|(
name|p
argument_list|,
name|fileNames
argument_list|)
expr_stmt|;
name|bigTblPartsToBucketNumber
operator|.
name|put
argument_list|(
name|p
argument_list|,
name|bucketCount
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|files
operator|.
name|add
argument_list|(
name|fileNames
argument_list|)
expr_stmt|;
name|buckets
operator|.
name|add
argument_list|(
name|bucketCount
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
operator|!
name|alias
operator|.
name|equals
argument_list|(
name|baseBigAlias
argument_list|)
condition|)
block|{
name|tblAliasToNumberOfBucketsInEachPartition
operator|.
name|put
argument_list|(
name|alias
argument_list|,
name|buckets
argument_list|)
expr_stmt|;
name|tblAliasToBucketedFilePathsInEachPartition
operator|.
name|put
argument_list|(
name|alias
argument_list|,
name|files
argument_list|)
expr_stmt|;
block|}
block|}
block|}
else|else
block|{
if|if
condition|(
operator|!
name|checkBucketColumns
argument_list|(
name|tbl
operator|.
name|getBucketCols
argument_list|()
argument_list|,
name|keys
argument_list|,
name|joinKeyOrder
argument_list|)
condition|)
block|{
return|return
literal|false
return|;
block|}
name|List
argument_list|<
name|String
argument_list|>
name|fileNames
init|=
name|getBucketFilePathsOfPartition
argument_list|(
name|tbl
operator|.
name|getDataLocation
argument_list|()
argument_list|,
name|pGraphContext
argument_list|)
decl_stmt|;
name|Integer
name|num
init|=
operator|new
name|Integer
argument_list|(
name|tbl
operator|.
name|getNumBuckets
argument_list|()
argument_list|)
decl_stmt|;
comment|// The number of files for the table should be same as number of buckets.
if|if
condition|(
name|fileNames
operator|.
name|size
argument_list|()
operator|!=
literal|0
operator|&&
name|fileNames
operator|.
name|size
argument_list|()
operator|!=
name|num
condition|)
block|{
name|String
name|msg
init|=
literal|"The number of buckets for table "
operator|+
name|tbl
operator|.
name|getTableName
argument_list|()
operator|+
literal|" is "
operator|+
name|tbl
operator|.
name|getNumBuckets
argument_list|()
operator|+
literal|", whereas the number of files is "
operator|+
name|fileNames
operator|.
name|size
argument_list|()
decl_stmt|;
throw|throw
operator|new
name|SemanticException
argument_list|(
name|ErrorMsg
operator|.
name|BUCKETED_TABLE_METADATA_INCORRECT
operator|.
name|getMsg
argument_list|(
name|msg
argument_list|)
argument_list|)
throw|;
block|}
if|if
condition|(
name|alias
operator|.
name|equals
argument_list|(
name|baseBigAlias
argument_list|)
condition|)
block|{
name|bigTblPartsToBucketFileNames
operator|.
name|put
argument_list|(
literal|null
argument_list|,
name|fileNames
argument_list|)
expr_stmt|;
name|bigTblPartsToBucketNumber
operator|.
name|put
argument_list|(
literal|null
argument_list|,
name|tbl
operator|.
name|getNumBuckets
argument_list|()
argument_list|)
expr_stmt|;
name|bigTablePartitioned
operator|=
literal|false
expr_stmt|;
block|}
else|else
block|{
name|tblAliasToNumberOfBucketsInEachPartition
operator|.
name|put
argument_list|(
name|alias
argument_list|,
name|Arrays
operator|.
name|asList
argument_list|(
name|num
argument_list|)
argument_list|)
expr_stmt|;
name|tblAliasToBucketedFilePathsInEachPartition
operator|.
name|put
argument_list|(
name|alias
argument_list|,
name|Arrays
operator|.
name|asList
argument_list|(
name|fileNames
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
block|}
comment|// All tables or partitions are bucketed, and their bucket number is
comment|// stored in 'bucketNumbers', we need to check if the number of buckets in
comment|// the big table can be divided by no of buckets in small tables.
for|for
control|(
name|Integer
name|numBucketsInPartitionOfBigTable
range|:
name|bigTblPartsToBucketNumber
operator|.
name|values
argument_list|()
control|)
block|{
if|if
condition|(
operator|!
name|checkNumberOfBucketsAgainstBigTable
argument_list|(
name|tblAliasToNumberOfBucketsInEachPartition
argument_list|,
name|numBucketsInPartitionOfBigTable
argument_list|)
condition|)
block|{
return|return
literal|false
return|;
block|}
block|}
name|context
operator|.
name|setTblAliasToNumberOfBucketsInEachPartition
argument_list|(
name|tblAliasToNumberOfBucketsInEachPartition
argument_list|)
expr_stmt|;
name|context
operator|.
name|setTblAliasToBucketedFilePathsInEachPartition
argument_list|(
name|tblAliasToBucketedFilePathsInEachPartition
argument_list|)
expr_stmt|;
name|context
operator|.
name|setBigTblPartsToBucketFileNames
argument_list|(
name|bigTblPartsToBucketFileNames
argument_list|)
expr_stmt|;
name|context
operator|.
name|setBigTblPartsToBucketNumber
argument_list|(
name|bigTblPartsToBucketNumber
argument_list|)
expr_stmt|;
name|context
operator|.
name|setJoinAliases
argument_list|(
name|joinAliases
argument_list|)
expr_stmt|;
name|context
operator|.
name|setBaseBigAlias
argument_list|(
name|baseBigAlias
argument_list|)
expr_stmt|;
name|context
operator|.
name|setBigTablePartitioned
argument_list|(
name|bigTablePartitioned
argument_list|)
expr_stmt|;
return|return
literal|true
return|;
block|}
comment|/*    * Convert mapjoin to a bucketed mapjoin.    * The operator tree is not changed, but the mapjoin descriptor in the big table is    * enhanced to keep the big table bucket -> small table buckets mapping.    */
specifier|protected
name|void
name|convertMapJoinToBucketMapJoin
parameter_list|(
name|MapJoinOperator
name|mapJoinOp
parameter_list|,
name|BucketJoinProcCtx
name|context
parameter_list|)
throws|throws
name|SemanticException
block|{
name|MapJoinDesc
name|desc
init|=
name|mapJoinOp
operator|.
name|getConf
argument_list|()
decl_stmt|;
name|Map
argument_list|<
name|String
argument_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|List
argument_list|<
name|String
argument_list|>
argument_list|>
argument_list|>
name|aliasBucketFileNameMapping
init|=
operator|new
name|LinkedHashMap
argument_list|<
name|String
argument_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|List
argument_list|<
name|String
argument_list|>
argument_list|>
argument_list|>
argument_list|()
decl_stmt|;
name|Map
argument_list|<
name|String
argument_list|,
name|List
argument_list|<
name|Integer
argument_list|>
argument_list|>
name|tblAliasToNumberOfBucketsInEachPartition
init|=
name|context
operator|.
name|getTblAliasToNumberOfBucketsInEachPartition
argument_list|()
decl_stmt|;
name|Map
argument_list|<
name|String
argument_list|,
name|List
argument_list|<
name|List
argument_list|<
name|String
argument_list|>
argument_list|>
argument_list|>
name|tblAliasToBucketedFilePathsInEachPartition
init|=
name|context
operator|.
name|getTblAliasToBucketedFilePathsInEachPartition
argument_list|()
decl_stmt|;
name|Map
argument_list|<
name|Partition
argument_list|,
name|List
argument_list|<
name|String
argument_list|>
argument_list|>
name|bigTblPartsToBucketFileNames
init|=
name|context
operator|.
name|getBigTblPartsToBucketFileNames
argument_list|()
decl_stmt|;
name|Map
argument_list|<
name|Partition
argument_list|,
name|Integer
argument_list|>
name|bigTblPartsToBucketNumber
init|=
name|context
operator|.
name|getBigTblPartsToBucketNumber
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|joinAliases
init|=
name|context
operator|.
name|getJoinAliases
argument_list|()
decl_stmt|;
name|String
name|baseBigAlias
init|=
name|context
operator|.
name|getBaseBigAlias
argument_list|()
decl_stmt|;
comment|// sort bucket names for the big table
for|for
control|(
name|List
argument_list|<
name|String
argument_list|>
name|partBucketNames
range|:
name|bigTblPartsToBucketFileNames
operator|.
name|values
argument_list|()
control|)
block|{
name|Collections
operator|.
name|sort
argument_list|(
name|partBucketNames
argument_list|)
expr_stmt|;
block|}
comment|// go through all small tables and get the mapping from bucket file name
comment|// in the big table to bucket file names in small tables.
for|for
control|(
name|int
name|j
init|=
literal|0
init|;
name|j
operator|<
name|joinAliases
operator|.
name|size
argument_list|()
condition|;
name|j
operator|++
control|)
block|{
name|String
name|alias
init|=
name|joinAliases
operator|.
name|get
argument_list|(
name|j
argument_list|)
decl_stmt|;
if|if
condition|(
name|alias
operator|.
name|equals
argument_list|(
name|baseBigAlias
argument_list|)
condition|)
block|{
continue|continue;
block|}
for|for
control|(
name|List
argument_list|<
name|String
argument_list|>
name|names
range|:
name|tblAliasToBucketedFilePathsInEachPartition
operator|.
name|get
argument_list|(
name|alias
argument_list|)
control|)
block|{
name|Collections
operator|.
name|sort
argument_list|(
name|names
argument_list|)
expr_stmt|;
block|}
name|List
argument_list|<
name|Integer
argument_list|>
name|smallTblBucketNums
init|=
name|tblAliasToNumberOfBucketsInEachPartition
operator|.
name|get
argument_list|(
name|alias
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|List
argument_list|<
name|String
argument_list|>
argument_list|>
name|smallTblFilesList
init|=
name|tblAliasToBucketedFilePathsInEachPartition
operator|.
name|get
argument_list|(
name|alias
argument_list|)
decl_stmt|;
name|Map
argument_list|<
name|String
argument_list|,
name|List
argument_list|<
name|String
argument_list|>
argument_list|>
name|mappingBigTableBucketFileNameToSmallTableBucketFileNames
init|=
operator|new
name|LinkedHashMap
argument_list|<
name|String
argument_list|,
name|List
argument_list|<
name|String
argument_list|>
argument_list|>
argument_list|()
decl_stmt|;
name|aliasBucketFileNameMapping
operator|.
name|put
argument_list|(
name|alias
argument_list|,
name|mappingBigTableBucketFileNameToSmallTableBucketFileNames
argument_list|)
expr_stmt|;
comment|// for each bucket file in big table, get the corresponding bucket file
comment|// name in the small table.
comment|// more than 1 partition in the big table, do the mapping for each partition
name|Iterator
argument_list|<
name|Entry
argument_list|<
name|Partition
argument_list|,
name|List
argument_list|<
name|String
argument_list|>
argument_list|>
argument_list|>
name|bigTblPartToBucketNames
init|=
name|bigTblPartsToBucketFileNames
operator|.
name|entrySet
argument_list|()
operator|.
name|iterator
argument_list|()
decl_stmt|;
name|Iterator
argument_list|<
name|Entry
argument_list|<
name|Partition
argument_list|,
name|Integer
argument_list|>
argument_list|>
name|bigTblPartToBucketNum
init|=
name|bigTblPartsToBucketNumber
operator|.
name|entrySet
argument_list|()
operator|.
name|iterator
argument_list|()
decl_stmt|;
while|while
condition|(
name|bigTblPartToBucketNames
operator|.
name|hasNext
argument_list|()
condition|)
block|{
assert|assert
name|bigTblPartToBucketNum
operator|.
name|hasNext
argument_list|()
assert|;
name|int
name|bigTblBucketNum
init|=
name|bigTblPartToBucketNum
operator|.
name|next
argument_list|()
operator|.
name|getValue
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|bigTblBucketNameList
init|=
name|bigTblPartToBucketNames
operator|.
name|next
argument_list|()
operator|.
name|getValue
argument_list|()
decl_stmt|;
name|fillMappingBigTableBucketFileNameToSmallTableBucketFileNames
argument_list|(
name|smallTblBucketNums
argument_list|,
name|smallTblFilesList
argument_list|,
name|mappingBigTableBucketFileNameToSmallTableBucketFileNames
argument_list|,
name|bigTblBucketNum
argument_list|,
name|bigTblBucketNameList
argument_list|,
name|desc
operator|.
name|getBigTableBucketNumMapping
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
name|desc
operator|.
name|setAliasBucketFileNameMapping
argument_list|(
name|aliasBucketFileNameMapping
argument_list|)
expr_stmt|;
name|desc
operator|.
name|setBigTableAlias
argument_list|(
name|baseBigAlias
argument_list|)
expr_stmt|;
name|boolean
name|bigTablePartitioned
init|=
name|context
operator|.
name|isBigTablePartitioned
argument_list|()
decl_stmt|;
if|if
condition|(
name|bigTablePartitioned
condition|)
block|{
name|desc
operator|.
name|setBigTablePartSpecToFileMapping
argument_list|(
name|convert
argument_list|(
name|bigTblPartsToBucketFileNames
argument_list|)
argument_list|)
expr_stmt|;
block|}
comment|// successfully convert to bucket map join
name|desc
operator|.
name|setBucketMapJoin
argument_list|(
literal|true
argument_list|)
expr_stmt|;
block|}
comment|// convert partition to partition spec string
specifier|private
specifier|static
name|Map
argument_list|<
name|String
argument_list|,
name|List
argument_list|<
name|String
argument_list|>
argument_list|>
name|convert
parameter_list|(
name|Map
argument_list|<
name|Partition
argument_list|,
name|List
argument_list|<
name|String
argument_list|>
argument_list|>
name|mapping
parameter_list|)
block|{
name|Map
argument_list|<
name|String
argument_list|,
name|List
argument_list|<
name|String
argument_list|>
argument_list|>
name|converted
init|=
operator|new
name|HashMap
argument_list|<
name|String
argument_list|,
name|List
argument_list|<
name|String
argument_list|>
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|Partition
argument_list|,
name|List
argument_list|<
name|String
argument_list|>
argument_list|>
name|entry
range|:
name|mapping
operator|.
name|entrySet
argument_list|()
control|)
block|{
name|converted
operator|.
name|put
argument_list|(
name|entry
operator|.
name|getKey
argument_list|()
operator|.
name|getName
argument_list|()
argument_list|,
name|entry
operator|.
name|getValue
argument_list|()
argument_list|)
expr_stmt|;
block|}
return|return
name|converted
return|;
block|}
specifier|public
name|List
argument_list|<
name|String
argument_list|>
name|toColumns
parameter_list|(
name|List
argument_list|<
name|ExprNodeDesc
argument_list|>
name|keys
parameter_list|)
block|{
name|List
argument_list|<
name|String
argument_list|>
name|columns
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|ExprNodeDesc
name|key
range|:
name|keys
control|)
block|{
if|if
condition|(
operator|!
operator|(
name|key
operator|instanceof
name|ExprNodeColumnDesc
operator|)
condition|)
block|{
return|return
literal|null
return|;
block|}
name|columns
operator|.
name|add
argument_list|(
operator|(
operator|(
name|ExprNodeColumnDesc
operator|)
name|key
operator|)
operator|.
name|getColumn
argument_list|()
argument_list|)
expr_stmt|;
block|}
return|return
name|columns
return|;
block|}
comment|// called for each partition of big table and populates mapping for each file in the partition
specifier|private
specifier|static
name|void
name|fillMappingBigTableBucketFileNameToSmallTableBucketFileNames
parameter_list|(
name|List
argument_list|<
name|Integer
argument_list|>
name|smallTblBucketNums
parameter_list|,
name|List
argument_list|<
name|List
argument_list|<
name|String
argument_list|>
argument_list|>
name|smallTblFilesList
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|List
argument_list|<
name|String
argument_list|>
argument_list|>
name|bigTableBucketFileNameToSmallTableBucketFileNames
parameter_list|,
name|int
name|bigTblBucketNum
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|bigTblBucketNameList
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|Integer
argument_list|>
name|bucketFileNameMapping
parameter_list|)
block|{
for|for
control|(
name|int
name|bindex
init|=
literal|0
init|;
name|bindex
operator|<
name|bigTblBucketNameList
operator|.
name|size
argument_list|()
condition|;
name|bindex
operator|++
control|)
block|{
name|ArrayList
argument_list|<
name|String
argument_list|>
name|resultFileNames
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|int
name|sindex
init|=
literal|0
init|;
name|sindex
operator|<
name|smallTblBucketNums
operator|.
name|size
argument_list|()
condition|;
name|sindex
operator|++
control|)
block|{
name|int
name|smallTblBucketNum
init|=
name|smallTblBucketNums
operator|.
name|get
argument_list|(
name|sindex
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|smallTblFileNames
init|=
name|smallTblFilesList
operator|.
name|get
argument_list|(
name|sindex
argument_list|)
decl_stmt|;
if|if
condition|(
name|bigTblBucketNum
operator|>=
name|smallTblBucketNum
condition|)
block|{
comment|// if the big table has more buckets than the current small table,
comment|// use "MOD" to get small table bucket names. For example, if the big
comment|// table has 4 buckets and the small table has 2 buckets, then the
comment|// mapping should be 0->0, 1->1, 2->0, 3->1.
name|int
name|toAddSmallIndex
init|=
name|bindex
operator|%
name|smallTblBucketNum
decl_stmt|;
name|resultFileNames
operator|.
name|add
argument_list|(
name|smallTblFileNames
operator|.
name|get
argument_list|(
name|toAddSmallIndex
argument_list|)
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|int
name|jump
init|=
name|smallTblBucketNum
operator|/
name|bigTblBucketNum
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
name|bindex
init|;
name|i
operator|<
name|smallTblFileNames
operator|.
name|size
argument_list|()
condition|;
name|i
operator|=
name|i
operator|+
name|jump
control|)
block|{
name|resultFileNames
operator|.
name|add
argument_list|(
name|smallTblFileNames
operator|.
name|get
argument_list|(
name|i
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
block|}
name|String
name|inputBigTBLBucket
init|=
name|bigTblBucketNameList
operator|.
name|get
argument_list|(
name|bindex
argument_list|)
decl_stmt|;
name|bigTableBucketFileNameToSmallTableBucketFileNames
operator|.
name|put
argument_list|(
name|inputBigTBLBucket
argument_list|,
name|resultFileNames
argument_list|)
expr_stmt|;
name|bucketFileNameMapping
operator|.
name|put
argument_list|(
name|inputBigTBLBucket
argument_list|,
name|bindex
argument_list|)
expr_stmt|;
block|}
block|}
block|}
end_class

end_unit

