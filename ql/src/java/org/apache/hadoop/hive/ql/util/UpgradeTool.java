begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/*  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|util
package|;
end_package

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|annotations
operator|.
name|VisibleForTesting
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|cli
operator|.
name|CommandLine
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|cli
operator|.
name|CommandLineParser
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|cli
operator|.
name|GnuParser
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|cli
operator|.
name|HelpFormatter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|cli
operator|.
name|Option
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|cli
operator|.
name|Options
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|cli
operator|.
name|ParseException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configuration
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|ContentSummary
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|LocatedFileStatus
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|RemoteIterator
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|StatsSetupConst
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|HiveMetaStoreClient
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|IMetaStoreClient
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|RetryingMetaStoreClient
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|TableType
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|Warehouse
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|EnvironmentContext
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|InvalidOperationException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|MetaException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|StorageDescriptor
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Table
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|hive_metastoreConstants
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|conf
operator|.
name|MetastoreConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|utils
operator|.
name|FileUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|utils
operator|.
name|FileUtils
operator|.
name|RemoteIteratorWithFilter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|AcidUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|BucketCodec
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|lockmgr
operator|.
name|HiveTxnManager
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|Hive
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|HiveException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|session
operator|.
name|SessionState
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|security
operator|.
name|UserGroupInformation
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hive
operator|.
name|common
operator|.
name|util
operator|.
name|HiveVersionInfo
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|thrift
operator|.
name|TException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|Logger
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|LoggerFactory
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|FileWriter
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|PrintWriter
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Collections
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|function
operator|.
name|Function
import|;
end_import

begin_comment
comment|/**  * A new type of transactional tables was added in 3.0 - insert-only tables.  These  * tables support ACID semantics and work with any Input/OutputFormat.  Any Managed tables may  * be made insert-only transactional table. These tables don't support Update/Delete/Merge commands.  *  * In postUpgrade mode, Hive 3.0 jars/hive-site.xml should be on the classpath. This utility will  * find all the tables that may be made transactional (with ful CRUD support) and generate  * Alter Table commands to do so.  It will also find all tables that may do not support full CRUD  * but can be made insert-only transactional tables and generate corresponding Alter Table commands.  *  * Note that to convert a table to full CRUD table requires that all files follow a naming  * convention, namely 0000N_0 or 0000N_0_copy_M, N>= 0, M> 0.  This utility can perform this  * rename with "execute" option.  It will also produce a script (with and w/o "execute" to  * perform the renames).  *  * "execute" option may be supplied in both modes to have the utility automatically execute the  * equivalent of the generated commands  *  * "location" option may be supplied followed by a path to set the location for the generated  * scripts.  *  * See also org.apache.hadoop.hive.upgrade.acid.PreUpgradeTool for steps which may be necessary to  * perform before upgrading to Hive 3.  */
end_comment

begin_class
specifier|public
class|class
name|UpgradeTool
block|{
specifier|private
specifier|static
specifier|final
name|Logger
name|LOG
init|=
name|LoggerFactory
operator|.
name|getLogger
argument_list|(
name|UpgradeTool
operator|.
name|class
argument_list|)
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|int
name|PARTITION_BATCH_SIZE
init|=
literal|10000
decl_stmt|;
specifier|private
specifier|final
name|Options
name|cmdLineOptions
init|=
operator|new
name|Options
argument_list|()
decl_stmt|;
specifier|public
specifier|static
name|void
name|main
parameter_list|(
name|String
index|[]
name|args
parameter_list|)
throws|throws
name|Exception
block|{
name|UpgradeTool
name|tool
init|=
operator|new
name|UpgradeTool
argument_list|()
decl_stmt|;
name|tool
operator|.
name|init
argument_list|()
expr_stmt|;
name|CommandLineParser
name|parser
init|=
operator|new
name|GnuParser
argument_list|()
decl_stmt|;
name|CommandLine
name|line
decl_stmt|;
name|String
name|outputDir
init|=
literal|"."
decl_stmt|;
name|boolean
name|execute
init|=
literal|false
decl_stmt|;
try|try
block|{
name|line
operator|=
name|parser
operator|.
name|parse
argument_list|(
name|tool
operator|.
name|cmdLineOptions
argument_list|,
name|args
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|ParseException
name|e
parameter_list|)
block|{
name|System
operator|.
name|err
operator|.
name|println
argument_list|(
literal|"UpgradeTool: Parsing failed.  Reason: "
operator|+
name|e
operator|.
name|getLocalizedMessage
argument_list|()
argument_list|)
expr_stmt|;
name|printAndExit
argument_list|(
name|tool
argument_list|)
expr_stmt|;
return|return;
block|}
if|if
condition|(
name|line
operator|.
name|hasOption
argument_list|(
literal|"help"
argument_list|)
condition|)
block|{
name|HelpFormatter
name|formatter
init|=
operator|new
name|HelpFormatter
argument_list|()
decl_stmt|;
name|formatter
operator|.
name|printHelp
argument_list|(
literal|"upgrade-acid"
argument_list|,
name|tool
operator|.
name|cmdLineOptions
argument_list|)
expr_stmt|;
return|return;
block|}
if|if
condition|(
name|line
operator|.
name|hasOption
argument_list|(
literal|"location"
argument_list|)
condition|)
block|{
name|outputDir
operator|=
name|line
operator|.
name|getOptionValue
argument_list|(
literal|"location"
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|line
operator|.
name|hasOption
argument_list|(
literal|"execute"
argument_list|)
condition|)
block|{
name|execute
operator|=
literal|true
expr_stmt|;
block|}
name|LOG
operator|.
name|info
argument_list|(
literal|"Starting with execute="
operator|+
name|execute
operator|+
literal|", location="
operator|+
name|outputDir
argument_list|)
expr_stmt|;
try|try
block|{
name|String
name|hiveVer
init|=
name|HiveVersionInfo
operator|.
name|getShortVersion
argument_list|()
decl_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Using Hive Version: "
operator|+
name|HiveVersionInfo
operator|.
name|getVersion
argument_list|()
operator|+
literal|" build: "
operator|+
name|HiveVersionInfo
operator|.
name|getBuildVersion
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
operator|(
name|hiveVer
operator|.
name|startsWith
argument_list|(
literal|"3."
argument_list|)
operator|||
name|hiveVer
operator|.
name|startsWith
argument_list|(
literal|"4."
argument_list|)
operator|)
condition|)
block|{
throw|throw
operator|new
name|IllegalStateException
argument_list|(
literal|"postUpgrade w/execute requires Hive 3.x.  Actual: "
operator|+
name|hiveVer
argument_list|)
throw|;
block|}
name|tool
operator|.
name|performUpgradeInternal
argument_list|(
name|outputDir
argument_list|,
name|execute
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|ex
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"UpgradeTool failed"
argument_list|,
name|ex
argument_list|)
expr_stmt|;
throw|throw
name|ex
throw|;
block|}
block|}
specifier|private
specifier|static
name|void
name|printAndExit
parameter_list|(
name|UpgradeTool
name|tool
parameter_list|)
block|{
name|HelpFormatter
name|formatter
init|=
operator|new
name|HelpFormatter
argument_list|()
decl_stmt|;
name|formatter
operator|.
name|printHelp
argument_list|(
literal|"upgrade-acid"
argument_list|,
name|tool
operator|.
name|cmdLineOptions
argument_list|)
expr_stmt|;
name|System
operator|.
name|exit
argument_list|(
literal|1
argument_list|)
expr_stmt|;
block|}
specifier|private
name|void
name|init
parameter_list|()
block|{
try|try
block|{
name|cmdLineOptions
operator|.
name|addOption
argument_list|(
operator|new
name|Option
argument_list|(
literal|"help"
argument_list|,
literal|"Generates a script to execute on 3.x "
operator|+
literal|"cluster.  This requires 3.x binaries on the classpath and hive-site.xml."
argument_list|)
argument_list|)
expr_stmt|;
name|Option
name|exec
init|=
operator|new
name|Option
argument_list|(
literal|"execute"
argument_list|,
literal|"Executes commands equivalent to generated scrips"
argument_list|)
decl_stmt|;
name|exec
operator|.
name|setOptionalArg
argument_list|(
literal|true
argument_list|)
expr_stmt|;
name|cmdLineOptions
operator|.
name|addOption
argument_list|(
name|exec
argument_list|)
expr_stmt|;
name|cmdLineOptions
operator|.
name|addOption
argument_list|(
operator|new
name|Option
argument_list|(
literal|"location"
argument_list|,
literal|true
argument_list|,
literal|"Location to write scripts to. Default is CWD."
argument_list|)
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|ex
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"init()"
argument_list|,
name|ex
argument_list|)
expr_stmt|;
throw|throw
name|ex
throw|;
block|}
block|}
specifier|private
specifier|static
name|IMetaStoreClient
name|getHMS
parameter_list|(
name|HiveConf
name|conf
parameter_list|)
block|{
name|UserGroupInformation
name|loggedInUser
init|=
literal|null
decl_stmt|;
try|try
block|{
name|loggedInUser
operator|=
name|UserGroupInformation
operator|.
name|getLoginUser
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Unable to get logged in user via UGI. err: {}"
argument_list|,
name|e
operator|.
name|getMessage
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|boolean
name|secureMode
init|=
name|loggedInUser
operator|!=
literal|null
operator|&&
name|loggedInUser
operator|.
name|hasKerberosCredentials
argument_list|()
decl_stmt|;
if|if
condition|(
name|secureMode
condition|)
block|{
name|MetastoreConf
operator|.
name|setBoolVar
argument_list|(
name|conf
argument_list|,
name|MetastoreConf
operator|.
name|ConfVars
operator|.
name|USE_THRIFT_SASL
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
try|try
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Creating metastore client for {}"
argument_list|,
literal|"PreUpgradeTool"
argument_list|)
expr_stmt|;
return|return
name|RetryingMetaStoreClient
operator|.
name|getProxy
argument_list|(
name|conf
argument_list|,
literal|true
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|MetaException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
literal|"Error connecting to Hive Metastore URI: "
operator|+
name|conf
operator|.
name|getVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|METASTOREURIS
argument_list|)
operator|+
literal|". "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    * todo: this should accept a file of table names to exclude from non-acid to acid conversion    * todo: change script comments to a preamble instead of a footer    */
specifier|private
name|void
name|performUpgradeInternal
parameter_list|(
name|String
name|scriptLocation
parameter_list|,
name|boolean
name|execute
parameter_list|)
throws|throws
name|HiveException
throws|,
name|TException
throws|,
name|IOException
block|{
name|HiveConf
name|conf
init|=
name|hiveConf
operator|!=
literal|null
condition|?
name|hiveConf
else|:
operator|new
name|HiveConf
argument_list|()
decl_stmt|;
name|boolean
name|isAcidEnabled
init|=
name|isAcidEnabled
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|IMetaStoreClient
name|hms
init|=
name|getHMS
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Looking for databases"
argument_list|)
expr_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|databases
init|=
name|hms
operator|.
name|getAllDatabases
argument_list|()
decl_stmt|;
comment|//TException
name|LOG
operator|.
name|debug
argument_list|(
literal|"Found "
operator|+
name|databases
operator|.
name|size
argument_list|()
operator|+
literal|" databases to process"
argument_list|)
expr_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|convertToAcid
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|convertToMM
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|()
decl_stmt|;
name|Hive
name|db
init|=
literal|null
decl_stmt|;
if|if
condition|(
name|execute
condition|)
block|{
name|db
operator|=
name|Hive
operator|.
name|get
argument_list|(
name|conf
argument_list|)
expr_stmt|;
block|}
name|PrintWriter
name|pw
init|=
name|makeRenameFileScript
argument_list|(
name|scriptLocation
argument_list|)
decl_stmt|;
for|for
control|(
name|String
name|dbName
range|:
name|databases
control|)
block|{
name|List
argument_list|<
name|String
argument_list|>
name|tables
init|=
name|hms
operator|.
name|getAllTables
argument_list|(
name|dbName
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"found "
operator|+
name|tables
operator|.
name|size
argument_list|()
operator|+
literal|" tables in "
operator|+
name|dbName
argument_list|)
expr_stmt|;
for|for
control|(
name|String
name|tableName
range|:
name|tables
control|)
block|{
name|Table
name|t
init|=
name|hms
operator|.
name|getTable
argument_list|(
name|dbName
argument_list|,
name|tableName
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"processing table "
operator|+
name|Warehouse
operator|.
name|getQualifiedName
argument_list|(
name|t
argument_list|)
argument_list|)
expr_stmt|;
if|if
condition|(
name|isAcidEnabled
condition|)
block|{
comment|//if acid is off post upgrade, you can't make any tables acid - will throw
name|processConversion
argument_list|(
name|t
argument_list|,
name|convertToAcid
argument_list|,
name|convertToMM
argument_list|,
name|hms
argument_list|,
name|db
argument_list|,
name|execute
argument_list|,
name|pw
argument_list|)
expr_stmt|;
block|}
block|}
block|}
name|pw
operator|.
name|close
argument_list|()
expr_stmt|;
name|makeConvertTableScript
argument_list|(
name|convertToAcid
argument_list|,
name|convertToMM
argument_list|,
name|scriptLocation
argument_list|)
expr_stmt|;
block|}
comment|/**    * Actually makes the table transactional    */
specifier|private
specifier|static
name|void
name|alterTable
parameter_list|(
name|Table
name|t
parameter_list|,
name|Hive
name|db
parameter_list|,
name|boolean
name|isMM
parameter_list|)
throws|throws
name|HiveException
throws|,
name|InvalidOperationException
block|{
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|Table
name|metaTable
init|=
comment|//clone to make sure new prop doesn't leak
operator|new
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|Table
argument_list|(
name|t
operator|.
name|deepCopy
argument_list|()
argument_list|)
decl_stmt|;
name|metaTable
operator|.
name|getParameters
argument_list|()
operator|.
name|put
argument_list|(
name|hive_metastoreConstants
operator|.
name|TABLE_IS_TRANSACTIONAL
argument_list|,
literal|"true"
argument_list|)
expr_stmt|;
if|if
condition|(
name|isMM
condition|)
block|{
name|metaTable
operator|.
name|getParameters
argument_list|()
operator|.
name|put
argument_list|(
name|hive_metastoreConstants
operator|.
name|TABLE_TRANSACTIONAL_PROPERTIES
argument_list|,
literal|"insert_only"
argument_list|)
expr_stmt|;
block|}
name|EnvironmentContext
name|ec
init|=
operator|new
name|EnvironmentContext
argument_list|()
decl_stmt|;
comment|/*we are not modifying any data so stats should be exactly the same*/
name|ec
operator|.
name|putToProperties
argument_list|(
name|StatsSetupConst
operator|.
name|DO_NOT_UPDATE_STATS
argument_list|,
name|StatsSetupConst
operator|.
name|TRUE
argument_list|)
expr_stmt|;
name|db
operator|.
name|alterTable
argument_list|(
name|Warehouse
operator|.
name|getQualifiedName
argument_list|(
name|t
argument_list|)
argument_list|,
name|metaTable
argument_list|,
literal|false
argument_list|,
name|ec
argument_list|,
literal|false
argument_list|)
expr_stmt|;
block|}
comment|/**    * assumes https://issues.apache.org/jira/browse/HIVE-19750 is in    * How does this work with Storage Based Auth?    * @param p partition root or table root if not partitioned    */
specifier|static
name|void
name|handleRenameFiles
parameter_list|(
name|Table
name|t
parameter_list|,
name|Path
name|p
parameter_list|,
name|boolean
name|execute
parameter_list|,
name|Configuration
name|conf
parameter_list|,
name|boolean
name|isBucketed
parameter_list|,
name|PrintWriter
name|pw
parameter_list|)
throws|throws
name|IOException
block|{
name|AcidUtils
operator|.
name|BUCKET_DIGIT_PATTERN
operator|.
name|matcher
argument_list|(
literal|"foo"
argument_list|)
expr_stmt|;
if|if
condition|(
name|isBucketed
condition|)
block|{
comment|/* For bucketed tables we assume that Hive wrote them and 0000M_0 and 0000M_0_copy_8       are the only possibilities.  Since we can't move files across buckets the only thing we       can do is put 0000M_0_copy_N into delta_N_N as 0000M_0.        If M> 4096 - should error out - better yet, make this table external one - can those       be bucketed?  don't think so       */
comment|//Known deltas
name|Map
argument_list|<
name|Integer
argument_list|,
name|List
argument_list|<
name|Path
argument_list|>
argument_list|>
name|deltaToFileMap
init|=
operator|new
name|HashMap
argument_list|<>
argument_list|()
decl_stmt|;
name|FileSystem
name|fs
init|=
name|FileSystem
operator|.
name|get
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|RemoteIteratorWithFilter
name|iter
init|=
operator|new
name|RemoteIteratorWithFilter
argument_list|(
name|fs
operator|.
name|listFiles
argument_list|(
name|p
argument_list|,
literal|true
argument_list|)
argument_list|,
name|RemoteIteratorWithFilter
operator|.
name|HIDDEN_FILES_FULL_PATH_FILTER
argument_list|)
decl_stmt|;
name|Function
argument_list|<
name|Integer
argument_list|,
name|List
argument_list|<
name|Path
argument_list|>
argument_list|>
name|makeList
init|=
operator|new
name|Function
argument_list|<
name|Integer
argument_list|,
name|List
argument_list|<
name|Path
argument_list|>
argument_list|>
argument_list|()
block|{
comment|//lambda?
annotation|@
name|Override
specifier|public
name|List
argument_list|<
name|Path
argument_list|>
name|apply
parameter_list|(
name|Integer
name|aVoid
parameter_list|)
block|{
return|return
operator|new
name|ArrayList
argument_list|<>
argument_list|()
return|;
block|}
block|}
decl_stmt|;
while|while
condition|(
name|iter
operator|.
name|hasNext
argument_list|()
condition|)
block|{
name|LocatedFileStatus
name|lfs
init|=
name|iter
operator|.
name|next
argument_list|()
decl_stmt|;
if|if
condition|(
name|lfs
operator|.
name|isDirectory
argument_list|()
condition|)
block|{
name|String
name|msg
init|=
name|Warehouse
operator|.
name|getQualifiedName
argument_list|(
name|t
argument_list|)
operator|+
literal|" is bucketed and has a subdirectory: "
operator|+
name|lfs
operator|.
name|getPath
argument_list|()
decl_stmt|;
name|LOG
operator|.
name|error
argument_list|(
name|msg
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|IllegalStateException
argument_list|(
name|msg
argument_list|)
throw|;
block|}
name|AcidUtils
operator|.
name|BucketMetaData
name|bmd
init|=
name|AcidUtils
operator|.
name|BucketMetaData
operator|.
name|parse
argument_list|(
name|lfs
operator|.
name|getPath
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|bmd
operator|.
name|bucketId
operator|<
literal|0
condition|)
block|{
comment|//non-standard file name - don't know what bucket the rows belong to and we can't
comment|//rename the file so tha it may end up treated like a different bucket id
name|String
name|msg
init|=
literal|"Bucketed table "
operator|+
name|Warehouse
operator|.
name|getQualifiedName
argument_list|(
name|t
argument_list|)
operator|+
literal|" contains file "
operator|+
name|lfs
operator|.
name|getPath
argument_list|()
operator|+
literal|" with non-standard name"
decl_stmt|;
name|LOG
operator|.
name|error
argument_list|(
name|msg
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
name|msg
argument_list|)
throw|;
block|}
else|else
block|{
if|if
condition|(
name|bmd
operator|.
name|bucketId
operator|>
name|BucketCodec
operator|.
name|MAX_BUCKET_ID
condition|)
block|{
name|String
name|msg
init|=
literal|"Bucketed table "
operator|+
name|Warehouse
operator|.
name|getQualifiedName
argument_list|(
name|t
argument_list|)
operator|+
literal|" contains file "
operator|+
name|lfs
operator|.
name|getPath
argument_list|()
operator|+
literal|" with bucketId="
operator|+
name|bmd
operator|.
name|bucketId
operator|+
literal|" that is out of range"
decl_stmt|;
name|LOG
operator|.
name|error
argument_list|(
name|msg
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
name|msg
argument_list|)
throw|;
block|}
if|if
condition|(
name|bmd
operator|.
name|copyNumber
operator|>
literal|0
condition|)
block|{
name|deltaToFileMap
operator|.
name|computeIfAbsent
argument_list|(
name|bmd
operator|.
name|copyNumber
argument_list|,
name|makeList
argument_list|)
operator|.
name|add
argument_list|(
name|lfs
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
block|}
if|if
condition|(
operator|!
name|deltaToFileMap
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|println
argument_list|(
name|pw
argument_list|,
literal|"#Begin file renames for bucketed table "
operator|+
name|Warehouse
operator|.
name|getQualifiedName
argument_list|(
name|t
argument_list|)
argument_list|)
expr_stmt|;
block|}
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|Integer
argument_list|,
name|List
argument_list|<
name|Path
argument_list|>
argument_list|>
name|ent
range|:
name|deltaToFileMap
operator|.
name|entrySet
argument_list|()
control|)
block|{
comment|/* create delta and move each files to it.  HIVE-19750 ensures wer have reserved          * enough write IDs to do this.*/
name|Path
name|deltaDir
init|=
operator|new
name|Path
argument_list|(
name|p
argument_list|,
name|AcidUtils
operator|.
name|deltaSubdir
argument_list|(
name|ent
operator|.
name|getKey
argument_list|()
argument_list|,
name|ent
operator|.
name|getKey
argument_list|()
argument_list|)
argument_list|)
decl_stmt|;
if|if
condition|(
name|execute
condition|)
block|{
if|if
condition|(
operator|!
name|fs
operator|.
name|mkdirs
argument_list|(
name|deltaDir
argument_list|)
condition|)
block|{
name|String
name|msg
init|=
literal|"Failed to create directory "
operator|+
name|deltaDir
decl_stmt|;
name|LOG
operator|.
name|error
argument_list|(
name|msg
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|IllegalStateException
argument_list|(
name|msg
argument_list|)
throw|;
block|}
block|}
comment|// Add to list of FS commands
name|makeDirectoryCommand
argument_list|(
name|deltaDir
argument_list|,
name|pw
argument_list|)
expr_stmt|;
for|for
control|(
name|Path
name|file
range|:
name|ent
operator|.
name|getValue
argument_list|()
control|)
block|{
name|Path
name|newFile
init|=
operator|new
name|Path
argument_list|(
name|deltaDir
argument_list|,
name|stripCopySuffix
argument_list|(
name|file
operator|.
name|getName
argument_list|()
argument_list|)
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"need to rename: "
operator|+
name|file
operator|+
literal|" to "
operator|+
name|newFile
argument_list|)
expr_stmt|;
if|if
condition|(
name|fs
operator|.
name|exists
argument_list|(
name|newFile
argument_list|)
condition|)
block|{
name|String
name|msg
init|=
name|Warehouse
operator|.
name|getQualifiedName
argument_list|(
name|t
argument_list|)
operator|+
literal|": "
operator|+
name|newFile
operator|+
literal|" already exists?!"
decl_stmt|;
name|LOG
operator|.
name|error
argument_list|(
name|msg
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|IllegalStateException
argument_list|(
name|msg
argument_list|)
throw|;
block|}
if|if
condition|(
name|execute
condition|)
block|{
if|if
condition|(
operator|!
name|fs
operator|.
name|rename
argument_list|(
name|file
argument_list|,
name|newFile
argument_list|)
condition|)
block|{
name|String
name|msg
init|=
name|Warehouse
operator|.
name|getQualifiedName
argument_list|(
name|t
argument_list|)
operator|+
literal|": "
operator|+
name|newFile
operator|+
literal|": failed to rename"
decl_stmt|;
name|LOG
operator|.
name|error
argument_list|(
name|msg
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|IllegalStateException
argument_list|(
name|msg
argument_list|)
throw|;
block|}
block|}
comment|//do this with and w/o execute to know what was done
name|makeRenameCommand
argument_list|(
name|file
argument_list|,
name|newFile
argument_list|,
name|pw
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
operator|!
name|deltaToFileMap
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|println
argument_list|(
name|pw
argument_list|,
literal|"#End file renames for bucketed table "
operator|+
name|Warehouse
operator|.
name|getQualifiedName
argument_list|(
name|t
argument_list|)
argument_list|)
expr_stmt|;
block|}
return|return;
block|}
name|List
argument_list|<
name|RenamePair
argument_list|>
name|renames
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|()
decl_stmt|;
name|FileSystem
name|fs
init|=
name|FileSystem
operator|.
name|get
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|RemoteIteratorWithFilter
name|iter
init|=
operator|new
name|RemoteIteratorWithFilter
argument_list|(
name|fs
operator|.
name|listFiles
argument_list|(
name|p
argument_list|,
literal|true
argument_list|)
argument_list|,
name|RemoteIteratorWithFilter
operator|.
name|HIDDEN_FILES_FULL_PATH_FILTER
argument_list|)
decl_stmt|;
comment|/**      * count some heuristics - bad file is something not in {@link AcidUtils#ORIGINAL_PATTERN} or      * {@link AcidUtils#ORIGINAL_PATTERN_COPY} format.  This has to be renamed for acid to work.      */
name|int
name|numBadFileNames
init|=
literal|0
decl_stmt|;
comment|/**      * count some heuristics - num files in {@link AcidUtils#ORIGINAL_PATTERN_COPY} format.  These      * are supported but if there are a lot of them there will be a perf hit on read until      * major compaction      */
name|int
name|numCopyNFiles
init|=
literal|0
decl_stmt|;
name|int
name|fileId
init|=
literal|0
decl_stmt|;
comment|//ordinal of the file in the iterator
name|long
name|numBytesInPartition
init|=
name|getDataSize
argument_list|(
name|p
argument_list|,
name|conf
argument_list|)
decl_stmt|;
name|int
name|numBuckets
init|=
name|guessNumBuckets
argument_list|(
name|numBytesInPartition
argument_list|)
decl_stmt|;
while|while
condition|(
name|iter
operator|.
name|hasNext
argument_list|()
condition|)
block|{
name|LocatedFileStatus
name|lfs
init|=
name|iter
operator|.
name|next
argument_list|()
decl_stmt|;
if|if
condition|(
name|lfs
operator|.
name|isDirectory
argument_list|()
condition|)
block|{
continue|continue;
block|}
name|AcidUtils
operator|.
name|BucketMetaData
name|bmd
init|=
name|AcidUtils
operator|.
name|BucketMetaData
operator|.
name|parse
argument_list|(
name|lfs
operator|.
name|getPath
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|bmd
operator|.
name|bucketId
operator|<
literal|0
condition|)
block|{
name|numBadFileNames
operator|++
expr_stmt|;
block|}
if|if
condition|(
name|bmd
operator|.
name|copyNumber
operator|>
literal|0
condition|)
block|{
comment|//todo: what about same file name in subdir like Union All?  ROW_ID generation will handle it
comment|//but will have to look at ORC footers - treat these as copyN files?
name|numCopyNFiles
operator|++
expr_stmt|;
block|}
name|int
name|wrtieId
init|=
name|fileId
operator|/
name|numBuckets
operator|+
literal|1
decl_stmt|;
comment|//start with delta_1 (not delta_0)
name|Path
name|deltaDir
init|=
operator|new
name|Path
argument_list|(
name|p
argument_list|,
name|AcidUtils
operator|.
name|deltaSubdir
argument_list|(
name|wrtieId
argument_list|,
name|wrtieId
argument_list|)
argument_list|)
decl_stmt|;
if|if
condition|(
name|execute
condition|)
block|{
if|if
condition|(
operator|!
name|fs
operator|.
name|mkdirs
argument_list|(
name|deltaDir
argument_list|)
condition|)
block|{
name|String
name|msg
init|=
literal|"Failed to create directory "
operator|+
name|deltaDir
decl_stmt|;
name|LOG
operator|.
name|error
argument_list|(
name|msg
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|IllegalStateException
argument_list|(
name|msg
argument_list|)
throw|;
block|}
block|}
comment|// Add to list of FS commands
name|makeDirectoryCommand
argument_list|(
name|deltaDir
argument_list|,
name|pw
argument_list|)
expr_stmt|;
name|Path
name|newPath
init|=
operator|new
name|Path
argument_list|(
name|deltaDir
argument_list|,
name|String
operator|.
name|format
argument_list|(
name|AcidUtils
operator|.
name|BUCKET_DIGITS
argument_list|,
name|fileId
operator|%
name|numBuckets
argument_list|)
operator|+
literal|"_0"
argument_list|)
decl_stmt|;
comment|/*we could track reason for rename in RenamePair so that the decision can be made later to        rename or not.  For example, if we need to minimize renames (say we are on S3), then we'd         only rename if it's absolutely required, i.e. if it's a 'bad file name'*/
name|renames
operator|.
name|add
argument_list|(
operator|new
name|RenamePair
argument_list|(
name|lfs
operator|.
name|getPath
argument_list|()
argument_list|,
name|newPath
argument_list|)
argument_list|)
expr_stmt|;
name|fileId
operator|++
expr_stmt|;
block|}
if|if
condition|(
name|numBadFileNames
operator|<=
literal|0
operator|&&
name|numCopyNFiles
operator|<=
literal|0
condition|)
block|{
comment|//if here, the only reason we'd want to rename is to spread the data into logical buckets to
comment|//help 3.0 Compactor generated more balanced splits
return|return;
block|}
if|if
condition|(
operator|!
name|renames
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|println
argument_list|(
name|pw
argument_list|,
literal|"#Begin file renames for unbucketed table "
operator|+
name|Warehouse
operator|.
name|getQualifiedName
argument_list|(
name|t
argument_list|)
argument_list|)
expr_stmt|;
block|}
for|for
control|(
name|RenamePair
name|renamePair
range|:
name|renames
control|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"need to rename: "
operator|+
name|renamePair
operator|.
name|getOldPath
argument_list|()
operator|+
literal|" to "
operator|+
name|renamePair
operator|.
name|getNewPath
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|fs
operator|.
name|exists
argument_list|(
name|renamePair
operator|.
name|getNewPath
argument_list|()
argument_list|)
condition|)
block|{
name|String
name|msg
init|=
name|Warehouse
operator|.
name|getQualifiedName
argument_list|(
name|t
argument_list|)
operator|+
literal|": "
operator|+
name|renamePair
operator|.
name|getNewPath
argument_list|()
operator|+
literal|" already exists?!"
decl_stmt|;
name|LOG
operator|.
name|error
argument_list|(
name|msg
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|IllegalStateException
argument_list|(
name|msg
argument_list|)
throw|;
block|}
if|if
condition|(
name|execute
condition|)
block|{
if|if
condition|(
operator|!
name|fs
operator|.
name|rename
argument_list|(
name|renamePair
operator|.
name|getOldPath
argument_list|()
argument_list|,
name|renamePair
operator|.
name|getNewPath
argument_list|()
argument_list|)
condition|)
block|{
name|String
name|msg
init|=
name|Warehouse
operator|.
name|getQualifiedName
argument_list|(
name|t
argument_list|)
operator|+
literal|": "
operator|+
name|renamePair
operator|.
name|getNewPath
argument_list|()
operator|+
literal|": failed to rename"
decl_stmt|;
name|LOG
operator|.
name|error
argument_list|(
name|msg
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|IllegalStateException
argument_list|(
name|msg
argument_list|)
throw|;
block|}
block|}
comment|//do this with and w/o execute to know what was done
name|makeRenameCommand
argument_list|(
name|renamePair
operator|.
name|getOldPath
argument_list|()
argument_list|,
name|renamePair
operator|.
name|getNewPath
argument_list|()
argument_list|,
name|pw
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
operator|!
name|renames
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|println
argument_list|(
name|pw
argument_list|,
literal|"#End file renames for unbucketed table "
operator|+
name|Warehouse
operator|.
name|getQualifiedName
argument_list|(
name|t
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
specifier|private
specifier|static
name|void
name|makeRenameCommand
parameter_list|(
name|Path
name|file
parameter_list|,
name|Path
name|newFile
parameter_list|,
name|PrintWriter
name|pw
parameter_list|)
block|{
comment|//https://hadoop.apache.org/docs/r3.0.0-alpha2/hadoop-project-dist/hadoop-common/FileSystemShell.html#mv
name|println
argument_list|(
name|pw
argument_list|,
literal|"hadoop fs -mv "
operator|+
name|file
operator|+
literal|" "
operator|+
name|newFile
operator|+
literal|";"
argument_list|)
expr_stmt|;
block|}
specifier|private
specifier|static
name|void
name|makeDirectoryCommand
parameter_list|(
name|Path
name|dir
parameter_list|,
name|PrintWriter
name|pw
parameter_list|)
block|{
name|println
argument_list|(
name|pw
argument_list|,
literal|"hadoop fs -mkdir "
operator|+
name|dir
operator|+
literal|";"
argument_list|)
expr_stmt|;
block|}
specifier|private
specifier|static
name|void
name|println
parameter_list|(
name|PrintWriter
name|pw
parameter_list|,
name|String
name|msg
parameter_list|)
block|{
if|if
condition|(
name|pw
operator|!=
literal|null
condition|)
block|{
name|pw
operator|.
name|println
argument_list|(
name|msg
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * Need better impl to be more memory efficient - there could be a lot of these objects.    * For example, remember partition root Path elsewhere,    * and have this object remember relative path to old file and bucketid/deletaid of new one    */
specifier|private
specifier|static
specifier|final
class|class
name|RenamePair
block|{
specifier|private
name|Path
name|oldPath
decl_stmt|;
specifier|private
name|Path
name|newPath
decl_stmt|;
specifier|private
name|RenamePair
parameter_list|(
name|Path
name|old
parameter_list|,
name|Path
name|newPath
parameter_list|)
block|{
name|this
operator|.
name|oldPath
operator|=
name|old
expr_stmt|;
name|this
operator|.
name|newPath
operator|=
name|newPath
expr_stmt|;
block|}
specifier|private
name|Path
name|getOldPath
parameter_list|()
block|{
return|return
name|oldPath
return|;
block|}
specifier|private
name|Path
name|getNewPath
parameter_list|()
block|{
return|return
name|newPath
return|;
block|}
block|}
comment|/**    * @param location - path to a partition (or table if not partitioned) dir    */
specifier|private
specifier|static
name|long
name|getDataSize
parameter_list|(
name|Path
name|location
parameter_list|,
name|Configuration
name|conf
parameter_list|)
throws|throws
name|IOException
block|{
name|FileSystem
name|fs
init|=
name|location
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|ContentSummary
name|cs
init|=
name|fs
operator|.
name|getContentSummary
argument_list|(
name|location
argument_list|)
decl_stmt|;
return|return
name|cs
operator|.
name|getLength
argument_list|()
return|;
block|}
comment|/**    * @param fileName - matching {@link AcidUtils#ORIGINAL_PATTERN_COPY}    */
specifier|private
specifier|static
name|String
name|stripCopySuffix
parameter_list|(
name|String
name|fileName
parameter_list|)
block|{
comment|//0000_0_copy_N -> 0000_0
return|return
name|fileName
operator|.
name|substring
argument_list|(
literal|0
argument_list|,
name|fileName
operator|.
name|indexOf
argument_list|(
literal|'_'
argument_list|,
literal|1
operator|+
name|fileName
operator|.
name|indexOf
argument_list|(
literal|'_'
argument_list|,
literal|0
argument_list|)
argument_list|)
argument_list|)
return|;
block|}
comment|/**    * Since current compactor derives its parallelism from file names, we need to name files in    * a way to control this parallelism.  This should be a function of data size.    * @param partitionSizeInBytes    * @return cannot exceed 4096    */
specifier|public
specifier|static
name|int
name|guessNumBuckets
parameter_list|(
name|long
name|partitionSizeInBytes
parameter_list|)
block|{
name|long
name|OneGB
init|=
literal|1000000000
decl_stmt|;
if|if
condition|(
name|partitionSizeInBytes
operator|<=
literal|1000000000
condition|)
block|{
return|return
literal|1
return|;
comment|//1 bucket
block|}
if|if
condition|(
name|partitionSizeInBytes
operator|<=
literal|100
operator|*
name|OneGB
condition|)
block|{
return|return
literal|8
return|;
block|}
if|if
condition|(
name|partitionSizeInBytes
operator|<=
literal|1000
operator|*
name|OneGB
condition|)
block|{
comment|//TB
return|return
literal|16
return|;
block|}
if|if
condition|(
name|partitionSizeInBytes
operator|<=
literal|10
operator|*
literal|1000
operator|*
name|OneGB
condition|)
block|{
comment|//10 TB
return|return
literal|32
return|;
block|}
if|if
condition|(
name|partitionSizeInBytes
operator|<=
literal|100
operator|*
literal|1000
operator|*
name|OneGB
condition|)
block|{
comment|//100TB
return|return
literal|64
return|;
block|}
if|if
condition|(
name|partitionSizeInBytes
operator|<=
literal|1000
operator|*
literal|1000
operator|*
name|OneGB
condition|)
block|{
comment|//PT
return|return
literal|128
return|;
block|}
if|if
condition|(
name|partitionSizeInBytes
operator|<=
literal|10
operator|*
literal|1000
operator|*
literal|1000
operator|*
name|OneGB
condition|)
block|{
comment|//10 PT
return|return
literal|256
return|;
block|}
if|if
condition|(
name|partitionSizeInBytes
operator|<=
literal|100
operator|*
literal|1000
operator|*
literal|1000
operator|*
name|OneGB
condition|)
block|{
comment|//100 PT
return|return
literal|512
return|;
block|}
if|if
condition|(
name|partitionSizeInBytes
operator|<=
literal|1000
operator|*
literal|1000
operator|*
literal|1000
operator|*
name|OneGB
condition|)
block|{
comment|//1000 PT
return|return
literal|1024
return|;
block|}
return|return
literal|2048
return|;
block|}
comment|/**    * todo: handle exclusion list    * Figures out which tables to make Acid, MM and (optionally, performs the operation)    */
specifier|private
specifier|static
name|void
name|processConversion
parameter_list|(
name|Table
name|t
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|convertToAcid
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|convertToMM
parameter_list|,
name|IMetaStoreClient
name|hms
parameter_list|,
name|Hive
name|db
parameter_list|,
name|boolean
name|execute
parameter_list|,
name|PrintWriter
name|pw
parameter_list|)
throws|throws
name|TException
throws|,
name|HiveException
throws|,
name|IOException
block|{
if|if
condition|(
name|isFullAcidTable
argument_list|(
name|t
argument_list|)
condition|)
block|{
return|return;
block|}
if|if
condition|(
operator|!
name|TableType
operator|.
name|MANAGED_TABLE
operator|.
name|name
argument_list|()
operator|.
name|equalsIgnoreCase
argument_list|(
name|t
operator|.
name|getTableType
argument_list|()
argument_list|)
condition|)
block|{
return|return;
block|}
comment|//todo: are HBase, Druid talbes managed in 2.x? 3.0?
name|String
name|fullTableName
init|=
name|Warehouse
operator|.
name|getQualifiedName
argument_list|(
name|t
argument_list|)
decl_stmt|;
comment|/*      * ORC uses table props for settings so things like bucketing, I/O Format, etc should      * be the same for each partition.      */
name|boolean
name|canBeMadeAcid
init|=
name|canBeMadeAcid
argument_list|(
name|fullTableName
argument_list|,
name|t
operator|.
name|getSd
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|t
operator|.
name|getPartitionKeysSize
argument_list|()
operator|<=
literal|0
condition|)
block|{
if|if
condition|(
name|canBeMadeAcid
condition|)
block|{
name|convertToAcid
operator|.
name|add
argument_list|(
literal|"ALTER TABLE "
operator|+
name|Warehouse
operator|.
name|getQualifiedName
argument_list|(
name|t
argument_list|)
operator|+
literal|" SET TBLPROPERTIES ("
operator|+
literal|"'transactional'='true')"
argument_list|)
expr_stmt|;
comment|//do this before alterTable in case files need to be renamed, else
comment|// TransactionalMetastoreListerner will squak
name|handleRenameFiles
argument_list|(
name|t
argument_list|,
operator|new
name|Path
argument_list|(
name|t
operator|.
name|getSd
argument_list|()
operator|.
name|getLocation
argument_list|()
argument_list|)
argument_list|,
name|execute
argument_list|,
name|db
operator|.
name|getConf
argument_list|()
argument_list|,
name|t
operator|.
name|getSd
argument_list|()
operator|.
name|getBucketColsSize
argument_list|()
operator|>
literal|0
argument_list|,
name|pw
argument_list|)
expr_stmt|;
if|if
condition|(
name|execute
condition|)
block|{
name|alterTable
argument_list|(
name|t
argument_list|,
name|db
argument_list|,
literal|false
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
name|convertToMM
operator|.
name|add
argument_list|(
literal|"ALTER TABLE "
operator|+
name|Warehouse
operator|.
name|getQualifiedName
argument_list|(
name|t
argument_list|)
operator|+
literal|" SET TBLPROPERTIES ("
operator|+
literal|"'transactional'='true', 'transactional_properties'='insert_only')"
argument_list|)
expr_stmt|;
if|if
condition|(
name|execute
condition|)
block|{
name|alterTable
argument_list|(
name|t
argument_list|,
name|db
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
block|}
block|}
else|else
block|{
if|if
condition|(
operator|!
name|canBeMadeAcid
condition|)
block|{
name|convertToMM
operator|.
name|add
argument_list|(
literal|"ALTER TABLE "
operator|+
name|Warehouse
operator|.
name|getQualifiedName
argument_list|(
name|t
argument_list|)
operator|+
literal|" SET TBLPROPERTIES ("
operator|+
literal|"'transactional'='true', 'transactional_properties'='insert_only')"
argument_list|)
expr_stmt|;
if|if
condition|(
name|execute
condition|)
block|{
name|alterTable
argument_list|(
name|t
argument_list|,
name|db
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
return|return;
block|}
comment|//now that we know it can be made acid, rename files as needed
comment|//process in batches in case there is a huge # of partitions
name|List
argument_list|<
name|String
argument_list|>
name|partNames
init|=
name|hms
operator|.
name|listPartitionNames
argument_list|(
name|t
operator|.
name|getDbName
argument_list|()
argument_list|,
name|t
operator|.
name|getTableName
argument_list|()
argument_list|,
operator|(
name|short
operator|)
operator|-
literal|1
argument_list|)
decl_stmt|;
name|int
name|batchSize
init|=
name|PARTITION_BATCH_SIZE
decl_stmt|;
name|int
name|numWholeBatches
init|=
name|partNames
operator|.
name|size
argument_list|()
operator|/
name|batchSize
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|numWholeBatches
condition|;
name|i
operator|++
control|)
block|{
name|List
argument_list|<
name|Partition
argument_list|>
name|partitionList
init|=
name|hms
operator|.
name|getPartitionsByNames
argument_list|(
name|t
operator|.
name|getDbName
argument_list|()
argument_list|,
name|t
operator|.
name|getTableName
argument_list|()
argument_list|,
name|partNames
operator|.
name|subList
argument_list|(
name|i
operator|*
name|batchSize
argument_list|,
operator|(
name|i
operator|+
literal|1
operator|)
operator|*
name|batchSize
argument_list|)
argument_list|)
decl_stmt|;
for|for
control|(
name|Partition
name|part
range|:
name|partitionList
control|)
block|{
name|handleRenameFiles
argument_list|(
name|t
argument_list|,
operator|new
name|Path
argument_list|(
name|part
operator|.
name|getSd
argument_list|()
operator|.
name|getLocation
argument_list|()
argument_list|)
argument_list|,
name|execute
argument_list|,
name|db
operator|.
name|getConf
argument_list|()
argument_list|,
name|t
operator|.
name|getSd
argument_list|()
operator|.
name|getBucketColsSize
argument_list|()
operator|>
literal|0
argument_list|,
name|pw
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
name|numWholeBatches
operator|*
name|batchSize
operator|<
name|partNames
operator|.
name|size
argument_list|()
condition|)
block|{
comment|//last partial batch
name|List
argument_list|<
name|Partition
argument_list|>
name|partitionList
init|=
name|hms
operator|.
name|getPartitionsByNames
argument_list|(
name|t
operator|.
name|getDbName
argument_list|()
argument_list|,
name|t
operator|.
name|getTableName
argument_list|()
argument_list|,
name|partNames
operator|.
name|subList
argument_list|(
name|numWholeBatches
operator|*
name|batchSize
argument_list|,
name|partNames
operator|.
name|size
argument_list|()
argument_list|)
argument_list|)
decl_stmt|;
for|for
control|(
name|Partition
name|part
range|:
name|partitionList
control|)
block|{
name|handleRenameFiles
argument_list|(
name|t
argument_list|,
operator|new
name|Path
argument_list|(
name|part
operator|.
name|getSd
argument_list|()
operator|.
name|getLocation
argument_list|()
argument_list|)
argument_list|,
name|execute
argument_list|,
name|db
operator|.
name|getConf
argument_list|()
argument_list|,
name|t
operator|.
name|getSd
argument_list|()
operator|.
name|getBucketColsSize
argument_list|()
operator|>
literal|0
argument_list|,
name|pw
argument_list|)
expr_stmt|;
block|}
block|}
comment|//if here, handled all parts and they are no wAcid compatible - make it acid
name|convertToAcid
operator|.
name|add
argument_list|(
literal|"ALTER TABLE "
operator|+
name|Warehouse
operator|.
name|getQualifiedName
argument_list|(
name|t
argument_list|)
operator|+
literal|" SET TBLPROPERTIES ("
operator|+
literal|"'transactional'='true')"
argument_list|)
expr_stmt|;
if|if
condition|(
name|execute
condition|)
block|{
name|alterTable
argument_list|(
name|t
argument_list|,
name|db
argument_list|,
literal|false
argument_list|)
expr_stmt|;
block|}
block|}
block|}
specifier|private
specifier|static
name|boolean
name|canBeMadeAcid
parameter_list|(
name|String
name|fullTableName
parameter_list|,
name|StorageDescriptor
name|sd
parameter_list|)
block|{
return|return
name|isAcidInputOutputFormat
argument_list|(
name|fullTableName
argument_list|,
name|sd
argument_list|)
operator|&&
name|sd
operator|.
name|getSortColsSize
argument_list|()
operator|<=
literal|0
return|;
block|}
specifier|private
specifier|static
name|boolean
name|isAcidInputOutputFormat
parameter_list|(
name|String
name|fullTableName
parameter_list|,
name|StorageDescriptor
name|sd
parameter_list|)
block|{
try|try
block|{
name|Class
name|inputFormatClass
init|=
name|sd
operator|.
name|getInputFormat
argument_list|()
operator|==
literal|null
condition|?
literal|null
else|:
name|Class
operator|.
name|forName
argument_list|(
name|sd
operator|.
name|getInputFormat
argument_list|()
argument_list|)
decl_stmt|;
name|Class
name|outputFormatClass
init|=
name|sd
operator|.
name|getOutputFormat
argument_list|()
operator|==
literal|null
condition|?
literal|null
else|:
name|Class
operator|.
name|forName
argument_list|(
name|sd
operator|.
name|getOutputFormat
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|inputFormatClass
operator|!=
literal|null
operator|&&
name|outputFormatClass
operator|!=
literal|null
operator|&&
name|Class
operator|.
name|forName
argument_list|(
literal|"org.apache.hadoop.hive.ql.io.AcidInputFormat"
argument_list|)
operator|.
name|isAssignableFrom
argument_list|(
name|inputFormatClass
argument_list|)
operator|&&
name|Class
operator|.
name|forName
argument_list|(
literal|"org.apache.hadoop.hive.ql.io.AcidOutputFormat"
argument_list|)
operator|.
name|isAssignableFrom
argument_list|(
name|outputFormatClass
argument_list|)
condition|)
block|{
return|return
literal|true
return|;
block|}
block|}
catch|catch
parameter_list|(
name|ClassNotFoundException
name|e
parameter_list|)
block|{
comment|//if a table is using some custom I/O format and it's not in the classpath, we won't mark
comment|//the table for Acid, but today (Hive 3.1 and earlier) OrcInput/OutputFormat is the only
comment|//Acid format
name|LOG
operator|.
name|error
argument_list|(
literal|"Could not determine if "
operator|+
name|fullTableName
operator|+
literal|" can be made Acid due to: "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
expr_stmt|;
return|return
literal|false
return|;
block|}
return|return
literal|false
return|;
block|}
specifier|private
specifier|static
name|void
name|makeConvertTableScript
parameter_list|(
name|List
argument_list|<
name|String
argument_list|>
name|alterTableAcid
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|alterTableMm
parameter_list|,
name|String
name|scriptLocation
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|alterTableAcid
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"No acid conversion is necessary"
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|String
name|fileName
init|=
literal|"convertToAcid_"
operator|+
name|System
operator|.
name|currentTimeMillis
argument_list|()
operator|+
literal|".sql"
decl_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Writing CRUD conversion commands to "
operator|+
name|fileName
argument_list|)
expr_stmt|;
try|try
init|(
name|PrintWriter
name|pw
init|=
name|createScript
argument_list|(
name|alterTableAcid
argument_list|,
name|fileName
argument_list|,
name|scriptLocation
argument_list|)
init|)
block|{
name|pw
operator|.
name|println
argument_list|(
literal|"-- These commands may be executed by Hive 3.x later"
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
name|alterTableMm
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"No managed table conversion is necessary"
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|String
name|fileName
init|=
literal|"convertToMM_"
operator|+
name|System
operator|.
name|currentTimeMillis
argument_list|()
operator|+
literal|".sql"
decl_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Writing managed table conversion commands to "
operator|+
name|fileName
argument_list|)
expr_stmt|;
try|try
init|(
name|PrintWriter
name|pw
init|=
name|createScript
argument_list|(
name|alterTableMm
argument_list|,
name|fileName
argument_list|,
name|scriptLocation
argument_list|)
init|)
block|{
name|pw
operator|.
name|println
argument_list|(
literal|"-- These commands must be executed by Hive 3.0 or later"
argument_list|)
expr_stmt|;
block|}
block|}
block|}
specifier|private
specifier|static
name|PrintWriter
name|createScript
parameter_list|(
name|List
argument_list|<
name|String
argument_list|>
name|commands
parameter_list|,
name|String
name|fileName
parameter_list|,
name|String
name|scriptLocation
parameter_list|)
throws|throws
name|IOException
block|{
name|FileWriter
name|fw
init|=
operator|new
name|FileWriter
argument_list|(
name|scriptLocation
operator|+
literal|"/"
operator|+
name|fileName
argument_list|)
decl_stmt|;
name|PrintWriter
name|pw
init|=
operator|new
name|PrintWriter
argument_list|(
name|fw
argument_list|)
decl_stmt|;
for|for
control|(
name|String
name|cmd
range|:
name|commands
control|)
block|{
name|pw
operator|.
name|println
argument_list|(
name|cmd
operator|+
literal|";"
argument_list|)
expr_stmt|;
block|}
return|return
name|pw
return|;
block|}
specifier|private
specifier|static
name|PrintWriter
name|makeRenameFileScript
parameter_list|(
name|String
name|scriptLocation
parameter_list|)
throws|throws
name|IOException
block|{
name|String
name|fileName
init|=
literal|"normalizeFileNames_"
operator|+
name|System
operator|.
name|currentTimeMillis
argument_list|()
operator|+
literal|".sh"
decl_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Writing file renaming commands to "
operator|+
name|fileName
argument_list|)
expr_stmt|;
return|return
name|createScript
argument_list|(
name|Collections
operator|.
name|emptyList
argument_list|()
argument_list|,
name|fileName
argument_list|,
name|scriptLocation
argument_list|)
return|;
block|}
specifier|private
specifier|static
name|boolean
name|isFullAcidTable
parameter_list|(
name|Table
name|t
parameter_list|)
block|{
if|if
condition|(
name|t
operator|.
name|getParametersSize
argument_list|()
operator|<=
literal|0
condition|)
block|{
comment|//cannot be acid
return|return
literal|false
return|;
block|}
name|String
name|transacationalValue
init|=
name|t
operator|.
name|getParameters
argument_list|()
operator|.
name|get
argument_list|(
name|hive_metastoreConstants
operator|.
name|TABLE_IS_TRANSACTIONAL
argument_list|)
decl_stmt|;
if|if
condition|(
name|transacationalValue
operator|!=
literal|null
operator|&&
literal|"true"
operator|.
name|equalsIgnoreCase
argument_list|(
name|transacationalValue
argument_list|)
condition|)
block|{
name|System
operator|.
name|out
operator|.
name|println
argument_list|(
literal|"Found Acid table: "
operator|+
name|Warehouse
operator|.
name|getQualifiedName
argument_list|(
name|t
argument_list|)
argument_list|)
expr_stmt|;
return|return
literal|true
return|;
block|}
return|return
literal|false
return|;
block|}
specifier|private
specifier|static
name|boolean
name|isAcidEnabled
parameter_list|(
name|HiveConf
name|hiveConf
parameter_list|)
block|{
name|String
name|txnMgr
init|=
name|hiveConf
operator|.
name|getVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_TXN_MANAGER
argument_list|)
decl_stmt|;
name|boolean
name|concurrency
init|=
name|hiveConf
operator|.
name|getBoolVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_SUPPORT_CONCURRENCY
argument_list|)
decl_stmt|;
name|String
name|dbTxnMgr
init|=
literal|"org.apache.hadoop.hive.ql.lockmgr.DbTxnManager"
decl_stmt|;
return|return
name|txnMgr
operator|.
name|equals
argument_list|(
name|dbTxnMgr
argument_list|)
operator|&&
name|concurrency
return|;
block|}
comment|/**    * can set it from tests to test when config needs something other than default values    * For example, that acid is enabled    */
annotation|@
name|VisibleForTesting
specifier|static
name|HiveConf
name|hiveConf
init|=
literal|null
decl_stmt|;
block|}
end_class

end_unit

