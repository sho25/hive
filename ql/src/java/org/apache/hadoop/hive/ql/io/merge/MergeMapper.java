begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/**  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|merge
package|;
end_package

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|Log
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|LogFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileStatus
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|Utilities
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|HiveException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|JobConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|MapReduceBase
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashSet
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Set
import|;
end_import

begin_class
specifier|public
class|class
name|MergeMapper
extends|extends
name|MapReduceBase
block|{
specifier|protected
name|JobConf
name|jc
decl_stmt|;
specifier|protected
name|Path
name|finalPath
decl_stmt|;
specifier|protected
name|FileSystem
name|fs
decl_stmt|;
specifier|protected
name|boolean
name|exception
init|=
literal|false
decl_stmt|;
specifier|protected
name|boolean
name|autoDelete
init|=
literal|false
decl_stmt|;
specifier|protected
name|Path
name|outPath
decl_stmt|;
specifier|protected
name|boolean
name|hasDynamicPartitions
init|=
literal|false
decl_stmt|;
specifier|protected
name|boolean
name|isListBucketingDML
init|=
literal|false
decl_stmt|;
specifier|protected
name|boolean
name|isListBucketingAlterTableConcatenate
init|=
literal|false
decl_stmt|;
comment|//used as depth for dir-calculation and if it is list bucketing case.
specifier|protected
name|int
name|listBucketingDepth
decl_stmt|;
specifier|protected
name|boolean
name|tmpPathFixedConcatenate
init|=
literal|false
decl_stmt|;
specifier|protected
name|boolean
name|tmpPathFixed
init|=
literal|false
decl_stmt|;
specifier|protected
name|Path
name|tmpPath
decl_stmt|;
specifier|protected
name|Path
name|taskTmpPath
decl_stmt|;
specifier|protected
name|Path
name|dpPath
decl_stmt|;
specifier|protected
name|Set
argument_list|<
name|Path
argument_list|>
name|incompatFileSet
decl_stmt|;
specifier|public
specifier|final
specifier|static
name|Log
name|LOG
init|=
name|LogFactory
operator|.
name|getLog
argument_list|(
literal|"MergeMapper"
argument_list|)
decl_stmt|;
annotation|@
name|Override
specifier|public
name|void
name|configure
parameter_list|(
name|JobConf
name|job
parameter_list|)
block|{
name|jc
operator|=
name|job
expr_stmt|;
name|hasDynamicPartitions
operator|=
name|HiveConf
operator|.
name|getBoolVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEMERGECURRENTJOBHASDYNAMICPARTITIONS
argument_list|)
expr_stmt|;
name|isListBucketingAlterTableConcatenate
operator|=
name|HiveConf
operator|.
name|getBoolVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEMERGECURRENTJOBCONCATENATELISTBUCKETING
argument_list|)
expr_stmt|;
name|listBucketingDepth
operator|=
name|HiveConf
operator|.
name|getIntVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEMERGECURRENTJOBCONCATENATELISTBUCKETINGDEPTH
argument_list|)
expr_stmt|;
name|Path
name|specPath
init|=
name|MergeOutputFormat
operator|.
name|getMergeOutputPath
argument_list|(
name|job
argument_list|)
decl_stmt|;
name|incompatFileSet
operator|=
operator|new
name|HashSet
argument_list|<
name|Path
argument_list|>
argument_list|()
expr_stmt|;
name|Path
name|tmpPath
init|=
name|Utilities
operator|.
name|toTempPath
argument_list|(
name|specPath
argument_list|)
decl_stmt|;
name|Path
name|taskTmpPath
init|=
name|Utilities
operator|.
name|toTaskTempPath
argument_list|(
name|specPath
argument_list|)
decl_stmt|;
name|updatePaths
argument_list|(
name|tmpPath
argument_list|,
name|taskTmpPath
argument_list|)
expr_stmt|;
try|try
block|{
name|fs
operator|=
name|specPath
operator|.
name|getFileSystem
argument_list|(
name|job
argument_list|)
expr_stmt|;
name|autoDelete
operator|=
name|fs
operator|.
name|deleteOnExit
argument_list|(
name|outPath
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|this
operator|.
name|exception
operator|=
literal|true
expr_stmt|;
throw|throw
operator|new
name|RuntimeException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|private
name|void
name|updatePaths
parameter_list|(
name|Path
name|tmpPath
parameter_list|,
name|Path
name|taskTmpPath
parameter_list|)
block|{
name|String
name|taskId
init|=
name|Utilities
operator|.
name|getTaskId
argument_list|(
name|jc
argument_list|)
decl_stmt|;
name|this
operator|.
name|tmpPath
operator|=
name|tmpPath
expr_stmt|;
name|this
operator|.
name|taskTmpPath
operator|=
name|taskTmpPath
expr_stmt|;
name|finalPath
operator|=
operator|new
name|Path
argument_list|(
name|tmpPath
argument_list|,
name|taskId
argument_list|)
expr_stmt|;
name|outPath
operator|=
operator|new
name|Path
argument_list|(
name|taskTmpPath
argument_list|,
name|Utilities
operator|.
name|toTempPath
argument_list|(
name|taskId
argument_list|)
argument_list|)
expr_stmt|;
block|}
comment|/**    * Validates that each input path belongs to the same partition since each mapper merges the input    * to a single output directory    * @param inputPath    * @throws HiveException    */
specifier|protected
name|void
name|checkPartitionsMatch
parameter_list|(
name|Path
name|inputPath
parameter_list|)
throws|throws
name|HiveException
block|{
if|if
condition|(
operator|!
name|dpPath
operator|.
name|equals
argument_list|(
name|inputPath
argument_list|)
condition|)
block|{
comment|// Temp partition input path does not match exist temp path
name|String
name|msg
init|=
literal|"Multiple partitions for one block merge mapper: "
operator|+
name|dpPath
operator|+
literal|" NOT EQUAL TO "
operator|+
name|inputPath
decl_stmt|;
name|LOG
operator|.
name|error
argument_list|(
name|msg
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|msg
argument_list|)
throw|;
block|}
block|}
comment|/**    * Fixes tmpPath to point to the correct partition. Before this is called, tmpPath will default to    * the root tmp table dir fixTmpPath(..) works for DP + LB + multiple skewed values + merge.    * reason: 1. fixTmpPath(..) compares inputPath and tmpDepth, find out path difference and put it    * into newPath. Then add newpath to existing this.tmpPath and this.taskTmpPath. 2. The path    * difference between inputPath and tmpDepth can be DP or DP+LB. It will automatically handle it.    * 3. For example, if inputpath is<prefix>/-ext-10002/hr=a1/HIVE_DEFAULT_LIST_BUCKETING_DIR_NAME/    * HIVE_DEFAULT_LIST_BUCKETING_DIR_NAME tmppath is<prefix>/_tmp.-ext-10000 newpath will be    * hr=a1/HIVE_DEFAULT_LIST_BUCKETING_DIR_NAME/HIVE_DEFAULT_LIST_BUCKETING_DIR_NAME Then,    * this.tmpPath and this.taskTmpPath will be update correctly. We have list_bucket_dml_6.q cover    * this case: DP + LP + multiple skewed values + merge.    * @param inputPath    * @throws HiveException    * @throws IOException    */
specifier|protected
name|void
name|fixTmpPath
parameter_list|(
name|Path
name|inputPath
parameter_list|)
throws|throws
name|HiveException
throws|,
name|IOException
block|{
name|dpPath
operator|=
name|inputPath
expr_stmt|;
name|Path
name|newPath
init|=
operator|new
name|Path
argument_list|(
literal|"."
argument_list|)
decl_stmt|;
name|int
name|inputDepth
init|=
name|inputPath
operator|.
name|depth
argument_list|()
decl_stmt|;
name|int
name|tmpDepth
init|=
name|tmpPath
operator|.
name|depth
argument_list|()
decl_stmt|;
comment|// Build the path from bottom up
while|while
condition|(
name|inputPath
operator|!=
literal|null
operator|&&
name|inputDepth
operator|>
name|tmpDepth
condition|)
block|{
name|newPath
operator|=
operator|new
name|Path
argument_list|(
name|inputPath
operator|.
name|getName
argument_list|()
argument_list|,
name|newPath
argument_list|)
expr_stmt|;
name|inputDepth
operator|--
expr_stmt|;
name|inputPath
operator|=
name|inputPath
operator|.
name|getParent
argument_list|()
expr_stmt|;
block|}
name|Path
name|newTmpPath
init|=
operator|new
name|Path
argument_list|(
name|tmpPath
argument_list|,
name|newPath
argument_list|)
decl_stmt|;
name|Path
name|newTaskTmpPath
init|=
operator|new
name|Path
argument_list|(
name|taskTmpPath
argument_list|,
name|newPath
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|fs
operator|.
name|exists
argument_list|(
name|newTmpPath
argument_list|)
condition|)
block|{
name|fs
operator|.
name|mkdirs
argument_list|(
name|newTmpPath
argument_list|)
expr_stmt|;
block|}
name|updatePaths
argument_list|(
name|newTmpPath
argument_list|,
name|newTaskTmpPath
argument_list|)
expr_stmt|;
block|}
comment|/**    * Fixes tmpPath to point to the correct list bucketing sub-directories. Before this is called,    * tmpPath will default to the root tmp table dir Reason to add a new method instead of changing    * fixTmpPath() Reason 1: logic has slightly difference fixTmpPath(..) needs 2 variables in order    * to decide path delta which is in variable newPath. 1. inputPath.depth() 2. tmpPath.depth()    * fixTmpPathConcatenate needs 2 variables too but one of them is different from fixTmpPath(..) 1.    * inputPath.depth() 2. listBucketingDepth Reason 2: less risks The existing logic is a little not    * trivial around map() and fixTmpPath(). In order to ensure minimum impact on existing flow, we    * try to avoid change on existing code/flow but add new code for new feature.    * @param inputPath    * @throws HiveException    * @throws IOException    */
specifier|protected
name|void
name|fixTmpPathConcatenate
parameter_list|(
name|Path
name|inputPath
parameter_list|)
throws|throws
name|HiveException
throws|,
name|IOException
block|{
name|dpPath
operator|=
name|inputPath
expr_stmt|;
name|Path
name|newPath
init|=
operator|new
name|Path
argument_list|(
literal|"."
argument_list|)
decl_stmt|;
name|int
name|depth
init|=
name|listBucketingDepth
decl_stmt|;
comment|// Build the path from bottom up. pick up list bucketing subdirectories
while|while
condition|(
operator|(
name|inputPath
operator|!=
literal|null
operator|)
operator|&&
operator|(
name|depth
operator|>
literal|0
operator|)
condition|)
block|{
name|newPath
operator|=
operator|new
name|Path
argument_list|(
name|inputPath
operator|.
name|getName
argument_list|()
argument_list|,
name|newPath
argument_list|)
expr_stmt|;
name|inputPath
operator|=
name|inputPath
operator|.
name|getParent
argument_list|()
expr_stmt|;
name|depth
operator|--
expr_stmt|;
block|}
name|Path
name|newTmpPath
init|=
operator|new
name|Path
argument_list|(
name|tmpPath
argument_list|,
name|newPath
argument_list|)
decl_stmt|;
name|Path
name|newTaskTmpPath
init|=
operator|new
name|Path
argument_list|(
name|taskTmpPath
argument_list|,
name|newPath
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|fs
operator|.
name|exists
argument_list|(
name|newTmpPath
argument_list|)
condition|)
block|{
name|fs
operator|.
name|mkdirs
argument_list|(
name|newTmpPath
argument_list|)
expr_stmt|;
block|}
name|updatePaths
argument_list|(
name|newTmpPath
argument_list|,
name|newTaskTmpPath
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|close
parameter_list|()
throws|throws
name|IOException
block|{
if|if
condition|(
operator|!
name|exception
condition|)
block|{
name|FileStatus
name|fss
init|=
name|fs
operator|.
name|getFileStatus
argument_list|(
name|outPath
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"renamed path "
operator|+
name|outPath
operator|+
literal|" to "
operator|+
name|finalPath
operator|+
literal|" . File size is "
operator|+
name|fss
operator|.
name|getLen
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|fs
operator|.
name|rename
argument_list|(
name|outPath
argument_list|,
name|finalPath
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Unable to rename output to "
operator|+
name|finalPath
argument_list|)
throw|;
block|}
comment|// move any incompatible files to final path
if|if
condition|(
operator|!
name|incompatFileSet
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
for|for
control|(
name|Path
name|incompatFile
range|:
name|incompatFileSet
control|)
block|{
name|String
name|fileName
init|=
name|incompatFile
operator|.
name|getName
argument_list|()
decl_stmt|;
name|Path
name|destFile
init|=
operator|new
name|Path
argument_list|(
name|finalPath
operator|.
name|getParent
argument_list|()
argument_list|,
name|fileName
argument_list|)
decl_stmt|;
try|try
block|{
name|Utilities
operator|.
name|renameOrMoveFiles
argument_list|(
name|fs
argument_list|,
name|incompatFile
argument_list|,
name|destFile
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Moved incompatible file "
operator|+
name|incompatFile
operator|+
literal|" to "
operator|+
name|destFile
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|HiveException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Unable to move "
operator|+
name|incompatFile
operator|+
literal|" to "
operator|+
name|destFile
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|IOException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
block|}
block|}
else|else
block|{
if|if
condition|(
operator|!
name|autoDelete
condition|)
block|{
name|fs
operator|.
name|delete
argument_list|(
name|outPath
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
block|}
block|}
specifier|protected
name|void
name|fixTmpPathAlterTable
parameter_list|(
name|Path
name|path
parameter_list|)
throws|throws
name|IOException
throws|,
name|HiveException
block|{
comment|/**      * 1. boolean isListBucketingAlterTableConcatenate will be true only if it is alter table ...      * concatenate on stored-as-dir so it will handle list bucketing alter table merge in the if      * cause with the help of fixTmpPathConcatenate 2. If it is DML,      * isListBucketingAlterTableConcatenate will be false so that it will be handled by else      * cause. In this else cause, we have another if check. 2.1 the if check will make sure DP or      * LB, we will fix path with the help of fixTmpPath(..). Since both has sub-directories. it      * includes SP + LB. 2.2 only SP without LB, we dont fix path.      */
comment|// Fix temp path for alter table ... concatenate
if|if
condition|(
name|isListBucketingAlterTableConcatenate
condition|)
block|{
if|if
condition|(
name|this
operator|.
name|tmpPathFixedConcatenate
condition|)
block|{
name|checkPartitionsMatch
argument_list|(
name|path
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|fixTmpPathConcatenate
argument_list|(
name|path
argument_list|)
expr_stmt|;
name|tmpPathFixedConcatenate
operator|=
literal|true
expr_stmt|;
block|}
block|}
else|else
block|{
if|if
condition|(
name|hasDynamicPartitions
operator|||
operator|(
name|listBucketingDepth
operator|>
literal|0
operator|)
condition|)
block|{
if|if
condition|(
name|tmpPathFixed
condition|)
block|{
name|checkPartitionsMatch
argument_list|(
name|path
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// We haven't fixed the TMP path for this mapper yet
name|fixTmpPath
argument_list|(
name|path
argument_list|)
expr_stmt|;
name|tmpPathFixed
operator|=
literal|true
expr_stmt|;
block|}
block|}
block|}
block|}
block|}
end_class

end_unit

