begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/*  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
package|;
end_package

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|DataOutputStream
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|Serializable
import|;
end_import

begin_import
import|import
name|java
operator|.
name|net
operator|.
name|URI
import|;
end_import

begin_import
import|import
name|java
operator|.
name|net
operator|.
name|URISyntaxException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Iterator
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|LinkedHashMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Set
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|collect
operator|.
name|Lists
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|lang
operator|.
name|StringUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FsShell
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|StatsSetupConst
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
operator|.
name|ConfVars
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|DefaultHiveMetaHook
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|HiveMetaHook
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|HiveMetaStoreUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|Msck
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|MsckInfo
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|TableType
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|Warehouse
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|CompactionResponse
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Database
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|EnvironmentContext
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|InvalidOperationException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|MetaException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Order
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|ShowCompactResponse
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|ShowCompactResponseElement
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|SkewedInfo
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|StorageDescriptor
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|txn
operator|.
name|TxnStore
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|CompilationOpContext
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|DriverContext
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|ErrorMsg
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|QueryPlan
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|QueryState
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|ArchiveUtils
operator|.
name|PartSpecInfo
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|tez
operator|.
name|TezTask
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|hooks
operator|.
name|ReadEntity
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|hooks
operator|.
name|WriteEntity
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|AcidUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|RCFileInputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|merge
operator|.
name|MergeFileTask
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|merge
operator|.
name|MergeFileWork
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|orc
operator|.
name|OrcInputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|orc
operator|.
name|OrcSerde
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|Hive
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|HiveException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|Partition
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|PartitionIterable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|Table
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|parse
operator|.
name|AlterTablePartMergeFilesDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|parse
operator|.
name|DDLSemanticAnalyzer
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|parse
operator|.
name|ExplainConfiguration
operator|.
name|AnalyzeState
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|parse
operator|.
name|ReplicationSpec
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|parse
operator|.
name|SemanticException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|parse
operator|.
name|repl
operator|.
name|dump
operator|.
name|Utils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|AlterTableDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|AlterTableDesc
operator|.
name|AlterTableTypes
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|AlterTableSimpleDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|CacheMetadataDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|DDLWork
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|FileMergeDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|InsertCommitHookDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|ListBucketingCtx
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|LoadMultiFilesDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|MoveWork
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|MsckDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|OperatorDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|OrcFileMergeDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|RCFileMergeDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|ReplRemoveFirstIncLoadPendFlagDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|ShowConfDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|TezWork
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|api
operator|.
name|StageType
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|repl
operator|.
name|util
operator|.
name|ReplUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|session
operator|.
name|SessionState
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|Deserializer
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|MRJobConfig
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|tools
operator|.
name|HadoopArchives
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|ToolRunner
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hive
operator|.
name|common
operator|.
name|util
operator|.
name|ReflectionUtil
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|thrift
operator|.
name|TException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|Logger
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|LoggerFactory
import|;
end_import

begin_comment
comment|/**  * DDLTask implementation.  *  **/
end_comment

begin_class
specifier|public
class|class
name|DDLTask
extends|extends
name|Task
argument_list|<
name|DDLWork
argument_list|>
implements|implements
name|Serializable
block|{
specifier|private
specifier|static
specifier|final
name|long
name|serialVersionUID
init|=
literal|1L
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|Logger
name|LOG
init|=
name|LoggerFactory
operator|.
name|getLogger
argument_list|(
literal|"hive.ql.exec.DDLTask"
argument_list|)
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|int
name|separator
init|=
name|Utilities
operator|.
name|tabCode
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|int
name|terminator
init|=
name|Utilities
operator|.
name|newLineCode
decl_stmt|;
comment|// These are suffixes attached to intermediate directory names used in the
comment|// archiving / un-archiving process.
specifier|private
specifier|static
name|String
name|INTERMEDIATE_ARCHIVED_DIR_SUFFIX
decl_stmt|;
specifier|private
specifier|static
name|String
name|INTERMEDIATE_ORIGINAL_DIR_SUFFIX
decl_stmt|;
specifier|private
specifier|static
name|String
name|INTERMEDIATE_EXTRACTED_DIR_SUFFIX
decl_stmt|;
annotation|@
name|Override
specifier|public
name|boolean
name|requireLock
parameter_list|()
block|{
return|return
name|this
operator|.
name|work
operator|!=
literal|null
operator|&&
name|this
operator|.
name|work
operator|.
name|getNeedLock
argument_list|()
return|;
block|}
specifier|public
name|DDLTask
parameter_list|()
block|{
name|super
argument_list|()
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|initialize
parameter_list|(
name|QueryState
name|queryState
parameter_list|,
name|QueryPlan
name|queryPlan
parameter_list|,
name|DriverContext
name|ctx
parameter_list|,
name|CompilationOpContext
name|opContext
parameter_list|)
block|{
name|super
operator|.
name|initialize
argument_list|(
name|queryState
argument_list|,
name|queryPlan
argument_list|,
name|ctx
argument_list|,
name|opContext
argument_list|)
expr_stmt|;
comment|// Pick the formatter to use to display the results.  Either the
comment|// normal human readable output or a json object.
name|INTERMEDIATE_ARCHIVED_DIR_SUFFIX
operator|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|conf
argument_list|,
name|ConfVars
operator|.
name|METASTORE_INT_ARCHIVED
argument_list|)
expr_stmt|;
name|INTERMEDIATE_ORIGINAL_DIR_SUFFIX
operator|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|conf
argument_list|,
name|ConfVars
operator|.
name|METASTORE_INT_ORIGINAL
argument_list|)
expr_stmt|;
name|INTERMEDIATE_EXTRACTED_DIR_SUFFIX
operator|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|conf
argument_list|,
name|ConfVars
operator|.
name|METASTORE_INT_EXTRACTED
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|int
name|execute
parameter_list|(
name|DriverContext
name|driverContext
parameter_list|)
block|{
if|if
condition|(
name|driverContext
operator|.
name|getCtx
argument_list|()
operator|.
name|getExplainAnalyze
argument_list|()
operator|==
name|AnalyzeState
operator|.
name|RUNNING
condition|)
block|{
return|return
literal|0
return|;
block|}
comment|// Create the db
name|Hive
name|db
decl_stmt|;
try|try
block|{
name|db
operator|=
name|Hive
operator|.
name|get
argument_list|(
name|conf
argument_list|)
expr_stmt|;
name|AlterTableDesc
name|alterTbl
init|=
name|work
operator|.
name|getAlterTblDesc
argument_list|()
decl_stmt|;
if|if
condition|(
name|alterTbl
operator|!=
literal|null
condition|)
block|{
if|if
condition|(
operator|!
name|allowOperationInReplicationScope
argument_list|(
name|db
argument_list|,
name|alterTbl
operator|.
name|getOldName
argument_list|()
argument_list|,
literal|null
argument_list|,
name|alterTbl
operator|.
name|getReplicationSpec
argument_list|()
argument_list|)
condition|)
block|{
comment|// no alter, the table is missing either due to drop/rename which follows the alter.
comment|// or the existing table is newer than our update.
name|LOG
operator|.
name|debug
argument_list|(
literal|"DDLTask: Alter Table is skipped as table {} is newer than update"
argument_list|,
name|alterTbl
operator|.
name|getOldName
argument_list|()
argument_list|)
expr_stmt|;
return|return
literal|0
return|;
block|}
return|return
name|alterTable
argument_list|(
name|db
argument_list|,
name|alterTbl
argument_list|)
return|;
block|}
name|AlterTableSimpleDesc
name|simpleDesc
init|=
name|work
operator|.
name|getAlterTblSimpleDesc
argument_list|()
decl_stmt|;
if|if
condition|(
name|simpleDesc
operator|!=
literal|null
condition|)
block|{
if|if
condition|(
name|simpleDesc
operator|.
name|getType
argument_list|()
operator|==
name|AlterTableTypes
operator|.
name|TOUCH
condition|)
block|{
return|return
name|touch
argument_list|(
name|db
argument_list|,
name|simpleDesc
argument_list|)
return|;
block|}
elseif|else
if|if
condition|(
name|simpleDesc
operator|.
name|getType
argument_list|()
operator|==
name|AlterTableTypes
operator|.
name|ARCHIVE
condition|)
block|{
return|return
name|archive
argument_list|(
name|db
argument_list|,
name|simpleDesc
argument_list|,
name|driverContext
argument_list|)
return|;
block|}
elseif|else
if|if
condition|(
name|simpleDesc
operator|.
name|getType
argument_list|()
operator|==
name|AlterTableTypes
operator|.
name|UNARCHIVE
condition|)
block|{
return|return
name|unarchive
argument_list|(
name|db
argument_list|,
name|simpleDesc
argument_list|)
return|;
block|}
elseif|else
if|if
condition|(
name|simpleDesc
operator|.
name|getType
argument_list|()
operator|==
name|AlterTableTypes
operator|.
name|COMPACT
condition|)
block|{
return|return
name|compact
argument_list|(
name|db
argument_list|,
name|simpleDesc
argument_list|)
return|;
block|}
block|}
name|MsckDesc
name|msckDesc
init|=
name|work
operator|.
name|getMsckDesc
argument_list|()
decl_stmt|;
if|if
condition|(
name|msckDesc
operator|!=
literal|null
condition|)
block|{
return|return
name|msck
argument_list|(
name|db
argument_list|,
name|msckDesc
argument_list|)
return|;
block|}
name|ShowConfDesc
name|showConf
init|=
name|work
operator|.
name|getShowConfDesc
argument_list|()
decl_stmt|;
if|if
condition|(
name|showConf
operator|!=
literal|null
condition|)
block|{
return|return
name|showConf
argument_list|(
name|db
argument_list|,
name|showConf
argument_list|)
return|;
block|}
name|AlterTablePartMergeFilesDesc
name|mergeFilesDesc
init|=
name|work
operator|.
name|getMergeFilesDesc
argument_list|()
decl_stmt|;
if|if
condition|(
name|mergeFilesDesc
operator|!=
literal|null
condition|)
block|{
return|return
name|mergeFiles
argument_list|(
name|db
argument_list|,
name|mergeFilesDesc
argument_list|,
name|driverContext
argument_list|)
return|;
block|}
name|CacheMetadataDesc
name|cacheMetadataDesc
init|=
name|work
operator|.
name|getCacheMetadataDesc
argument_list|()
decl_stmt|;
if|if
condition|(
name|cacheMetadataDesc
operator|!=
literal|null
condition|)
block|{
return|return
name|cacheMetadata
argument_list|(
name|db
argument_list|,
name|cacheMetadataDesc
argument_list|)
return|;
block|}
name|InsertCommitHookDesc
name|insertCommitHookDesc
init|=
name|work
operator|.
name|getInsertCommitHookDesc
argument_list|()
decl_stmt|;
if|if
condition|(
name|insertCommitHookDesc
operator|!=
literal|null
condition|)
block|{
return|return
name|insertCommitWork
argument_list|(
name|db
argument_list|,
name|insertCommitHookDesc
argument_list|)
return|;
block|}
if|if
condition|(
name|work
operator|.
name|getReplSetFirstIncLoadFlagDesc
argument_list|()
operator|!=
literal|null
condition|)
block|{
return|return
name|remFirstIncPendFlag
argument_list|(
name|db
argument_list|,
name|work
operator|.
name|getReplSetFirstIncLoadFlagDesc
argument_list|()
argument_list|)
return|;
block|}
block|}
catch|catch
parameter_list|(
name|Throwable
name|e
parameter_list|)
block|{
name|failed
argument_list|(
name|e
argument_list|)
expr_stmt|;
return|return
literal|1
return|;
block|}
assert|assert
literal|false
assert|;
return|return
literal|0
return|;
block|}
specifier|private
name|int
name|insertCommitWork
parameter_list|(
name|Hive
name|db
parameter_list|,
name|InsertCommitHookDesc
name|insertCommitHookDesc
parameter_list|)
throws|throws
name|MetaException
block|{
name|boolean
name|failed
init|=
literal|true
decl_stmt|;
name|HiveMetaHook
name|hook
init|=
name|insertCommitHookDesc
operator|.
name|getTable
argument_list|()
operator|.
name|getStorageHandler
argument_list|()
operator|.
name|getMetaHook
argument_list|()
decl_stmt|;
if|if
condition|(
name|hook
operator|==
literal|null
operator|||
operator|!
operator|(
name|hook
operator|instanceof
name|DefaultHiveMetaHook
operator|)
condition|)
block|{
return|return
literal|0
return|;
block|}
name|DefaultHiveMetaHook
name|hiveMetaHook
init|=
operator|(
name|DefaultHiveMetaHook
operator|)
name|hook
decl_stmt|;
try|try
block|{
name|hiveMetaHook
operator|.
name|commitInsertTable
argument_list|(
name|insertCommitHookDesc
operator|.
name|getTable
argument_list|()
operator|.
name|getTTable
argument_list|()
argument_list|,
name|insertCommitHookDesc
operator|.
name|isOverwrite
argument_list|()
argument_list|)
expr_stmt|;
name|failed
operator|=
literal|false
expr_stmt|;
block|}
finally|finally
block|{
if|if
condition|(
name|failed
condition|)
block|{
name|hiveMetaHook
operator|.
name|rollbackInsertTable
argument_list|(
name|insertCommitHookDesc
operator|.
name|getTable
argument_list|()
operator|.
name|getTTable
argument_list|()
argument_list|,
name|insertCommitHookDesc
operator|.
name|isOverwrite
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
return|return
literal|0
return|;
block|}
specifier|private
name|int
name|cacheMetadata
parameter_list|(
name|Hive
name|db
parameter_list|,
name|CacheMetadataDesc
name|desc
parameter_list|)
throws|throws
name|HiveException
block|{
name|db
operator|.
name|cacheFileMetadata
argument_list|(
name|desc
operator|.
name|getDbName
argument_list|()
argument_list|,
name|desc
operator|.
name|getTableName
argument_list|()
argument_list|,
name|desc
operator|.
name|getPartName
argument_list|()
argument_list|,
name|desc
operator|.
name|isAllParts
argument_list|()
argument_list|)
expr_stmt|;
return|return
literal|0
return|;
block|}
specifier|private
name|void
name|failed
parameter_list|(
name|Throwable
name|e
parameter_list|)
block|{
while|while
condition|(
name|e
operator|.
name|getCause
argument_list|()
operator|!=
literal|null
operator|&&
name|e
operator|.
name|getClass
argument_list|()
operator|==
name|RuntimeException
operator|.
name|class
condition|)
block|{
name|e
operator|=
name|e
operator|.
name|getCause
argument_list|()
expr_stmt|;
block|}
name|setException
argument_list|(
name|e
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|error
argument_list|(
literal|"Failed"
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
specifier|private
name|int
name|showConf
parameter_list|(
name|Hive
name|db
parameter_list|,
name|ShowConfDesc
name|showConf
parameter_list|)
throws|throws
name|Exception
block|{
name|ConfVars
name|conf
init|=
name|HiveConf
operator|.
name|getConfVars
argument_list|(
name|showConf
operator|.
name|getConfName
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|conf
operator|==
literal|null
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"invalid configuration name "
operator|+
name|showConf
operator|.
name|getConfName
argument_list|()
argument_list|)
throw|;
block|}
name|String
name|description
init|=
name|conf
operator|.
name|getDescription
argument_list|()
decl_stmt|;
name|String
name|defaultValue
init|=
name|conf
operator|.
name|getDefaultValue
argument_list|()
decl_stmt|;
name|DataOutputStream
name|output
init|=
name|getOutputStream
argument_list|(
name|showConf
operator|.
name|getResFile
argument_list|()
argument_list|)
decl_stmt|;
try|try
block|{
if|if
condition|(
name|defaultValue
operator|!=
literal|null
condition|)
block|{
name|output
operator|.
name|write
argument_list|(
name|defaultValue
operator|.
name|getBytes
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|output
operator|.
name|write
argument_list|(
name|separator
argument_list|)
expr_stmt|;
name|output
operator|.
name|write
argument_list|(
name|conf
operator|.
name|typeString
argument_list|()
operator|.
name|getBytes
argument_list|()
argument_list|)
expr_stmt|;
name|output
operator|.
name|write
argument_list|(
name|separator
argument_list|)
expr_stmt|;
if|if
condition|(
name|description
operator|!=
literal|null
condition|)
block|{
name|output
operator|.
name|write
argument_list|(
name|description
operator|.
name|replaceAll
argument_list|(
literal|" *\n *"
argument_list|,
literal|" "
argument_list|)
operator|.
name|getBytes
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|output
operator|.
name|write
argument_list|(
name|terminator
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
name|output
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
return|return
literal|0
return|;
block|}
specifier|private
name|DataOutputStream
name|getOutputStream
parameter_list|(
name|Path
name|outputFile
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|FileSystem
name|fs
init|=
name|outputFile
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
return|return
name|fs
operator|.
name|create
argument_list|(
name|outputFile
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    * First, make sure the source table/partition is not    * archived/indexes/non-rcfile. If either of these is true, throw an    * exception.    *    * The way how it does the merge is to create a BlockMergeTask from the    * mergeFilesDesc.    *    * @param db    * @param mergeFilesDesc    * @return    * @throws HiveException    */
specifier|private
name|int
name|mergeFiles
parameter_list|(
name|Hive
name|db
parameter_list|,
name|AlterTablePartMergeFilesDesc
name|mergeFilesDesc
parameter_list|,
name|DriverContext
name|driverContext
parameter_list|)
throws|throws
name|HiveException
block|{
name|ListBucketingCtx
name|lbCtx
init|=
name|mergeFilesDesc
operator|.
name|getLbCtx
argument_list|()
decl_stmt|;
name|boolean
name|lbatc
init|=
name|lbCtx
operator|==
literal|null
condition|?
literal|false
else|:
name|lbCtx
operator|.
name|isSkewedStoredAsDir
argument_list|()
decl_stmt|;
name|int
name|lbd
init|=
name|lbCtx
operator|==
literal|null
condition|?
literal|0
else|:
name|lbCtx
operator|.
name|calculateListBucketingLevel
argument_list|()
decl_stmt|;
comment|// merge work only needs input and output.
name|MergeFileWork
name|mergeWork
init|=
operator|new
name|MergeFileWork
argument_list|(
name|mergeFilesDesc
operator|.
name|getInputDir
argument_list|()
argument_list|,
name|mergeFilesDesc
operator|.
name|getOutputDir
argument_list|()
argument_list|,
name|mergeFilesDesc
operator|.
name|getInputFormatClass
argument_list|()
operator|.
name|getName
argument_list|()
argument_list|,
name|mergeFilesDesc
operator|.
name|getTableDesc
argument_list|()
argument_list|)
decl_stmt|;
name|LinkedHashMap
argument_list|<
name|Path
argument_list|,
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|>
name|pathToAliases
init|=
operator|new
name|LinkedHashMap
argument_list|<>
argument_list|()
decl_stmt|;
name|ArrayList
argument_list|<
name|String
argument_list|>
name|inputDirstr
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|(
literal|1
argument_list|)
decl_stmt|;
name|inputDirstr
operator|.
name|add
argument_list|(
name|mergeFilesDesc
operator|.
name|getInputDir
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
name|pathToAliases
operator|.
name|put
argument_list|(
name|mergeFilesDesc
operator|.
name|getInputDir
argument_list|()
operator|.
name|get
argument_list|(
literal|0
argument_list|)
argument_list|,
name|inputDirstr
argument_list|)
expr_stmt|;
name|mergeWork
operator|.
name|setPathToAliases
argument_list|(
name|pathToAliases
argument_list|)
expr_stmt|;
name|mergeWork
operator|.
name|setListBucketingCtx
argument_list|(
name|mergeFilesDesc
operator|.
name|getLbCtx
argument_list|()
argument_list|)
expr_stmt|;
name|mergeWork
operator|.
name|resolveConcatenateMerge
argument_list|(
name|db
operator|.
name|getConf
argument_list|()
argument_list|)
expr_stmt|;
name|mergeWork
operator|.
name|setMapperCannotSpanPartns
argument_list|(
literal|true
argument_list|)
expr_stmt|;
name|mergeWork
operator|.
name|setSourceTableInputFormat
argument_list|(
name|mergeFilesDesc
operator|.
name|getInputFormatClass
argument_list|()
operator|.
name|getName
argument_list|()
argument_list|)
expr_stmt|;
specifier|final
name|FileMergeDesc
name|fmd
decl_stmt|;
if|if
condition|(
name|mergeFilesDesc
operator|.
name|getInputFormatClass
argument_list|()
operator|.
name|equals
argument_list|(
name|RCFileInputFormat
operator|.
name|class
argument_list|)
condition|)
block|{
name|fmd
operator|=
operator|new
name|RCFileMergeDesc
argument_list|()
expr_stmt|;
block|}
else|else
block|{
comment|// safe to assume else is ORC as semantic analyzer will check for RC/ORC
name|fmd
operator|=
operator|new
name|OrcFileMergeDesc
argument_list|()
expr_stmt|;
block|}
name|fmd
operator|.
name|setDpCtx
argument_list|(
literal|null
argument_list|)
expr_stmt|;
name|fmd
operator|.
name|setHasDynamicPartitions
argument_list|(
literal|false
argument_list|)
expr_stmt|;
name|fmd
operator|.
name|setListBucketingAlterTableConcatenate
argument_list|(
name|lbatc
argument_list|)
expr_stmt|;
name|fmd
operator|.
name|setListBucketingDepth
argument_list|(
name|lbd
argument_list|)
expr_stmt|;
name|fmd
operator|.
name|setOutputPath
argument_list|(
name|mergeFilesDesc
operator|.
name|getOutputDir
argument_list|()
argument_list|)
expr_stmt|;
name|CompilationOpContext
name|opContext
init|=
name|driverContext
operator|.
name|getCtx
argument_list|()
operator|.
name|getOpContext
argument_list|()
decl_stmt|;
name|Operator
argument_list|<
name|?
extends|extends
name|OperatorDesc
argument_list|>
name|mergeOp
init|=
name|OperatorFactory
operator|.
name|get
argument_list|(
name|opContext
argument_list|,
name|fmd
argument_list|)
decl_stmt|;
name|LinkedHashMap
argument_list|<
name|String
argument_list|,
name|Operator
argument_list|<
name|?
extends|extends
name|OperatorDesc
argument_list|>
argument_list|>
name|aliasToWork
init|=
operator|new
name|LinkedHashMap
argument_list|<
name|String
argument_list|,
name|Operator
argument_list|<
name|?
extends|extends
name|OperatorDesc
argument_list|>
argument_list|>
argument_list|()
decl_stmt|;
name|aliasToWork
operator|.
name|put
argument_list|(
name|mergeFilesDesc
operator|.
name|getInputDir
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|,
name|mergeOp
argument_list|)
expr_stmt|;
name|mergeWork
operator|.
name|setAliasToWork
argument_list|(
name|aliasToWork
argument_list|)
expr_stmt|;
name|DriverContext
name|driverCxt
init|=
operator|new
name|DriverContext
argument_list|()
decl_stmt|;
name|Task
argument_list|<
name|?
argument_list|>
name|task
decl_stmt|;
if|if
condition|(
name|conf
operator|.
name|getVar
argument_list|(
name|ConfVars
operator|.
name|HIVE_EXECUTION_ENGINE
argument_list|)
operator|.
name|equals
argument_list|(
literal|"tez"
argument_list|)
condition|)
block|{
name|TezWork
name|tezWork
init|=
operator|new
name|TezWork
argument_list|(
name|queryState
operator|.
name|getQueryId
argument_list|()
argument_list|,
name|conf
argument_list|)
decl_stmt|;
name|mergeWork
operator|.
name|setName
argument_list|(
literal|"File Merge"
argument_list|)
expr_stmt|;
name|tezWork
operator|.
name|add
argument_list|(
name|mergeWork
argument_list|)
expr_stmt|;
name|task
operator|=
operator|new
name|TezTask
argument_list|()
expr_stmt|;
operator|(
operator|(
name|TezTask
operator|)
name|task
operator|)
operator|.
name|setWork
argument_list|(
name|tezWork
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|task
operator|=
operator|new
name|MergeFileTask
argument_list|()
expr_stmt|;
operator|(
operator|(
name|MergeFileTask
operator|)
name|task
operator|)
operator|.
name|setWork
argument_list|(
name|mergeWork
argument_list|)
expr_stmt|;
block|}
comment|// initialize the task and execute
name|task
operator|.
name|initialize
argument_list|(
name|queryState
argument_list|,
name|getQueryPlan
argument_list|()
argument_list|,
name|driverCxt
argument_list|,
name|opContext
argument_list|)
expr_stmt|;
name|Task
argument_list|<
name|?
extends|extends
name|Serializable
argument_list|>
name|subtask
init|=
name|task
decl_stmt|;
name|int
name|ret
init|=
name|task
operator|.
name|execute
argument_list|(
name|driverCxt
argument_list|)
decl_stmt|;
if|if
condition|(
name|subtask
operator|.
name|getException
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|setException
argument_list|(
name|subtask
operator|.
name|getException
argument_list|()
argument_list|)
expr_stmt|;
block|}
return|return
name|ret
return|;
block|}
comment|/**    * Rewrite the partition's metadata and force the pre/post execute hooks to    * be fired.    *    * @param db    * @param touchDesc    * @return    * @throws HiveException    */
specifier|private
name|int
name|touch
parameter_list|(
name|Hive
name|db
parameter_list|,
name|AlterTableSimpleDesc
name|touchDesc
parameter_list|)
throws|throws
name|HiveException
block|{
comment|// TODO: catalog
name|Table
name|tbl
init|=
name|db
operator|.
name|getTable
argument_list|(
name|touchDesc
operator|.
name|getTableName
argument_list|()
argument_list|)
decl_stmt|;
name|EnvironmentContext
name|environmentContext
init|=
operator|new
name|EnvironmentContext
argument_list|()
decl_stmt|;
name|environmentContext
operator|.
name|putToProperties
argument_list|(
name|StatsSetupConst
operator|.
name|DO_NOT_UPDATE_STATS
argument_list|,
name|StatsSetupConst
operator|.
name|TRUE
argument_list|)
expr_stmt|;
if|if
condition|(
name|touchDesc
operator|.
name|getPartSpec
argument_list|()
operator|==
literal|null
condition|)
block|{
name|db
operator|.
name|alterTable
argument_list|(
name|tbl
argument_list|,
literal|false
argument_list|,
name|environmentContext
argument_list|,
literal|true
argument_list|)
expr_stmt|;
name|work
operator|.
name|getInputs
argument_list|()
operator|.
name|add
argument_list|(
operator|new
name|ReadEntity
argument_list|(
name|tbl
argument_list|)
argument_list|)
expr_stmt|;
name|addIfAbsentByName
argument_list|(
operator|new
name|WriteEntity
argument_list|(
name|tbl
argument_list|,
name|WriteEntity
operator|.
name|WriteType
operator|.
name|DDL_NO_LOCK
argument_list|)
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|Partition
name|part
init|=
name|db
operator|.
name|getPartition
argument_list|(
name|tbl
argument_list|,
name|touchDesc
operator|.
name|getPartSpec
argument_list|()
argument_list|,
literal|false
argument_list|)
decl_stmt|;
if|if
condition|(
name|part
operator|==
literal|null
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Specified partition does not exist"
argument_list|)
throw|;
block|}
try|try
block|{
name|db
operator|.
name|alterPartition
argument_list|(
name|tbl
operator|.
name|getCatalogName
argument_list|()
argument_list|,
name|tbl
operator|.
name|getDbName
argument_list|()
argument_list|,
name|tbl
operator|.
name|getTableName
argument_list|()
argument_list|,
name|part
argument_list|,
name|environmentContext
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|InvalidOperationException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
name|work
operator|.
name|getInputs
argument_list|()
operator|.
name|add
argument_list|(
operator|new
name|ReadEntity
argument_list|(
name|part
argument_list|)
argument_list|)
expr_stmt|;
name|addIfAbsentByName
argument_list|(
operator|new
name|WriteEntity
argument_list|(
name|part
argument_list|,
name|WriteEntity
operator|.
name|WriteType
operator|.
name|DDL_NO_LOCK
argument_list|)
argument_list|)
expr_stmt|;
block|}
return|return
literal|0
return|;
block|}
comment|/**    * Sets archiving flag locally; it has to be pushed into metastore    * @param p partition to set flag    * @param state desired state of IS_ARCHIVED flag    * @param level desired level for state == true, anything for false    */
specifier|private
name|void
name|setIsArchived
parameter_list|(
name|Partition
name|p
parameter_list|,
name|boolean
name|state
parameter_list|,
name|int
name|level
parameter_list|)
block|{
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|params
init|=
name|p
operator|.
name|getParameters
argument_list|()
decl_stmt|;
if|if
condition|(
name|state
condition|)
block|{
name|params
operator|.
name|put
argument_list|(
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|hive_metastoreConstants
operator|.
name|IS_ARCHIVED
argument_list|,
literal|"true"
argument_list|)
expr_stmt|;
name|params
operator|.
name|put
argument_list|(
name|ArchiveUtils
operator|.
name|ARCHIVING_LEVEL
argument_list|,
name|Integer
operator|.
name|toString
argument_list|(
name|level
argument_list|)
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|params
operator|.
name|remove
argument_list|(
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|hive_metastoreConstants
operator|.
name|IS_ARCHIVED
argument_list|)
expr_stmt|;
name|params
operator|.
name|remove
argument_list|(
name|ArchiveUtils
operator|.
name|ARCHIVING_LEVEL
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * Returns original partition of archived partition, null for unarchived one    */
specifier|private
name|String
name|getOriginalLocation
parameter_list|(
name|Partition
name|p
parameter_list|)
block|{
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|params
init|=
name|p
operator|.
name|getParameters
argument_list|()
decl_stmt|;
return|return
name|params
operator|.
name|get
argument_list|(
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|hive_metastoreConstants
operator|.
name|ORIGINAL_LOCATION
argument_list|)
return|;
block|}
comment|/**    * Sets original location of partition which is to be archived    */
specifier|private
name|void
name|setOriginalLocation
parameter_list|(
name|Partition
name|p
parameter_list|,
name|String
name|loc
parameter_list|)
block|{
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|params
init|=
name|p
operator|.
name|getParameters
argument_list|()
decl_stmt|;
if|if
condition|(
name|loc
operator|==
literal|null
condition|)
block|{
name|params
operator|.
name|remove
argument_list|(
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|hive_metastoreConstants
operator|.
name|ORIGINAL_LOCATION
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|params
operator|.
name|put
argument_list|(
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|hive_metastoreConstants
operator|.
name|ORIGINAL_LOCATION
argument_list|,
name|loc
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * Sets the appropriate attributes in the supplied Partition object to mark    * it as archived. Note that the metastore is not touched - a separate    * call to alter_partition is needed.    *    * @param p - the partition object to modify    * @param harPath - new location of partition (har schema URI)    */
specifier|private
name|void
name|setArchived
parameter_list|(
name|Partition
name|p
parameter_list|,
name|Path
name|harPath
parameter_list|,
name|int
name|level
parameter_list|)
block|{
assert|assert
operator|(
name|ArchiveUtils
operator|.
name|isArchived
argument_list|(
name|p
argument_list|)
operator|==
literal|false
operator|)
assert|;
name|setIsArchived
argument_list|(
name|p
argument_list|,
literal|true
argument_list|,
name|level
argument_list|)
expr_stmt|;
name|setOriginalLocation
argument_list|(
name|p
argument_list|,
name|p
operator|.
name|getLocation
argument_list|()
argument_list|)
expr_stmt|;
name|p
operator|.
name|setLocation
argument_list|(
name|harPath
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
comment|/**    * Sets the appropriate attributes in the supplied Partition object to mark    * it as not archived. Note that the metastore is not touched - a separate    * call to alter_partition is needed.    *    * @param p - the partition to modify    */
specifier|private
name|void
name|setUnArchived
parameter_list|(
name|Partition
name|p
parameter_list|)
block|{
assert|assert
operator|(
name|ArchiveUtils
operator|.
name|isArchived
argument_list|(
name|p
argument_list|)
operator|==
literal|true
operator|)
assert|;
name|String
name|parentDir
init|=
name|getOriginalLocation
argument_list|(
name|p
argument_list|)
decl_stmt|;
name|setIsArchived
argument_list|(
name|p
argument_list|,
literal|false
argument_list|,
literal|0
argument_list|)
expr_stmt|;
name|setOriginalLocation
argument_list|(
name|p
argument_list|,
literal|null
argument_list|)
expr_stmt|;
assert|assert
operator|(
name|parentDir
operator|!=
literal|null
operator|)
assert|;
name|p
operator|.
name|setLocation
argument_list|(
name|parentDir
argument_list|)
expr_stmt|;
block|}
specifier|private
name|boolean
name|pathExists
parameter_list|(
name|Path
name|p
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|FileSystem
name|fs
init|=
name|p
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
return|return
name|fs
operator|.
name|exists
argument_list|(
name|p
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|private
name|void
name|moveDir
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|Path
name|from
parameter_list|,
name|Path
name|to
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
if|if
condition|(
operator|!
name|fs
operator|.
name|rename
argument_list|(
name|from
argument_list|,
name|to
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Moving "
operator|+
name|from
operator|+
literal|" to "
operator|+
name|to
operator|+
literal|" failed!"
argument_list|)
throw|;
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|private
name|void
name|deleteDir
parameter_list|(
name|Path
name|dir
parameter_list|,
name|Database
name|db
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|Warehouse
name|wh
init|=
operator|new
name|Warehouse
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|wh
operator|.
name|deleteDir
argument_list|(
name|dir
argument_list|,
literal|true
argument_list|,
name|db
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|MetaException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    * Checks in partition is in custom (not-standard) location.    * @param tbl - table in which partition is    * @param p - partition    * @return true if partition location is custom, false if it is standard    */
name|boolean
name|partitionInCustomLocation
parameter_list|(
name|Table
name|tbl
parameter_list|,
name|Partition
name|p
parameter_list|)
throws|throws
name|HiveException
block|{
name|String
name|subdir
init|=
literal|null
decl_stmt|;
try|try
block|{
name|subdir
operator|=
name|Warehouse
operator|.
name|makePartName
argument_list|(
name|tbl
operator|.
name|getPartCols
argument_list|()
argument_list|,
name|p
operator|.
name|getValues
argument_list|()
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|MetaException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Unable to get partition's directory"
argument_list|,
name|e
argument_list|)
throw|;
block|}
name|Path
name|tableDir
init|=
name|tbl
operator|.
name|getDataLocation
argument_list|()
decl_stmt|;
if|if
condition|(
name|tableDir
operator|==
literal|null
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Table has no location set"
argument_list|)
throw|;
block|}
name|String
name|standardLocation
init|=
operator|(
operator|new
name|Path
argument_list|(
name|tableDir
argument_list|,
name|subdir
argument_list|)
operator|)
operator|.
name|toString
argument_list|()
decl_stmt|;
if|if
condition|(
name|ArchiveUtils
operator|.
name|isArchived
argument_list|(
name|p
argument_list|)
condition|)
block|{
return|return
operator|!
name|getOriginalLocation
argument_list|(
name|p
argument_list|)
operator|.
name|equals
argument_list|(
name|standardLocation
argument_list|)
return|;
block|}
else|else
block|{
return|return
operator|!
name|p
operator|.
name|getLocation
argument_list|()
operator|.
name|equals
argument_list|(
name|standardLocation
argument_list|)
return|;
block|}
block|}
specifier|private
name|int
name|archive
parameter_list|(
name|Hive
name|db
parameter_list|,
name|AlterTableSimpleDesc
name|simpleDesc
parameter_list|,
name|DriverContext
name|driverContext
parameter_list|)
throws|throws
name|HiveException
block|{
name|Table
name|tbl
init|=
name|db
operator|.
name|getTable
argument_list|(
name|simpleDesc
operator|.
name|getTableName
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|tbl
operator|.
name|getTableType
argument_list|()
operator|!=
name|TableType
operator|.
name|MANAGED_TABLE
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"ARCHIVE can only be performed on managed tables"
argument_list|)
throw|;
block|}
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|partSpec
init|=
name|simpleDesc
operator|.
name|getPartSpec
argument_list|()
decl_stmt|;
name|PartSpecInfo
name|partSpecInfo
init|=
name|PartSpecInfo
operator|.
name|create
argument_list|(
name|tbl
argument_list|,
name|partSpec
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|Partition
argument_list|>
name|partitions
init|=
name|db
operator|.
name|getPartitions
argument_list|(
name|tbl
argument_list|,
name|partSpec
argument_list|)
decl_stmt|;
name|Path
name|originalDir
init|=
literal|null
decl_stmt|;
comment|// when we have partial partitions specification we must assume partitions
comment|// lie in standard place - if they were in custom locations putting
comment|// them into one archive would involve mass amount of copying
comment|// in full partition specification case we allow custom locations
comment|// to keep backward compatibility
if|if
condition|(
name|partitions
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"No partition matches the specification"
argument_list|)
throw|;
block|}
elseif|else
if|if
condition|(
name|partSpecInfo
operator|.
name|values
operator|.
name|size
argument_list|()
operator|!=
name|tbl
operator|.
name|getPartCols
argument_list|()
operator|.
name|size
argument_list|()
condition|)
block|{
comment|// for partial specifications we need partitions to follow the scheme
for|for
control|(
name|Partition
name|p
range|:
name|partitions
control|)
block|{
if|if
condition|(
name|partitionInCustomLocation
argument_list|(
name|tbl
argument_list|,
name|p
argument_list|)
condition|)
block|{
name|String
name|message
init|=
name|String
operator|.
name|format
argument_list|(
literal|"ARCHIVE cannot run for partition "
operator|+
literal|"groups with custom locations like %s"
argument_list|,
name|p
operator|.
name|getLocation
argument_list|()
argument_list|)
decl_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|message
argument_list|)
throw|;
block|}
block|}
name|originalDir
operator|=
name|partSpecInfo
operator|.
name|createPath
argument_list|(
name|tbl
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|Partition
name|p
init|=
name|partitions
operator|.
name|get
argument_list|(
literal|0
argument_list|)
decl_stmt|;
comment|// partition can be archived if during recovery
if|if
condition|(
name|ArchiveUtils
operator|.
name|isArchived
argument_list|(
name|p
argument_list|)
condition|)
block|{
name|originalDir
operator|=
operator|new
name|Path
argument_list|(
name|getOriginalLocation
argument_list|(
name|p
argument_list|)
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|originalDir
operator|=
name|p
operator|.
name|getDataLocation
argument_list|()
expr_stmt|;
block|}
block|}
name|Path
name|intermediateArchivedDir
init|=
operator|new
name|Path
argument_list|(
name|originalDir
operator|.
name|getParent
argument_list|()
argument_list|,
name|originalDir
operator|.
name|getName
argument_list|()
operator|+
name|INTERMEDIATE_ARCHIVED_DIR_SUFFIX
argument_list|)
decl_stmt|;
name|Path
name|intermediateOriginalDir
init|=
operator|new
name|Path
argument_list|(
name|originalDir
operator|.
name|getParent
argument_list|()
argument_list|,
name|originalDir
operator|.
name|getName
argument_list|()
operator|+
name|INTERMEDIATE_ORIGINAL_DIR_SUFFIX
argument_list|)
decl_stmt|;
name|console
operator|.
name|printInfo
argument_list|(
literal|"intermediate.archived is "
operator|+
name|intermediateArchivedDir
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
name|console
operator|.
name|printInfo
argument_list|(
literal|"intermediate.original is "
operator|+
name|intermediateOriginalDir
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
name|String
name|archiveName
init|=
literal|"data.har"
decl_stmt|;
name|FileSystem
name|fs
init|=
literal|null
decl_stmt|;
try|try
block|{
name|fs
operator|=
name|originalDir
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
name|URI
name|archiveUri
init|=
operator|(
operator|new
name|Path
argument_list|(
name|originalDir
argument_list|,
name|archiveName
argument_list|)
operator|)
operator|.
name|toUri
argument_list|()
decl_stmt|;
name|URI
name|originalUri
init|=
name|ArchiveUtils
operator|.
name|addSlash
argument_list|(
name|originalDir
operator|.
name|toUri
argument_list|()
argument_list|)
decl_stmt|;
name|ArchiveUtils
operator|.
name|HarPathHelper
name|harHelper
init|=
operator|new
name|ArchiveUtils
operator|.
name|HarPathHelper
argument_list|(
name|conf
argument_list|,
name|archiveUri
argument_list|,
name|originalUri
argument_list|)
decl_stmt|;
comment|// we checked if partitions matching specification are marked as archived
comment|// in the metadata; if they are and their levels are the same as we would
comment|// set it later it means previous run failed and we have to do the recovery;
comment|// if they are different, we throw an error
for|for
control|(
name|Partition
name|p
range|:
name|partitions
control|)
block|{
if|if
condition|(
name|ArchiveUtils
operator|.
name|isArchived
argument_list|(
name|p
argument_list|)
condition|)
block|{
if|if
condition|(
name|ArchiveUtils
operator|.
name|getArchivingLevel
argument_list|(
name|p
argument_list|)
operator|!=
name|partSpecInfo
operator|.
name|values
operator|.
name|size
argument_list|()
condition|)
block|{
name|String
name|name
init|=
name|ArchiveUtils
operator|.
name|getPartialName
argument_list|(
name|p
argument_list|,
name|ArchiveUtils
operator|.
name|getArchivingLevel
argument_list|(
name|p
argument_list|)
argument_list|)
decl_stmt|;
name|String
name|m
init|=
name|String
operator|.
name|format
argument_list|(
literal|"Conflict with existing archive %s"
argument_list|,
name|name
argument_list|)
decl_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|m
argument_list|)
throw|;
block|}
else|else
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Partition(s) already archived"
argument_list|)
throw|;
block|}
block|}
block|}
name|boolean
name|recovery
init|=
literal|false
decl_stmt|;
if|if
condition|(
name|pathExists
argument_list|(
name|intermediateArchivedDir
argument_list|)
operator|||
name|pathExists
argument_list|(
name|intermediateOriginalDir
argument_list|)
condition|)
block|{
name|recovery
operator|=
literal|true
expr_stmt|;
name|console
operator|.
name|printInfo
argument_list|(
literal|"Starting recovery after failed ARCHIVE"
argument_list|)
expr_stmt|;
block|}
comment|// The following steps seem roundabout, but they are meant to aid in
comment|// recovery if a failure occurs and to keep a consistent state in the FS
comment|// Steps:
comment|// 1. Create the archive in a temporary folder
comment|// 2. Move the archive dir to an intermediate dir that is in at the same
comment|//    dir as the original partition dir. Call the new dir
comment|//    intermediate-archive.
comment|// 3. Rename the original partition dir to an intermediate dir. Call the
comment|//    renamed dir intermediate-original
comment|// 4. Rename intermediate-archive to the original partition dir
comment|// 5. Change the metadata
comment|// 6. Delete the original partition files in intermediate-original
comment|// The original partition files are deleted after the metadata change
comment|// because the presence of those files are used to indicate whether
comment|// the original partition directory contains archived or unarchived files.
comment|// Create an archived version of the partition in a directory ending in
comment|// ARCHIVE_INTERMEDIATE_DIR_SUFFIX that's the same level as the partition,
comment|// if it does not already exist. If it does exist, we assume the dir is good
comment|// to use as the move operation that created it is atomic.
if|if
condition|(
operator|!
name|pathExists
argument_list|(
name|intermediateArchivedDir
argument_list|)
operator|&&
operator|!
name|pathExists
argument_list|(
name|intermediateOriginalDir
argument_list|)
condition|)
block|{
comment|// First create the archive in a tmp dir so that if the job fails, the
comment|// bad files don't pollute the filesystem
name|Path
name|tmpPath
init|=
operator|new
name|Path
argument_list|(
name|driverContext
operator|.
name|getCtx
argument_list|()
operator|.
name|getExternalTmpPath
argument_list|(
name|originalDir
argument_list|)
argument_list|,
literal|"partlevel"
argument_list|)
decl_stmt|;
name|console
operator|.
name|printInfo
argument_list|(
literal|"Creating "
operator|+
name|archiveName
operator|+
literal|" for "
operator|+
name|originalDir
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
name|console
operator|.
name|printInfo
argument_list|(
literal|"in "
operator|+
name|tmpPath
argument_list|)
expr_stmt|;
name|console
operator|.
name|printInfo
argument_list|(
literal|"Please wait... (this may take a while)"
argument_list|)
expr_stmt|;
comment|// Create the Hadoop archive
name|int
name|ret
init|=
literal|0
decl_stmt|;
try|try
block|{
name|int
name|maxJobNameLen
init|=
name|conf
operator|.
name|getIntVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEJOBNAMELENGTH
argument_list|)
decl_stmt|;
name|String
name|jobname
init|=
name|String
operator|.
name|format
argument_list|(
literal|"Archiving %s@%s"
argument_list|,
name|tbl
operator|.
name|getTableName
argument_list|()
argument_list|,
name|partSpecInfo
operator|.
name|getName
argument_list|()
argument_list|)
decl_stmt|;
name|jobname
operator|=
name|Utilities
operator|.
name|abbreviate
argument_list|(
name|jobname
argument_list|,
name|maxJobNameLen
operator|-
literal|6
argument_list|)
expr_stmt|;
name|conf
operator|.
name|set
argument_list|(
name|MRJobConfig
operator|.
name|JOB_NAME
argument_list|,
name|jobname
argument_list|)
expr_stmt|;
name|HadoopArchives
name|har
init|=
operator|new
name|HadoopArchives
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|args
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
name|args
operator|.
name|add
argument_list|(
literal|"-archiveName"
argument_list|)
expr_stmt|;
name|args
operator|.
name|add
argument_list|(
name|archiveName
argument_list|)
expr_stmt|;
name|args
operator|.
name|add
argument_list|(
literal|"-p"
argument_list|)
expr_stmt|;
name|args
operator|.
name|add
argument_list|(
name|originalDir
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
name|args
operator|.
name|add
argument_list|(
name|tmpPath
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
name|ret
operator|=
name|ToolRunner
operator|.
name|run
argument_list|(
name|har
argument_list|,
name|args
operator|.
name|toArray
argument_list|(
operator|new
name|String
index|[
literal|0
index|]
argument_list|)
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
if|if
condition|(
name|ret
operator|!=
literal|0
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Error while creating HAR"
argument_list|)
throw|;
block|}
comment|// Move from the tmp dir to an intermediate directory, in the same level as
comment|// the partition directory. e.g. .../hr=12-intermediate-archived
try|try
block|{
name|console
operator|.
name|printInfo
argument_list|(
literal|"Moving "
operator|+
name|tmpPath
operator|+
literal|" to "
operator|+
name|intermediateArchivedDir
argument_list|)
expr_stmt|;
if|if
condition|(
name|pathExists
argument_list|(
name|intermediateArchivedDir
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"The intermediate archive directory already exists."
argument_list|)
throw|;
block|}
name|fs
operator|.
name|rename
argument_list|(
name|tmpPath
argument_list|,
name|intermediateArchivedDir
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Error while moving tmp directory"
argument_list|)
throw|;
block|}
block|}
else|else
block|{
if|if
condition|(
name|pathExists
argument_list|(
name|intermediateArchivedDir
argument_list|)
condition|)
block|{
name|console
operator|.
name|printInfo
argument_list|(
literal|"Intermediate archive directory "
operator|+
name|intermediateArchivedDir
operator|+
literal|" already exists. Assuming it contains an archived version of the partition"
argument_list|)
expr_stmt|;
block|}
block|}
comment|// If we get to here, we know that we've archived the partition files, but
comment|// they may be in the original partition location, or in the intermediate
comment|// original dir.
comment|// Move the original parent directory to the intermediate original directory
comment|// if the move hasn't been made already
if|if
condition|(
operator|!
name|pathExists
argument_list|(
name|intermediateOriginalDir
argument_list|)
condition|)
block|{
name|console
operator|.
name|printInfo
argument_list|(
literal|"Moving "
operator|+
name|originalDir
operator|+
literal|" to "
operator|+
name|intermediateOriginalDir
argument_list|)
expr_stmt|;
name|moveDir
argument_list|(
name|fs
argument_list|,
name|originalDir
argument_list|,
name|intermediateOriginalDir
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|console
operator|.
name|printInfo
argument_list|(
name|intermediateOriginalDir
operator|+
literal|" already exists. "
operator|+
literal|"Assuming it contains the original files in the partition"
argument_list|)
expr_stmt|;
block|}
comment|// If there's a failure from here to when the metadata is updated,
comment|// there will be no data in the partition, or an error while trying to read
comment|// the partition (if the archive files have been moved to the original
comment|// partition directory.) But re-running the archive command will allow
comment|// recovery
comment|// Move the intermediate archived directory to the original parent directory
if|if
condition|(
operator|!
name|pathExists
argument_list|(
name|originalDir
argument_list|)
condition|)
block|{
name|console
operator|.
name|printInfo
argument_list|(
literal|"Moving "
operator|+
name|intermediateArchivedDir
operator|+
literal|" to "
operator|+
name|originalDir
argument_list|)
expr_stmt|;
name|moveDir
argument_list|(
name|fs
argument_list|,
name|intermediateArchivedDir
argument_list|,
name|originalDir
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|console
operator|.
name|printInfo
argument_list|(
name|originalDir
operator|+
literal|" already exists. "
operator|+
literal|"Assuming it contains the archived version of the partition"
argument_list|)
expr_stmt|;
block|}
comment|// Record this change in the metastore
try|try
block|{
for|for
control|(
name|Partition
name|p
range|:
name|partitions
control|)
block|{
name|URI
name|originalPartitionUri
init|=
name|ArchiveUtils
operator|.
name|addSlash
argument_list|(
name|p
operator|.
name|getDataLocation
argument_list|()
operator|.
name|toUri
argument_list|()
argument_list|)
decl_stmt|;
name|URI
name|harPartitionDir
init|=
name|harHelper
operator|.
name|getHarUri
argument_list|(
name|originalPartitionUri
argument_list|)
decl_stmt|;
name|StringBuilder
name|authority
init|=
operator|new
name|StringBuilder
argument_list|()
decl_stmt|;
if|if
condition|(
name|harPartitionDir
operator|.
name|getUserInfo
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|authority
operator|.
name|append
argument_list|(
name|harPartitionDir
operator|.
name|getUserInfo
argument_list|()
argument_list|)
operator|.
name|append
argument_list|(
literal|"@"
argument_list|)
expr_stmt|;
block|}
name|authority
operator|.
name|append
argument_list|(
name|harPartitionDir
operator|.
name|getHost
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|harPartitionDir
operator|.
name|getPort
argument_list|()
operator|!=
operator|-
literal|1
condition|)
block|{
name|authority
operator|.
name|append
argument_list|(
literal|":"
argument_list|)
operator|.
name|append
argument_list|(
name|harPartitionDir
operator|.
name|getPort
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|Path
name|harPath
init|=
operator|new
name|Path
argument_list|(
name|harPartitionDir
operator|.
name|getScheme
argument_list|()
argument_list|,
name|authority
operator|.
name|toString
argument_list|()
argument_list|,
name|harPartitionDir
operator|.
name|getPath
argument_list|()
argument_list|)
decl_stmt|;
comment|// make in Path to ensure no slash at the end
name|setArchived
argument_list|(
name|p
argument_list|,
name|harPath
argument_list|,
name|partSpecInfo
operator|.
name|values
operator|.
name|size
argument_list|()
argument_list|)
expr_stmt|;
comment|// TODO: catalog
name|db
operator|.
name|alterPartition
argument_list|(
name|simpleDesc
operator|.
name|getTableName
argument_list|()
argument_list|,
name|p
argument_list|,
literal|null
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Unable to change the partition info for HAR"
argument_list|,
name|e
argument_list|)
throw|;
block|}
comment|// If a failure occurs here, the directory containing the original files
comment|// will not be deleted. The user will run ARCHIVE again to clear this up
if|if
condition|(
name|pathExists
argument_list|(
name|intermediateOriginalDir
argument_list|)
condition|)
block|{
name|deleteDir
argument_list|(
name|intermediateOriginalDir
argument_list|,
name|db
operator|.
name|getDatabase
argument_list|(
name|tbl
operator|.
name|getDbName
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|recovery
condition|)
block|{
name|console
operator|.
name|printInfo
argument_list|(
literal|"Recovery after ARCHIVE succeeded"
argument_list|)
expr_stmt|;
block|}
return|return
literal|0
return|;
block|}
specifier|private
name|int
name|unarchive
parameter_list|(
name|Hive
name|db
parameter_list|,
name|AlterTableSimpleDesc
name|simpleDesc
parameter_list|)
throws|throws
name|HiveException
throws|,
name|URISyntaxException
block|{
name|Table
name|tbl
init|=
name|db
operator|.
name|getTable
argument_list|(
name|simpleDesc
operator|.
name|getTableName
argument_list|()
argument_list|)
decl_stmt|;
comment|// Means user specified a table, not a partition
if|if
condition|(
name|simpleDesc
operator|.
name|getPartSpec
argument_list|()
operator|==
literal|null
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"UNARCHIVE is for partitions only"
argument_list|)
throw|;
block|}
if|if
condition|(
name|tbl
operator|.
name|getTableType
argument_list|()
operator|!=
name|TableType
operator|.
name|MANAGED_TABLE
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"UNARCHIVE can only be performed on managed tables"
argument_list|)
throw|;
block|}
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|partSpec
init|=
name|simpleDesc
operator|.
name|getPartSpec
argument_list|()
decl_stmt|;
name|PartSpecInfo
name|partSpecInfo
init|=
name|PartSpecInfo
operator|.
name|create
argument_list|(
name|tbl
argument_list|,
name|partSpec
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|Partition
argument_list|>
name|partitions
init|=
name|db
operator|.
name|getPartitions
argument_list|(
name|tbl
argument_list|,
name|partSpec
argument_list|)
decl_stmt|;
name|int
name|partSpecLevel
init|=
name|partSpec
operator|.
name|size
argument_list|()
decl_stmt|;
name|Path
name|originalDir
init|=
literal|null
decl_stmt|;
comment|// when we have partial partitions specification we must assume partitions
comment|// lie in standard place - if they were in custom locations putting
comment|// them into one archive would involve mass amount of copying
comment|// in full partition specification case we allow custom locations
comment|// to keep backward compatibility
if|if
condition|(
name|partitions
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"No partition matches the specification"
argument_list|)
throw|;
block|}
elseif|else
if|if
condition|(
name|partSpecInfo
operator|.
name|values
operator|.
name|size
argument_list|()
operator|!=
name|tbl
operator|.
name|getPartCols
argument_list|()
operator|.
name|size
argument_list|()
condition|)
block|{
comment|// for partial specifications we need partitions to follow the scheme
for|for
control|(
name|Partition
name|p
range|:
name|partitions
control|)
block|{
if|if
condition|(
name|partitionInCustomLocation
argument_list|(
name|tbl
argument_list|,
name|p
argument_list|)
condition|)
block|{
name|String
name|message
init|=
name|String
operator|.
name|format
argument_list|(
literal|"UNARCHIVE cannot run for partition "
operator|+
literal|"groups with custom locations like %s"
argument_list|,
name|p
operator|.
name|getLocation
argument_list|()
argument_list|)
decl_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|message
argument_list|)
throw|;
block|}
block|}
name|originalDir
operator|=
name|partSpecInfo
operator|.
name|createPath
argument_list|(
name|tbl
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|Partition
name|p
init|=
name|partitions
operator|.
name|get
argument_list|(
literal|0
argument_list|)
decl_stmt|;
if|if
condition|(
name|ArchiveUtils
operator|.
name|isArchived
argument_list|(
name|p
argument_list|)
condition|)
block|{
name|originalDir
operator|=
operator|new
name|Path
argument_list|(
name|getOriginalLocation
argument_list|(
name|p
argument_list|)
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|originalDir
operator|=
operator|new
name|Path
argument_list|(
name|p
operator|.
name|getLocation
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
name|URI
name|originalUri
init|=
name|ArchiveUtils
operator|.
name|addSlash
argument_list|(
name|originalDir
operator|.
name|toUri
argument_list|()
argument_list|)
decl_stmt|;
name|Path
name|intermediateArchivedDir
init|=
operator|new
name|Path
argument_list|(
name|originalDir
operator|.
name|getParent
argument_list|()
argument_list|,
name|originalDir
operator|.
name|getName
argument_list|()
operator|+
name|INTERMEDIATE_ARCHIVED_DIR_SUFFIX
argument_list|)
decl_stmt|;
name|Path
name|intermediateExtractedDir
init|=
operator|new
name|Path
argument_list|(
name|originalDir
operator|.
name|getParent
argument_list|()
argument_list|,
name|originalDir
operator|.
name|getName
argument_list|()
operator|+
name|INTERMEDIATE_EXTRACTED_DIR_SUFFIX
argument_list|)
decl_stmt|;
name|boolean
name|recovery
init|=
literal|false
decl_stmt|;
if|if
condition|(
name|pathExists
argument_list|(
name|intermediateArchivedDir
argument_list|)
operator|||
name|pathExists
argument_list|(
name|intermediateExtractedDir
argument_list|)
condition|)
block|{
name|recovery
operator|=
literal|true
expr_stmt|;
name|console
operator|.
name|printInfo
argument_list|(
literal|"Starting recovery after failed UNARCHIVE"
argument_list|)
expr_stmt|;
block|}
for|for
control|(
name|Partition
name|p
range|:
name|partitions
control|)
block|{
name|checkArchiveProperty
argument_list|(
name|partSpecLevel
argument_list|,
name|recovery
argument_list|,
name|p
argument_list|)
expr_stmt|;
block|}
name|String
name|archiveName
init|=
literal|"data.har"
decl_stmt|;
name|FileSystem
name|fs
init|=
literal|null
decl_stmt|;
try|try
block|{
name|fs
operator|=
name|originalDir
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
comment|// assume the archive is in the original dir, check if it exists
name|Path
name|archivePath
init|=
operator|new
name|Path
argument_list|(
name|originalDir
argument_list|,
name|archiveName
argument_list|)
decl_stmt|;
name|URI
name|archiveUri
init|=
name|archivePath
operator|.
name|toUri
argument_list|()
decl_stmt|;
name|ArchiveUtils
operator|.
name|HarPathHelper
name|harHelper
init|=
operator|new
name|ArchiveUtils
operator|.
name|HarPathHelper
argument_list|(
name|conf
argument_list|,
name|archiveUri
argument_list|,
name|originalUri
argument_list|)
decl_stmt|;
name|URI
name|sourceUri
init|=
name|harHelper
operator|.
name|getHarUri
argument_list|(
name|originalUri
argument_list|)
decl_stmt|;
name|Path
name|sourceDir
init|=
operator|new
name|Path
argument_list|(
name|sourceUri
operator|.
name|getScheme
argument_list|()
argument_list|,
name|sourceUri
operator|.
name|getAuthority
argument_list|()
argument_list|,
name|sourceUri
operator|.
name|getPath
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|pathExists
argument_list|(
name|intermediateArchivedDir
argument_list|)
operator|&&
operator|!
name|pathExists
argument_list|(
name|archivePath
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Haven't found any archive where it should be"
argument_list|)
throw|;
block|}
name|Path
name|tmpPath
init|=
name|driverContext
operator|.
name|getCtx
argument_list|()
operator|.
name|getExternalTmpPath
argument_list|(
name|originalDir
argument_list|)
decl_stmt|;
try|try
block|{
name|fs
operator|=
name|tmpPath
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
comment|// Clarification of terms:
comment|// - The originalDir directory represents the original directory of the
comment|//   partitions' files. They now contain an archived version of those files
comment|//   eg. hdfs:/warehouse/myTable/ds=1/
comment|// - The source directory is the directory containing all the files that
comment|//   should be in the partitions. e.g. har:/warehouse/myTable/ds=1/myTable.har/
comment|//   Note the har:/ scheme
comment|// Steps:
comment|// 1. Extract the archive in a temporary folder
comment|// 2. Move the archive dir to an intermediate dir that is in at the same
comment|//    dir as originalLocation. Call the new dir intermediate-extracted.
comment|// 3. Rename the original partitions dir to an intermediate dir. Call the
comment|//    renamed dir intermediate-archive
comment|// 4. Rename intermediate-extracted to the original partitions dir
comment|// 5. Change the metadata
comment|// 6. Delete the archived partitions files in intermediate-archive
if|if
condition|(
operator|!
name|pathExists
argument_list|(
name|intermediateExtractedDir
argument_list|)
operator|&&
operator|!
name|pathExists
argument_list|(
name|intermediateArchivedDir
argument_list|)
condition|)
block|{
try|try
block|{
comment|// Copy the files out of the archive into the temporary directory
name|String
name|copySource
init|=
name|sourceDir
operator|.
name|toString
argument_list|()
decl_stmt|;
name|String
name|copyDest
init|=
name|tmpPath
operator|.
name|toString
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|args
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
name|args
operator|.
name|add
argument_list|(
literal|"-cp"
argument_list|)
expr_stmt|;
name|args
operator|.
name|add
argument_list|(
name|copySource
argument_list|)
expr_stmt|;
name|args
operator|.
name|add
argument_list|(
name|copyDest
argument_list|)
expr_stmt|;
name|console
operator|.
name|printInfo
argument_list|(
literal|"Copying "
operator|+
name|copySource
operator|+
literal|" to "
operator|+
name|copyDest
argument_list|)
expr_stmt|;
name|FileSystem
name|srcFs
init|=
name|FileSystem
operator|.
name|get
argument_list|(
name|sourceDir
operator|.
name|toUri
argument_list|()
argument_list|,
name|conf
argument_list|)
decl_stmt|;
name|srcFs
operator|.
name|initialize
argument_list|(
name|sourceDir
operator|.
name|toUri
argument_list|()
argument_list|,
name|conf
argument_list|)
expr_stmt|;
name|FsShell
name|fss
init|=
operator|new
name|FsShell
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|int
name|ret
init|=
literal|0
decl_stmt|;
try|try
block|{
name|ret
operator|=
name|ToolRunner
operator|.
name|run
argument_list|(
name|fss
argument_list|,
name|args
operator|.
name|toArray
argument_list|(
operator|new
name|String
index|[
literal|0
index|]
argument_list|)
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
if|if
condition|(
name|ret
operator|!=
literal|0
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Error while copying files from archive, return code="
operator|+
name|ret
argument_list|)
throw|;
block|}
else|else
block|{
name|console
operator|.
name|printInfo
argument_list|(
literal|"Successfully Copied "
operator|+
name|copySource
operator|+
literal|" to "
operator|+
name|copyDest
argument_list|)
expr_stmt|;
block|}
name|console
operator|.
name|printInfo
argument_list|(
literal|"Moving "
operator|+
name|tmpPath
operator|+
literal|" to "
operator|+
name|intermediateExtractedDir
argument_list|)
expr_stmt|;
if|if
condition|(
name|fs
operator|.
name|exists
argument_list|(
name|intermediateExtractedDir
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Invalid state: the intermediate extracted "
operator|+
literal|"directory already exists."
argument_list|)
throw|;
block|}
name|fs
operator|.
name|rename
argument_list|(
name|tmpPath
argument_list|,
name|intermediateExtractedDir
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
comment|// At this point, we know that the extracted files are in the intermediate
comment|// extracted dir, or in the the original directory.
if|if
condition|(
operator|!
name|pathExists
argument_list|(
name|intermediateArchivedDir
argument_list|)
condition|)
block|{
try|try
block|{
name|console
operator|.
name|printInfo
argument_list|(
literal|"Moving "
operator|+
name|originalDir
operator|+
literal|" to "
operator|+
name|intermediateArchivedDir
argument_list|)
expr_stmt|;
name|fs
operator|.
name|rename
argument_list|(
name|originalDir
argument_list|,
name|intermediateArchivedDir
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
else|else
block|{
name|console
operator|.
name|printInfo
argument_list|(
name|intermediateArchivedDir
operator|+
literal|" already exists. "
operator|+
literal|"Assuming it contains the archived version of the partition"
argument_list|)
expr_stmt|;
block|}
comment|// If there is a failure from here to until when the metadata is changed,
comment|// the partition will be empty or throw errors on read.
comment|// If the original location exists here, then it must be the extracted files
comment|// because in the previous step, we moved the previous original location
comment|// (containing the archived version of the files) to intermediateArchiveDir
if|if
condition|(
operator|!
name|pathExists
argument_list|(
name|originalDir
argument_list|)
condition|)
block|{
try|try
block|{
name|console
operator|.
name|printInfo
argument_list|(
literal|"Moving "
operator|+
name|intermediateExtractedDir
operator|+
literal|" to "
operator|+
name|originalDir
argument_list|)
expr_stmt|;
name|fs
operator|.
name|rename
argument_list|(
name|intermediateExtractedDir
argument_list|,
name|originalDir
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
else|else
block|{
name|console
operator|.
name|printInfo
argument_list|(
name|originalDir
operator|+
literal|" already exists. "
operator|+
literal|"Assuming it contains the extracted files in the partition"
argument_list|)
expr_stmt|;
block|}
for|for
control|(
name|Partition
name|p
range|:
name|partitions
control|)
block|{
name|setUnArchived
argument_list|(
name|p
argument_list|)
expr_stmt|;
try|try
block|{
comment|// TODO: catalog
name|db
operator|.
name|alterPartition
argument_list|(
name|simpleDesc
operator|.
name|getTableName
argument_list|()
argument_list|,
name|p
argument_list|,
literal|null
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|InvalidOperationException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
comment|// If a failure happens here, the intermediate archive files won't be
comment|// deleted. The user will need to call unarchive again to clear those up.
if|if
condition|(
name|pathExists
argument_list|(
name|intermediateArchivedDir
argument_list|)
condition|)
block|{
name|deleteDir
argument_list|(
name|intermediateArchivedDir
argument_list|,
name|db
operator|.
name|getDatabase
argument_list|(
name|tbl
operator|.
name|getDbName
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|recovery
condition|)
block|{
name|console
operator|.
name|printInfo
argument_list|(
literal|"Recovery after UNARCHIVE succeeded"
argument_list|)
expr_stmt|;
block|}
return|return
literal|0
return|;
block|}
specifier|private
name|void
name|checkArchiveProperty
parameter_list|(
name|int
name|partSpecLevel
parameter_list|,
name|boolean
name|recovery
parameter_list|,
name|Partition
name|p
parameter_list|)
throws|throws
name|HiveException
block|{
if|if
condition|(
operator|!
name|ArchiveUtils
operator|.
name|isArchived
argument_list|(
name|p
argument_list|)
operator|&&
operator|!
name|recovery
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Partition "
operator|+
name|p
operator|.
name|getName
argument_list|()
operator|+
literal|" is not archived."
argument_list|)
throw|;
block|}
name|int
name|archiveLevel
init|=
name|ArchiveUtils
operator|.
name|getArchivingLevel
argument_list|(
name|p
argument_list|)
decl_stmt|;
if|if
condition|(
name|partSpecLevel
operator|>
name|archiveLevel
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Partition "
operator|+
name|p
operator|.
name|getName
argument_list|()
operator|+
literal|" is archived at level "
operator|+
name|archiveLevel
operator|+
literal|", and given partspec only has "
operator|+
name|partSpecLevel
operator|+
literal|" specs."
argument_list|)
throw|;
block|}
block|}
specifier|private
name|int
name|compact
parameter_list|(
name|Hive
name|db
parameter_list|,
name|AlterTableSimpleDesc
name|desc
parameter_list|)
throws|throws
name|HiveException
block|{
name|Table
name|tbl
init|=
name|db
operator|.
name|getTable
argument_list|(
name|desc
operator|.
name|getTableName
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|AcidUtils
operator|.
name|isTransactionalTable
argument_list|(
name|tbl
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|ErrorMsg
operator|.
name|NONACID_COMPACTION_NOT_SUPPORTED
argument_list|,
name|tbl
operator|.
name|getDbName
argument_list|()
argument_list|,
name|tbl
operator|.
name|getTableName
argument_list|()
argument_list|)
throw|;
block|}
name|String
name|partName
init|=
literal|null
decl_stmt|;
if|if
condition|(
name|desc
operator|.
name|getPartSpec
argument_list|()
operator|==
literal|null
condition|)
block|{
comment|// Compaction can only be done on the whole table if the table is non-partitioned.
if|if
condition|(
name|tbl
operator|.
name|isPartitioned
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|ErrorMsg
operator|.
name|NO_COMPACTION_PARTITION
argument_list|)
throw|;
block|}
block|}
else|else
block|{
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|partSpec
init|=
name|desc
operator|.
name|getPartSpec
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|Partition
argument_list|>
name|partitions
init|=
name|db
operator|.
name|getPartitions
argument_list|(
name|tbl
argument_list|,
name|partSpec
argument_list|)
decl_stmt|;
if|if
condition|(
name|partitions
operator|.
name|size
argument_list|()
operator|>
literal|1
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|ErrorMsg
operator|.
name|TOO_MANY_COMPACTION_PARTITIONS
argument_list|)
throw|;
block|}
elseif|else
if|if
condition|(
name|partitions
operator|.
name|size
argument_list|()
operator|==
literal|0
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|ErrorMsg
operator|.
name|INVALID_PARTITION_SPEC
argument_list|)
throw|;
block|}
name|partName
operator|=
name|partitions
operator|.
name|get
argument_list|(
literal|0
argument_list|)
operator|.
name|getName
argument_list|()
expr_stmt|;
block|}
name|CompactionResponse
name|resp
init|=
name|db
operator|.
name|compact2
argument_list|(
name|tbl
operator|.
name|getDbName
argument_list|()
argument_list|,
name|tbl
operator|.
name|getTableName
argument_list|()
argument_list|,
name|partName
argument_list|,
name|desc
operator|.
name|getCompactionType
argument_list|()
argument_list|,
name|desc
operator|.
name|getProps
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|resp
operator|.
name|isAccepted
argument_list|()
condition|)
block|{
name|console
operator|.
name|printInfo
argument_list|(
literal|"Compaction enqueued with id "
operator|+
name|resp
operator|.
name|getId
argument_list|()
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|console
operator|.
name|printInfo
argument_list|(
literal|"Compaction already enqueued with id "
operator|+
name|resp
operator|.
name|getId
argument_list|()
operator|+
literal|"; State is "
operator|+
name|resp
operator|.
name|getState
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|desc
operator|.
name|isBlocking
argument_list|()
operator|&&
name|resp
operator|.
name|isAccepted
argument_list|()
condition|)
block|{
name|StringBuilder
name|progressDots
init|=
operator|new
name|StringBuilder
argument_list|()
decl_stmt|;
name|long
name|waitTimeMs
init|=
literal|1000
decl_stmt|;
name|wait
label|:
while|while
condition|(
literal|true
condition|)
block|{
comment|//double wait time until 5min
name|waitTimeMs
operator|=
name|waitTimeMs
operator|*
literal|2
expr_stmt|;
name|waitTimeMs
operator|=
name|waitTimeMs
operator|<
literal|5
operator|*
literal|60
operator|*
literal|1000
condition|?
name|waitTimeMs
else|:
literal|5
operator|*
literal|60
operator|*
literal|1000
expr_stmt|;
try|try
block|{
name|Thread
operator|.
name|sleep
argument_list|(
name|waitTimeMs
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|InterruptedException
name|ex
parameter_list|)
block|{
name|console
operator|.
name|printInfo
argument_list|(
literal|"Interrupted while waiting for compaction with id="
operator|+
name|resp
operator|.
name|getId
argument_list|()
argument_list|)
expr_stmt|;
break|break;
block|}
comment|//this could be expensive when there are a lot of compactions....
comment|//todo: update to search by ID once HIVE-13353 is done
name|ShowCompactResponse
name|allCompactions
init|=
name|db
operator|.
name|showCompactions
argument_list|()
decl_stmt|;
for|for
control|(
name|ShowCompactResponseElement
name|compaction
range|:
name|allCompactions
operator|.
name|getCompacts
argument_list|()
control|)
block|{
if|if
condition|(
name|resp
operator|.
name|getId
argument_list|()
operator|!=
name|compaction
operator|.
name|getId
argument_list|()
condition|)
block|{
continue|continue;
block|}
switch|switch
condition|(
name|compaction
operator|.
name|getState
argument_list|()
condition|)
block|{
case|case
name|TxnStore
operator|.
name|WORKING_RESPONSE
case|:
case|case
name|TxnStore
operator|.
name|INITIATED_RESPONSE
case|:
comment|//still working
name|console
operator|.
name|printInfo
argument_list|(
name|progressDots
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
name|progressDots
operator|.
name|append
argument_list|(
literal|"."
argument_list|)
expr_stmt|;
continue|continue
name|wait
continue|;
default|default:
comment|//done
name|console
operator|.
name|printInfo
argument_list|(
literal|"Compaction with id "
operator|+
name|resp
operator|.
name|getId
argument_list|()
operator|+
literal|" finished with status: "
operator|+
name|compaction
operator|.
name|getState
argument_list|()
argument_list|)
expr_stmt|;
break|break
name|wait
break|;
block|}
block|}
block|}
block|}
return|return
literal|0
return|;
block|}
comment|/**    * MetastoreCheck, see if the data in the metastore matches what is on the    * dfs. Current version checks for tables and partitions that are either    * missing on disk on in the metastore.    *    * @param db    *          The database in question.    * @param msckDesc    *          Information about the tables and partitions we want to check for.    * @return Returns 0 when execution succeeds and above 0 if it fails.    */
specifier|private
name|int
name|msck
parameter_list|(
name|Hive
name|db
parameter_list|,
name|MsckDesc
name|msckDesc
parameter_list|)
block|{
name|Msck
name|msck
decl_stmt|;
try|try
block|{
name|msck
operator|=
operator|new
name|Msck
argument_list|(
literal|false
argument_list|,
literal|false
argument_list|)
expr_stmt|;
name|msck
operator|.
name|init
argument_list|(
name|db
operator|.
name|getConf
argument_list|()
argument_list|)
expr_stmt|;
name|String
index|[]
name|names
init|=
name|Utilities
operator|.
name|getDbTableName
argument_list|(
name|msckDesc
operator|.
name|getTableName
argument_list|()
argument_list|)
decl_stmt|;
name|MsckInfo
name|msckInfo
init|=
operator|new
name|MsckInfo
argument_list|(
name|SessionState
operator|.
name|get
argument_list|()
operator|.
name|getCurrentCatalog
argument_list|()
argument_list|,
name|names
index|[
literal|0
index|]
argument_list|,
name|names
index|[
literal|1
index|]
argument_list|,
name|msckDesc
operator|.
name|getPartSpecs
argument_list|()
argument_list|,
name|msckDesc
operator|.
name|getResFile
argument_list|()
argument_list|,
name|msckDesc
operator|.
name|isRepairPartitions
argument_list|()
argument_list|,
name|msckDesc
operator|.
name|isAddPartitions
argument_list|()
argument_list|,
name|msckDesc
operator|.
name|isDropPartitions
argument_list|()
argument_list|,
operator|-
literal|1
argument_list|)
decl_stmt|;
return|return
name|msck
operator|.
name|repair
argument_list|(
name|msckInfo
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|MetaException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Unable to create msck instance."
argument_list|,
name|e
argument_list|)
expr_stmt|;
return|return
literal|1
return|;
block|}
catch|catch
parameter_list|(
name|SemanticException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Msck failed."
argument_list|,
name|e
argument_list|)
expr_stmt|;
return|return
literal|1
return|;
block|}
block|}
comment|/**    * Alter a given table.    *    * @param db    *          The database in question.    * @param alterTbl    *          This is the table we're altering.    * @return Returns 0 when execution succeeds and above 0 if it fails.    * @throws HiveException    *           Throws this exception if an unexpected error occurs.    */
specifier|private
name|int
name|alterTable
parameter_list|(
name|Hive
name|db
parameter_list|,
name|AlterTableDesc
name|alterTbl
parameter_list|)
throws|throws
name|HiveException
block|{
if|if
condition|(
name|alterTbl
operator|.
name|getOp
argument_list|()
operator|==
name|AlterTableDesc
operator|.
name|AlterTableTypes
operator|.
name|RENAME
condition|)
block|{
name|String
name|names
index|[]
init|=
name|Utilities
operator|.
name|getDbTableName
argument_list|(
name|alterTbl
operator|.
name|getOldName
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|Utils
operator|.
name|isBootstrapDumpInProgress
argument_list|(
name|db
argument_list|,
name|names
index|[
literal|0
index|]
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"DDLTask: Rename Table not allowed as bootstrap dump in progress"
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Rename Table: Not allowed as bootstrap dump in progress"
argument_list|)
throw|;
block|}
block|}
comment|// alter the table
name|Table
name|tbl
init|=
name|db
operator|.
name|getTable
argument_list|(
name|alterTbl
operator|.
name|getOldName
argument_list|()
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|Partition
argument_list|>
name|allPartitions
init|=
literal|null
decl_stmt|;
if|if
condition|(
name|alterTbl
operator|.
name|getPartSpec
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|partSpec
init|=
name|alterTbl
operator|.
name|getPartSpec
argument_list|()
decl_stmt|;
if|if
condition|(
name|DDLSemanticAnalyzer
operator|.
name|isFullSpec
argument_list|(
name|tbl
argument_list|,
name|partSpec
argument_list|)
condition|)
block|{
name|allPartitions
operator|=
operator|new
name|ArrayList
argument_list|<
name|Partition
argument_list|>
argument_list|()
expr_stmt|;
name|Partition
name|part
init|=
name|db
operator|.
name|getPartition
argument_list|(
name|tbl
argument_list|,
name|partSpec
argument_list|,
literal|false
argument_list|)
decl_stmt|;
if|if
condition|(
name|part
operator|==
literal|null
condition|)
block|{
comment|// User provided a fully specified partition spec but it doesn't exist, fail.
throw|throw
operator|new
name|HiveException
argument_list|(
name|ErrorMsg
operator|.
name|INVALID_PARTITION
argument_list|,
name|StringUtils
operator|.
name|join
argument_list|(
name|alterTbl
operator|.
name|getPartSpec
argument_list|()
operator|.
name|keySet
argument_list|()
argument_list|,
literal|','
argument_list|)
operator|+
literal|" for table "
operator|+
name|alterTbl
operator|.
name|getOldName
argument_list|()
argument_list|)
throw|;
block|}
name|allPartitions
operator|.
name|add
argument_list|(
name|part
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// DDLSemanticAnalyzer has already checked if partial partition specs are allowed,
comment|// thus we should not need to check it here.
name|allPartitions
operator|=
name|db
operator|.
name|getPartitions
argument_list|(
name|tbl
argument_list|,
name|alterTbl
operator|.
name|getPartSpec
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
comment|// Don't change the table object returned by the metastore, as we'll mess with it's caches.
name|Table
name|oldTbl
init|=
name|tbl
decl_stmt|;
name|tbl
operator|=
name|oldTbl
operator|.
name|copy
argument_list|()
expr_stmt|;
comment|// Handle child tasks here. We could add them directly whereever we need,
comment|// but let's make it a little bit more explicit.
if|if
condition|(
name|allPartitions
operator|!=
literal|null
condition|)
block|{
comment|// Alter all partitions
for|for
control|(
name|Partition
name|part
range|:
name|allPartitions
control|)
block|{
name|addChildTasks
argument_list|(
name|alterTableOrSinglePartition
argument_list|(
name|alterTbl
argument_list|,
name|tbl
argument_list|,
name|part
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
comment|// Just alter the table
name|addChildTasks
argument_list|(
name|alterTableOrSinglePartition
argument_list|(
name|alterTbl
argument_list|,
name|tbl
argument_list|,
literal|null
argument_list|)
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|allPartitions
operator|==
literal|null
condition|)
block|{
name|updateModifiedParameters
argument_list|(
name|tbl
operator|.
name|getTTable
argument_list|()
operator|.
name|getParameters
argument_list|()
argument_list|,
name|conf
argument_list|)
expr_stmt|;
name|tbl
operator|.
name|checkValidity
argument_list|(
name|conf
argument_list|)
expr_stmt|;
block|}
else|else
block|{
for|for
control|(
name|Partition
name|tmpPart
range|:
name|allPartitions
control|)
block|{
name|updateModifiedParameters
argument_list|(
name|tmpPart
operator|.
name|getParameters
argument_list|()
argument_list|,
name|conf
argument_list|)
expr_stmt|;
block|}
block|}
try|try
block|{
name|EnvironmentContext
name|environmentContext
init|=
name|alterTbl
operator|.
name|getEnvironmentContext
argument_list|()
decl_stmt|;
if|if
condition|(
name|environmentContext
operator|==
literal|null
condition|)
block|{
name|environmentContext
operator|=
operator|new
name|EnvironmentContext
argument_list|()
expr_stmt|;
block|}
name|environmentContext
operator|.
name|putToProperties
argument_list|(
name|HiveMetaHook
operator|.
name|ALTER_TABLE_OPERATION_TYPE
argument_list|,
name|alterTbl
operator|.
name|getOp
argument_list|()
operator|.
name|name
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|allPartitions
operator|==
literal|null
condition|)
block|{
name|long
name|writeId
init|=
name|alterTbl
operator|.
name|getWriteId
argument_list|()
operator|!=
literal|null
condition|?
name|alterTbl
operator|.
name|getWriteId
argument_list|()
else|:
literal|0
decl_stmt|;
if|if
condition|(
name|alterTbl
operator|.
name|getReplicationSpec
argument_list|()
operator|!=
literal|null
operator|&&
name|alterTbl
operator|.
name|getReplicationSpec
argument_list|()
operator|.
name|isMigratingToTxnTable
argument_list|()
condition|)
block|{
name|Long
name|tmpWriteId
init|=
name|ReplUtils
operator|.
name|getMigrationCurrentTblWriteId
argument_list|(
name|conf
argument_list|)
decl_stmt|;
if|if
condition|(
name|tmpWriteId
operator|==
literal|null
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"DDLTask : Write id is not set in the config by open txn task for migration"
argument_list|)
throw|;
block|}
name|writeId
operator|=
name|tmpWriteId
expr_stmt|;
block|}
name|db
operator|.
name|alterTable
argument_list|(
name|alterTbl
operator|.
name|getOldName
argument_list|()
argument_list|,
name|tbl
argument_list|,
name|alterTbl
operator|.
name|getIsCascade
argument_list|()
argument_list|,
name|environmentContext
argument_list|,
literal|true
argument_list|,
name|writeId
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// Note: this is necessary for UPDATE_STATISTICS command, that operates via ADDPROPS (why?).
comment|//       For any other updates, we don't want to do txn check on partitions when altering table.
name|boolean
name|isTxn
init|=
literal|false
decl_stmt|;
if|if
condition|(
name|alterTbl
operator|.
name|getPartSpec
argument_list|()
operator|!=
literal|null
operator|&&
name|alterTbl
operator|.
name|getOp
argument_list|()
operator|==
name|AlterTableTypes
operator|.
name|ADDPROPS
condition|)
block|{
comment|// ADDPROPS is used to add replication properties like repl.last.id, which isn't
comment|// transactional change. In case of replication check for transactional properties
comment|// explicitly.
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|props
init|=
name|alterTbl
operator|.
name|getProps
argument_list|()
decl_stmt|;
if|if
condition|(
name|alterTbl
operator|.
name|getReplicationSpec
argument_list|()
operator|!=
literal|null
operator|&&
name|alterTbl
operator|.
name|getReplicationSpec
argument_list|()
operator|.
name|isInReplicationScope
argument_list|()
condition|)
block|{
name|isTxn
operator|=
operator|(
name|props
operator|.
name|get
argument_list|(
name|StatsSetupConst
operator|.
name|COLUMN_STATS_ACCURATE
argument_list|)
operator|!=
literal|null
operator|)
expr_stmt|;
block|}
else|else
block|{
name|isTxn
operator|=
literal|true
expr_stmt|;
block|}
block|}
name|db
operator|.
name|alterPartitions
argument_list|(
name|Warehouse
operator|.
name|getQualifiedName
argument_list|(
name|tbl
operator|.
name|getTTable
argument_list|()
argument_list|)
argument_list|,
name|allPartitions
argument_list|,
name|environmentContext
argument_list|,
name|isTxn
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|InvalidOperationException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"alter table: "
argument_list|,
name|e
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|,
name|ErrorMsg
operator|.
name|GENERIC_ERROR
argument_list|)
throw|;
block|}
comment|// This is kind of hacky - the read entity contains the old table, whereas
comment|// the write entity
comment|// contains the new table. This is needed for rename - both the old and the
comment|// new table names are
comment|// passed
comment|// Don't acquire locks for any of these, we have already asked for them in DDLSemanticAnalyzer.
if|if
condition|(
name|allPartitions
operator|!=
literal|null
condition|)
block|{
for|for
control|(
name|Partition
name|tmpPart
range|:
name|allPartitions
control|)
block|{
name|work
operator|.
name|getInputs
argument_list|()
operator|.
name|add
argument_list|(
operator|new
name|ReadEntity
argument_list|(
name|tmpPart
argument_list|)
argument_list|)
expr_stmt|;
name|addIfAbsentByName
argument_list|(
operator|new
name|WriteEntity
argument_list|(
name|tmpPart
argument_list|,
name|WriteEntity
operator|.
name|WriteType
operator|.
name|DDL_NO_LOCK
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
name|work
operator|.
name|getInputs
argument_list|()
operator|.
name|add
argument_list|(
operator|new
name|ReadEntity
argument_list|(
name|oldTbl
argument_list|)
argument_list|)
expr_stmt|;
name|addIfAbsentByName
argument_list|(
operator|new
name|WriteEntity
argument_list|(
name|tbl
argument_list|,
name|WriteEntity
operator|.
name|WriteType
operator|.
name|DDL_NO_LOCK
argument_list|)
argument_list|)
expr_stmt|;
block|}
return|return
literal|0
return|;
block|}
comment|/**    * There are many places where "duplicate" Read/WriteEnity objects are added.  The way this was    * initially implemented, the duplicate just replaced the previous object.    * (work.getOutputs() is a Set and WriteEntity#equals() relies on name)    * This may be benign for ReadEntity and perhaps was benign for WriteEntity before WriteType was    * added. Now that WriteEntity has a WriteType it replaces it with one with possibly different    * {@link org.apache.hadoop.hive.ql.hooks.WriteEntity.WriteType}.  It's hard to imagine    * how this is desirable.    *    * As of HIVE-14993, WriteEntity with different WriteType must be considered different.    * So WriteEntity created in DDLTask cause extra output in golden files, but only because    * DDLTask sets a different WriteType for the same Entity.    *    * In the spirit of bug-for-bug compatibility, this method ensures we only add new    * WriteEntity if it's really new.    *    * @return {@code true} if item was added    */
specifier|static
name|boolean
name|addIfAbsentByName
parameter_list|(
name|WriteEntity
name|newWriteEntity
parameter_list|,
name|Set
argument_list|<
name|WriteEntity
argument_list|>
name|outputs
parameter_list|)
block|{
for|for
control|(
name|WriteEntity
name|writeEntity
range|:
name|outputs
control|)
block|{
if|if
condition|(
name|writeEntity
operator|.
name|getName
argument_list|()
operator|.
name|equalsIgnoreCase
argument_list|(
name|newWriteEntity
operator|.
name|getName
argument_list|()
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Ignoring request to add {} because {} is present"
argument_list|,
name|newWriteEntity
operator|.
name|toStringDetail
argument_list|()
argument_list|,
name|writeEntity
operator|.
name|toStringDetail
argument_list|()
argument_list|)
expr_stmt|;
return|return
literal|false
return|;
block|}
block|}
name|outputs
operator|.
name|add
argument_list|(
name|newWriteEntity
argument_list|)
expr_stmt|;
return|return
literal|true
return|;
block|}
specifier|private
name|boolean
name|addIfAbsentByName
parameter_list|(
name|WriteEntity
name|newWriteEntity
parameter_list|)
block|{
return|return
name|addIfAbsentByName
argument_list|(
name|newWriteEntity
argument_list|,
name|work
operator|.
name|getOutputs
argument_list|()
argument_list|)
return|;
block|}
specifier|private
name|void
name|addChildTasks
parameter_list|(
name|List
argument_list|<
name|Task
argument_list|<
name|?
argument_list|>
argument_list|>
name|extraTasks
parameter_list|)
block|{
if|if
condition|(
name|extraTasks
operator|==
literal|null
condition|)
block|{
return|return;
block|}
for|for
control|(
name|Task
argument_list|<
name|?
argument_list|>
name|newTask
range|:
name|extraTasks
control|)
block|{
name|addDependentTask
argument_list|(
name|newTask
argument_list|)
expr_stmt|;
block|}
block|}
specifier|private
name|boolean
name|isSchemaEvolutionEnabled
parameter_list|(
name|Table
name|tbl
parameter_list|)
block|{
name|boolean
name|isAcid
init|=
name|AcidUtils
operator|.
name|isTablePropertyTransactional
argument_list|(
name|tbl
operator|.
name|getMetadata
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|isAcid
operator|||
name|HiveConf
operator|.
name|getBoolVar
argument_list|(
name|conf
argument_list|,
name|ConfVars
operator|.
name|HIVE_SCHEMA_EVOLUTION
argument_list|)
condition|)
block|{
return|return
literal|true
return|;
block|}
return|return
literal|false
return|;
block|}
specifier|private
specifier|static
name|StorageDescriptor
name|retrieveStorageDescriptor
parameter_list|(
name|Table
name|tbl
parameter_list|,
name|Partition
name|part
parameter_list|)
block|{
return|return
operator|(
name|part
operator|==
literal|null
condition|?
name|tbl
operator|.
name|getTTable
argument_list|()
operator|.
name|getSd
argument_list|()
else|:
name|part
operator|.
name|getTPartition
argument_list|()
operator|.
name|getSd
argument_list|()
operator|)
return|;
block|}
specifier|private
name|List
argument_list|<
name|Task
argument_list|<
name|?
argument_list|>
argument_list|>
name|alterTableOrSinglePartition
parameter_list|(
name|AlterTableDesc
name|alterTbl
parameter_list|,
name|Table
name|tbl
parameter_list|,
name|Partition
name|part
parameter_list|)
throws|throws
name|HiveException
block|{
name|EnvironmentContext
name|environmentContext
init|=
name|alterTbl
operator|.
name|getEnvironmentContext
argument_list|()
decl_stmt|;
if|if
condition|(
name|environmentContext
operator|==
literal|null
condition|)
block|{
name|environmentContext
operator|=
operator|new
name|EnvironmentContext
argument_list|()
expr_stmt|;
name|alterTbl
operator|.
name|setEnvironmentContext
argument_list|(
name|environmentContext
argument_list|)
expr_stmt|;
block|}
comment|// do not need update stats in alter table/partition operations
if|if
condition|(
name|environmentContext
operator|.
name|getProperties
argument_list|()
operator|==
literal|null
operator|||
name|environmentContext
operator|.
name|getProperties
argument_list|()
operator|.
name|get
argument_list|(
name|StatsSetupConst
operator|.
name|DO_NOT_UPDATE_STATS
argument_list|)
operator|==
literal|null
condition|)
block|{
name|environmentContext
operator|.
name|putToProperties
argument_list|(
name|StatsSetupConst
operator|.
name|DO_NOT_UPDATE_STATS
argument_list|,
name|StatsSetupConst
operator|.
name|TRUE
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|alterTbl
operator|.
name|getOp
argument_list|()
operator|==
name|AlterTableDesc
operator|.
name|AlterTableTypes
operator|.
name|RENAME
condition|)
block|{
name|tbl
operator|.
name|setDbName
argument_list|(
name|Utilities
operator|.
name|getDatabaseName
argument_list|(
name|alterTbl
operator|.
name|getNewName
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
name|tbl
operator|.
name|setTableName
argument_list|(
name|Utilities
operator|.
name|getTableName
argument_list|(
name|alterTbl
operator|.
name|getNewName
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|alterTbl
operator|.
name|getOp
argument_list|()
operator|==
name|AlterTableDesc
operator|.
name|AlterTableTypes
operator|.
name|ADDPROPS
condition|)
block|{
return|return
name|alterTableAddProps
argument_list|(
name|alterTbl
argument_list|,
name|tbl
argument_list|,
name|part
argument_list|,
name|environmentContext
argument_list|)
return|;
block|}
elseif|else
if|if
condition|(
name|alterTbl
operator|.
name|getOp
argument_list|()
operator|==
name|AlterTableDesc
operator|.
name|AlterTableTypes
operator|.
name|DROPPROPS
condition|)
block|{
return|return
name|alterTableDropProps
argument_list|(
name|alterTbl
argument_list|,
name|tbl
argument_list|,
name|part
argument_list|,
name|environmentContext
argument_list|)
return|;
block|}
elseif|else
if|if
condition|(
name|alterTbl
operator|.
name|getOp
argument_list|()
operator|==
name|AlterTableDesc
operator|.
name|AlterTableTypes
operator|.
name|ADDSERDEPROPS
condition|)
block|{
name|StorageDescriptor
name|sd
init|=
name|retrieveStorageDescriptor
argument_list|(
name|tbl
argument_list|,
name|part
argument_list|)
decl_stmt|;
name|sd
operator|.
name|getSerdeInfo
argument_list|()
operator|.
name|getParameters
argument_list|()
operator|.
name|putAll
argument_list|(
name|alterTbl
operator|.
name|getProps
argument_list|()
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|alterTbl
operator|.
name|getOp
argument_list|()
operator|==
name|AlterTableDesc
operator|.
name|AlterTableTypes
operator|.
name|ADDSERDE
condition|)
block|{
name|StorageDescriptor
name|sd
init|=
name|retrieveStorageDescriptor
argument_list|(
name|tbl
argument_list|,
name|part
argument_list|)
decl_stmt|;
name|String
name|serdeName
init|=
name|alterTbl
operator|.
name|getSerdeName
argument_list|()
decl_stmt|;
name|String
name|oldSerdeName
init|=
name|sd
operator|.
name|getSerdeInfo
argument_list|()
operator|.
name|getSerializationLib
argument_list|()
decl_stmt|;
comment|// if orc table, restrict changing the serde as it can break schema evolution
if|if
condition|(
name|isSchemaEvolutionEnabled
argument_list|(
name|tbl
argument_list|)
operator|&&
name|oldSerdeName
operator|.
name|equalsIgnoreCase
argument_list|(
name|OrcSerde
operator|.
name|class
operator|.
name|getName
argument_list|()
argument_list|)
operator|&&
operator|!
name|serdeName
operator|.
name|equalsIgnoreCase
argument_list|(
name|OrcSerde
operator|.
name|class
operator|.
name|getName
argument_list|()
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|ErrorMsg
operator|.
name|CANNOT_CHANGE_SERDE
argument_list|,
name|OrcSerde
operator|.
name|class
operator|.
name|getSimpleName
argument_list|()
argument_list|,
name|alterTbl
operator|.
name|getOldName
argument_list|()
argument_list|)
throw|;
block|}
name|sd
operator|.
name|getSerdeInfo
argument_list|()
operator|.
name|setSerializationLib
argument_list|(
name|serdeName
argument_list|)
expr_stmt|;
if|if
condition|(
operator|(
name|alterTbl
operator|.
name|getProps
argument_list|()
operator|!=
literal|null
operator|)
operator|&&
operator|(
name|alterTbl
operator|.
name|getProps
argument_list|()
operator|.
name|size
argument_list|()
operator|>
literal|0
operator|)
condition|)
block|{
name|sd
operator|.
name|getSerdeInfo
argument_list|()
operator|.
name|getParameters
argument_list|()
operator|.
name|putAll
argument_list|(
name|alterTbl
operator|.
name|getProps
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|part
operator|!=
literal|null
condition|)
block|{
comment|// TODO: wtf? This doesn't do anything.
name|part
operator|.
name|getTPartition
argument_list|()
operator|.
name|getSd
argument_list|()
operator|.
name|setCols
argument_list|(
name|part
operator|.
name|getTPartition
argument_list|()
operator|.
name|getSd
argument_list|()
operator|.
name|getCols
argument_list|()
argument_list|)
expr_stmt|;
block|}
else|else
block|{
if|if
condition|(
name|Table
operator|.
name|shouldStoreFieldsInMetastore
argument_list|(
name|conf
argument_list|,
name|serdeName
argument_list|,
name|tbl
operator|.
name|getParameters
argument_list|()
argument_list|)
operator|&&
operator|!
name|Table
operator|.
name|hasMetastoreBasedSchema
argument_list|(
name|conf
argument_list|,
name|oldSerdeName
argument_list|)
condition|)
block|{
comment|// If new SerDe needs to store fields in metastore, but the old serde doesn't, save
comment|// the fields so that new SerDe could operate. Note that this may fail if some fields
comment|// from old SerDe are too long to be stored in metastore, but there's nothing we can do.
try|try
block|{
name|Deserializer
name|oldSerde
init|=
name|HiveMetaStoreUtils
operator|.
name|getDeserializer
argument_list|(
name|conf
argument_list|,
name|tbl
operator|.
name|getTTable
argument_list|()
argument_list|,
literal|false
argument_list|,
name|oldSerdeName
argument_list|)
decl_stmt|;
name|tbl
operator|.
name|setFields
argument_list|(
name|Hive
operator|.
name|getFieldsFromDeserializer
argument_list|(
name|tbl
operator|.
name|getTableName
argument_list|()
argument_list|,
name|oldSerde
argument_list|)
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|MetaException
name|ex
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|ex
argument_list|)
throw|;
block|}
block|}
block|}
block|}
elseif|else
if|if
condition|(
name|alterTbl
operator|.
name|getOp
argument_list|()
operator|==
name|AlterTableDesc
operator|.
name|AlterTableTypes
operator|.
name|ADDFILEFORMAT
condition|)
block|{
name|StorageDescriptor
name|sd
init|=
name|retrieveStorageDescriptor
argument_list|(
name|tbl
argument_list|,
name|part
argument_list|)
decl_stmt|;
comment|// if orc table, restrict changing the file format as it can break schema evolution
if|if
condition|(
name|isSchemaEvolutionEnabled
argument_list|(
name|tbl
argument_list|)
operator|&&
name|sd
operator|.
name|getInputFormat
argument_list|()
operator|.
name|equals
argument_list|(
name|OrcInputFormat
operator|.
name|class
operator|.
name|getName
argument_list|()
argument_list|)
operator|&&
operator|!
name|alterTbl
operator|.
name|getInputFormat
argument_list|()
operator|.
name|equals
argument_list|(
name|OrcInputFormat
operator|.
name|class
operator|.
name|getName
argument_list|()
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|ErrorMsg
operator|.
name|CANNOT_CHANGE_FILEFORMAT
argument_list|,
literal|"ORC"
argument_list|,
name|alterTbl
operator|.
name|getOldName
argument_list|()
argument_list|)
throw|;
block|}
name|sd
operator|.
name|setInputFormat
argument_list|(
name|alterTbl
operator|.
name|getInputFormat
argument_list|()
argument_list|)
expr_stmt|;
name|sd
operator|.
name|setOutputFormat
argument_list|(
name|alterTbl
operator|.
name|getOutputFormat
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|alterTbl
operator|.
name|getSerdeName
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|sd
operator|.
name|getSerdeInfo
argument_list|()
operator|.
name|setSerializationLib
argument_list|(
name|alterTbl
operator|.
name|getSerdeName
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
elseif|else
if|if
condition|(
name|alterTbl
operator|.
name|getOp
argument_list|()
operator|==
name|AlterTableDesc
operator|.
name|AlterTableTypes
operator|.
name|ADDCLUSTERSORTCOLUMN
condition|)
block|{
name|StorageDescriptor
name|sd
init|=
name|retrieveStorageDescriptor
argument_list|(
name|tbl
argument_list|,
name|part
argument_list|)
decl_stmt|;
comment|// validate sort columns and bucket columns
name|List
argument_list|<
name|String
argument_list|>
name|columns
init|=
name|Utilities
operator|.
name|getColumnNamesFromFieldSchema
argument_list|(
name|tbl
operator|.
name|getCols
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|alterTbl
operator|.
name|isTurnOffSorting
argument_list|()
condition|)
block|{
name|Utilities
operator|.
name|validateColumnNames
argument_list|(
name|columns
argument_list|,
name|alterTbl
operator|.
name|getBucketColumns
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|alterTbl
operator|.
name|getSortColumns
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|Utilities
operator|.
name|validateColumnNames
argument_list|(
name|columns
argument_list|,
name|Utilities
operator|.
name|getColumnNamesFromSortCols
argument_list|(
name|alterTbl
operator|.
name|getSortColumns
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|alterTbl
operator|.
name|isTurnOffSorting
argument_list|()
condition|)
block|{
name|sd
operator|.
name|setSortCols
argument_list|(
operator|new
name|ArrayList
argument_list|<
name|Order
argument_list|>
argument_list|()
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|alterTbl
operator|.
name|getNumberBuckets
argument_list|()
operator|==
operator|-
literal|1
condition|)
block|{
comment|// -1 buckets means to turn off bucketing
name|sd
operator|.
name|setBucketCols
argument_list|(
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
argument_list|)
expr_stmt|;
name|sd
operator|.
name|setNumBuckets
argument_list|(
operator|-
literal|1
argument_list|)
expr_stmt|;
name|sd
operator|.
name|setSortCols
argument_list|(
operator|new
name|ArrayList
argument_list|<
name|Order
argument_list|>
argument_list|()
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|sd
operator|.
name|setBucketCols
argument_list|(
name|alterTbl
operator|.
name|getBucketColumns
argument_list|()
argument_list|)
expr_stmt|;
name|sd
operator|.
name|setNumBuckets
argument_list|(
name|alterTbl
operator|.
name|getNumberBuckets
argument_list|()
argument_list|)
expr_stmt|;
name|sd
operator|.
name|setSortCols
argument_list|(
name|alterTbl
operator|.
name|getSortColumns
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
elseif|else
if|if
condition|(
name|alterTbl
operator|.
name|getOp
argument_list|()
operator|==
name|AlterTableDesc
operator|.
name|AlterTableTypes
operator|.
name|ALTERLOCATION
condition|)
block|{
name|StorageDescriptor
name|sd
init|=
name|retrieveStorageDescriptor
argument_list|(
name|tbl
argument_list|,
name|part
argument_list|)
decl_stmt|;
name|String
name|newLocation
init|=
name|alterTbl
operator|.
name|getNewLocation
argument_list|()
decl_stmt|;
try|try
block|{
name|URI
name|locUri
init|=
operator|new
name|URI
argument_list|(
name|newLocation
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
operator|new
name|Path
argument_list|(
name|locUri
argument_list|)
operator|.
name|isAbsolute
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|ErrorMsg
operator|.
name|BAD_LOCATION_VALUE
argument_list|,
name|newLocation
argument_list|)
throw|;
block|}
name|sd
operator|.
name|setLocation
argument_list|(
name|newLocation
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|URISyntaxException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
name|environmentContext
operator|.
name|getProperties
argument_list|()
operator|.
name|remove
argument_list|(
name|StatsSetupConst
operator|.
name|DO_NOT_UPDATE_STATS
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|alterTbl
operator|.
name|getOp
argument_list|()
operator|==
name|AlterTableDesc
operator|.
name|AlterTableTypes
operator|.
name|ADDSKEWEDBY
condition|)
block|{
comment|// Validation's been done at compile time. no validation is needed here.
name|List
argument_list|<
name|String
argument_list|>
name|skewedColNames
init|=
literal|null
decl_stmt|;
name|List
argument_list|<
name|List
argument_list|<
name|String
argument_list|>
argument_list|>
name|skewedValues
init|=
literal|null
decl_stmt|;
if|if
condition|(
name|alterTbl
operator|.
name|isTurnOffSkewed
argument_list|()
condition|)
block|{
comment|// Convert skewed table to non-skewed table.
name|skewedColNames
operator|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
expr_stmt|;
name|skewedValues
operator|=
operator|new
name|ArrayList
argument_list|<
name|List
argument_list|<
name|String
argument_list|>
argument_list|>
argument_list|()
expr_stmt|;
block|}
else|else
block|{
name|skewedColNames
operator|=
name|alterTbl
operator|.
name|getSkewedColNames
argument_list|()
expr_stmt|;
name|skewedValues
operator|=
name|alterTbl
operator|.
name|getSkewedColValues
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
literal|null
operator|==
name|tbl
operator|.
name|getSkewedInfo
argument_list|()
condition|)
block|{
comment|// Convert non-skewed table to skewed table.
name|SkewedInfo
name|skewedInfo
init|=
operator|new
name|SkewedInfo
argument_list|()
decl_stmt|;
name|skewedInfo
operator|.
name|setSkewedColNames
argument_list|(
name|skewedColNames
argument_list|)
expr_stmt|;
name|skewedInfo
operator|.
name|setSkewedColValues
argument_list|(
name|skewedValues
argument_list|)
expr_stmt|;
name|tbl
operator|.
name|setSkewedInfo
argument_list|(
name|skewedInfo
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|tbl
operator|.
name|setSkewedColNames
argument_list|(
name|skewedColNames
argument_list|)
expr_stmt|;
name|tbl
operator|.
name|setSkewedColValues
argument_list|(
name|skewedValues
argument_list|)
expr_stmt|;
block|}
name|tbl
operator|.
name|setStoredAsSubDirectories
argument_list|(
name|alterTbl
operator|.
name|isStoredAsSubDirectories
argument_list|()
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|alterTbl
operator|.
name|getOp
argument_list|()
operator|==
name|AlterTableDesc
operator|.
name|AlterTableTypes
operator|.
name|OWNER
condition|)
block|{
if|if
condition|(
name|alterTbl
operator|.
name|getOwnerPrincipal
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|tbl
operator|.
name|setOwner
argument_list|(
name|alterTbl
operator|.
name|getOwnerPrincipal
argument_list|()
operator|.
name|getName
argument_list|()
argument_list|)
expr_stmt|;
name|tbl
operator|.
name|setOwnerType
argument_list|(
name|alterTbl
operator|.
name|getOwnerPrincipal
argument_list|()
operator|.
name|getType
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
elseif|else
if|if
condition|(
name|alterTbl
operator|.
name|getOp
argument_list|()
operator|==
name|AlterTableDesc
operator|.
name|AlterTableTypes
operator|.
name|ALTERSKEWEDLOCATION
condition|)
block|{
comment|// process location one-by-one
name|Map
argument_list|<
name|List
argument_list|<
name|String
argument_list|>
argument_list|,
name|String
argument_list|>
name|locMaps
init|=
name|alterTbl
operator|.
name|getSkewedLocations
argument_list|()
decl_stmt|;
name|Set
argument_list|<
name|List
argument_list|<
name|String
argument_list|>
argument_list|>
name|keys
init|=
name|locMaps
operator|.
name|keySet
argument_list|()
decl_stmt|;
for|for
control|(
name|List
argument_list|<
name|String
argument_list|>
name|key
range|:
name|keys
control|)
block|{
name|String
name|newLocation
init|=
name|locMaps
operator|.
name|get
argument_list|(
name|key
argument_list|)
decl_stmt|;
try|try
block|{
name|URI
name|locUri
init|=
operator|new
name|URI
argument_list|(
name|newLocation
argument_list|)
decl_stmt|;
if|if
condition|(
name|part
operator|!=
literal|null
condition|)
block|{
name|List
argument_list|<
name|String
argument_list|>
name|slk
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|(
name|key
argument_list|)
decl_stmt|;
name|part
operator|.
name|setSkewedValueLocationMap
argument_list|(
name|slk
argument_list|,
name|locUri
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|List
argument_list|<
name|String
argument_list|>
name|slk
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|(
name|key
argument_list|)
decl_stmt|;
name|tbl
operator|.
name|setSkewedValueLocationMap
argument_list|(
name|slk
argument_list|,
name|locUri
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|URISyntaxException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
name|environmentContext
operator|.
name|getProperties
argument_list|()
operator|.
name|remove
argument_list|(
name|StatsSetupConst
operator|.
name|DO_NOT_UPDATE_STATS
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|alterTbl
operator|.
name|getOp
argument_list|()
operator|==
name|AlterTableTypes
operator|.
name|ALTERBUCKETNUM
condition|)
block|{
if|if
condition|(
name|part
operator|!=
literal|null
condition|)
block|{
if|if
condition|(
name|part
operator|.
name|getBucketCount
argument_list|()
operator|==
name|alterTbl
operator|.
name|getNumberBuckets
argument_list|()
condition|)
block|{
return|return
literal|null
return|;
block|}
name|part
operator|.
name|setBucketCount
argument_list|(
name|alterTbl
operator|.
name|getNumberBuckets
argument_list|()
argument_list|)
expr_stmt|;
block|}
else|else
block|{
if|if
condition|(
name|tbl
operator|.
name|getNumBuckets
argument_list|()
operator|==
name|alterTbl
operator|.
name|getNumberBuckets
argument_list|()
condition|)
block|{
return|return
literal|null
return|;
block|}
name|tbl
operator|.
name|setNumBuckets
argument_list|(
name|alterTbl
operator|.
name|getNumberBuckets
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|ErrorMsg
operator|.
name|UNSUPPORTED_ALTER_TBL_OP
argument_list|,
name|alterTbl
operator|.
name|getOp
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|)
throw|;
block|}
return|return
literal|null
return|;
block|}
specifier|private
name|List
argument_list|<
name|Task
argument_list|<
name|?
argument_list|>
argument_list|>
name|alterTableDropProps
parameter_list|(
name|AlterTableDesc
name|alterTbl
parameter_list|,
name|Table
name|tbl
parameter_list|,
name|Partition
name|part
parameter_list|,
name|EnvironmentContext
name|environmentContext
parameter_list|)
throws|throws
name|HiveException
block|{
if|if
condition|(
name|StatsSetupConst
operator|.
name|USER
operator|.
name|equals
argument_list|(
name|environmentContext
operator|.
name|getProperties
argument_list|()
operator|.
name|get
argument_list|(
name|StatsSetupConst
operator|.
name|STATS_GENERATED
argument_list|)
argument_list|)
condition|)
block|{
comment|// drop a stats parameter, which triggers recompute stats update automatically
name|environmentContext
operator|.
name|getProperties
argument_list|()
operator|.
name|remove
argument_list|(
name|StatsSetupConst
operator|.
name|DO_NOT_UPDATE_STATS
argument_list|)
expr_stmt|;
block|}
name|List
argument_list|<
name|Task
argument_list|<
name|?
argument_list|>
argument_list|>
name|result
init|=
literal|null
decl_stmt|;
if|if
condition|(
name|part
operator|==
literal|null
condition|)
block|{
name|Set
argument_list|<
name|String
argument_list|>
name|removedSet
init|=
name|alterTbl
operator|.
name|getProps
argument_list|()
operator|.
name|keySet
argument_list|()
decl_stmt|;
name|boolean
name|isFromMmTable
init|=
name|AcidUtils
operator|.
name|isInsertOnlyTable
argument_list|(
name|tbl
operator|.
name|getParameters
argument_list|()
argument_list|)
decl_stmt|,
name|isRemoved
init|=
name|AcidUtils
operator|.
name|isRemovedInsertOnlyTable
argument_list|(
name|removedSet
argument_list|)
decl_stmt|;
if|if
condition|(
name|isFromMmTable
operator|&&
name|isRemoved
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Cannot convert an ACID table to non-ACID"
argument_list|)
throw|;
block|}
comment|// Check if external table property being removed
if|if
condition|(
name|removedSet
operator|.
name|contains
argument_list|(
literal|"EXTERNAL"
argument_list|)
operator|&&
name|tbl
operator|.
name|getTableType
argument_list|()
operator|==
name|TableType
operator|.
name|EXTERNAL_TABLE
condition|)
block|{
name|tbl
operator|.
name|setTableType
argument_list|(
name|TableType
operator|.
name|MANAGED_TABLE
argument_list|)
expr_stmt|;
block|}
block|}
name|Iterator
argument_list|<
name|String
argument_list|>
name|keyItr
init|=
name|alterTbl
operator|.
name|getProps
argument_list|()
operator|.
name|keySet
argument_list|()
operator|.
name|iterator
argument_list|()
decl_stmt|;
while|while
condition|(
name|keyItr
operator|.
name|hasNext
argument_list|()
condition|)
block|{
if|if
condition|(
name|part
operator|!=
literal|null
condition|)
block|{
name|part
operator|.
name|getTPartition
argument_list|()
operator|.
name|getParameters
argument_list|()
operator|.
name|remove
argument_list|(
name|keyItr
operator|.
name|next
argument_list|()
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|tbl
operator|.
name|getTTable
argument_list|()
operator|.
name|getParameters
argument_list|()
operator|.
name|remove
argument_list|(
name|keyItr
operator|.
name|next
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|result
return|;
block|}
specifier|private
name|void
name|checkMmLb
parameter_list|(
name|Table
name|tbl
parameter_list|)
throws|throws
name|HiveException
block|{
if|if
condition|(
operator|!
name|tbl
operator|.
name|isStoredAsSubDirectories
argument_list|()
condition|)
block|{
return|return;
block|}
comment|// TODO [MM gap?]: by design; no-one seems to use LB tables. They will work, but not convert.
comment|//                 It's possible to work around this by re-creating and re-inserting the table.
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Converting list bucketed tables stored as subdirectories "
operator|+
literal|" to MM is not supported. Please re-create a table in the desired format."
argument_list|)
throw|;
block|}
specifier|private
name|void
name|checkMmLb
parameter_list|(
name|Partition
name|part
parameter_list|)
throws|throws
name|HiveException
block|{
if|if
condition|(
operator|!
name|part
operator|.
name|isStoredAsSubDirectories
argument_list|()
condition|)
block|{
return|return;
block|}
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Converting list bucketed tables stored as subdirectories "
operator|+
literal|" to MM is not supported. Please re-create a table in the desired format."
argument_list|)
throw|;
block|}
specifier|private
name|List
argument_list|<
name|Task
argument_list|<
name|?
argument_list|>
argument_list|>
name|generateAddMmTasks
parameter_list|(
name|Table
name|tbl
parameter_list|,
name|Long
name|writeId
parameter_list|)
throws|throws
name|HiveException
block|{
comment|// We will move all the files in the table/partition directories into the first MM
comment|// directory, then commit the first write ID.
name|List
argument_list|<
name|Path
argument_list|>
name|srcs
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|()
decl_stmt|,
name|tgts
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|()
decl_stmt|;
if|if
condition|(
name|writeId
operator|==
literal|null
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Internal error - write ID not set for MM conversion"
argument_list|)
throw|;
block|}
name|int
name|stmtId
init|=
literal|0
decl_stmt|;
name|String
name|mmDir
init|=
name|AcidUtils
operator|.
name|deltaSubdir
argument_list|(
name|writeId
argument_list|,
name|writeId
argument_list|,
name|stmtId
argument_list|)
decl_stmt|;
name|Hive
name|db
init|=
name|getHive
argument_list|()
decl_stmt|;
if|if
condition|(
name|tbl
operator|.
name|getPartitionKeys
argument_list|()
operator|.
name|size
argument_list|()
operator|>
literal|0
condition|)
block|{
name|PartitionIterable
name|parts
init|=
operator|new
name|PartitionIterable
argument_list|(
name|db
argument_list|,
name|tbl
argument_list|,
literal|null
argument_list|,
name|HiveConf
operator|.
name|getIntVar
argument_list|(
name|conf
argument_list|,
name|ConfVars
operator|.
name|METASTORE_BATCH_RETRIEVE_MAX
argument_list|)
argument_list|)
decl_stmt|;
name|Iterator
argument_list|<
name|Partition
argument_list|>
name|partIter
init|=
name|parts
operator|.
name|iterator
argument_list|()
decl_stmt|;
while|while
condition|(
name|partIter
operator|.
name|hasNext
argument_list|()
condition|)
block|{
name|Partition
name|part
init|=
name|partIter
operator|.
name|next
argument_list|()
decl_stmt|;
name|checkMmLb
argument_list|(
name|part
argument_list|)
expr_stmt|;
name|Path
name|src
init|=
name|part
operator|.
name|getDataLocation
argument_list|()
decl_stmt|,
name|tgt
init|=
operator|new
name|Path
argument_list|(
name|src
argument_list|,
name|mmDir
argument_list|)
decl_stmt|;
name|srcs
operator|.
name|add
argument_list|(
name|src
argument_list|)
expr_stmt|;
name|tgts
operator|.
name|add
argument_list|(
name|tgt
argument_list|)
expr_stmt|;
if|if
condition|(
name|Utilities
operator|.
name|FILE_OP_LOGGER
operator|.
name|isTraceEnabled
argument_list|()
condition|)
block|{
name|Utilities
operator|.
name|FILE_OP_LOGGER
operator|.
name|trace
argument_list|(
literal|"Will move "
operator|+
name|src
operator|+
literal|" to "
operator|+
name|tgt
argument_list|)
expr_stmt|;
block|}
block|}
block|}
else|else
block|{
name|checkMmLb
argument_list|(
name|tbl
argument_list|)
expr_stmt|;
name|Path
name|src
init|=
name|tbl
operator|.
name|getDataLocation
argument_list|()
decl_stmt|,
name|tgt
init|=
operator|new
name|Path
argument_list|(
name|src
argument_list|,
name|mmDir
argument_list|)
decl_stmt|;
name|srcs
operator|.
name|add
argument_list|(
name|src
argument_list|)
expr_stmt|;
name|tgts
operator|.
name|add
argument_list|(
name|tgt
argument_list|)
expr_stmt|;
if|if
condition|(
name|Utilities
operator|.
name|FILE_OP_LOGGER
operator|.
name|isTraceEnabled
argument_list|()
condition|)
block|{
name|Utilities
operator|.
name|FILE_OP_LOGGER
operator|.
name|trace
argument_list|(
literal|"Will move "
operator|+
name|src
operator|+
literal|" to "
operator|+
name|tgt
argument_list|)
expr_stmt|;
block|}
block|}
comment|// Don't set inputs and outputs - the locks have already been taken so it's pointless.
name|MoveWork
name|mw
init|=
operator|new
name|MoveWork
argument_list|(
literal|null
argument_list|,
literal|null
argument_list|,
literal|null
argument_list|,
literal|null
argument_list|,
literal|false
argument_list|)
decl_stmt|;
name|mw
operator|.
name|setMultiFilesDesc
argument_list|(
operator|new
name|LoadMultiFilesDesc
argument_list|(
name|srcs
argument_list|,
name|tgts
argument_list|,
literal|true
argument_list|,
literal|null
argument_list|,
literal|null
argument_list|)
argument_list|)
expr_stmt|;
return|return
name|Lists
operator|.
expr|<
name|Task
argument_list|<
name|?
argument_list|>
operator|>
name|newArrayList
argument_list|(
name|TaskFactory
operator|.
name|get
argument_list|(
name|mw
argument_list|)
argument_list|)
return|;
block|}
specifier|private
name|List
argument_list|<
name|Task
argument_list|<
name|?
argument_list|>
argument_list|>
name|alterTableAddProps
parameter_list|(
name|AlterTableDesc
name|alterTbl
parameter_list|,
name|Table
name|tbl
parameter_list|,
name|Partition
name|part
parameter_list|,
name|EnvironmentContext
name|environmentContext
parameter_list|)
throws|throws
name|HiveException
block|{
if|if
condition|(
name|StatsSetupConst
operator|.
name|USER
operator|.
name|equals
argument_list|(
name|environmentContext
operator|.
name|getProperties
argument_list|()
operator|.
name|get
argument_list|(
name|StatsSetupConst
operator|.
name|STATS_GENERATED
argument_list|)
argument_list|)
condition|)
block|{
name|environmentContext
operator|.
name|getProperties
argument_list|()
operator|.
name|remove
argument_list|(
name|StatsSetupConst
operator|.
name|DO_NOT_UPDATE_STATS
argument_list|)
expr_stmt|;
block|}
name|List
argument_list|<
name|Task
argument_list|<
name|?
argument_list|>
argument_list|>
name|result
init|=
literal|null
decl_stmt|;
if|if
condition|(
name|part
operator|!=
literal|null
condition|)
block|{
name|part
operator|.
name|getTPartition
argument_list|()
operator|.
name|getParameters
argument_list|()
operator|.
name|putAll
argument_list|(
name|alterTbl
operator|.
name|getProps
argument_list|()
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|boolean
name|isFromMmTable
init|=
name|AcidUtils
operator|.
name|isInsertOnlyTable
argument_list|(
name|tbl
operator|.
name|getParameters
argument_list|()
argument_list|)
decl_stmt|;
name|Boolean
name|isToMmTable
init|=
name|AcidUtils
operator|.
name|isToInsertOnlyTable
argument_list|(
name|tbl
argument_list|,
name|alterTbl
operator|.
name|getProps
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|isToMmTable
operator|!=
literal|null
condition|)
block|{
if|if
condition|(
operator|!
name|isFromMmTable
operator|&&
name|isToMmTable
condition|)
block|{
if|if
condition|(
operator|!
name|HiveConf
operator|.
name|getBoolVar
argument_list|(
name|conf
argument_list|,
name|ConfVars
operator|.
name|HIVE_MM_ALLOW_ORIGINALS
argument_list|)
condition|)
block|{
name|result
operator|=
name|generateAddMmTasks
argument_list|(
name|tbl
argument_list|,
name|alterTbl
operator|.
name|getWriteId
argument_list|()
argument_list|)
expr_stmt|;
block|}
else|else
block|{
if|if
condition|(
name|tbl
operator|.
name|getPartitionKeys
argument_list|()
operator|.
name|size
argument_list|()
operator|>
literal|0
condition|)
block|{
name|Hive
name|db
init|=
name|getHive
argument_list|()
decl_stmt|;
name|PartitionIterable
name|parts
init|=
operator|new
name|PartitionIterable
argument_list|(
name|db
argument_list|,
name|tbl
argument_list|,
literal|null
argument_list|,
name|HiveConf
operator|.
name|getIntVar
argument_list|(
name|conf
argument_list|,
name|ConfVars
operator|.
name|METASTORE_BATCH_RETRIEVE_MAX
argument_list|)
argument_list|)
decl_stmt|;
name|Iterator
argument_list|<
name|Partition
argument_list|>
name|partIter
init|=
name|parts
operator|.
name|iterator
argument_list|()
decl_stmt|;
while|while
condition|(
name|partIter
operator|.
name|hasNext
argument_list|()
condition|)
block|{
name|Partition
name|part0
init|=
name|partIter
operator|.
name|next
argument_list|()
decl_stmt|;
name|checkMmLb
argument_list|(
name|part0
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
name|checkMmLb
argument_list|(
name|tbl
argument_list|)
expr_stmt|;
block|}
block|}
block|}
elseif|else
if|if
condition|(
name|isFromMmTable
operator|&&
operator|!
name|isToMmTable
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Cannot convert an ACID table to non-ACID"
argument_list|)
throw|;
block|}
block|}
comment|// Converting to/from external table
name|String
name|externalProp
init|=
name|alterTbl
operator|.
name|getProps
argument_list|()
operator|.
name|get
argument_list|(
literal|"EXTERNAL"
argument_list|)
decl_stmt|;
if|if
condition|(
name|externalProp
operator|!=
literal|null
condition|)
block|{
if|if
condition|(
name|Boolean
operator|.
name|parseBoolean
argument_list|(
name|externalProp
argument_list|)
operator|&&
name|tbl
operator|.
name|getTableType
argument_list|()
operator|==
name|TableType
operator|.
name|MANAGED_TABLE
condition|)
block|{
name|tbl
operator|.
name|setTableType
argument_list|(
name|TableType
operator|.
name|EXTERNAL_TABLE
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
operator|!
name|Boolean
operator|.
name|parseBoolean
argument_list|(
name|externalProp
argument_list|)
operator|&&
name|tbl
operator|.
name|getTableType
argument_list|()
operator|==
name|TableType
operator|.
name|EXTERNAL_TABLE
condition|)
block|{
name|tbl
operator|.
name|setTableType
argument_list|(
name|TableType
operator|.
name|MANAGED_TABLE
argument_list|)
expr_stmt|;
block|}
block|}
name|tbl
operator|.
name|getTTable
argument_list|()
operator|.
name|getParameters
argument_list|()
operator|.
name|putAll
argument_list|(
name|alterTbl
operator|.
name|getProps
argument_list|()
argument_list|)
expr_stmt|;
block|}
return|return
name|result
return|;
block|}
comment|/**    * Update last_modified_by and last_modified_time parameters in parameter map.    *    * @param params    *          Parameters.    * @param conf    *          HiveConf of session    */
specifier|private
name|boolean
name|updateModifiedParameters
parameter_list|(
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|params
parameter_list|,
name|HiveConf
name|conf
parameter_list|)
throws|throws
name|HiveException
block|{
name|String
name|user
init|=
literal|null
decl_stmt|;
name|user
operator|=
name|SessionState
operator|.
name|getUserFromAuthenticator
argument_list|()
expr_stmt|;
name|params
operator|.
name|put
argument_list|(
literal|"last_modified_by"
argument_list|,
name|user
argument_list|)
expr_stmt|;
name|params
operator|.
name|put
argument_list|(
literal|"last_modified_time"
argument_list|,
name|Long
operator|.
name|toString
argument_list|(
name|System
operator|.
name|currentTimeMillis
argument_list|()
operator|/
literal|1000
argument_list|)
argument_list|)
expr_stmt|;
return|return
literal|true
return|;
block|}
comment|/**    * Check if the given serde is valid.    */
specifier|public
specifier|static
name|void
name|validateSerDe
parameter_list|(
name|String
name|serdeName
parameter_list|,
name|HiveConf
name|conf
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|Deserializer
name|d
init|=
name|ReflectionUtil
operator|.
name|newInstance
argument_list|(
name|conf
operator|.
name|getClassByName
argument_list|(
name|serdeName
argument_list|)
operator|.
name|asSubclass
argument_list|(
name|Deserializer
operator|.
name|class
argument_list|)
argument_list|,
name|conf
argument_list|)
decl_stmt|;
if|if
condition|(
name|d
operator|!=
literal|null
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Found class for {}"
argument_list|,
name|serdeName
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Cannot validate serde: "
operator|+
name|serdeName
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
annotation|@
name|Override
specifier|public
name|StageType
name|getType
parameter_list|()
block|{
return|return
name|StageType
operator|.
name|DDL
return|;
block|}
annotation|@
name|Override
specifier|public
name|String
name|getName
parameter_list|()
block|{
return|return
literal|"DDL"
return|;
block|}
comment|/**    * Validate if the given table/partition is eligible for update    *    * @param db Database.    * @param tableName Table name of format db.table    * @param partSpec Partition spec for the partition    * @param replicationSpec Replications specification    *    * @return boolean true if allow the operation    * @throws HiveException    */
specifier|private
name|boolean
name|allowOperationInReplicationScope
parameter_list|(
name|Hive
name|db
parameter_list|,
name|String
name|tableName
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|partSpec
parameter_list|,
name|ReplicationSpec
name|replicationSpec
parameter_list|)
throws|throws
name|HiveException
block|{
if|if
condition|(
operator|(
literal|null
operator|==
name|replicationSpec
operator|)
operator|||
operator|(
operator|!
name|replicationSpec
operator|.
name|isInReplicationScope
argument_list|()
operator|)
condition|)
block|{
comment|// Always allow the operation if it is not in replication scope.
return|return
literal|true
return|;
block|}
comment|// If the table/partition exist and is older than the event, then just apply
comment|// the event else noop.
name|Table
name|existingTable
init|=
name|db
operator|.
name|getTable
argument_list|(
name|tableName
argument_list|,
literal|false
argument_list|)
decl_stmt|;
if|if
condition|(
operator|(
name|existingTable
operator|!=
literal|null
operator|)
operator|&&
name|replicationSpec
operator|.
name|allowEventReplacementInto
argument_list|(
name|existingTable
operator|.
name|getParameters
argument_list|()
argument_list|)
condition|)
block|{
comment|// Table exists and is older than the update. Now, need to ensure if update allowed on the
comment|// partition.
if|if
condition|(
name|partSpec
operator|!=
literal|null
condition|)
block|{
name|Partition
name|existingPtn
init|=
name|db
operator|.
name|getPartition
argument_list|(
name|existingTable
argument_list|,
name|partSpec
argument_list|,
literal|false
argument_list|)
decl_stmt|;
return|return
operator|(
operator|(
name|existingPtn
operator|!=
literal|null
operator|)
operator|&&
name|replicationSpec
operator|.
name|allowEventReplacementInto
argument_list|(
name|existingPtn
operator|.
name|getParameters
argument_list|()
argument_list|)
operator|)
return|;
block|}
comment|// Replacement is allowed as the existing table is older than event
return|return
literal|true
return|;
block|}
comment|// The table is missing either due to drop/rename which follows the operation.
comment|// Or the existing table is newer than our update. So, don't allow the update.
return|return
literal|false
return|;
block|}
specifier|private
name|int
name|remFirstIncPendFlag
parameter_list|(
name|Hive
name|hive
parameter_list|,
name|ReplRemoveFirstIncLoadPendFlagDesc
name|desc
parameter_list|)
throws|throws
name|HiveException
throws|,
name|TException
block|{
name|String
name|dbNameOrPattern
init|=
name|desc
operator|.
name|getDatabaseName
argument_list|()
decl_stmt|;
name|String
name|tableNameOrPattern
init|=
name|desc
operator|.
name|getTableName
argument_list|()
decl_stmt|;
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|parameters
decl_stmt|;
comment|// For database level load tableNameOrPattern will be null. Flag is set only in database for db level load.
if|if
condition|(
name|tableNameOrPattern
operator|!=
literal|null
operator|&&
operator|!
name|tableNameOrPattern
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
comment|// For table level load, dbNameOrPattern is db name and not a pattern.
for|for
control|(
name|String
name|tableName
range|:
name|Utils
operator|.
name|matchesTbl
argument_list|(
name|hive
argument_list|,
name|dbNameOrPattern
argument_list|,
name|tableNameOrPattern
argument_list|)
control|)
block|{
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Table
name|tbl
init|=
name|hive
operator|.
name|getMSC
argument_list|()
operator|.
name|getTable
argument_list|(
name|dbNameOrPattern
argument_list|,
name|tableName
argument_list|)
decl_stmt|;
name|parameters
operator|=
name|tbl
operator|.
name|getParameters
argument_list|()
expr_stmt|;
name|String
name|incPendPara
init|=
name|parameters
operator|!=
literal|null
condition|?
name|parameters
operator|.
name|get
argument_list|(
name|ReplUtils
operator|.
name|REPL_FIRST_INC_PENDING_FLAG
argument_list|)
else|:
literal|null
decl_stmt|;
if|if
condition|(
name|incPendPara
operator|!=
literal|null
condition|)
block|{
name|parameters
operator|.
name|remove
argument_list|(
name|ReplUtils
operator|.
name|REPL_FIRST_INC_PENDING_FLAG
argument_list|)
expr_stmt|;
name|hive
operator|.
name|getMSC
argument_list|()
operator|.
name|alter_table
argument_list|(
name|dbNameOrPattern
argument_list|,
name|tableName
argument_list|,
name|tbl
argument_list|)
expr_stmt|;
block|}
block|}
block|}
else|else
block|{
for|for
control|(
name|String
name|dbName
range|:
name|Utils
operator|.
name|matchesDb
argument_list|(
name|hive
argument_list|,
name|dbNameOrPattern
argument_list|)
control|)
block|{
name|Database
name|database
init|=
name|hive
operator|.
name|getMSC
argument_list|()
operator|.
name|getDatabase
argument_list|(
name|dbName
argument_list|)
decl_stmt|;
name|parameters
operator|=
name|database
operator|.
name|getParameters
argument_list|()
expr_stmt|;
name|String
name|incPendPara
init|=
name|parameters
operator|!=
literal|null
condition|?
name|parameters
operator|.
name|get
argument_list|(
name|ReplUtils
operator|.
name|REPL_FIRST_INC_PENDING_FLAG
argument_list|)
else|:
literal|null
decl_stmt|;
if|if
condition|(
name|incPendPara
operator|!=
literal|null
condition|)
block|{
name|parameters
operator|.
name|remove
argument_list|(
name|ReplUtils
operator|.
name|REPL_FIRST_INC_PENDING_FLAG
argument_list|)
expr_stmt|;
name|hive
operator|.
name|getMSC
argument_list|()
operator|.
name|alterDatabase
argument_list|(
name|dbName
argument_list|,
name|database
argument_list|)
expr_stmt|;
block|}
block|}
block|}
return|return
literal|0
return|;
block|}
comment|/*   uses the authorizer from SessionState will need some more work to get this to run in parallel,   however this should not be a bottle neck so might not need to parallelize this.    */
annotation|@
name|Override
specifier|public
name|boolean
name|canExecuteInParallel
parameter_list|()
block|{
return|return
literal|false
return|;
block|}
block|}
end_class

end_unit

