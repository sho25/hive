begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/*  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
package|;
end_package

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|DataInput
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|DataOutput
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|Serializable
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Comparator
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashSet
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Iterator
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
operator|.
name|Entry
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Set
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|ConcurrentHashMap
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|FileUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|StringInternUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|ValidTxnWriteIdList
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|ValidWriteIdList
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|SerializationUtilities
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hive
operator|.
name|common
operator|.
name|util
operator|.
name|HiveStringUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hive
operator|.
name|common
operator|.
name|util
operator|.
name|Ref
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|Logger
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|LoggerFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configurable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configuration
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileStatus
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
operator|.
name|ConfVars
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|io
operator|.
name|HiveIOExceptionHandlerUtil
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|llap
operator|.
name|io
operator|.
name|api
operator|.
name|LlapIo
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|llap
operator|.
name|io
operator|.
name|api
operator|.
name|LlapProxy
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|Operator
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|TableScanOperator
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|Utilities
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|vector
operator|.
name|VectorizedRowBatch
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|log
operator|.
name|PerfLogger
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|HiveException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|HiveStoragePredicateHandler
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|ExprNodeGenericFuncDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|MapWork
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|OperatorDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|PartitionDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|TableDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|TableScanDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|session
operator|.
name|SessionState
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|ColumnProjectionUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|Deserializer
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|Writable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|WritableComparable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|FileInputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|FileSplit
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|InputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|InputSplit
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|JobConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|JobConfigurable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|RecordReader
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|Reporter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|StringUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hive
operator|.
name|common
operator|.
name|util
operator|.
name|ReflectionUtil
import|;
end_import

begin_comment
comment|/**  * HiveInputFormat is a parameterized InputFormat which looks at the path name  * and determine the correct InputFormat for that path name from  * mapredPlan.pathToPartitionInfo(). It can be used to read files with different  * input format in the same map-reduce job.  */
end_comment

begin_class
specifier|public
class|class
name|HiveInputFormat
parameter_list|<
name|K
extends|extends
name|WritableComparable
parameter_list|,
name|V
extends|extends
name|Writable
parameter_list|>
implements|implements
name|InputFormat
argument_list|<
name|K
argument_list|,
name|V
argument_list|>
implements|,
name|JobConfigurable
block|{
specifier|private
specifier|static
specifier|final
name|String
name|CLASS_NAME
init|=
name|HiveInputFormat
operator|.
name|class
operator|.
name|getName
argument_list|()
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|Logger
name|LOG
init|=
name|LoggerFactory
operator|.
name|getLogger
argument_list|(
name|CLASS_NAME
argument_list|)
decl_stmt|;
comment|/**    * A cache of InputFormat instances.    */
specifier|private
specifier|static
specifier|final
name|Map
argument_list|<
name|Class
argument_list|,
name|InputFormat
argument_list|<
name|WritableComparable
argument_list|,
name|Writable
argument_list|>
argument_list|>
name|inputFormats
init|=
operator|new
name|ConcurrentHashMap
argument_list|<
name|Class
argument_list|,
name|InputFormat
argument_list|<
name|WritableComparable
argument_list|,
name|Writable
argument_list|>
argument_list|>
argument_list|()
decl_stmt|;
specifier|private
name|JobConf
name|job
decl_stmt|;
comment|// both classes access by subclasses
specifier|protected
name|Map
argument_list|<
name|Path
argument_list|,
name|PartitionDesc
argument_list|>
name|pathToPartitionInfo
decl_stmt|;
specifier|protected
name|MapWork
name|mrwork
decl_stmt|;
specifier|public
specifier|static
specifier|final
class|class
name|HiveInputSplitComparator
implements|implements
name|Comparator
argument_list|<
name|HiveInputSplit
argument_list|>
block|{
annotation|@
name|Override
specifier|public
name|int
name|compare
parameter_list|(
name|HiveInputSplit
name|o1
parameter_list|,
name|HiveInputSplit
name|o2
parameter_list|)
block|{
name|int
name|pathCompare
init|=
name|comparePath
argument_list|(
name|o1
operator|.
name|getPath
argument_list|()
argument_list|,
name|o2
operator|.
name|getPath
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|pathCompare
operator|!=
literal|0
condition|)
block|{
return|return
name|pathCompare
return|;
block|}
return|return
name|Long
operator|.
name|compare
argument_list|(
name|o1
operator|.
name|getStart
argument_list|()
argument_list|,
name|o2
operator|.
name|getStart
argument_list|()
argument_list|)
return|;
block|}
specifier|private
name|int
name|comparePath
parameter_list|(
name|Path
name|p1
parameter_list|,
name|Path
name|p2
parameter_list|)
block|{
return|return
name|p1
operator|.
name|compareTo
argument_list|(
name|p2
argument_list|)
return|;
block|}
block|}
comment|/**    * HiveInputSplit encapsulates an InputSplit with its corresponding    * inputFormatClass. The reason that it derives from FileSplit is to make sure    * "map.input.file" in MapTask.    */
specifier|public
specifier|static
class|class
name|HiveInputSplit
extends|extends
name|FileSplit
implements|implements
name|InputSplit
implements|,
name|Configurable
block|{
name|InputSplit
name|inputSplit
decl_stmt|;
name|String
name|inputFormatClassName
decl_stmt|;
specifier|public
name|HiveInputSplit
parameter_list|()
block|{
comment|// This is the only public constructor of FileSplit
name|super
argument_list|(
operator|(
name|Path
operator|)
literal|null
argument_list|,
literal|0
argument_list|,
literal|0
argument_list|,
operator|(
name|String
index|[]
operator|)
literal|null
argument_list|)
expr_stmt|;
block|}
specifier|public
name|HiveInputSplit
parameter_list|(
name|InputSplit
name|inputSplit
parameter_list|,
name|String
name|inputFormatClassName
parameter_list|)
block|{
comment|// This is the only public constructor of FileSplit
name|super
argument_list|(
operator|(
name|Path
operator|)
literal|null
argument_list|,
literal|0
argument_list|,
literal|0
argument_list|,
operator|(
name|String
index|[]
operator|)
literal|null
argument_list|)
expr_stmt|;
name|this
operator|.
name|inputSplit
operator|=
name|inputSplit
expr_stmt|;
name|this
operator|.
name|inputFormatClassName
operator|=
name|inputFormatClassName
expr_stmt|;
block|}
specifier|public
name|InputSplit
name|getInputSplit
parameter_list|()
block|{
return|return
name|inputSplit
return|;
block|}
specifier|public
name|String
name|inputFormatClassName
parameter_list|()
block|{
return|return
name|inputFormatClassName
return|;
block|}
annotation|@
name|Override
specifier|public
name|Path
name|getPath
parameter_list|()
block|{
if|if
condition|(
name|inputSplit
operator|instanceof
name|FileSplit
condition|)
block|{
return|return
operator|(
operator|(
name|FileSplit
operator|)
name|inputSplit
operator|)
operator|.
name|getPath
argument_list|()
return|;
block|}
throw|throw
operator|new
name|RuntimeException
argument_list|(
name|inputSplit
operator|+
literal|" is not a FileSplit"
argument_list|)
throw|;
block|}
comment|/** The position of the first byte in the file to process. */
annotation|@
name|Override
specifier|public
name|long
name|getStart
parameter_list|()
block|{
if|if
condition|(
name|inputSplit
operator|instanceof
name|FileSplit
condition|)
block|{
return|return
operator|(
operator|(
name|FileSplit
operator|)
name|inputSplit
operator|)
operator|.
name|getStart
argument_list|()
return|;
block|}
return|return
literal|0
return|;
block|}
annotation|@
name|Override
specifier|public
name|String
name|toString
parameter_list|()
block|{
return|return
name|inputFormatClassName
operator|+
literal|":"
operator|+
name|inputSplit
operator|.
name|toString
argument_list|()
return|;
block|}
annotation|@
name|Override
specifier|public
name|long
name|getLength
parameter_list|()
block|{
name|long
name|r
init|=
literal|0
decl_stmt|;
try|try
block|{
name|r
operator|=
name|inputSplit
operator|.
name|getLength
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
name|e
argument_list|)
throw|;
block|}
return|return
name|r
return|;
block|}
annotation|@
name|Override
specifier|public
name|String
index|[]
name|getLocations
parameter_list|()
throws|throws
name|IOException
block|{
return|return
name|inputSplit
operator|.
name|getLocations
argument_list|()
return|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|readFields
parameter_list|(
name|DataInput
name|in
parameter_list|)
throws|throws
name|IOException
block|{
name|String
name|inputSplitClassName
init|=
name|in
operator|.
name|readUTF
argument_list|()
decl_stmt|;
try|try
block|{
name|inputSplit
operator|=
operator|(
name|InputSplit
operator|)
name|ReflectionUtil
operator|.
name|newInstance
argument_list|(
name|conf
operator|.
name|getClassByName
argument_list|(
name|inputSplitClassName
argument_list|)
argument_list|,
name|conf
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Cannot create an instance of InputSplit class = "
operator|+
name|inputSplitClassName
operator|+
literal|":"
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
name|inputSplit
operator|.
name|readFields
argument_list|(
name|in
argument_list|)
expr_stmt|;
name|inputFormatClassName
operator|=
name|in
operator|.
name|readUTF
argument_list|()
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|write
parameter_list|(
name|DataOutput
name|out
parameter_list|)
throws|throws
name|IOException
block|{
name|out
operator|.
name|writeUTF
argument_list|(
name|inputSplit
operator|.
name|getClass
argument_list|()
operator|.
name|getName
argument_list|()
argument_list|)
expr_stmt|;
name|inputSplit
operator|.
name|write
argument_list|(
name|out
argument_list|)
expr_stmt|;
name|out
operator|.
name|writeUTF
argument_list|(
name|inputFormatClassName
argument_list|)
expr_stmt|;
block|}
name|Configuration
name|conf
decl_stmt|;
annotation|@
name|Override
specifier|public
name|Configuration
name|getConf
parameter_list|()
block|{
return|return
name|conf
return|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|setConf
parameter_list|(
name|Configuration
name|conf
parameter_list|)
block|{
name|this
operator|.
name|conf
operator|=
name|conf
expr_stmt|;
block|}
block|}
annotation|@
name|Override
specifier|public
name|void
name|configure
parameter_list|(
name|JobConf
name|job
parameter_list|)
block|{
name|this
operator|.
name|job
operator|=
name|job
expr_stmt|;
block|}
specifier|public
specifier|static
name|InputFormat
argument_list|<
name|WritableComparable
argument_list|,
name|Writable
argument_list|>
name|wrapForLlap
parameter_list|(
name|InputFormat
argument_list|<
name|WritableComparable
argument_list|,
name|Writable
argument_list|>
name|inputFormat
parameter_list|,
name|Configuration
name|conf
parameter_list|,
name|PartitionDesc
name|part
parameter_list|)
throws|throws
name|HiveException
block|{
if|if
condition|(
operator|!
name|HiveConf
operator|.
name|getBoolVar
argument_list|(
name|conf
argument_list|,
name|ConfVars
operator|.
name|LLAP_IO_ENABLED
argument_list|,
name|LlapProxy
operator|.
name|isDaemon
argument_list|()
argument_list|)
condition|)
block|{
return|return
name|inputFormat
return|;
comment|// LLAP not enabled, no-op.
block|}
name|String
name|ifName
init|=
name|inputFormat
operator|.
name|getClass
argument_list|()
operator|.
name|getCanonicalName
argument_list|()
decl_stmt|;
name|boolean
name|isSupported
init|=
name|inputFormat
operator|instanceof
name|LlapWrappableInputFormatInterface
decl_stmt|;
name|boolean
name|isCacheOnly
init|=
name|inputFormat
operator|instanceof
name|LlapCacheOnlyInputFormatInterface
decl_stmt|;
name|boolean
name|isVectorized
init|=
name|Utilities
operator|.
name|getIsVectorized
argument_list|(
name|conf
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|isVectorized
condition|)
block|{
comment|// Pretend it's vectorized if the non-vector wrapped is enabled.
name|isVectorized
operator|=
name|HiveConf
operator|.
name|getBoolVar
argument_list|(
name|conf
argument_list|,
name|ConfVars
operator|.
name|LLAP_IO_NONVECTOR_WRAPPER_ENABLED
argument_list|)
operator|&&
operator|(
name|Utilities
operator|.
name|getPlanPath
argument_list|(
name|conf
argument_list|)
operator|!=
literal|null
operator|)
expr_stmt|;
block|}
name|boolean
name|isSerdeBased
init|=
literal|false
decl_stmt|;
if|if
condition|(
name|isVectorized
operator|&&
operator|!
name|isSupported
operator|&&
name|HiveConf
operator|.
name|getBoolVar
argument_list|(
name|conf
argument_list|,
name|ConfVars
operator|.
name|LLAP_IO_ENCODE_ENABLED
argument_list|)
condition|)
block|{
comment|// See if we can use re-encoding to read the format thru IO elevator.
name|isSupported
operator|=
name|isSerdeBased
operator|=
name|checkInputFormatForLlapEncode
argument_list|(
name|conf
argument_list|,
name|ifName
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
operator|(
operator|!
name|isSupported
operator|||
operator|!
name|isVectorized
operator|)
operator|&&
operator|!
name|isCacheOnly
condition|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isInfoEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Not using llap for "
operator|+
name|ifName
operator|+
literal|": supported = "
operator|+
name|isSupported
operator|+
literal|", vectorized = "
operator|+
name|isVectorized
operator|+
literal|", cache only = "
operator|+
name|isCacheOnly
argument_list|)
expr_stmt|;
block|}
return|return
name|inputFormat
return|;
block|}
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Processing "
operator|+
name|ifName
argument_list|)
expr_stmt|;
block|}
annotation|@
name|SuppressWarnings
argument_list|(
literal|"unchecked"
argument_list|)
name|LlapIo
argument_list|<
name|VectorizedRowBatch
argument_list|>
name|llapIo
init|=
name|LlapProxy
operator|.
name|getIo
argument_list|()
decl_stmt|;
if|if
condition|(
name|llapIo
operator|==
literal|null
condition|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isInfoEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Not using LLAP IO because it is not initialized"
argument_list|)
expr_stmt|;
block|}
return|return
name|inputFormat
return|;
block|}
name|Deserializer
name|serde
init|=
literal|null
decl_stmt|;
if|if
condition|(
name|isSerdeBased
condition|)
block|{
if|if
condition|(
name|part
operator|==
literal|null
condition|)
block|{
if|if
condition|(
name|isCacheOnly
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Using cache only because there's no partition spec for SerDe-based IF"
argument_list|)
expr_stmt|;
name|injectLlapCaches
argument_list|(
name|inputFormat
argument_list|,
name|llapIo
argument_list|,
name|conf
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Not using LLAP IO because there's no partition spec for SerDe-based IF"
argument_list|)
expr_stmt|;
block|}
return|return
name|inputFormat
return|;
block|}
try|try
block|{
name|serde
operator|=
name|part
operator|.
name|getDeserializer
argument_list|(
name|conf
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Error creating SerDe for LLAP IO"
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
if|if
condition|(
name|isSupported
operator|&&
name|isVectorized
condition|)
block|{
name|InputFormat
argument_list|<
name|?
argument_list|,
name|?
argument_list|>
name|wrappedIf
init|=
name|llapIo
operator|.
name|getInputFormat
argument_list|(
name|inputFormat
argument_list|,
name|serde
argument_list|)
decl_stmt|;
comment|// null means we cannot wrap; the cause is logged inside.
if|if
condition|(
name|wrappedIf
operator|!=
literal|null
condition|)
block|{
return|return
name|castInputFormat
argument_list|(
name|wrappedIf
argument_list|)
return|;
block|}
block|}
if|if
condition|(
name|isCacheOnly
condition|)
block|{
name|injectLlapCaches
argument_list|(
name|inputFormat
argument_list|,
name|llapIo
argument_list|,
name|conf
argument_list|)
expr_stmt|;
block|}
return|return
name|inputFormat
return|;
block|}
specifier|private
specifier|static
name|boolean
name|checkInputFormatForLlapEncode
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|String
name|ifName
parameter_list|)
block|{
name|String
name|formatList
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|conf
argument_list|,
name|ConfVars
operator|.
name|LLAP_IO_ENCODE_FORMATS
argument_list|)
decl_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Checking "
operator|+
name|ifName
operator|+
literal|" against "
operator|+
name|formatList
argument_list|)
expr_stmt|;
block|}
name|String
index|[]
name|formats
init|=
name|StringUtils
operator|.
name|getStrings
argument_list|(
name|formatList
argument_list|)
decl_stmt|;
if|if
condition|(
name|formats
operator|!=
literal|null
condition|)
block|{
for|for
control|(
name|String
name|format
range|:
name|formats
control|)
block|{
comment|// TODO: should we check isAssignableFrom?
if|if
condition|(
name|ifName
operator|.
name|equals
argument_list|(
name|format
argument_list|)
condition|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isInfoEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Using SerDe-based LLAP reader for "
operator|+
name|ifName
argument_list|)
expr_stmt|;
block|}
return|return
literal|true
return|;
block|}
block|}
block|}
return|return
literal|false
return|;
block|}
specifier|public
specifier|static
name|void
name|injectLlapCaches
parameter_list|(
name|InputFormat
argument_list|<
name|WritableComparable
argument_list|,
name|Writable
argument_list|>
name|inputFormat
parameter_list|,
name|LlapIo
argument_list|<
name|VectorizedRowBatch
argument_list|>
name|llapIo
parameter_list|,
name|Configuration
name|conf
parameter_list|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Injecting LLAP caches into "
operator|+
name|inputFormat
operator|.
name|getClass
argument_list|()
operator|.
name|getCanonicalName
argument_list|()
argument_list|)
expr_stmt|;
name|conf
operator|.
name|setInt
argument_list|(
literal|"parquet.read.allocation.size"
argument_list|,
literal|1024
operator|*
literal|1024
operator|*
literal|1024
argument_list|)
expr_stmt|;
comment|// Disable buffer splitting for now.
name|llapIo
operator|.
name|initCacheOnlyInputFormat
argument_list|(
name|inputFormat
argument_list|)
expr_stmt|;
block|}
specifier|public
specifier|static
name|boolean
name|canWrapForLlap
parameter_list|(
name|Class
argument_list|<
name|?
extends|extends
name|InputFormat
argument_list|>
name|clazz
parameter_list|,
name|boolean
name|checkVector
parameter_list|)
block|{
return|return
name|LlapWrappableInputFormatInterface
operator|.
name|class
operator|.
name|isAssignableFrom
argument_list|(
name|clazz
argument_list|)
operator|&&
operator|(
operator|!
name|checkVector
operator|||
name|BatchToRowInputFormat
operator|.
name|class
operator|.
name|isAssignableFrom
argument_list|(
name|clazz
argument_list|)
operator|)
return|;
block|}
specifier|public
specifier|static
name|boolean
name|canInjectCaches
parameter_list|(
name|Class
argument_list|<
name|?
extends|extends
name|InputFormat
argument_list|>
name|clazz
parameter_list|)
block|{
return|return
name|LlapCacheOnlyInputFormatInterface
operator|.
name|class
operator|.
name|isAssignableFrom
argument_list|(
name|clazz
argument_list|)
return|;
block|}
annotation|@
name|SuppressWarnings
argument_list|(
literal|"unchecked"
argument_list|)
specifier|private
specifier|static
parameter_list|<
name|T
parameter_list|,
name|U
parameter_list|,
name|V
parameter_list|,
name|W
parameter_list|>
name|InputFormat
argument_list|<
name|T
argument_list|,
name|U
argument_list|>
name|castInputFormat
parameter_list|(
name|InputFormat
argument_list|<
name|V
argument_list|,
name|W
argument_list|>
name|from
parameter_list|)
block|{
comment|// This is ugly in two ways...
comment|// 1) We assume that LlapWrappableInputFormatInterface has NullWritable as first parameter.
comment|//    Since we are using Java and not, say, a programming language, there's no way to check.
comment|// 2) We ignore the fact that 2nd arg is completely incompatible (VRB -> Writable), because
comment|//    vectorization currently works by magic, getting VRB from IF with non-VRB value param.
comment|// So we just cast blindly and hope for the best (which is obviously what happens).
return|return
operator|(
name|InputFormat
argument_list|<
name|T
argument_list|,
name|U
argument_list|>
operator|)
name|from
return|;
block|}
comment|/** NOTE: this no longer wraps the IF for LLAP. Call wrapForLlap manually if needed. */
specifier|public
specifier|static
name|InputFormat
argument_list|<
name|WritableComparable
argument_list|,
name|Writable
argument_list|>
name|getInputFormatFromCache
parameter_list|(
name|Class
name|inputFormatClass
parameter_list|,
name|JobConf
name|job
parameter_list|)
throws|throws
name|IOException
block|{
name|InputFormat
argument_list|<
name|WritableComparable
argument_list|,
name|Writable
argument_list|>
name|instance
init|=
name|inputFormats
operator|.
name|get
argument_list|(
name|inputFormatClass
argument_list|)
decl_stmt|;
if|if
condition|(
name|instance
operator|==
literal|null
condition|)
block|{
try|try
block|{
name|instance
operator|=
operator|(
name|InputFormat
argument_list|<
name|WritableComparable
argument_list|,
name|Writable
argument_list|>
operator|)
name|ReflectionUtil
operator|.
name|newInstance
argument_list|(
name|inputFormatClass
argument_list|,
name|job
argument_list|)
expr_stmt|;
comment|// HBase input formats are not thread safe today. See HIVE-8808.
name|String
name|inputFormatName
init|=
name|inputFormatClass
operator|.
name|getName
argument_list|()
operator|.
name|toLowerCase
argument_list|()
decl_stmt|;
if|if
condition|(
operator|!
name|inputFormatName
operator|.
name|contains
argument_list|(
literal|"hbase"
argument_list|)
condition|)
block|{
name|inputFormats
operator|.
name|put
argument_list|(
name|inputFormatClass
argument_list|,
name|instance
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Cannot create an instance of InputFormat class "
operator|+
name|inputFormatClass
operator|.
name|getName
argument_list|()
operator|+
literal|" as specified in mapredWork!"
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
return|return
name|instance
return|;
block|}
annotation|@
name|Override
specifier|public
name|RecordReader
name|getRecordReader
parameter_list|(
name|InputSplit
name|split
parameter_list|,
name|JobConf
name|job
parameter_list|,
name|Reporter
name|reporter
parameter_list|)
throws|throws
name|IOException
block|{
name|HiveInputSplit
name|hsplit
init|=
operator|(
name|HiveInputSplit
operator|)
name|split
decl_stmt|;
name|InputSplit
name|inputSplit
init|=
name|hsplit
operator|.
name|getInputSplit
argument_list|()
decl_stmt|;
name|String
name|inputFormatClassName
init|=
literal|null
decl_stmt|;
name|Class
name|inputFormatClass
init|=
literal|null
decl_stmt|;
try|try
block|{
name|inputFormatClassName
operator|=
name|hsplit
operator|.
name|inputFormatClassName
argument_list|()
expr_stmt|;
name|inputFormatClass
operator|=
name|job
operator|.
name|getClassByName
argument_list|(
name|inputFormatClassName
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"cannot find class "
operator|+
name|inputFormatClassName
argument_list|,
name|e
argument_list|)
throw|;
block|}
if|if
condition|(
name|this
operator|.
name|mrwork
operator|==
literal|null
operator|||
name|pathToPartitionInfo
operator|==
literal|null
condition|)
block|{
name|init
argument_list|(
name|job
argument_list|)
expr_stmt|;
block|}
name|boolean
name|nonNative
init|=
literal|false
decl_stmt|;
name|PartitionDesc
name|part
init|=
name|HiveFileFormatUtils
operator|.
name|getFromPathRecursively
argument_list|(
name|pathToPartitionInfo
argument_list|,
name|hsplit
operator|.
name|getPath
argument_list|()
argument_list|,
literal|null
argument_list|)
decl_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Found spec for "
operator|+
name|hsplit
operator|.
name|getPath
argument_list|()
operator|+
literal|" "
operator|+
name|part
operator|+
literal|" from "
operator|+
name|pathToPartitionInfo
argument_list|)
expr_stmt|;
block|}
try|try
block|{
if|if
condition|(
operator|(
name|part
operator|!=
literal|null
operator|)
operator|&&
operator|(
name|part
operator|.
name|getTableDesc
argument_list|()
operator|!=
literal|null
operator|)
condition|)
block|{
name|Utilities
operator|.
name|copyTableJobPropertiesToConf
argument_list|(
name|part
operator|.
name|getTableDesc
argument_list|()
argument_list|,
name|job
argument_list|)
expr_stmt|;
name|nonNative
operator|=
name|part
operator|.
name|getTableDesc
argument_list|()
operator|.
name|isNonNative
argument_list|()
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|HiveException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
name|e
argument_list|)
throw|;
block|}
name|Path
name|splitPath
init|=
name|hsplit
operator|.
name|getPath
argument_list|()
decl_stmt|;
name|pushProjectionsAndFilters
argument_list|(
name|job
argument_list|,
name|inputFormatClass
argument_list|,
name|splitPath
argument_list|,
name|nonNative
argument_list|)
expr_stmt|;
name|InputFormat
name|inputFormat
init|=
name|getInputFormatFromCache
argument_list|(
name|inputFormatClass
argument_list|,
name|job
argument_list|)
decl_stmt|;
try|try
block|{
name|inputFormat
operator|=
name|HiveInputFormat
operator|.
name|wrapForLlap
argument_list|(
name|inputFormat
argument_list|,
name|job
argument_list|,
name|part
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|HiveException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
name|e
argument_list|)
throw|;
block|}
name|RecordReader
name|innerReader
init|=
literal|null
decl_stmt|;
try|try
block|{
name|innerReader
operator|=
name|inputFormat
operator|.
name|getRecordReader
argument_list|(
name|inputSplit
argument_list|,
name|job
argument_list|,
name|reporter
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|innerReader
operator|=
name|HiveIOExceptionHandlerUtil
operator|.
name|handleRecordReaderCreationException
argument_list|(
name|e
argument_list|,
name|job
argument_list|)
expr_stmt|;
block|}
name|HiveRecordReader
argument_list|<
name|K
argument_list|,
name|V
argument_list|>
name|rr
init|=
operator|new
name|HiveRecordReader
argument_list|(
name|innerReader
argument_list|,
name|job
argument_list|)
decl_stmt|;
name|rr
operator|.
name|initIOContext
argument_list|(
name|hsplit
argument_list|,
name|job
argument_list|,
name|inputFormatClass
argument_list|,
name|innerReader
argument_list|)
expr_stmt|;
return|return
name|rr
return|;
block|}
specifier|protected
name|void
name|init
parameter_list|(
name|JobConf
name|job
parameter_list|)
block|{
if|if
condition|(
name|mrwork
operator|==
literal|null
operator|||
name|pathToPartitionInfo
operator|==
literal|null
condition|)
block|{
if|if
condition|(
name|HiveConf
operator|.
name|getVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_EXECUTION_ENGINE
argument_list|)
operator|.
name|equals
argument_list|(
literal|"tez"
argument_list|)
condition|)
block|{
name|mrwork
operator|=
operator|(
name|MapWork
operator|)
name|Utilities
operator|.
name|getMergeWork
argument_list|(
name|job
argument_list|)
expr_stmt|;
if|if
condition|(
name|mrwork
operator|==
literal|null
condition|)
block|{
name|mrwork
operator|=
name|Utilities
operator|.
name|getMapWork
argument_list|(
name|job
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
name|mrwork
operator|=
name|Utilities
operator|.
name|getMapWork
argument_list|(
name|job
argument_list|)
expr_stmt|;
block|}
name|pathToPartitionInfo
operator|=
name|mrwork
operator|.
name|getPathToPartitionInfo
argument_list|()
expr_stmt|;
block|}
block|}
comment|/*    * AddSplitsForGroup collects separate calls to setInputPaths into one where possible.    * The reason for this is that this is faster on some InputFormats. E.g.: Orc will start    * a threadpool to do the work and calling it multiple times unnecessarily will create a lot    * of unnecessary thread pools.    */
specifier|private
name|void
name|addSplitsForGroup
parameter_list|(
name|List
argument_list|<
name|Path
argument_list|>
name|dirs
parameter_list|,
name|TableScanOperator
name|tableScan
parameter_list|,
name|JobConf
name|conf
parameter_list|,
name|InputFormat
name|inputFormat
parameter_list|,
name|Class
argument_list|<
name|?
extends|extends
name|InputFormat
argument_list|>
name|inputFormatClass
parameter_list|,
name|int
name|splits
parameter_list|,
name|TableDesc
name|table
parameter_list|,
name|List
argument_list|<
name|InputSplit
argument_list|>
name|result
parameter_list|)
throws|throws
name|IOException
block|{
name|String
name|tableName
init|=
name|table
operator|.
name|getTableName
argument_list|()
decl_stmt|;
name|ValidWriteIdList
name|validWriteIdList
init|=
name|AcidUtils
operator|.
name|getTableValidWriteIdList
argument_list|(
name|conf
argument_list|,
name|tableName
operator|==
literal|null
condition|?
literal|null
else|:
name|HiveStringUtils
operator|.
name|normalizeIdentifier
argument_list|(
name|tableName
argument_list|)
argument_list|)
decl_stmt|;
name|ValidWriteIdList
name|validMmWriteIdList
init|=
name|getMmValidWriteIds
argument_list|(
name|conf
argument_list|,
name|table
argument_list|,
name|validWriteIdList
argument_list|)
decl_stmt|;
try|try
block|{
name|Utilities
operator|.
name|copyJobSecretToTableProperties
argument_list|(
name|table
argument_list|)
expr_stmt|;
name|Utilities
operator|.
name|copyTablePropertiesToConf
argument_list|(
name|table
argument_list|,
name|conf
argument_list|)
expr_stmt|;
if|if
condition|(
name|tableScan
operator|!=
literal|null
condition|)
block|{
name|AcidUtils
operator|.
name|setAcidOperationalProperties
argument_list|(
name|conf
argument_list|,
name|tableScan
operator|.
name|getConf
argument_list|()
operator|.
name|isTranscationalTable
argument_list|()
argument_list|,
name|tableScan
operator|.
name|getConf
argument_list|()
operator|.
name|getAcidOperationalProperties
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|tableScan
operator|.
name|getConf
argument_list|()
operator|.
name|isTranscationalTable
argument_list|()
operator|&&
operator|(
name|validWriteIdList
operator|==
literal|null
operator|)
condition|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Acid table: "
operator|+
name|table
operator|.
name|getTableName
argument_list|()
operator|+
literal|" is missing from the ValidWriteIdList config: "
operator|+
name|conf
operator|.
name|get
argument_list|(
name|ValidTxnWriteIdList
operator|.
name|VALID_TABLES_WRITEIDS_KEY
argument_list|)
argument_list|)
throw|;
block|}
if|if
condition|(
name|validWriteIdList
operator|!=
literal|null
condition|)
block|{
name|AcidUtils
operator|.
name|setValidWriteIdList
argument_list|(
name|conf
argument_list|,
name|validWriteIdList
argument_list|)
expr_stmt|;
block|}
block|}
block|}
catch|catch
parameter_list|(
name|HiveException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
name|e
argument_list|)
throw|;
block|}
if|if
condition|(
name|tableScan
operator|!=
literal|null
condition|)
block|{
name|pushFilters
argument_list|(
name|conf
argument_list|,
name|tableScan
argument_list|,
name|this
operator|.
name|mrwork
argument_list|)
expr_stmt|;
block|}
name|List
argument_list|<
name|Path
argument_list|>
name|dirsWithFileOriginals
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|()
decl_stmt|,
name|finalDirs
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|()
decl_stmt|;
name|processPathsForMmRead
argument_list|(
name|dirs
argument_list|,
name|conf
argument_list|,
name|validMmWriteIdList
argument_list|,
name|finalDirs
argument_list|,
name|dirsWithFileOriginals
argument_list|)
expr_stmt|;
if|if
condition|(
name|finalDirs
operator|.
name|isEmpty
argument_list|()
operator|&&
name|dirsWithFileOriginals
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
comment|// This is for transactional tables.
if|if
condition|(
operator|!
name|conf
operator|.
name|getBoolean
argument_list|(
name|Utilities
operator|.
name|ENSURE_OPERATORS_EXECUTED
argument_list|,
literal|false
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"No valid inputs found in "
operator|+
name|dirs
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|validMmWriteIdList
operator|!=
literal|null
condition|)
block|{
comment|// AcidUtils.getAcidState() is already called to verify there is no input split.
comment|// Thus for a GroupByOperator summary row, set finalDirs and add a Dummy split here.
name|result
operator|.
name|add
argument_list|(
operator|new
name|HiveInputSplit
argument_list|(
operator|new
name|NullRowsInputFormat
operator|.
name|DummyInputSplit
argument_list|(
name|dirs
operator|.
name|get
argument_list|(
literal|0
argument_list|)
operator|.
name|toString
argument_list|()
argument_list|)
argument_list|,
name|ZeroRowsInputFormat
operator|.
name|class
operator|.
name|getName
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
return|return;
comment|// No valid inputs.
block|}
name|conf
operator|.
name|setInputFormat
argument_list|(
name|inputFormat
operator|.
name|getClass
argument_list|()
argument_list|)
expr_stmt|;
name|int
name|headerCount
init|=
literal|0
decl_stmt|;
name|int
name|footerCount
init|=
literal|0
decl_stmt|;
if|if
condition|(
name|table
operator|!=
literal|null
condition|)
block|{
name|headerCount
operator|=
name|Utilities
operator|.
name|getHeaderCount
argument_list|(
name|table
argument_list|)
expr_stmt|;
name|footerCount
operator|=
name|Utilities
operator|.
name|getFooterCount
argument_list|(
name|table
argument_list|,
name|conf
argument_list|)
expr_stmt|;
if|if
condition|(
name|headerCount
operator|!=
literal|0
operator|||
name|footerCount
operator|!=
literal|0
condition|)
block|{
comment|// Input file has header or footer, cannot be splitted.
name|HiveConf
operator|.
name|setLongVar
argument_list|(
name|conf
argument_list|,
name|ConfVars
operator|.
name|MAPREDMINSPLITSIZE
argument_list|,
name|Long
operator|.
name|MAX_VALUE
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
operator|!
name|finalDirs
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|FileInputFormat
operator|.
name|setInputPaths
argument_list|(
name|conf
argument_list|,
name|finalDirs
operator|.
name|toArray
argument_list|(
operator|new
name|Path
index|[
name|finalDirs
operator|.
name|size
argument_list|()
index|]
argument_list|)
argument_list|)
expr_stmt|;
name|InputSplit
index|[]
name|iss
init|=
name|inputFormat
operator|.
name|getSplits
argument_list|(
name|conf
argument_list|,
name|splits
argument_list|)
decl_stmt|;
for|for
control|(
name|InputSplit
name|is
range|:
name|iss
control|)
block|{
name|result
operator|.
name|add
argument_list|(
operator|new
name|HiveInputSplit
argument_list|(
name|is
argument_list|,
name|inputFormatClass
operator|.
name|getName
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
operator|!
name|dirsWithFileOriginals
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
comment|// We are going to add splits for these directories with recursive = false, so we ignore
comment|// any subdirectories (deltas or original directories) and only read the original files.
comment|// The fact that there's a loop calling addSplitsForGroup already implies it's ok to
comment|// the real input format multiple times... however some split concurrency/etc configs
comment|// that are applied separately in each call will effectively be ignored for such splits.
name|JobConf
name|nonRecConf
init|=
name|createConfForMmOriginalsSplit
argument_list|(
name|conf
argument_list|,
name|dirsWithFileOriginals
argument_list|)
decl_stmt|;
name|InputSplit
index|[]
name|iss
init|=
name|inputFormat
operator|.
name|getSplits
argument_list|(
name|nonRecConf
argument_list|,
name|splits
argument_list|)
decl_stmt|;
for|for
control|(
name|InputSplit
name|is
range|:
name|iss
control|)
block|{
name|result
operator|.
name|add
argument_list|(
operator|new
name|HiveInputSplit
argument_list|(
name|is
argument_list|,
name|inputFormatClass
operator|.
name|getName
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
name|result
operator|.
name|isEmpty
argument_list|()
operator|&&
name|conf
operator|.
name|getBoolean
argument_list|(
name|Utilities
operator|.
name|ENSURE_OPERATORS_EXECUTED
argument_list|,
literal|false
argument_list|)
condition|)
block|{
comment|// If there are no inputs; the Execution engine skips the operator tree.
comment|// To prevent it from happening; an opaque  ZeroRows input is added here - when needed.
name|result
operator|.
name|add
argument_list|(
operator|new
name|HiveInputSplit
argument_list|(
operator|new
name|NullRowsInputFormat
operator|.
name|DummyInputSplit
argument_list|(
name|finalDirs
operator|.
name|get
argument_list|(
literal|0
argument_list|)
operator|.
name|toString
argument_list|()
argument_list|)
argument_list|,
name|ZeroRowsInputFormat
operator|.
name|class
operator|.
name|getName
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
specifier|public
specifier|static
name|JobConf
name|createConfForMmOriginalsSplit
parameter_list|(
name|JobConf
name|conf
parameter_list|,
name|List
argument_list|<
name|Path
argument_list|>
name|dirsWithFileOriginals
parameter_list|)
block|{
name|JobConf
name|nonRecConf
init|=
operator|new
name|JobConf
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|FileInputFormat
operator|.
name|setInputPaths
argument_list|(
name|nonRecConf
argument_list|,
name|dirsWithFileOriginals
operator|.
name|toArray
argument_list|(
operator|new
name|Path
index|[
name|dirsWithFileOriginals
operator|.
name|size
argument_list|()
index|]
argument_list|)
argument_list|)
expr_stmt|;
name|nonRecConf
operator|.
name|setBoolean
argument_list|(
name|FileInputFormat
operator|.
name|INPUT_DIR_RECURSIVE
argument_list|,
literal|false
argument_list|)
expr_stmt|;
name|nonRecConf
operator|.
name|setBoolean
argument_list|(
literal|"mapred.input.dir.recursive"
argument_list|,
literal|false
argument_list|)
expr_stmt|;
comment|// TODO: change to FileInputFormat.... field after MAPREDUCE-7086.
name|nonRecConf
operator|.
name|setBoolean
argument_list|(
literal|"mapreduce.input.fileinputformat.input.dir.nonrecursive.ignore.subdirs"
argument_list|,
literal|true
argument_list|)
expr_stmt|;
return|return
name|nonRecConf
return|;
block|}
specifier|protected
name|ValidWriteIdList
name|getMmValidWriteIds
parameter_list|(
name|JobConf
name|conf
parameter_list|,
name|TableDesc
name|table
parameter_list|,
name|ValidWriteIdList
name|validWriteIdList
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
operator|!
name|AcidUtils
operator|.
name|isInsertOnlyTable
argument_list|(
name|table
operator|.
name|getProperties
argument_list|()
argument_list|)
condition|)
block|{
return|return
literal|null
return|;
block|}
if|if
condition|(
name|validWriteIdList
operator|==
literal|null
condition|)
block|{
name|validWriteIdList
operator|=
name|AcidUtils
operator|.
name|getTableValidWriteIdList
argument_list|(
name|conf
argument_list|,
name|table
operator|.
name|getTableName
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|validWriteIdList
operator|==
literal|null
condition|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Insert-Only table: "
operator|+
name|table
operator|.
name|getTableName
argument_list|()
operator|+
literal|" is missing from the ValidWriteIdList config: "
operator|+
name|conf
operator|.
name|get
argument_list|(
name|ValidTxnWriteIdList
operator|.
name|VALID_TABLES_WRITEIDS_KEY
argument_list|)
argument_list|)
throw|;
block|}
block|}
return|return
name|validWriteIdList
return|;
block|}
specifier|public
specifier|static
name|void
name|processPathsForMmRead
parameter_list|(
name|List
argument_list|<
name|Path
argument_list|>
name|dirs
parameter_list|,
name|Configuration
name|conf
parameter_list|,
name|ValidWriteIdList
name|validWriteIdList
parameter_list|,
name|List
argument_list|<
name|Path
argument_list|>
name|finalPaths
parameter_list|,
name|List
argument_list|<
name|Path
argument_list|>
name|pathsWithFileOriginals
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|validWriteIdList
operator|==
literal|null
condition|)
block|{
name|finalPaths
operator|.
name|addAll
argument_list|(
name|dirs
argument_list|)
expr_stmt|;
return|return;
block|}
name|boolean
name|allowOriginals
init|=
name|HiveConf
operator|.
name|getBoolVar
argument_list|(
name|conf
argument_list|,
name|ConfVars
operator|.
name|HIVE_MM_ALLOW_ORIGINALS
argument_list|)
decl_stmt|;
for|for
control|(
name|Path
name|dir
range|:
name|dirs
control|)
block|{
name|processForWriteIdsForMmRead
argument_list|(
name|dir
argument_list|,
name|conf
argument_list|,
name|validWriteIdList
argument_list|,
name|allowOriginals
argument_list|,
name|finalPaths
argument_list|,
name|pathsWithFileOriginals
argument_list|)
expr_stmt|;
block|}
block|}
specifier|private
specifier|static
name|void
name|processForWriteIdsForMmRead
parameter_list|(
name|Path
name|dir
parameter_list|,
name|Configuration
name|conf
parameter_list|,
name|ValidWriteIdList
name|validWriteIdList
parameter_list|,
name|boolean
name|allowOriginals
parameter_list|,
name|List
argument_list|<
name|Path
argument_list|>
name|finalPaths
parameter_list|,
name|List
argument_list|<
name|Path
argument_list|>
name|pathsWithFileOriginals
parameter_list|)
throws|throws
name|IOException
block|{
name|FileSystem
name|fs
init|=
name|dir
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|Utilities
operator|.
name|FILE_OP_LOGGER
operator|.
name|trace
argument_list|(
literal|"Checking {} for inputs"
argument_list|,
name|dir
argument_list|)
expr_stmt|;
comment|// Ignore nullscan-optimized paths.
if|if
condition|(
name|fs
operator|instanceof
name|NullScanFileSystem
condition|)
block|{
name|finalPaths
operator|.
name|add
argument_list|(
name|dir
argument_list|)
expr_stmt|;
return|return;
block|}
comment|// We need to iterate to detect original directories, that are supported in MM but not ACID.
name|boolean
name|hasOriginalFiles
init|=
literal|false
decl_stmt|,
name|hasAcidDirs
init|=
literal|false
decl_stmt|;
name|List
argument_list|<
name|Path
argument_list|>
name|originalDirectories
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|()
decl_stmt|;
for|for
control|(
name|FileStatus
name|file
range|:
name|fs
operator|.
name|listStatus
argument_list|(
name|dir
argument_list|,
name|AcidUtils
operator|.
name|hiddenFileFilter
argument_list|)
control|)
block|{
name|Path
name|currDir
init|=
name|file
operator|.
name|getPath
argument_list|()
decl_stmt|;
name|Utilities
operator|.
name|FILE_OP_LOGGER
operator|.
name|trace
argument_list|(
literal|"Checking {} for being an input"
argument_list|,
name|currDir
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|file
operator|.
name|isDirectory
argument_list|()
condition|)
block|{
name|hasOriginalFiles
operator|=
literal|true
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|AcidUtils
operator|.
name|extractWriteId
argument_list|(
name|currDir
argument_list|)
operator|==
literal|null
condition|)
block|{
if|if
condition|(
name|allowOriginals
condition|)
block|{
name|originalDirectories
operator|.
name|add
argument_list|(
name|currDir
argument_list|)
expr_stmt|;
comment|// Add as is; it would become a recursive split.
block|}
else|else
block|{
name|Utilities
operator|.
name|FILE_OP_LOGGER
operator|.
name|debug
argument_list|(
literal|"Ignoring unknown (original?) directory {}"
argument_list|,
name|currDir
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
name|hasAcidDirs
operator|=
literal|true
expr_stmt|;
block|}
block|}
if|if
condition|(
name|hasAcidDirs
condition|)
block|{
name|AcidUtils
operator|.
name|Directory
name|dirInfo
init|=
name|AcidUtils
operator|.
name|getAcidState
argument_list|(
name|fs
argument_list|,
name|dir
argument_list|,
name|conf
argument_list|,
name|validWriteIdList
argument_list|,
name|Ref
operator|.
name|from
argument_list|(
literal|false
argument_list|)
argument_list|,
literal|true
argument_list|,
literal|null
argument_list|,
literal|false
argument_list|)
decl_stmt|;
comment|// Find the base, created for IOW.
name|Path
name|base
init|=
name|dirInfo
operator|.
name|getBaseDirectory
argument_list|()
decl_stmt|;
if|if
condition|(
name|base
operator|!=
literal|null
condition|)
block|{
name|Utilities
operator|.
name|FILE_OP_LOGGER
operator|.
name|debug
argument_list|(
literal|"Adding input {}"
argument_list|,
name|base
argument_list|)
expr_stmt|;
name|finalPaths
operator|.
name|add
argument_list|(
name|base
argument_list|)
expr_stmt|;
comment|// Base means originals no longer matter.
name|originalDirectories
operator|.
name|clear
argument_list|()
expr_stmt|;
name|hasOriginalFiles
operator|=
literal|false
expr_stmt|;
block|}
comment|// Find the parsed delta files.
for|for
control|(
name|AcidUtils
operator|.
name|ParsedDelta
name|delta
range|:
name|dirInfo
operator|.
name|getCurrentDirectories
argument_list|()
control|)
block|{
name|Utilities
operator|.
name|FILE_OP_LOGGER
operator|.
name|debug
argument_list|(
literal|"Adding input {}"
argument_list|,
name|delta
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
name|finalPaths
operator|.
name|add
argument_list|(
name|delta
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
operator|!
name|originalDirectories
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|Utilities
operator|.
name|FILE_OP_LOGGER
operator|.
name|debug
argument_list|(
literal|"Adding original directories {}"
argument_list|,
name|originalDirectories
argument_list|)
expr_stmt|;
name|finalPaths
operator|.
name|addAll
argument_list|(
name|originalDirectories
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|hasOriginalFiles
condition|)
block|{
if|if
condition|(
name|allowOriginals
condition|)
block|{
name|Utilities
operator|.
name|FILE_OP_LOGGER
operator|.
name|debug
argument_list|(
literal|"Directory has original files {}"
argument_list|,
name|dir
argument_list|)
expr_stmt|;
name|pathsWithFileOriginals
operator|.
name|add
argument_list|(
name|dir
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|Utilities
operator|.
name|FILE_OP_LOGGER
operator|.
name|debug
argument_list|(
literal|"Ignoring unknown (original?) files in {}"
argument_list|,
name|dir
argument_list|)
expr_stmt|;
block|}
block|}
block|}
name|Path
index|[]
name|getInputPaths
parameter_list|(
name|JobConf
name|job
parameter_list|)
throws|throws
name|IOException
block|{
name|Path
index|[]
name|dirs
decl_stmt|;
if|if
condition|(
name|HiveConf
operator|.
name|getVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_EXECUTION_ENGINE
argument_list|)
operator|.
name|equals
argument_list|(
literal|"spark"
argument_list|)
condition|)
block|{
name|dirs
operator|=
name|mrwork
operator|.
name|getPathToPartitionInfo
argument_list|()
operator|.
name|keySet
argument_list|()
operator|.
name|toArray
argument_list|(
operator|new
name|Path
index|[]
block|{}
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|dirs
operator|=
name|FileInputFormat
operator|.
name|getInputPaths
argument_list|(
name|job
argument_list|)
expr_stmt|;
if|if
condition|(
name|dirs
operator|.
name|length
operator|==
literal|0
condition|)
block|{
comment|// on tez we're avoiding to duplicate the file info in FileInputFormat.
if|if
condition|(
name|HiveConf
operator|.
name|getVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_EXECUTION_ENGINE
argument_list|)
operator|.
name|equals
argument_list|(
literal|"tez"
argument_list|)
condition|)
block|{
try|try
block|{
name|List
argument_list|<
name|Path
argument_list|>
name|paths
init|=
name|Utilities
operator|.
name|getInputPathsTez
argument_list|(
name|job
argument_list|,
name|mrwork
argument_list|)
decl_stmt|;
name|dirs
operator|=
name|paths
operator|.
name|toArray
argument_list|(
operator|new
name|Path
index|[
name|paths
operator|.
name|size
argument_list|()
index|]
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Could not create input files"
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
else|else
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"No input paths specified in job"
argument_list|)
throw|;
block|}
block|}
block|}
name|StringInternUtils
operator|.
name|internUriStringsInPathArray
argument_list|(
name|dirs
argument_list|)
expr_stmt|;
return|return
name|dirs
return|;
block|}
annotation|@
name|Override
specifier|public
name|InputSplit
index|[]
name|getSplits
parameter_list|(
name|JobConf
name|job
parameter_list|,
name|int
name|numSplits
parameter_list|)
throws|throws
name|IOException
block|{
name|PerfLogger
name|perfLogger
init|=
name|SessionState
operator|.
name|getPerfLogger
argument_list|()
decl_stmt|;
name|perfLogger
operator|.
name|PerfLogBegin
argument_list|(
name|CLASS_NAME
argument_list|,
name|PerfLogger
operator|.
name|GET_SPLITS
argument_list|)
expr_stmt|;
name|init
argument_list|(
name|job
argument_list|)
expr_stmt|;
name|Path
index|[]
name|dirs
init|=
name|getInputPaths
argument_list|(
name|job
argument_list|)
decl_stmt|;
name|JobConf
name|newjob
init|=
operator|new
name|JobConf
argument_list|(
name|job
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|InputSplit
argument_list|>
name|result
init|=
operator|new
name|ArrayList
argument_list|<
name|InputSplit
argument_list|>
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|Path
argument_list|>
name|currentDirs
init|=
operator|new
name|ArrayList
argument_list|<
name|Path
argument_list|>
argument_list|()
decl_stmt|;
name|Class
argument_list|<
name|?
extends|extends
name|InputFormat
argument_list|>
name|currentInputFormatClass
init|=
literal|null
decl_stmt|;
name|TableDesc
name|currentTable
init|=
literal|null
decl_stmt|;
name|TableScanOperator
name|currentTableScan
init|=
literal|null
decl_stmt|;
name|boolean
name|pushDownProjection
init|=
literal|false
decl_stmt|;
comment|//Buffers to hold filter pushdown information
name|StringBuilder
name|readColumnsBuffer
init|=
operator|new
name|StringBuilder
argument_list|(
name|newjob
operator|.
name|get
argument_list|(
name|ColumnProjectionUtils
operator|.
name|READ_COLUMN_IDS_CONF_STR
argument_list|,
literal|""
argument_list|)
argument_list|)
decl_stmt|;
empty_stmt|;
name|StringBuilder
name|readColumnNamesBuffer
init|=
operator|new
name|StringBuilder
argument_list|(
name|newjob
operator|.
name|get
argument_list|(
name|ColumnProjectionUtils
operator|.
name|READ_COLUMN_NAMES_CONF_STR
argument_list|,
literal|""
argument_list|)
argument_list|)
decl_stmt|;
comment|// for each dir, get the InputFormat, and do getSplits.
for|for
control|(
name|Path
name|dir
range|:
name|dirs
control|)
block|{
name|PartitionDesc
name|part
init|=
name|getPartitionDescFromPath
argument_list|(
name|pathToPartitionInfo
argument_list|,
name|dir
argument_list|)
decl_stmt|;
name|Class
argument_list|<
name|?
extends|extends
name|InputFormat
argument_list|>
name|inputFormatClass
init|=
name|part
operator|.
name|getInputFileFormatClass
argument_list|()
decl_stmt|;
name|TableDesc
name|table
init|=
name|part
operator|.
name|getTableDesc
argument_list|()
decl_stmt|;
name|TableScanOperator
name|tableScan
init|=
literal|null
decl_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|aliases
init|=
name|mrwork
operator|.
name|getPathToAliases
argument_list|()
operator|.
name|get
argument_list|(
name|dir
argument_list|)
decl_stmt|;
comment|// Make filter pushdown information available to getSplits.
if|if
condition|(
operator|(
name|aliases
operator|!=
literal|null
operator|)
operator|&&
operator|(
name|aliases
operator|.
name|size
argument_list|()
operator|==
literal|1
operator|)
condition|)
block|{
name|Operator
name|op
init|=
name|mrwork
operator|.
name|getAliasToWork
argument_list|()
operator|.
name|get
argument_list|(
name|aliases
operator|.
name|get
argument_list|(
literal|0
argument_list|)
argument_list|)
decl_stmt|;
if|if
condition|(
operator|(
name|op
operator|!=
literal|null
operator|)
operator|&&
operator|(
name|op
operator|instanceof
name|TableScanOperator
operator|)
condition|)
block|{
name|tableScan
operator|=
operator|(
name|TableScanOperator
operator|)
name|op
expr_stmt|;
comment|//Reset buffers to store filter push down columns
name|readColumnsBuffer
operator|.
name|setLength
argument_list|(
literal|0
argument_list|)
expr_stmt|;
name|readColumnNamesBuffer
operator|.
name|setLength
argument_list|(
literal|0
argument_list|)
expr_stmt|;
comment|// push down projections.
name|ColumnProjectionUtils
operator|.
name|appendReadColumns
argument_list|(
name|readColumnsBuffer
argument_list|,
name|readColumnNamesBuffer
argument_list|,
name|tableScan
operator|.
name|getNeededColumnIDs
argument_list|()
argument_list|,
name|tableScan
operator|.
name|getNeededColumns
argument_list|()
argument_list|)
expr_stmt|;
name|pushDownProjection
operator|=
literal|true
expr_stmt|;
comment|// push down filters
name|pushFilters
argument_list|(
name|newjob
argument_list|,
name|tableScan
argument_list|,
name|this
operator|.
name|mrwork
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"aliases: {} pathToAliases: {} dir: {}"
argument_list|,
name|aliases
argument_list|,
name|mrwork
operator|.
name|getPathToAliases
argument_list|()
argument_list|,
name|dir
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
operator|!
name|currentDirs
operator|.
name|isEmpty
argument_list|()
operator|&&
name|inputFormatClass
operator|.
name|equals
argument_list|(
name|currentInputFormatClass
argument_list|)
operator|&&
name|table
operator|.
name|equals
argument_list|(
name|currentTable
argument_list|)
operator|&&
name|tableScan
operator|==
name|currentTableScan
condition|)
block|{
name|currentDirs
operator|.
name|add
argument_list|(
name|dir
argument_list|)
expr_stmt|;
continue|continue;
block|}
if|if
condition|(
operator|!
name|currentDirs
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isInfoEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Generating splits as currentDirs is not empty. currentDirs: {}"
argument_list|,
name|currentDirs
argument_list|)
expr_stmt|;
block|}
comment|// set columns to read in conf
if|if
condition|(
name|pushDownProjection
condition|)
block|{
name|pushProjection
argument_list|(
name|newjob
argument_list|,
name|readColumnsBuffer
argument_list|,
name|readColumnNamesBuffer
argument_list|)
expr_stmt|;
block|}
name|addSplitsForGroup
argument_list|(
name|currentDirs
argument_list|,
name|currentTableScan
argument_list|,
name|newjob
argument_list|,
name|getInputFormatFromCache
argument_list|(
name|currentInputFormatClass
argument_list|,
name|job
argument_list|)
argument_list|,
name|currentInputFormatClass
argument_list|,
name|currentDirs
operator|.
name|size
argument_list|()
operator|*
operator|(
name|numSplits
operator|/
name|dirs
operator|.
name|length
operator|)
argument_list|,
name|currentTable
argument_list|,
name|result
argument_list|)
expr_stmt|;
block|}
name|currentDirs
operator|.
name|clear
argument_list|()
expr_stmt|;
name|currentDirs
operator|.
name|add
argument_list|(
name|dir
argument_list|)
expr_stmt|;
name|currentTableScan
operator|=
name|tableScan
expr_stmt|;
name|currentTable
operator|=
name|table
expr_stmt|;
name|currentInputFormatClass
operator|=
name|inputFormatClass
expr_stmt|;
block|}
comment|// set columns to read in conf
if|if
condition|(
name|pushDownProjection
condition|)
block|{
name|pushProjection
argument_list|(
name|newjob
argument_list|,
name|readColumnsBuffer
argument_list|,
name|readColumnNamesBuffer
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|dirs
operator|.
name|length
operator|!=
literal|0
condition|)
block|{
comment|// TODO: should this be currentDirs?
if|if
condition|(
name|LOG
operator|.
name|isInfoEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Generating splits for dirs: {}"
argument_list|,
name|dirs
argument_list|)
expr_stmt|;
block|}
name|addSplitsForGroup
argument_list|(
name|currentDirs
argument_list|,
name|currentTableScan
argument_list|,
name|newjob
argument_list|,
name|getInputFormatFromCache
argument_list|(
name|currentInputFormatClass
argument_list|,
name|job
argument_list|)
argument_list|,
name|currentInputFormatClass
argument_list|,
name|currentDirs
operator|.
name|size
argument_list|()
operator|*
operator|(
name|numSplits
operator|/
name|dirs
operator|.
name|length
operator|)
argument_list|,
name|currentTable
argument_list|,
name|result
argument_list|)
expr_stmt|;
block|}
name|Utilities
operator|.
name|clearWorkMapForConf
argument_list|(
name|job
argument_list|)
expr_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isInfoEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"number of splits "
operator|+
name|result
operator|.
name|size
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|perfLogger
operator|.
name|PerfLogEnd
argument_list|(
name|CLASS_NAME
argument_list|,
name|PerfLogger
operator|.
name|GET_SPLITS
argument_list|)
expr_stmt|;
return|return
name|result
operator|.
name|toArray
argument_list|(
operator|new
name|HiveInputSplit
index|[
name|result
operator|.
name|size
argument_list|()
index|]
argument_list|)
return|;
block|}
specifier|private
name|void
name|pushProjection
parameter_list|(
specifier|final
name|JobConf
name|newjob
parameter_list|,
specifier|final
name|StringBuilder
name|readColumnsBuffer
parameter_list|,
specifier|final
name|StringBuilder
name|readColumnNamesBuffer
parameter_list|)
block|{
name|String
name|readColIds
init|=
name|readColumnsBuffer
operator|.
name|toString
argument_list|()
decl_stmt|;
name|String
name|readColNames
init|=
name|readColumnNamesBuffer
operator|.
name|toString
argument_list|()
decl_stmt|;
name|newjob
operator|.
name|setBoolean
argument_list|(
name|ColumnProjectionUtils
operator|.
name|READ_ALL_COLUMNS
argument_list|,
literal|false
argument_list|)
expr_stmt|;
name|newjob
operator|.
name|set
argument_list|(
name|ColumnProjectionUtils
operator|.
name|READ_COLUMN_IDS_CONF_STR
argument_list|,
name|readColIds
argument_list|)
expr_stmt|;
name|newjob
operator|.
name|set
argument_list|(
name|ColumnProjectionUtils
operator|.
name|READ_COLUMN_NAMES_CONF_STR
argument_list|,
name|readColNames
argument_list|)
expr_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isInfoEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"{} = {}"
argument_list|,
name|ColumnProjectionUtils
operator|.
name|READ_COLUMN_IDS_CONF_STR
argument_list|,
name|readColIds
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"{} = {}"
argument_list|,
name|ColumnProjectionUtils
operator|.
name|READ_COLUMN_NAMES_CONF_STR
argument_list|,
name|readColNames
argument_list|)
expr_stmt|;
block|}
block|}
specifier|protected
specifier|static
name|PartitionDesc
name|getPartitionDescFromPath
parameter_list|(
name|Map
argument_list|<
name|Path
argument_list|,
name|PartitionDesc
argument_list|>
name|pathToPartitionInfo
parameter_list|,
name|Path
name|dir
parameter_list|)
throws|throws
name|IOException
block|{
name|PartitionDesc
name|partDesc
init|=
name|pathToPartitionInfo
operator|.
name|get
argument_list|(
name|dir
argument_list|)
decl_stmt|;
if|if
condition|(
name|partDesc
operator|==
literal|null
condition|)
block|{
comment|// Note: we could call HiveFileFormatUtils.getPartitionDescFromPathRecursively for MM tables.
comment|//       The recursive call is usually needed for non-MM tables, because the path management
comment|//       is not strict and the code does whatever. That should not happen for MM tables.
comment|//       Keep it like this for now; may need replacement if we find a valid use case.
name|partDesc
operator|=
name|pathToPartitionInfo
operator|.
name|get
argument_list|(
name|Path
operator|.
name|getPathWithoutSchemeAndAuthority
argument_list|(
name|dir
argument_list|)
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|partDesc
operator|==
literal|null
condition|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"cannot find dir = "
operator|+
name|dir
operator|.
name|toString
argument_list|()
operator|+
literal|" in "
operator|+
name|pathToPartitionInfo
argument_list|)
throw|;
block|}
return|return
name|partDesc
return|;
block|}
specifier|public
specifier|static
name|void
name|pushFilters
parameter_list|(
name|JobConf
name|jobConf
parameter_list|,
name|TableScanOperator
name|tableScan
parameter_list|,
specifier|final
name|MapWork
name|mrwork
parameter_list|)
block|{
comment|// ensure filters are not set from previous pushFilters
name|jobConf
operator|.
name|unset
argument_list|(
name|TableScanDesc
operator|.
name|FILTER_TEXT_CONF_STR
argument_list|)
expr_stmt|;
name|jobConf
operator|.
name|unset
argument_list|(
name|TableScanDesc
operator|.
name|FILTER_EXPR_CONF_STR
argument_list|)
expr_stmt|;
name|Utilities
operator|.
name|unsetSchemaEvolution
argument_list|(
name|jobConf
argument_list|)
expr_stmt|;
name|TableScanDesc
name|scanDesc
init|=
name|tableScan
operator|.
name|getConf
argument_list|()
decl_stmt|;
if|if
condition|(
name|scanDesc
operator|==
literal|null
condition|)
block|{
return|return;
block|}
name|Utilities
operator|.
name|addTableSchemaToConf
argument_list|(
name|jobConf
argument_list|,
name|tableScan
argument_list|)
expr_stmt|;
comment|// construct column name list and types for reference by filter push down
name|Utilities
operator|.
name|setColumnNameList
argument_list|(
name|jobConf
argument_list|,
name|tableScan
argument_list|)
expr_stmt|;
name|Utilities
operator|.
name|setColumnTypeList
argument_list|(
name|jobConf
argument_list|,
name|tableScan
argument_list|)
expr_stmt|;
comment|// push down filters
name|ExprNodeGenericFuncDesc
name|filterExpr
init|=
name|scanDesc
operator|.
name|getFilterExpr
argument_list|()
decl_stmt|;
if|if
condition|(
name|filterExpr
operator|==
literal|null
condition|)
block|{
return|return;
block|}
comment|// disable filter pushdown for mapreduce(except for storage handlers) when there are more than one table aliases,
comment|// since we don't clone jobConf per alias
if|if
condition|(
name|mrwork
operator|!=
literal|null
operator|&&
name|mrwork
operator|.
name|getAliases
argument_list|()
operator|!=
literal|null
operator|&&
name|mrwork
operator|.
name|getAliases
argument_list|()
operator|.
name|size
argument_list|()
operator|>
literal|1
operator|&&
name|jobConf
operator|.
name|get
argument_list|(
name|ConfVars
operator|.
name|HIVE_EXECUTION_ENGINE
operator|.
name|varname
argument_list|)
operator|.
name|equals
argument_list|(
literal|"mr"
argument_list|)
operator|&&
operator|(
name|scanDesc
operator|.
name|getTableMetadata
argument_list|()
operator|==
literal|null
operator|||
operator|!
operator|(
name|scanDesc
operator|.
name|getTableMetadata
argument_list|()
operator|.
name|getStorageHandler
argument_list|()
operator|instanceof
name|HiveStoragePredicateHandler
operator|)
operator|)
condition|)
block|{
return|return;
block|}
name|String
name|serializedFilterObj
init|=
name|scanDesc
operator|.
name|getSerializedFilterObject
argument_list|()
decl_stmt|;
name|String
name|serializedFilterExpr
init|=
name|scanDesc
operator|.
name|getSerializedFilterExpr
argument_list|()
decl_stmt|;
name|boolean
name|hasObj
init|=
name|serializedFilterObj
operator|!=
literal|null
decl_stmt|,
name|hasExpr
init|=
name|serializedFilterExpr
operator|!=
literal|null
decl_stmt|;
if|if
condition|(
operator|!
name|hasObj
condition|)
block|{
name|Serializable
name|filterObject
init|=
name|scanDesc
operator|.
name|getFilterObject
argument_list|()
decl_stmt|;
if|if
condition|(
name|filterObject
operator|!=
literal|null
condition|)
block|{
name|serializedFilterObj
operator|=
name|SerializationUtilities
operator|.
name|serializeObject
argument_list|(
name|filterObject
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
name|serializedFilterObj
operator|!=
literal|null
condition|)
block|{
name|jobConf
operator|.
name|set
argument_list|(
name|TableScanDesc
operator|.
name|FILTER_OBJECT_CONF_STR
argument_list|,
name|serializedFilterObj
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
operator|!
name|hasExpr
condition|)
block|{
name|serializedFilterExpr
operator|=
name|SerializationUtilities
operator|.
name|serializeExpression
argument_list|(
name|filterExpr
argument_list|)
expr_stmt|;
block|}
name|String
name|filterText
init|=
name|filterExpr
operator|.
name|getExprString
argument_list|()
decl_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Pushdown initiated with filterText = "
operator|+
name|filterText
operator|+
literal|", filterExpr = "
operator|+
name|filterExpr
operator|+
literal|", serializedFilterExpr = "
operator|+
name|serializedFilterExpr
operator|+
literal|" ("
operator|+
operator|(
name|hasExpr
condition|?
literal|"desc"
else|:
literal|"new"
operator|)
operator|+
literal|")"
operator|+
operator|(
name|serializedFilterObj
operator|==
literal|null
condition|?
literal|""
else|:
operator|(
literal|", serializedFilterObj = "
operator|+
name|serializedFilterObj
operator|+
literal|" ("
operator|+
operator|(
name|hasObj
condition|?
literal|"desc"
else|:
literal|"new"
operator|)
operator|+
literal|")"
operator|)
operator|)
argument_list|)
expr_stmt|;
block|}
name|jobConf
operator|.
name|set
argument_list|(
name|TableScanDesc
operator|.
name|FILTER_TEXT_CONF_STR
argument_list|,
name|filterText
argument_list|)
expr_stmt|;
name|jobConf
operator|.
name|set
argument_list|(
name|TableScanDesc
operator|.
name|FILTER_EXPR_CONF_STR
argument_list|,
name|serializedFilterExpr
argument_list|)
expr_stmt|;
block|}
specifier|protected
name|void
name|pushProjectionsAndFilters
parameter_list|(
name|JobConf
name|jobConf
parameter_list|,
name|Class
name|inputFormatClass
parameter_list|,
name|Path
name|splitPath
parameter_list|)
block|{
name|pushProjectionsAndFilters
argument_list|(
name|jobConf
argument_list|,
name|inputFormatClass
argument_list|,
name|splitPath
argument_list|,
literal|false
argument_list|)
expr_stmt|;
block|}
specifier|protected
name|void
name|pushProjectionsAndFilters
parameter_list|(
name|JobConf
name|jobConf
parameter_list|,
name|Class
name|inputFormatClass
parameter_list|,
name|Path
name|splitPath
parameter_list|,
name|boolean
name|nonNative
parameter_list|)
block|{
name|Path
name|splitPathWithNoSchema
init|=
name|Path
operator|.
name|getPathWithoutSchemeAndAuthority
argument_list|(
name|splitPath
argument_list|)
decl_stmt|;
if|if
condition|(
name|this
operator|.
name|mrwork
operator|==
literal|null
condition|)
block|{
name|init
argument_list|(
name|job
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|this
operator|.
name|mrwork
operator|.
name|getPathToAliases
argument_list|()
operator|==
literal|null
condition|)
block|{
return|return;
block|}
name|ArrayList
argument_list|<
name|String
argument_list|>
name|aliases
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
name|Iterator
argument_list|<
name|Entry
argument_list|<
name|Path
argument_list|,
name|List
argument_list|<
name|String
argument_list|>
argument_list|>
argument_list|>
name|iterator
init|=
name|this
operator|.
name|mrwork
operator|.
name|getPathToAliases
argument_list|()
operator|.
name|entrySet
argument_list|()
operator|.
name|iterator
argument_list|()
decl_stmt|;
name|Set
argument_list|<
name|Path
argument_list|>
name|splitParentPaths
init|=
literal|null
decl_stmt|;
name|int
name|pathsSize
init|=
name|this
operator|.
name|mrwork
operator|.
name|getPathToAliases
argument_list|()
operator|.
name|entrySet
argument_list|()
operator|.
name|size
argument_list|()
decl_stmt|;
while|while
condition|(
name|iterator
operator|.
name|hasNext
argument_list|()
condition|)
block|{
name|Entry
argument_list|<
name|Path
argument_list|,
name|List
argument_list|<
name|String
argument_list|>
argument_list|>
name|entry
init|=
name|iterator
operator|.
name|next
argument_list|()
decl_stmt|;
name|Path
name|key
init|=
name|entry
operator|.
name|getKey
argument_list|()
decl_stmt|;
name|boolean
name|match
decl_stmt|;
if|if
condition|(
name|nonNative
condition|)
block|{
comment|// For non-native tables, we need to do an exact match to avoid
comment|// HIVE-1903.  (The table location contains no files, and the string
comment|// representation of its path does not have a trailing slash.)
name|match
operator|=
name|splitPath
operator|.
name|equals
argument_list|(
name|key
argument_list|)
operator|||
name|splitPathWithNoSchema
operator|.
name|equals
argument_list|(
name|key
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// But for native tables, we need to do a prefix match for
comment|// subdirectories.  (Unlike non-native tables, prefix mixups don't seem
comment|// to be a potential problem here since we are always dealing with the
comment|// path to something deeper than the table location.)
if|if
condition|(
name|pathsSize
operator|>
literal|1
condition|)
block|{
comment|// Comparing paths multiple times creates lots of objects&
comment|// creates GC pressure for tables having large number of partitions.
comment|// In such cases, use pre-computed paths for comparison
if|if
condition|(
name|splitParentPaths
operator|==
literal|null
condition|)
block|{
name|splitParentPaths
operator|=
operator|new
name|HashSet
argument_list|<>
argument_list|()
expr_stmt|;
name|FileUtils
operator|.
name|populateParentPaths
argument_list|(
name|splitParentPaths
argument_list|,
name|splitPath
argument_list|)
expr_stmt|;
name|FileUtils
operator|.
name|populateParentPaths
argument_list|(
name|splitParentPaths
argument_list|,
name|splitPathWithNoSchema
argument_list|)
expr_stmt|;
block|}
name|match
operator|=
name|splitParentPaths
operator|.
name|contains
argument_list|(
name|key
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|match
operator|=
name|FileUtils
operator|.
name|isPathWithinSubtree
argument_list|(
name|splitPath
argument_list|,
name|key
argument_list|)
operator|||
name|FileUtils
operator|.
name|isPathWithinSubtree
argument_list|(
name|splitPathWithNoSchema
argument_list|,
name|key
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
name|match
condition|)
block|{
name|List
argument_list|<
name|String
argument_list|>
name|list
init|=
name|entry
operator|.
name|getValue
argument_list|()
decl_stmt|;
for|for
control|(
name|String
name|val
range|:
name|list
control|)
block|{
name|aliases
operator|.
name|add
argument_list|(
name|val
argument_list|)
expr_stmt|;
block|}
block|}
block|}
for|for
control|(
name|String
name|alias
range|:
name|aliases
control|)
block|{
name|Operator
argument_list|<
name|?
extends|extends
name|OperatorDesc
argument_list|>
name|op
init|=
name|this
operator|.
name|mrwork
operator|.
name|getAliasToWork
argument_list|()
operator|.
name|get
argument_list|(
name|alias
argument_list|)
decl_stmt|;
if|if
condition|(
name|op
operator|instanceof
name|TableScanOperator
condition|)
block|{
name|TableScanOperator
name|ts
init|=
operator|(
name|TableScanOperator
operator|)
name|op
decl_stmt|;
comment|// push down projections.
name|ColumnProjectionUtils
operator|.
name|appendReadColumns
argument_list|(
name|jobConf
argument_list|,
name|ts
operator|.
name|getNeededColumnIDs
argument_list|()
argument_list|,
name|ts
operator|.
name|getNeededColumns
argument_list|()
argument_list|,
name|ts
operator|.
name|getNeededNestedColumnPaths
argument_list|()
argument_list|)
expr_stmt|;
comment|// push down filters
name|pushFilters
argument_list|(
name|jobConf
argument_list|,
name|ts
argument_list|,
name|this
operator|.
name|mrwork
argument_list|)
expr_stmt|;
name|AcidUtils
operator|.
name|setAcidOperationalProperties
argument_list|(
name|job
argument_list|,
name|ts
operator|.
name|getConf
argument_list|()
operator|.
name|isTranscationalTable
argument_list|()
argument_list|,
name|ts
operator|.
name|getConf
argument_list|()
operator|.
name|getAcidOperationalProperties
argument_list|()
argument_list|)
expr_stmt|;
name|AcidUtils
operator|.
name|setValidWriteIdList
argument_list|(
name|job
argument_list|,
name|ts
operator|.
name|getConf
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
end_class

end_unit

