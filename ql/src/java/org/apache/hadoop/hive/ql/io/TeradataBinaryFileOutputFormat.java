begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/*  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
package|;
end_package

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|OutputStream
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Properties
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|io
operator|.
name|EndianUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|Log
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|LogFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|FileSinkOperator
operator|.
name|RecordWriter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|Utilities
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|BytesWritable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|Writable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|WritableComparable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|JobConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|Progressable
import|;
end_import

begin_import
import|import static
name|java
operator|.
name|lang
operator|.
name|String
operator|.
name|format
import|;
end_import

begin_comment
comment|/**  * https://cwiki.apache.org/confluence/display/Hive/TeradataBinarySerde.  * FileOutputFormat for Teradata binary files.  *  * In the Teradata Binary File, each record constructs as below:  * The first 2 bytes represents the length of the bytes next for this record (null bitmap and fields).  * Then the null bitmap whose length is depended on the number of fields is followe.  * Then each field of the record is serialized into bytes - the serialization strategy is decided by the type of field.  * At last, there is one byte (0x0a) in the end of the record.  *  * Teradata binary files are using little endian.  */
end_comment

begin_class
specifier|public
class|class
name|TeradataBinaryFileOutputFormat
parameter_list|<
name|K
extends|extends
name|WritableComparable
parameter_list|,
name|V
extends|extends
name|Writable
parameter_list|>
extends|extends
name|HiveIgnoreKeyTextOutputFormat
argument_list|<
name|K
argument_list|,
name|V
argument_list|>
block|{
specifier|private
specifier|static
specifier|final
name|Log
name|LOG
init|=
name|LogFactory
operator|.
name|getLog
argument_list|(
name|TeradataBinaryFileOutputFormat
operator|.
name|class
argument_list|)
decl_stmt|;
specifier|static
specifier|final
name|byte
name|RECORD_END_BYTE
init|=
operator|(
name|byte
operator|)
literal|0x0a
decl_stmt|;
comment|/**    * create the final out file, and output row by row. After one row is    * appended, a configured row separator is appended    *    * @param jc    *          the job configuration file    * @param outPath    *          the final output file to be created    * @param valueClass    *          the value class used for create    * @param isCompressed    *          whether the content is compressed or not    * @param tableProperties    *          the tableProperties of this file's corresponding table    * @param progress    *          progress used for status report    * @return the RecordWriter    */
annotation|@
name|Override
specifier|public
name|RecordWriter
name|getHiveRecordWriter
parameter_list|(
name|JobConf
name|jc
parameter_list|,
name|Path
name|outPath
parameter_list|,
name|Class
argument_list|<
name|?
extends|extends
name|Writable
argument_list|>
name|valueClass
parameter_list|,
name|boolean
name|isCompressed
parameter_list|,
name|Properties
name|tableProperties
parameter_list|,
name|Progressable
name|progress
parameter_list|)
throws|throws
name|IOException
block|{
name|FileSystem
name|fs
init|=
name|outPath
operator|.
name|getFileSystem
argument_list|(
name|jc
argument_list|)
decl_stmt|;
specifier|final
name|OutputStream
name|outStream
init|=
name|Utilities
operator|.
name|createCompressedStream
argument_list|(
name|jc
argument_list|,
name|fs
operator|.
name|create
argument_list|(
name|outPath
argument_list|,
name|progress
argument_list|)
argument_list|,
name|isCompressed
argument_list|)
decl_stmt|;
return|return
operator|new
name|RecordWriter
argument_list|()
block|{
annotation|@
name|Override
specifier|public
name|void
name|write
parameter_list|(
name|Writable
name|r
parameter_list|)
throws|throws
name|IOException
block|{
name|BytesWritable
name|bw
init|=
operator|(
name|BytesWritable
operator|)
name|r
decl_stmt|;
name|int
name|recordLength
init|=
name|bw
operator|.
name|getLength
argument_list|()
decl_stmt|;
comment|//Based on the row length to decide if the length is int or short
name|String
name|rowLength
init|=
name|tableProperties
operator|.
name|getProperty
argument_list|(
name|TeradataBinaryRecordReader
operator|.
name|TD_ROW_LENGTH
argument_list|,
name|TeradataBinaryRecordReader
operator|.
name|DEFAULT_TD_ROW_LENGTH
argument_list|)
operator|.
name|toLowerCase
argument_list|()
decl_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
name|format
argument_list|(
literal|"The table property %s is: %s"
argument_list|,
name|TeradataBinaryRecordReader
operator|.
name|TD_ROW_LENGTH
argument_list|,
name|rowLength
argument_list|)
argument_list|)
expr_stmt|;
if|if
condition|(
name|TeradataBinaryRecordReader
operator|.
name|TD_ROW_LENGTH_TO_BYTE_NUM
operator|.
name|containsKey
argument_list|(
name|rowLength
argument_list|)
condition|)
block|{
if|if
condition|(
name|rowLength
operator|.
name|equals
argument_list|(
name|TeradataBinaryRecordReader
operator|.
name|DEFAULT_TD_ROW_LENGTH
argument_list|)
condition|)
block|{
name|EndianUtils
operator|.
name|writeSwappedShort
argument_list|(
name|outStream
argument_list|,
operator|(
name|short
operator|)
name|recordLength
argument_list|)
expr_stmt|;
comment|// write the length using little endian
block|}
elseif|else
if|if
condition|(
name|rowLength
operator|.
name|equals
argument_list|(
name|TeradataBinaryRecordReader
operator|.
name|TD_ROW_LENGTH_1MB
argument_list|)
condition|)
block|{
name|EndianUtils
operator|.
name|writeSwappedInteger
argument_list|(
name|outStream
argument_list|,
name|recordLength
argument_list|)
expr_stmt|;
comment|// write the length using little endian
block|}
block|}
else|else
block|{
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
name|format
argument_list|(
literal|"%s doesn't support the value %s, the supported values are %s"
argument_list|,
name|TeradataBinaryRecordReader
operator|.
name|TD_ROW_LENGTH
argument_list|,
name|rowLength
argument_list|,
name|TeradataBinaryRecordReader
operator|.
name|TD_ROW_LENGTH_TO_BYTE_NUM
operator|.
name|keySet
argument_list|()
argument_list|)
argument_list|)
throw|;
block|}
name|outStream
operator|.
name|write
argument_list|(
name|bw
operator|.
name|getBytes
argument_list|()
argument_list|,
literal|0
argument_list|,
name|bw
operator|.
name|getLength
argument_list|()
argument_list|)
expr_stmt|;
comment|// write the content (the content is in little endian)
name|outStream
operator|.
name|write
argument_list|(
name|RECORD_END_BYTE
argument_list|)
expr_stmt|;
comment|//write the record ending
block|}
annotation|@
name|Override
specifier|public
name|void
name|close
parameter_list|(
name|boolean
name|abort
parameter_list|)
throws|throws
name|IOException
block|{
name|outStream
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
return|;
block|}
block|}
end_class

end_unit

