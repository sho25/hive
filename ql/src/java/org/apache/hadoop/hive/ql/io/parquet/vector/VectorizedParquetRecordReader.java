begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/**  * Licensed under the Apache License, Version 2.0 (the "License");  * you may not use this file except in compliance with the License.  * You may obtain a copy of the License at  *  * http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|parquet
operator|.
name|vector
package|;
end_package

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|annotations
operator|.
name|VisibleForTesting
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configuration
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileStatus
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|io
operator|.
name|DataCache
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|io
operator|.
name|FileMetadataCache
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|io
operator|.
name|encoded
operator|.
name|MemoryBufferOrBuffers
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
operator|.
name|ConfVars
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|llap
operator|.
name|LlapCacheAwareFs
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|Utilities
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|vector
operator|.
name|VectorizedRowBatch
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|vector
operator|.
name|VectorizedRowBatchCtx
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|HdfsUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|IOConstants
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|parquet
operator|.
name|ParquetRecordReaderBase
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|parquet
operator|.
name|ProjectionPusher
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|parquet
operator|.
name|read
operator|.
name|DataWritableReadSupport
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|ColumnProjectionUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|SerDeStats
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|typeinfo
operator|.
name|ListTypeInfo
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|typeinfo
operator|.
name|PrimitiveTypeInfo
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|typeinfo
operator|.
name|StructTypeInfo
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|typeinfo
operator|.
name|TypeInfo
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|NullWritable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|FileSplit
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|JobConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|RecordReader
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|InputSplit
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|parquet
operator|.
name|ParquetRuntimeException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|parquet
operator|.
name|bytes
operator|.
name|BytesUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|parquet
operator|.
name|column
operator|.
name|ColumnDescriptor
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|parquet
operator|.
name|column
operator|.
name|page
operator|.
name|PageReadStore
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|parquet
operator|.
name|filter2
operator|.
name|compat
operator|.
name|FilterCompat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|parquet
operator|.
name|format
operator|.
name|converter
operator|.
name|ParquetMetadataConverter
operator|.
name|MetadataFilter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|parquet
operator|.
name|hadoop
operator|.
name|ParquetFileReader
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|parquet
operator|.
name|hadoop
operator|.
name|ParquetFileWriter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|parquet
operator|.
name|hadoop
operator|.
name|ParquetInputSplit
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|parquet
operator|.
name|hadoop
operator|.
name|metadata
operator|.
name|BlockMetaData
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|parquet
operator|.
name|hadoop
operator|.
name|metadata
operator|.
name|ColumnChunkMetaData
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|parquet
operator|.
name|hadoop
operator|.
name|metadata
operator|.
name|ColumnPath
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|parquet
operator|.
name|hadoop
operator|.
name|metadata
operator|.
name|ParquetMetadata
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|parquet
operator|.
name|hadoop
operator|.
name|util
operator|.
name|HadoopStreams
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|parquet
operator|.
name|io
operator|.
name|InputFile
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|parquet
operator|.
name|io
operator|.
name|SeekableInputStream
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|parquet
operator|.
name|schema
operator|.
name|InvalidSchemaException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|parquet
operator|.
name|schema
operator|.
name|MessageType
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|parquet
operator|.
name|schema
operator|.
name|Type
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|Logger
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|LoggerFactory
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Arrays
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashSet
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Set
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|TreeMap
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|parquet
operator|.
name|filter2
operator|.
name|compat
operator|.
name|RowGroupFilter
operator|.
name|filterRowGroups
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|parquet
operator|.
name|format
operator|.
name|converter
operator|.
name|ParquetMetadataConverter
operator|.
name|NO_FILTER
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|parquet
operator|.
name|format
operator|.
name|converter
operator|.
name|ParquetMetadataConverter
operator|.
name|range
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|parquet
operator|.
name|hadoop
operator|.
name|ParquetInputFormat
operator|.
name|getFilter
import|;
end_import

begin_comment
comment|/**  * This reader is used to read a batch of record from inputsplit, part of the code is referred  * from Apache Spark and Apache Parquet.  */
end_comment

begin_class
specifier|public
class|class
name|VectorizedParquetRecordReader
extends|extends
name|ParquetRecordReaderBase
implements|implements
name|RecordReader
argument_list|<
name|NullWritable
argument_list|,
name|VectorizedRowBatch
argument_list|>
block|{
specifier|public
specifier|static
specifier|final
name|Logger
name|LOG
init|=
name|LoggerFactory
operator|.
name|getLogger
argument_list|(
name|VectorizedParquetRecordReader
operator|.
name|class
argument_list|)
decl_stmt|;
specifier|private
name|List
argument_list|<
name|Integer
argument_list|>
name|colsToInclude
decl_stmt|;
specifier|protected
name|MessageType
name|fileSchema
decl_stmt|;
specifier|protected
name|MessageType
name|requestedSchema
decl_stmt|;
specifier|private
name|List
argument_list|<
name|String
argument_list|>
name|columnNamesList
decl_stmt|;
specifier|private
name|List
argument_list|<
name|TypeInfo
argument_list|>
name|columnTypesList
decl_stmt|;
specifier|private
name|VectorizedRowBatchCtx
name|rbCtx
decl_stmt|;
specifier|private
name|Object
index|[]
name|partitionValues
decl_stmt|;
specifier|private
name|Path
name|cacheFsPath
decl_stmt|;
comment|/**    * For each request column, the reader to read this column. This is NULL if this column    * is missing from the file, in which case we populate the attribute with NULL.    */
specifier|private
name|VectorizedColumnReader
index|[]
name|columnReaders
decl_stmt|;
comment|/**    * The number of rows that have been returned.    */
specifier|private
name|long
name|rowsReturned
init|=
literal|0
decl_stmt|;
comment|/**    * The number of rows that have been reading, including the current in flight row group.    */
specifier|private
name|long
name|totalCountLoadedSoFar
init|=
literal|0
decl_stmt|;
comment|/**    * The total number of rows this RecordReader will eventually read. The sum of the    * rows of all the row groups.    */
specifier|protected
name|long
name|totalRowCount
init|=
literal|0
decl_stmt|;
specifier|public
name|VectorizedParquetRecordReader
parameter_list|(
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|InputSplit
name|oldInputSplit
parameter_list|,
name|JobConf
name|conf
parameter_list|)
block|{
name|this
argument_list|(
name|oldInputSplit
argument_list|,
name|conf
argument_list|,
literal|null
argument_list|,
literal|null
argument_list|,
literal|null
argument_list|)
expr_stmt|;
block|}
specifier|public
name|VectorizedParquetRecordReader
parameter_list|(
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|InputSplit
name|oldInputSplit
parameter_list|,
name|JobConf
name|conf
parameter_list|,
name|FileMetadataCache
name|metadataCache
parameter_list|,
name|DataCache
name|dataCache
parameter_list|,
name|Configuration
name|cacheConf
parameter_list|)
block|{
try|try
block|{
name|this
operator|.
name|metadataCache
operator|=
name|metadataCache
expr_stmt|;
name|this
operator|.
name|cache
operator|=
name|dataCache
expr_stmt|;
name|this
operator|.
name|cacheConf
operator|=
name|cacheConf
expr_stmt|;
name|serDeStats
operator|=
operator|new
name|SerDeStats
argument_list|()
expr_stmt|;
name|projectionPusher
operator|=
operator|new
name|ProjectionPusher
argument_list|()
expr_stmt|;
name|colsToInclude
operator|=
name|ColumnProjectionUtils
operator|.
name|getReadColumnIDs
argument_list|(
name|conf
argument_list|)
expr_stmt|;
comment|//initialize the rowbatchContext
name|jobConf
operator|=
name|conf
expr_stmt|;
name|rbCtx
operator|=
name|Utilities
operator|.
name|getVectorizedRowBatchCtx
argument_list|(
name|jobConf
argument_list|)
expr_stmt|;
name|ParquetInputSplit
name|inputSplit
init|=
name|getSplit
argument_list|(
name|oldInputSplit
argument_list|,
name|conf
argument_list|)
decl_stmt|;
if|if
condition|(
name|inputSplit
operator|!=
literal|null
condition|)
block|{
name|initialize
argument_list|(
name|inputSplit
argument_list|,
name|conf
argument_list|)
expr_stmt|;
block|}
name|initPartitionValues
argument_list|(
operator|(
name|FileSplit
operator|)
name|oldInputSplit
argument_list|,
name|conf
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Throwable
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Failed to create the vectorized reader due to exception "
operator|+
name|e
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|RuntimeException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|private
name|void
name|initPartitionValues
parameter_list|(
name|FileSplit
name|fileSplit
parameter_list|,
name|JobConf
name|conf
parameter_list|)
throws|throws
name|IOException
block|{
name|int
name|partitionColumnCount
init|=
name|rbCtx
operator|.
name|getPartitionColumnCount
argument_list|()
decl_stmt|;
if|if
condition|(
name|partitionColumnCount
operator|>
literal|0
condition|)
block|{
name|partitionValues
operator|=
operator|new
name|Object
index|[
name|partitionColumnCount
index|]
expr_stmt|;
name|VectorizedRowBatchCtx
operator|.
name|getPartitionValues
argument_list|(
name|rbCtx
argument_list|,
name|conf
argument_list|,
name|fileSplit
argument_list|,
name|partitionValues
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|partitionValues
operator|=
literal|null
expr_stmt|;
block|}
block|}
annotation|@
name|SuppressWarnings
argument_list|(
literal|"deprecation"
argument_list|)
specifier|public
name|void
name|initialize
parameter_list|(
name|InputSplit
name|oldSplit
parameter_list|,
name|JobConf
name|configuration
parameter_list|)
throws|throws
name|IOException
throws|,
name|InterruptedException
block|{
comment|// the oldSplit may be null during the split phase
if|if
condition|(
name|oldSplit
operator|==
literal|null
condition|)
block|{
return|return;
block|}
name|ParquetMetadata
name|footer
decl_stmt|;
name|List
argument_list|<
name|BlockMetaData
argument_list|>
name|blocks
decl_stmt|;
name|ParquetInputSplit
name|split
init|=
operator|(
name|ParquetInputSplit
operator|)
name|oldSplit
decl_stmt|;
name|boolean
name|indexAccess
init|=
name|configuration
operator|.
name|getBoolean
argument_list|(
name|DataWritableReadSupport
operator|.
name|PARQUET_COLUMN_INDEX_ACCESS
argument_list|,
literal|false
argument_list|)
decl_stmt|;
name|this
operator|.
name|file
operator|=
name|split
operator|.
name|getPath
argument_list|()
expr_stmt|;
name|long
index|[]
name|rowGroupOffsets
init|=
name|split
operator|.
name|getRowGroupOffsets
argument_list|()
decl_stmt|;
name|String
name|columnNames
init|=
name|configuration
operator|.
name|get
argument_list|(
name|IOConstants
operator|.
name|COLUMNS
argument_list|)
decl_stmt|;
name|columnNamesList
operator|=
name|DataWritableReadSupport
operator|.
name|getColumnNames
argument_list|(
name|columnNames
argument_list|)
expr_stmt|;
name|String
name|columnTypes
init|=
name|configuration
operator|.
name|get
argument_list|(
name|IOConstants
operator|.
name|COLUMNS_TYPES
argument_list|)
decl_stmt|;
name|columnTypesList
operator|=
name|DataWritableReadSupport
operator|.
name|getColumnTypes
argument_list|(
name|columnTypes
argument_list|)
expr_stmt|;
comment|// if task.side.metadata is set, rowGroupOffsets is null
name|Object
name|cacheKey
init|=
literal|null
decl_stmt|;
comment|// TODO: also support fileKey in splits, like OrcSplit does
if|if
condition|(
name|metadataCache
operator|!=
literal|null
condition|)
block|{
name|cacheKey
operator|=
name|HdfsUtils
operator|.
name|getFileId
argument_list|(
name|file
operator|.
name|getFileSystem
argument_list|(
name|configuration
argument_list|)
argument_list|,
name|file
argument_list|,
name|HiveConf
operator|.
name|getBoolVar
argument_list|(
name|cacheConf
argument_list|,
name|ConfVars
operator|.
name|LLAP_CACHE_ALLOW_SYNTHETIC_FILEID
argument_list|)
argument_list|,
name|HiveConf
operator|.
name|getBoolVar
argument_list|(
name|cacheConf
argument_list|,
name|ConfVars
operator|.
name|LLAP_CACHE_DEFAULT_FS_FILE_ID
argument_list|)
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|cacheKey
operator|!=
literal|null
condition|)
block|{
comment|// If we are going to use cache, change the path to depend on file ID for extra consistency.
name|FileSystem
name|fs
init|=
name|file
operator|.
name|getFileSystem
argument_list|(
name|configuration
argument_list|)
decl_stmt|;
if|if
condition|(
name|cacheKey
operator|instanceof
name|Long
operator|&&
name|HiveConf
operator|.
name|getBoolVar
argument_list|(
name|cacheConf
argument_list|,
name|ConfVars
operator|.
name|LLAP_IO_USE_FILEID_PATH
argument_list|)
condition|)
block|{
name|file
operator|=
name|HdfsUtils
operator|.
name|getFileIdPath
argument_list|(
name|fs
argument_list|,
name|file
argument_list|,
operator|(
name|long
operator|)
name|cacheKey
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
name|rowGroupOffsets
operator|==
literal|null
condition|)
block|{
comment|//TODO check whether rowGroupOffSets can be null
comment|// then we need to apply the predicate push down filter
name|footer
operator|=
name|readSplitFooter
argument_list|(
name|configuration
argument_list|,
name|file
argument_list|,
name|cacheKey
argument_list|,
name|range
argument_list|(
name|split
operator|.
name|getStart
argument_list|()
argument_list|,
name|split
operator|.
name|getEnd
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
name|MessageType
name|fileSchema
init|=
name|footer
operator|.
name|getFileMetaData
argument_list|()
operator|.
name|getSchema
argument_list|()
decl_stmt|;
name|FilterCompat
operator|.
name|Filter
name|filter
init|=
name|getFilter
argument_list|(
name|configuration
argument_list|)
decl_stmt|;
name|blocks
operator|=
name|filterRowGroups
argument_list|(
name|filter
argument_list|,
name|footer
operator|.
name|getBlocks
argument_list|()
argument_list|,
name|fileSchema
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// otherwise we find the row groups that were selected on the client
name|footer
operator|=
name|readSplitFooter
argument_list|(
name|configuration
argument_list|,
name|file
argument_list|,
name|cacheKey
argument_list|,
name|NO_FILTER
argument_list|)
expr_stmt|;
name|Set
argument_list|<
name|Long
argument_list|>
name|offsets
init|=
operator|new
name|HashSet
argument_list|<>
argument_list|()
decl_stmt|;
for|for
control|(
name|long
name|offset
range|:
name|rowGroupOffsets
control|)
block|{
name|offsets
operator|.
name|add
argument_list|(
name|offset
argument_list|)
expr_stmt|;
block|}
name|blocks
operator|=
operator|new
name|ArrayList
argument_list|<>
argument_list|()
expr_stmt|;
for|for
control|(
name|BlockMetaData
name|block
range|:
name|footer
operator|.
name|getBlocks
argument_list|()
control|)
block|{
if|if
condition|(
name|offsets
operator|.
name|contains
argument_list|(
name|block
operator|.
name|getStartingPos
argument_list|()
argument_list|)
condition|)
block|{
name|blocks
operator|.
name|add
argument_list|(
name|block
argument_list|)
expr_stmt|;
block|}
block|}
comment|// verify we found them all
if|if
condition|(
name|blocks
operator|.
name|size
argument_list|()
operator|!=
name|rowGroupOffsets
operator|.
name|length
condition|)
block|{
name|long
index|[]
name|foundRowGroupOffsets
init|=
operator|new
name|long
index|[
name|footer
operator|.
name|getBlocks
argument_list|()
operator|.
name|size
argument_list|()
index|]
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|foundRowGroupOffsets
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|foundRowGroupOffsets
index|[
name|i
index|]
operator|=
name|footer
operator|.
name|getBlocks
argument_list|()
operator|.
name|get
argument_list|(
name|i
argument_list|)
operator|.
name|getStartingPos
argument_list|()
expr_stmt|;
block|}
comment|// this should never happen.
comment|// provide a good error message in case there's a bug
throw|throw
operator|new
name|IllegalStateException
argument_list|(
literal|"All the offsets listed in the split should be found in the file."
operator|+
literal|" expected: "
operator|+
name|Arrays
operator|.
name|toString
argument_list|(
name|rowGroupOffsets
argument_list|)
operator|+
literal|" found: "
operator|+
name|blocks
operator|+
literal|" out of: "
operator|+
name|Arrays
operator|.
name|toString
argument_list|(
name|foundRowGroupOffsets
argument_list|)
operator|+
literal|" in range "
operator|+
name|split
operator|.
name|getStart
argument_list|()
operator|+
literal|", "
operator|+
name|split
operator|.
name|getEnd
argument_list|()
argument_list|)
throw|;
block|}
block|}
for|for
control|(
name|BlockMetaData
name|block
range|:
name|blocks
control|)
block|{
name|this
operator|.
name|totalRowCount
operator|+=
name|block
operator|.
name|getRowCount
argument_list|()
expr_stmt|;
block|}
name|this
operator|.
name|fileSchema
operator|=
name|footer
operator|.
name|getFileMetaData
argument_list|()
operator|.
name|getSchema
argument_list|()
expr_stmt|;
name|colsToInclude
operator|=
name|ColumnProjectionUtils
operator|.
name|getReadColumnIDs
argument_list|(
name|configuration
argument_list|)
expr_stmt|;
name|requestedSchema
operator|=
name|DataWritableReadSupport
operator|.
name|getRequestedSchema
argument_list|(
name|indexAccess
argument_list|,
name|columnNamesList
argument_list|,
name|columnTypesList
argument_list|,
name|fileSchema
argument_list|,
name|configuration
argument_list|)
expr_stmt|;
name|Path
name|path
init|=
name|wrapPathForCache
argument_list|(
name|file
argument_list|,
name|cacheKey
argument_list|,
name|configuration
argument_list|,
name|blocks
argument_list|)
decl_stmt|;
name|this
operator|.
name|reader
operator|=
operator|new
name|ParquetFileReader
argument_list|(
name|configuration
argument_list|,
name|footer
operator|.
name|getFileMetaData
argument_list|()
argument_list|,
name|path
argument_list|,
name|blocks
argument_list|,
name|requestedSchema
operator|.
name|getColumns
argument_list|()
argument_list|)
expr_stmt|;
block|}
specifier|private
name|Path
name|wrapPathForCache
parameter_list|(
name|Path
name|path
parameter_list|,
name|Object
name|fileKey
parameter_list|,
name|JobConf
name|configuration
parameter_list|,
name|List
argument_list|<
name|BlockMetaData
argument_list|>
name|blocks
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|fileKey
operator|==
literal|null
operator|||
name|cache
operator|==
literal|null
condition|)
block|{
return|return
name|path
return|;
block|}
name|HashSet
argument_list|<
name|ColumnPath
argument_list|>
name|includedCols
init|=
operator|new
name|HashSet
argument_list|<>
argument_list|()
decl_stmt|;
for|for
control|(
name|ColumnDescriptor
name|col
range|:
name|requestedSchema
operator|.
name|getColumns
argument_list|()
control|)
block|{
name|includedCols
operator|.
name|add
argument_list|(
name|ColumnPath
operator|.
name|get
argument_list|(
name|col
operator|.
name|getPath
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
comment|// We could make some assumptions given how the reader currently does the work (consecutive
comment|// chunks, etc.; blocks and columns stored in offset order in the lists), but we won't -
comment|// just save all the chunk boundaries and lengths for now.
name|TreeMap
argument_list|<
name|Long
argument_list|,
name|Long
argument_list|>
name|chunkIndex
init|=
operator|new
name|TreeMap
argument_list|<>
argument_list|()
decl_stmt|;
for|for
control|(
name|BlockMetaData
name|block
range|:
name|blocks
control|)
block|{
for|for
control|(
name|ColumnChunkMetaData
name|mc
range|:
name|block
operator|.
name|getColumns
argument_list|()
control|)
block|{
if|if
condition|(
operator|!
name|includedCols
operator|.
name|contains
argument_list|(
name|mc
operator|.
name|getPath
argument_list|()
argument_list|)
condition|)
continue|continue;
name|chunkIndex
operator|.
name|put
argument_list|(
name|mc
operator|.
name|getStartingPos
argument_list|()
argument_list|,
name|mc
operator|.
name|getStartingPos
argument_list|()
operator|+
name|mc
operator|.
name|getTotalSize
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
comment|// Register the cache-aware path so that Parquet reader would go thru it.
name|configuration
operator|.
name|set
argument_list|(
literal|"fs."
operator|+
name|LlapCacheAwareFs
operator|.
name|SCHEME
operator|+
literal|".impl"
argument_list|,
name|LlapCacheAwareFs
operator|.
name|class
operator|.
name|getCanonicalName
argument_list|()
argument_list|)
expr_stmt|;
name|path
operator|=
name|LlapCacheAwareFs
operator|.
name|registerFile
argument_list|(
name|cache
argument_list|,
name|path
argument_list|,
name|fileKey
argument_list|,
name|chunkIndex
argument_list|,
name|configuration
argument_list|)
expr_stmt|;
name|this
operator|.
name|cacheFsPath
operator|=
name|path
expr_stmt|;
return|return
name|path
return|;
block|}
specifier|private
name|ParquetMetadata
name|readSplitFooter
parameter_list|(
name|JobConf
name|configuration
parameter_list|,
specifier|final
name|Path
name|file
parameter_list|,
name|Object
name|cacheKey
parameter_list|,
name|MetadataFilter
name|filter
parameter_list|)
throws|throws
name|IOException
block|{
name|MemoryBufferOrBuffers
name|footerData
init|=
operator|(
name|cacheKey
operator|==
literal|null
operator|||
name|metadataCache
operator|==
literal|null
operator|)
condition|?
literal|null
else|:
name|metadataCache
operator|.
name|getFileMetadata
argument_list|(
name|cacheKey
argument_list|)
decl_stmt|;
if|if
condition|(
name|footerData
operator|!=
literal|null
condition|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isInfoEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Found the footer in cache for "
operator|+
name|cacheKey
argument_list|)
expr_stmt|;
block|}
try|try
block|{
return|return
name|ParquetFileReader
operator|.
name|readFooter
argument_list|(
operator|new
name|ParquetFooterInputFromCache
argument_list|(
name|footerData
argument_list|)
argument_list|,
name|filter
argument_list|)
return|;
block|}
finally|finally
block|{
name|metadataCache
operator|.
name|decRefBuffer
argument_list|(
name|footerData
argument_list|)
expr_stmt|;
block|}
block|}
specifier|final
name|FileSystem
name|fs
init|=
name|file
operator|.
name|getFileSystem
argument_list|(
name|configuration
argument_list|)
decl_stmt|;
specifier|final
name|FileStatus
name|stat
init|=
name|fs
operator|.
name|getFileStatus
argument_list|(
name|file
argument_list|)
decl_stmt|;
if|if
condition|(
name|cacheKey
operator|==
literal|null
operator|||
name|metadataCache
operator|==
literal|null
condition|)
block|{
return|return
name|readFooterFromFile
argument_list|(
name|file
argument_list|,
name|fs
argument_list|,
name|stat
argument_list|,
name|filter
argument_list|)
return|;
block|}
comment|// To avoid reading the footer twice, we will cache it first and then read from cache.
comment|// Parquet calls protobuf methods directly on the stream and we can't get bytes after the fact.
try|try
init|(
name|SeekableInputStream
name|stream
init|=
name|HadoopStreams
operator|.
name|wrap
argument_list|(
name|fs
operator|.
name|open
argument_list|(
name|file
argument_list|)
argument_list|)
init|)
block|{
name|long
name|footerLengthIndex
init|=
name|stat
operator|.
name|getLen
argument_list|()
operator|-
name|ParquetFooterInputFromCache
operator|.
name|FOOTER_LENGTH_SIZE
operator|-
name|ParquetFileWriter
operator|.
name|MAGIC
operator|.
name|length
decl_stmt|;
name|stream
operator|.
name|seek
argument_list|(
name|footerLengthIndex
argument_list|)
expr_stmt|;
name|int
name|footerLength
init|=
name|BytesUtils
operator|.
name|readIntLittleEndian
argument_list|(
name|stream
argument_list|)
decl_stmt|;
name|stream
operator|.
name|seek
argument_list|(
name|footerLengthIndex
operator|-
name|footerLength
argument_list|)
expr_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isInfoEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Caching the footer of length "
operator|+
name|footerLength
operator|+
literal|" for "
operator|+
name|cacheKey
argument_list|)
expr_stmt|;
block|}
name|footerData
operator|=
name|metadataCache
operator|.
name|putFileMetadata
argument_list|(
name|cacheKey
argument_list|,
name|footerLength
argument_list|,
name|stream
argument_list|)
expr_stmt|;
try|try
block|{
return|return
name|ParquetFileReader
operator|.
name|readFooter
argument_list|(
operator|new
name|ParquetFooterInputFromCache
argument_list|(
name|footerData
argument_list|)
argument_list|,
name|filter
argument_list|)
return|;
block|}
finally|finally
block|{
name|metadataCache
operator|.
name|decRefBuffer
argument_list|(
name|footerData
argument_list|)
expr_stmt|;
block|}
block|}
block|}
specifier|private
name|ParquetMetadata
name|readFooterFromFile
parameter_list|(
specifier|final
name|Path
name|file
parameter_list|,
specifier|final
name|FileSystem
name|fs
parameter_list|,
specifier|final
name|FileStatus
name|stat
parameter_list|,
name|MetadataFilter
name|filter
parameter_list|)
throws|throws
name|IOException
block|{
name|InputFile
name|inputFile
init|=
operator|new
name|InputFile
argument_list|()
block|{
annotation|@
name|Override
specifier|public
name|SeekableInputStream
name|newStream
parameter_list|()
throws|throws
name|IOException
block|{
return|return
name|HadoopStreams
operator|.
name|wrap
argument_list|(
name|fs
operator|.
name|open
argument_list|(
name|file
argument_list|)
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|public
name|long
name|getLength
parameter_list|()
throws|throws
name|IOException
block|{
return|return
name|stat
operator|.
name|getLen
argument_list|()
return|;
block|}
block|}
decl_stmt|;
return|return
name|ParquetFileReader
operator|.
name|readFooter
argument_list|(
name|inputFile
argument_list|,
name|filter
argument_list|)
return|;
block|}
specifier|private
name|FileMetadataCache
name|metadataCache
decl_stmt|;
specifier|private
name|DataCache
name|cache
decl_stmt|;
specifier|private
name|Configuration
name|cacheConf
decl_stmt|;
annotation|@
name|Override
specifier|public
name|boolean
name|next
parameter_list|(
name|NullWritable
name|nullWritable
parameter_list|,
name|VectorizedRowBatch
name|vectorizedRowBatch
parameter_list|)
throws|throws
name|IOException
block|{
return|return
name|nextBatch
argument_list|(
name|vectorizedRowBatch
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|public
name|NullWritable
name|createKey
parameter_list|()
block|{
return|return
name|NullWritable
operator|.
name|get
argument_list|()
return|;
block|}
annotation|@
name|Override
specifier|public
name|VectorizedRowBatch
name|createValue
parameter_list|()
block|{
return|return
name|rbCtx
operator|.
name|createVectorizedRowBatch
argument_list|()
return|;
block|}
annotation|@
name|Override
specifier|public
name|long
name|getPos
parameter_list|()
throws|throws
name|IOException
block|{
comment|//TODO
return|return
literal|0
return|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|close
parameter_list|()
throws|throws
name|IOException
block|{
if|if
condition|(
name|cacheFsPath
operator|!=
literal|null
condition|)
block|{
name|LlapCacheAwareFs
operator|.
name|unregisterFile
argument_list|(
name|cacheFsPath
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|reader
operator|!=
literal|null
condition|)
block|{
name|reader
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
annotation|@
name|Override
specifier|public
name|float
name|getProgress
parameter_list|()
throws|throws
name|IOException
block|{
comment|//TODO
return|return
literal|0
return|;
block|}
comment|/**    * Advances to the next batch of rows. Returns false if there are no more.    */
specifier|private
name|boolean
name|nextBatch
parameter_list|(
name|VectorizedRowBatch
name|columnarBatch
parameter_list|)
throws|throws
name|IOException
block|{
name|columnarBatch
operator|.
name|reset
argument_list|()
expr_stmt|;
if|if
condition|(
name|rowsReturned
operator|>=
name|totalRowCount
condition|)
block|{
return|return
literal|false
return|;
block|}
comment|// Add partition cols if necessary (see VectorizedOrcInputFormat for details).
if|if
condition|(
name|partitionValues
operator|!=
literal|null
condition|)
block|{
name|rbCtx
operator|.
name|addPartitionColsToBatch
argument_list|(
name|columnarBatch
argument_list|,
name|partitionValues
argument_list|)
expr_stmt|;
block|}
name|checkEndOfRowGroup
argument_list|()
expr_stmt|;
name|int
name|num
init|=
operator|(
name|int
operator|)
name|Math
operator|.
name|min
argument_list|(
name|VectorizedRowBatch
operator|.
name|DEFAULT_SIZE
argument_list|,
name|totalCountLoadedSoFar
operator|-
name|rowsReturned
argument_list|)
decl_stmt|;
if|if
condition|(
name|colsToInclude
operator|.
name|size
argument_list|()
operator|>
literal|0
condition|)
block|{
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|columnReaders
operator|.
name|length
condition|;
operator|++
name|i
control|)
block|{
if|if
condition|(
name|columnReaders
index|[
name|i
index|]
operator|==
literal|null
condition|)
block|{
continue|continue;
block|}
name|columnarBatch
operator|.
name|cols
index|[
name|colsToInclude
operator|.
name|get
argument_list|(
name|i
argument_list|)
index|]
operator|.
name|isRepeating
operator|=
literal|true
expr_stmt|;
name|columnReaders
index|[
name|i
index|]
operator|.
name|readBatch
argument_list|(
name|num
argument_list|,
name|columnarBatch
operator|.
name|cols
index|[
name|colsToInclude
operator|.
name|get
argument_list|(
name|i
argument_list|)
index|]
argument_list|,
name|columnTypesList
operator|.
name|get
argument_list|(
name|colsToInclude
operator|.
name|get
argument_list|(
name|i
argument_list|)
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
name|rowsReturned
operator|+=
name|num
expr_stmt|;
name|columnarBatch
operator|.
name|size
operator|=
name|num
expr_stmt|;
return|return
literal|true
return|;
block|}
specifier|private
name|void
name|checkEndOfRowGroup
parameter_list|()
throws|throws
name|IOException
block|{
if|if
condition|(
name|rowsReturned
operator|!=
name|totalCountLoadedSoFar
condition|)
block|{
return|return;
block|}
name|PageReadStore
name|pages
init|=
name|reader
operator|.
name|readNextRowGroup
argument_list|()
decl_stmt|;
if|if
condition|(
name|pages
operator|==
literal|null
condition|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"expecting more rows but reached last block. Read "
operator|+
name|rowsReturned
operator|+
literal|" out of "
operator|+
name|totalRowCount
argument_list|)
throw|;
block|}
name|List
argument_list|<
name|ColumnDescriptor
argument_list|>
name|columns
init|=
name|requestedSchema
operator|.
name|getColumns
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|Type
argument_list|>
name|types
init|=
name|requestedSchema
operator|.
name|getFields
argument_list|()
decl_stmt|;
name|columnReaders
operator|=
operator|new
name|VectorizedColumnReader
index|[
name|columns
operator|.
name|size
argument_list|()
index|]
expr_stmt|;
if|if
condition|(
operator|!
name|ColumnProjectionUtils
operator|.
name|isReadAllColumns
argument_list|(
name|jobConf
argument_list|)
condition|)
block|{
comment|//certain queries like select count(*) from table do not have
comment|//any projected columns and still have isReadAllColumns as false
comment|//in such cases columnReaders are not needed
comment|//However, if colsToInclude is not empty we should initialize each columnReader
if|if
condition|(
operator|!
name|colsToInclude
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|types
operator|.
name|size
argument_list|()
condition|;
operator|++
name|i
control|)
block|{
name|columnReaders
index|[
name|i
index|]
operator|=
name|buildVectorizedParquetReader
argument_list|(
name|columnTypesList
operator|.
name|get
argument_list|(
name|colsToInclude
operator|.
name|get
argument_list|(
name|i
argument_list|)
argument_list|)
argument_list|,
name|types
operator|.
name|get
argument_list|(
name|i
argument_list|)
argument_list|,
name|pages
argument_list|,
name|requestedSchema
operator|.
name|getColumns
argument_list|()
argument_list|,
name|skipTimestampConversion
argument_list|,
literal|0
argument_list|)
expr_stmt|;
block|}
block|}
block|}
else|else
block|{
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|types
operator|.
name|size
argument_list|()
condition|;
operator|++
name|i
control|)
block|{
name|columnReaders
index|[
name|i
index|]
operator|=
name|buildVectorizedParquetReader
argument_list|(
name|columnTypesList
operator|.
name|get
argument_list|(
name|i
argument_list|)
argument_list|,
name|types
operator|.
name|get
argument_list|(
name|i
argument_list|)
argument_list|,
name|pages
argument_list|,
name|requestedSchema
operator|.
name|getColumns
argument_list|()
argument_list|,
name|skipTimestampConversion
argument_list|,
literal|0
argument_list|)
expr_stmt|;
block|}
block|}
name|totalCountLoadedSoFar
operator|+=
name|pages
operator|.
name|getRowCount
argument_list|()
expr_stmt|;
block|}
specifier|private
name|List
argument_list|<
name|ColumnDescriptor
argument_list|>
name|getAllColumnDescriptorByType
parameter_list|(
name|int
name|depth
parameter_list|,
name|Type
name|type
parameter_list|,
name|List
argument_list|<
name|ColumnDescriptor
argument_list|>
name|columns
parameter_list|)
throws|throws
name|ParquetRuntimeException
block|{
name|List
argument_list|<
name|ColumnDescriptor
argument_list|>
name|res
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|()
decl_stmt|;
for|for
control|(
name|ColumnDescriptor
name|descriptor
range|:
name|columns
control|)
block|{
if|if
condition|(
name|depth
operator|>=
name|descriptor
operator|.
name|getPath
argument_list|()
operator|.
name|length
condition|)
block|{
throw|throw
operator|new
name|InvalidSchemaException
argument_list|(
literal|"Corrupted Parquet schema"
argument_list|)
throw|;
block|}
if|if
condition|(
name|type
operator|.
name|getName
argument_list|()
operator|.
name|equals
argument_list|(
name|descriptor
operator|.
name|getPath
argument_list|()
index|[
name|depth
index|]
argument_list|)
condition|)
block|{
name|res
operator|.
name|add
argument_list|(
name|descriptor
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|res
return|;
block|}
comment|// Build VectorizedParquetColumnReader via Hive typeInfo and Parquet schema
specifier|private
name|VectorizedColumnReader
name|buildVectorizedParquetReader
parameter_list|(
name|TypeInfo
name|typeInfo
parameter_list|,
name|Type
name|type
parameter_list|,
name|PageReadStore
name|pages
parameter_list|,
name|List
argument_list|<
name|ColumnDescriptor
argument_list|>
name|columnDescriptors
parameter_list|,
name|boolean
name|skipTimestampConversion
parameter_list|,
name|int
name|depth
parameter_list|)
throws|throws
name|IOException
block|{
name|List
argument_list|<
name|ColumnDescriptor
argument_list|>
name|descriptors
init|=
name|getAllColumnDescriptorByType
argument_list|(
name|depth
argument_list|,
name|type
argument_list|,
name|columnDescriptors
argument_list|)
decl_stmt|;
switch|switch
condition|(
name|typeInfo
operator|.
name|getCategory
argument_list|()
condition|)
block|{
case|case
name|PRIMITIVE
case|:
if|if
condition|(
name|columnDescriptors
operator|==
literal|null
operator|||
name|columnDescriptors
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
literal|"Failed to find related Parquet column descriptor with type "
operator|+
name|type
argument_list|)
throw|;
block|}
else|else
block|{
return|return
operator|new
name|VectorizedPrimitiveColumnReader
argument_list|(
name|descriptors
operator|.
name|get
argument_list|(
literal|0
argument_list|)
argument_list|,
name|pages
operator|.
name|getPageReader
argument_list|(
name|descriptors
operator|.
name|get
argument_list|(
literal|0
argument_list|)
argument_list|)
argument_list|,
name|skipTimestampConversion
argument_list|,
name|type
argument_list|)
return|;
block|}
case|case
name|STRUCT
case|:
name|StructTypeInfo
name|structTypeInfo
init|=
operator|(
name|StructTypeInfo
operator|)
name|typeInfo
decl_stmt|;
name|List
argument_list|<
name|VectorizedColumnReader
argument_list|>
name|fieldReaders
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|TypeInfo
argument_list|>
name|fieldTypes
init|=
name|structTypeInfo
operator|.
name|getAllStructFieldTypeInfos
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|Type
argument_list|>
name|types
init|=
name|type
operator|.
name|asGroupType
argument_list|()
operator|.
name|getFields
argument_list|()
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|fieldTypes
operator|.
name|size
argument_list|()
condition|;
name|i
operator|++
control|)
block|{
name|VectorizedColumnReader
name|r
init|=
name|buildVectorizedParquetReader
argument_list|(
name|fieldTypes
operator|.
name|get
argument_list|(
name|i
argument_list|)
argument_list|,
name|types
operator|.
name|get
argument_list|(
name|i
argument_list|)
argument_list|,
name|pages
argument_list|,
name|descriptors
argument_list|,
name|skipTimestampConversion
argument_list|,
name|depth
operator|+
literal|1
argument_list|)
decl_stmt|;
if|if
condition|(
name|r
operator|!=
literal|null
condition|)
block|{
name|fieldReaders
operator|.
name|add
argument_list|(
name|r
argument_list|)
expr_stmt|;
block|}
else|else
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
literal|"Fail to build Parquet vectorized reader based on Hive type "
operator|+
name|fieldTypes
operator|.
name|get
argument_list|(
name|i
argument_list|)
operator|.
name|getTypeName
argument_list|()
operator|+
literal|" and Parquet type"
operator|+
name|types
operator|.
name|get
argument_list|(
name|i
argument_list|)
operator|.
name|toString
argument_list|()
argument_list|)
throw|;
block|}
block|}
return|return
operator|new
name|VectorizedStructColumnReader
argument_list|(
name|fieldReaders
argument_list|)
return|;
case|case
name|LIST
case|:
name|checkListColumnSupport
argument_list|(
operator|(
operator|(
name|ListTypeInfo
operator|)
name|typeInfo
operator|)
operator|.
name|getListElementTypeInfo
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|columnDescriptors
operator|==
literal|null
operator|||
name|columnDescriptors
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
literal|"Failed to find related Parquet column descriptor with type "
operator|+
name|type
argument_list|)
throw|;
block|}
return|return
operator|new
name|VectorizedListColumnReader
argument_list|(
name|descriptors
operator|.
name|get
argument_list|(
literal|0
argument_list|)
argument_list|,
name|pages
operator|.
name|getPageReader
argument_list|(
name|descriptors
operator|.
name|get
argument_list|(
literal|0
argument_list|)
argument_list|)
argument_list|,
name|skipTimestampConversion
argument_list|,
name|type
argument_list|)
return|;
case|case
name|MAP
case|:
case|case
name|UNION
case|:
default|default:
throw|throw
operator|new
name|RuntimeException
argument_list|(
literal|"Unsupported category "
operator|+
name|typeInfo
operator|.
name|getCategory
argument_list|()
operator|.
name|name
argument_list|()
argument_list|)
throw|;
block|}
block|}
comment|/**    * Check if the element type in list is supported by vectorization read.    * Supported type: INT, BYTE, SHORT, DATE, INTERVAL_YEAR_MONTH, LONG, BOOLEAN, DOUBLE, BINARY, STRING, CHAR, VARCHAR,    *                 FLOAT, DECIMAL    */
specifier|private
name|void
name|checkListColumnSupport
parameter_list|(
name|TypeInfo
name|elementType
parameter_list|)
block|{
if|if
condition|(
name|elementType
operator|instanceof
name|PrimitiveTypeInfo
condition|)
block|{
switch|switch
condition|(
operator|(
operator|(
name|PrimitiveTypeInfo
operator|)
name|elementType
operator|)
operator|.
name|getPrimitiveCategory
argument_list|()
condition|)
block|{
case|case
name|INTERVAL_DAY_TIME
case|:
case|case
name|TIMESTAMP
case|:
throw|throw
operator|new
name|RuntimeException
argument_list|(
literal|"Unsupported primitive type used in list:: "
operator|+
name|elementType
argument_list|)
throw|;
block|}
block|}
else|else
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
literal|"Unsupported type used in list:"
operator|+
name|elementType
argument_list|)
throw|;
block|}
block|}
block|}
end_class

end_unit

