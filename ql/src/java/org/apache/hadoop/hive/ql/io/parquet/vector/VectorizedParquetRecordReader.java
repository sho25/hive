begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/**  * Licensed under the Apache License, Version 2.0 (the "License");  * you may not use this file except in compliance with the License.  * You may obtain a copy of the License at  *  * http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|parquet
operator|.
name|vector
package|;
end_package

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|annotations
operator|.
name|VisibleForTesting
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|Utilities
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|vector
operator|.
name|VectorizedRowBatch
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|vector
operator|.
name|VectorizedRowBatchCtx
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|IOConstants
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|parquet
operator|.
name|ParquetRecordReaderBase
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|parquet
operator|.
name|ProjectionPusher
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|parquet
operator|.
name|read
operator|.
name|DataWritableReadSupport
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|ColumnProjectionUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|SerDeStats
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|typeinfo
operator|.
name|TypeInfo
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|NullWritable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|JobConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|RecordReader
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|InputSplit
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|parquet
operator|.
name|column
operator|.
name|ColumnDescriptor
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|parquet
operator|.
name|column
operator|.
name|page
operator|.
name|PageReadStore
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|parquet
operator|.
name|filter2
operator|.
name|compat
operator|.
name|FilterCompat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|parquet
operator|.
name|hadoop
operator|.
name|ParquetFileReader
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|parquet
operator|.
name|hadoop
operator|.
name|ParquetInputSplit
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|parquet
operator|.
name|hadoop
operator|.
name|metadata
operator|.
name|BlockMetaData
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|parquet
operator|.
name|hadoop
operator|.
name|metadata
operator|.
name|ParquetMetadata
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|parquet
operator|.
name|schema
operator|.
name|MessageType
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|parquet
operator|.
name|schema
operator|.
name|Type
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|Logger
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|LoggerFactory
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Arrays
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashSet
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Set
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|parquet
operator|.
name|filter2
operator|.
name|compat
operator|.
name|RowGroupFilter
operator|.
name|filterRowGroups
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|parquet
operator|.
name|format
operator|.
name|converter
operator|.
name|ParquetMetadataConverter
operator|.
name|NO_FILTER
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|parquet
operator|.
name|format
operator|.
name|converter
operator|.
name|ParquetMetadataConverter
operator|.
name|range
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|parquet
operator|.
name|hadoop
operator|.
name|ParquetFileReader
operator|.
name|readFooter
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|parquet
operator|.
name|hadoop
operator|.
name|ParquetInputFormat
operator|.
name|getFilter
import|;
end_import

begin_comment
comment|/**  * This reader is used to read a batch of record from inputsplit, part of the code is referred  * from Apache Spark and Apache Parquet.  */
end_comment

begin_class
specifier|public
class|class
name|VectorizedParquetRecordReader
extends|extends
name|ParquetRecordReaderBase
implements|implements
name|RecordReader
argument_list|<
name|NullWritable
argument_list|,
name|VectorizedRowBatch
argument_list|>
block|{
specifier|public
specifier|static
specifier|final
name|Logger
name|LOG
init|=
name|LoggerFactory
operator|.
name|getLogger
argument_list|(
name|VectorizedParquetRecordReader
operator|.
name|class
argument_list|)
decl_stmt|;
specifier|private
name|List
argument_list|<
name|Integer
argument_list|>
name|colsToInclude
decl_stmt|;
specifier|protected
name|MessageType
name|fileSchema
decl_stmt|;
specifier|protected
name|MessageType
name|requestedSchema
decl_stmt|;
specifier|private
name|List
argument_list|<
name|String
argument_list|>
name|columnNamesList
decl_stmt|;
specifier|private
name|List
argument_list|<
name|TypeInfo
argument_list|>
name|columnTypesList
decl_stmt|;
specifier|private
name|VectorizedRowBatchCtx
name|rbCtx
decl_stmt|;
comment|/**    * For each request column, the reader to read this column. This is NULL if this column    * is missing from the file, in which case we populate the attribute with NULL.    */
specifier|private
name|VectorizedColumnReader
index|[]
name|columnReaders
decl_stmt|;
comment|/**    * The number of rows that have been returned.    */
specifier|private
name|long
name|rowsReturned
decl_stmt|;
comment|/**    * The number of rows that have been reading, including the current in flight row group.    */
specifier|private
name|long
name|totalCountLoadedSoFar
init|=
literal|0
decl_stmt|;
comment|/**    * The total number of rows this RecordReader will eventually read. The sum of the    * rows of all the row groups.    */
specifier|protected
name|long
name|totalRowCount
decl_stmt|;
annotation|@
name|VisibleForTesting
specifier|public
name|VectorizedParquetRecordReader
parameter_list|(
name|InputSplit
name|inputSplit
parameter_list|,
name|JobConf
name|conf
parameter_list|)
block|{
try|try
block|{
name|serDeStats
operator|=
operator|new
name|SerDeStats
argument_list|()
expr_stmt|;
name|projectionPusher
operator|=
operator|new
name|ProjectionPusher
argument_list|()
expr_stmt|;
name|initialize
argument_list|(
name|inputSplit
argument_list|,
name|conf
argument_list|)
expr_stmt|;
name|colsToInclude
operator|=
name|ColumnProjectionUtils
operator|.
name|getReadColumnIDs
argument_list|(
name|conf
argument_list|)
expr_stmt|;
name|rbCtx
operator|=
name|Utilities
operator|.
name|getVectorizedRowBatchCtx
argument_list|(
name|conf
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Throwable
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Failed to create the vectorized reader due to exception "
operator|+
name|e
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|RuntimeException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|VectorizedParquetRecordReader
parameter_list|(
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|InputSplit
name|oldInputSplit
parameter_list|,
name|JobConf
name|conf
parameter_list|)
block|{
try|try
block|{
name|serDeStats
operator|=
operator|new
name|SerDeStats
argument_list|()
expr_stmt|;
name|projectionPusher
operator|=
operator|new
name|ProjectionPusher
argument_list|()
expr_stmt|;
name|initialize
argument_list|(
name|getSplit
argument_list|(
name|oldInputSplit
argument_list|,
name|conf
argument_list|)
argument_list|,
name|conf
argument_list|)
expr_stmt|;
name|colsToInclude
operator|=
name|ColumnProjectionUtils
operator|.
name|getReadColumnIDs
argument_list|(
name|conf
argument_list|)
expr_stmt|;
name|rbCtx
operator|=
name|Utilities
operator|.
name|getVectorizedRowBatchCtx
argument_list|(
name|conf
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Throwable
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Failed to create the vectorized reader due to exception "
operator|+
name|e
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|RuntimeException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|void
name|initialize
parameter_list|(
name|InputSplit
name|oldSplit
parameter_list|,
name|JobConf
name|configuration
parameter_list|)
throws|throws
name|IOException
throws|,
name|InterruptedException
block|{
name|jobConf
operator|=
name|configuration
expr_stmt|;
name|ParquetMetadata
name|footer
decl_stmt|;
name|List
argument_list|<
name|BlockMetaData
argument_list|>
name|blocks
decl_stmt|;
name|ParquetInputSplit
name|split
init|=
operator|(
name|ParquetInputSplit
operator|)
name|oldSplit
decl_stmt|;
name|boolean
name|indexAccess
init|=
name|configuration
operator|.
name|getBoolean
argument_list|(
name|DataWritableReadSupport
operator|.
name|PARQUET_COLUMN_INDEX_ACCESS
argument_list|,
literal|false
argument_list|)
decl_stmt|;
name|this
operator|.
name|file
operator|=
name|split
operator|.
name|getPath
argument_list|()
expr_stmt|;
name|long
index|[]
name|rowGroupOffsets
init|=
name|split
operator|.
name|getRowGroupOffsets
argument_list|()
decl_stmt|;
name|String
name|columnNames
init|=
name|configuration
operator|.
name|get
argument_list|(
name|IOConstants
operator|.
name|COLUMNS
argument_list|)
decl_stmt|;
name|columnNamesList
operator|=
name|DataWritableReadSupport
operator|.
name|getColumnNames
argument_list|(
name|columnNames
argument_list|)
expr_stmt|;
name|String
name|columnTypes
init|=
name|configuration
operator|.
name|get
argument_list|(
name|IOConstants
operator|.
name|COLUMNS_TYPES
argument_list|)
decl_stmt|;
name|columnTypesList
operator|=
name|DataWritableReadSupport
operator|.
name|getColumnTypes
argument_list|(
name|columnTypes
argument_list|)
expr_stmt|;
comment|// if task.side.metadata is set, rowGroupOffsets is null
if|if
condition|(
name|rowGroupOffsets
operator|==
literal|null
condition|)
block|{
comment|//TODO check whether rowGroupOffSets can be null
comment|// then we need to apply the predicate push down filter
name|footer
operator|=
name|readFooter
argument_list|(
name|configuration
argument_list|,
name|file
argument_list|,
name|range
argument_list|(
name|split
operator|.
name|getStart
argument_list|()
argument_list|,
name|split
operator|.
name|getEnd
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
name|MessageType
name|fileSchema
init|=
name|footer
operator|.
name|getFileMetaData
argument_list|()
operator|.
name|getSchema
argument_list|()
decl_stmt|;
name|FilterCompat
operator|.
name|Filter
name|filter
init|=
name|getFilter
argument_list|(
name|configuration
argument_list|)
decl_stmt|;
name|blocks
operator|=
name|filterRowGroups
argument_list|(
name|filter
argument_list|,
name|footer
operator|.
name|getBlocks
argument_list|()
argument_list|,
name|fileSchema
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// otherwise we find the row groups that were selected on the client
name|footer
operator|=
name|readFooter
argument_list|(
name|configuration
argument_list|,
name|file
argument_list|,
name|NO_FILTER
argument_list|)
expr_stmt|;
name|Set
argument_list|<
name|Long
argument_list|>
name|offsets
init|=
operator|new
name|HashSet
argument_list|<>
argument_list|()
decl_stmt|;
for|for
control|(
name|long
name|offset
range|:
name|rowGroupOffsets
control|)
block|{
name|offsets
operator|.
name|add
argument_list|(
name|offset
argument_list|)
expr_stmt|;
block|}
name|blocks
operator|=
operator|new
name|ArrayList
argument_list|<>
argument_list|()
expr_stmt|;
for|for
control|(
name|BlockMetaData
name|block
range|:
name|footer
operator|.
name|getBlocks
argument_list|()
control|)
block|{
if|if
condition|(
name|offsets
operator|.
name|contains
argument_list|(
name|block
operator|.
name|getStartingPos
argument_list|()
argument_list|)
condition|)
block|{
name|blocks
operator|.
name|add
argument_list|(
name|block
argument_list|)
expr_stmt|;
block|}
block|}
comment|// verify we found them all
if|if
condition|(
name|blocks
operator|.
name|size
argument_list|()
operator|!=
name|rowGroupOffsets
operator|.
name|length
condition|)
block|{
name|long
index|[]
name|foundRowGroupOffsets
init|=
operator|new
name|long
index|[
name|footer
operator|.
name|getBlocks
argument_list|()
operator|.
name|size
argument_list|()
index|]
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|foundRowGroupOffsets
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|foundRowGroupOffsets
index|[
name|i
index|]
operator|=
name|footer
operator|.
name|getBlocks
argument_list|()
operator|.
name|get
argument_list|(
name|i
argument_list|)
operator|.
name|getStartingPos
argument_list|()
expr_stmt|;
block|}
comment|// this should never happen.
comment|// provide a good error message in case there's a bug
throw|throw
operator|new
name|IllegalStateException
argument_list|(
literal|"All the offsets listed in the split should be found in the file."
operator|+
literal|" expected: "
operator|+
name|Arrays
operator|.
name|toString
argument_list|(
name|rowGroupOffsets
argument_list|)
operator|+
literal|" found: "
operator|+
name|blocks
operator|+
literal|" out of: "
operator|+
name|Arrays
operator|.
name|toString
argument_list|(
name|foundRowGroupOffsets
argument_list|)
operator|+
literal|" in range "
operator|+
name|split
operator|.
name|getStart
argument_list|()
operator|+
literal|", "
operator|+
name|split
operator|.
name|getEnd
argument_list|()
argument_list|)
throw|;
block|}
block|}
for|for
control|(
name|BlockMetaData
name|block
range|:
name|blocks
control|)
block|{
name|this
operator|.
name|totalRowCount
operator|+=
name|block
operator|.
name|getRowCount
argument_list|()
expr_stmt|;
block|}
name|this
operator|.
name|fileSchema
operator|=
name|footer
operator|.
name|getFileMetaData
argument_list|()
operator|.
name|getSchema
argument_list|()
expr_stmt|;
name|MessageType
name|tableSchema
decl_stmt|;
if|if
condition|(
name|indexAccess
condition|)
block|{
name|List
argument_list|<
name|Integer
argument_list|>
name|indexSequence
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|()
decl_stmt|;
comment|// Generates a sequence list of indexes
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|columnNamesList
operator|.
name|size
argument_list|()
condition|;
name|i
operator|++
control|)
block|{
name|indexSequence
operator|.
name|add
argument_list|(
name|i
argument_list|)
expr_stmt|;
block|}
name|tableSchema
operator|=
name|DataWritableReadSupport
operator|.
name|getSchemaByIndex
argument_list|(
name|fileSchema
argument_list|,
name|columnNamesList
argument_list|,
name|indexSequence
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|tableSchema
operator|=
name|DataWritableReadSupport
operator|.
name|getSchemaByName
argument_list|(
name|fileSchema
argument_list|,
name|columnNamesList
argument_list|,
name|columnTypesList
argument_list|)
expr_stmt|;
block|}
name|List
argument_list|<
name|Integer
argument_list|>
name|indexColumnsWanted
init|=
name|ColumnProjectionUtils
operator|.
name|getReadColumnIDs
argument_list|(
name|configuration
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|ColumnProjectionUtils
operator|.
name|isReadAllColumns
argument_list|(
name|configuration
argument_list|)
operator|&&
operator|!
name|indexColumnsWanted
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|requestedSchema
operator|=
name|DataWritableReadSupport
operator|.
name|getSchemaByIndex
argument_list|(
name|tableSchema
argument_list|,
name|columnNamesList
argument_list|,
name|indexColumnsWanted
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|requestedSchema
operator|=
name|fileSchema
expr_stmt|;
block|}
name|this
operator|.
name|reader
operator|=
operator|new
name|ParquetFileReader
argument_list|(
name|configuration
argument_list|,
name|footer
operator|.
name|getFileMetaData
argument_list|()
argument_list|,
name|file
argument_list|,
name|blocks
argument_list|,
name|requestedSchema
operator|.
name|getColumns
argument_list|()
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|next
parameter_list|(
name|NullWritable
name|nullWritable
parameter_list|,
name|VectorizedRowBatch
name|vectorizedRowBatch
parameter_list|)
throws|throws
name|IOException
block|{
return|return
name|nextBatch
argument_list|(
name|vectorizedRowBatch
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|public
name|NullWritable
name|createKey
parameter_list|()
block|{
return|return
name|NullWritable
operator|.
name|get
argument_list|()
return|;
block|}
annotation|@
name|Override
specifier|public
name|VectorizedRowBatch
name|createValue
parameter_list|()
block|{
return|return
name|rbCtx
operator|.
name|createVectorizedRowBatch
argument_list|()
return|;
block|}
annotation|@
name|Override
specifier|public
name|long
name|getPos
parameter_list|()
throws|throws
name|IOException
block|{
comment|//TODO
return|return
literal|0
return|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|close
parameter_list|()
throws|throws
name|IOException
block|{   }
annotation|@
name|Override
specifier|public
name|float
name|getProgress
parameter_list|()
throws|throws
name|IOException
block|{
comment|//TODO
return|return
literal|0
return|;
block|}
comment|/**    * Advances to the next batch of rows. Returns false if there are no more.    */
specifier|private
name|boolean
name|nextBatch
parameter_list|(
name|VectorizedRowBatch
name|columnarBatch
parameter_list|)
throws|throws
name|IOException
block|{
name|columnarBatch
operator|.
name|reset
argument_list|()
expr_stmt|;
if|if
condition|(
name|rowsReturned
operator|>=
name|totalRowCount
condition|)
block|{
return|return
literal|false
return|;
block|}
name|checkEndOfRowGroup
argument_list|()
expr_stmt|;
name|int
name|num
init|=
operator|(
name|int
operator|)
name|Math
operator|.
name|min
argument_list|(
name|VectorizedRowBatch
operator|.
name|DEFAULT_SIZE
argument_list|,
name|totalCountLoadedSoFar
operator|-
name|rowsReturned
argument_list|)
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|columnReaders
operator|.
name|length
condition|;
operator|++
name|i
control|)
block|{
if|if
condition|(
name|columnReaders
index|[
name|i
index|]
operator|==
literal|null
condition|)
block|{
continue|continue;
block|}
name|columnarBatch
operator|.
name|cols
index|[
name|colsToInclude
operator|.
name|get
argument_list|(
name|i
argument_list|)
index|]
operator|.
name|isRepeating
operator|=
literal|true
expr_stmt|;
name|columnReaders
index|[
name|i
index|]
operator|.
name|readBatch
argument_list|(
name|num
argument_list|,
name|columnarBatch
operator|.
name|cols
index|[
name|colsToInclude
operator|.
name|get
argument_list|(
name|i
argument_list|)
index|]
argument_list|,
name|columnTypesList
operator|.
name|get
argument_list|(
name|colsToInclude
operator|.
name|get
argument_list|(
name|i
argument_list|)
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|rowsReturned
operator|+=
name|num
expr_stmt|;
name|columnarBatch
operator|.
name|size
operator|=
name|num
expr_stmt|;
return|return
literal|true
return|;
block|}
specifier|private
name|void
name|checkEndOfRowGroup
parameter_list|()
throws|throws
name|IOException
block|{
if|if
condition|(
name|rowsReturned
operator|!=
name|totalCountLoadedSoFar
condition|)
block|{
return|return;
block|}
name|PageReadStore
name|pages
init|=
name|reader
operator|.
name|readNextRowGroup
argument_list|()
decl_stmt|;
if|if
condition|(
name|pages
operator|==
literal|null
condition|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"expecting more rows but reached last block. Read "
operator|+
name|rowsReturned
operator|+
literal|" out of "
operator|+
name|totalRowCount
argument_list|)
throw|;
block|}
name|List
argument_list|<
name|ColumnDescriptor
argument_list|>
name|columns
init|=
name|requestedSchema
operator|.
name|getColumns
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|Type
argument_list|>
name|types
init|=
name|requestedSchema
operator|.
name|getFields
argument_list|()
decl_stmt|;
name|columnReaders
operator|=
operator|new
name|VectorizedColumnReader
index|[
name|columns
operator|.
name|size
argument_list|()
index|]
expr_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|columns
operator|.
name|size
argument_list|()
condition|;
operator|++
name|i
control|)
block|{
name|columnReaders
index|[
name|i
index|]
operator|=
operator|new
name|VectorizedColumnReader
argument_list|(
name|columns
operator|.
name|get
argument_list|(
name|i
argument_list|)
argument_list|,
name|pages
operator|.
name|getPageReader
argument_list|(
name|columns
operator|.
name|get
argument_list|(
name|i
argument_list|)
argument_list|)
argument_list|,
name|skipTimestampConversion
argument_list|,
name|types
operator|.
name|get
argument_list|(
name|i
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|totalCountLoadedSoFar
operator|+=
name|pages
operator|.
name|getRowCount
argument_list|()
expr_stmt|;
block|}
block|}
end_class

end_unit

