begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/*  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|spark
package|;
end_package

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Collection
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Iterator
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|LinkedList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|regex
operator|.
name|Pattern
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|annotations
operator|.
name|VisibleForTesting
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|base
operator|.
name|Strings
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|base
operator|.
name|Throwables
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|metrics
operator|.
name|common
operator|.
name|Metrics
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|metrics
operator|.
name|common
operator|.
name|MetricsConstant
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|spark
operator|.
name|Statistic
operator|.
name|SparkStatisticsNames
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|Logger
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|LoggerFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|CompilationOpContext
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|DriverContext
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|ErrorMsg
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|QueryPlan
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|QueryState
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|FileSinkOperator
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|JoinOperator
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|MapOperator
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|Operator
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|ReduceSinkOperator
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|ScriptOperator
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|Task
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|Utilities
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|spark
operator|.
name|Statistic
operator|.
name|SparkStatistic
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|spark
operator|.
name|Statistic
operator|.
name|SparkStatisticGroup
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|spark
operator|.
name|Statistic
operator|.
name|SparkStatistics
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|spark
operator|.
name|session
operator|.
name|SparkSession
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|spark
operator|.
name|session
operator|.
name|SparkSessionManager
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|spark
operator|.
name|session
operator|.
name|SparkSessionManagerImpl
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|spark
operator|.
name|status
operator|.
name|SparkJobRef
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|spark
operator|.
name|status
operator|.
name|SparkJobStatus
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|spark
operator|.
name|status
operator|.
name|SparkStageProgress
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|history
operator|.
name|HiveHistory
operator|.
name|Keys
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|log
operator|.
name|PerfLogger
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|HiveException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|BaseWork
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|MapWork
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|OperatorDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|ReduceWork
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|SparkWork
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|api
operator|.
name|StageType
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|session
operator|.
name|SessionState
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|session
operator|.
name|SessionState
operator|.
name|LogHelper
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|StringUtils
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|collect
operator|.
name|Lists
import|;
end_import

begin_class
specifier|public
class|class
name|SparkTask
extends|extends
name|Task
argument_list|<
name|SparkWork
argument_list|>
block|{
specifier|private
specifier|static
specifier|final
name|String
name|CLASS_NAME
init|=
name|SparkTask
operator|.
name|class
operator|.
name|getName
argument_list|()
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|Logger
name|LOG
init|=
name|LoggerFactory
operator|.
name|getLogger
argument_list|(
name|CLASS_NAME
argument_list|)
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|LogHelper
name|console
init|=
operator|new
name|LogHelper
argument_list|(
name|LOG
argument_list|)
decl_stmt|;
specifier|private
name|PerfLogger
name|perfLogger
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|long
name|serialVersionUID
init|=
literal|1L
decl_stmt|;
comment|// The id of the actual Spark job
specifier|private
specifier|transient
name|int
name|sparkJobID
decl_stmt|;
comment|// The id of the JobHandle used to track the actual Spark job
specifier|private
specifier|transient
name|String
name|sparkJobHandleId
decl_stmt|;
specifier|private
specifier|transient
name|SparkStatistics
name|sparkStatistics
decl_stmt|;
specifier|private
specifier|transient
name|long
name|submitTime
decl_stmt|;
specifier|private
specifier|transient
name|long
name|startTime
decl_stmt|;
specifier|private
specifier|transient
name|long
name|finishTime
decl_stmt|;
specifier|private
specifier|transient
name|int
name|succeededTaskCount
decl_stmt|;
specifier|private
specifier|transient
name|int
name|totalTaskCount
decl_stmt|;
specifier|private
specifier|transient
name|int
name|failedTaskCount
decl_stmt|;
specifier|private
specifier|transient
name|List
argument_list|<
name|Integer
argument_list|>
name|stageIds
decl_stmt|;
specifier|private
specifier|transient
name|SparkJobRef
name|jobRef
init|=
literal|null
decl_stmt|;
specifier|private
specifier|transient
name|boolean
name|isShutdown
init|=
literal|false
decl_stmt|;
specifier|private
specifier|transient
name|boolean
name|jobKilled
init|=
literal|false
decl_stmt|;
annotation|@
name|Override
specifier|public
name|void
name|initialize
parameter_list|(
name|QueryState
name|queryState
parameter_list|,
name|QueryPlan
name|queryPlan
parameter_list|,
name|DriverContext
name|driverContext
parameter_list|,
name|CompilationOpContext
name|opContext
parameter_list|)
block|{
name|super
operator|.
name|initialize
argument_list|(
name|queryState
argument_list|,
name|queryPlan
argument_list|,
name|driverContext
argument_list|,
name|opContext
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|int
name|execute
parameter_list|(
name|DriverContext
name|driverContext
parameter_list|)
block|{
name|int
name|rc
init|=
literal|0
decl_stmt|;
name|perfLogger
operator|=
name|SessionState
operator|.
name|getPerfLogger
argument_list|()
expr_stmt|;
name|SparkSession
name|sparkSession
init|=
literal|null
decl_stmt|;
name|SparkSessionManager
name|sparkSessionManager
init|=
literal|null
decl_stmt|;
try|try
block|{
name|printConfigInfo
argument_list|()
expr_stmt|;
name|sparkSessionManager
operator|=
name|SparkSessionManagerImpl
operator|.
name|getInstance
argument_list|()
expr_stmt|;
name|sparkSession
operator|=
name|SparkUtilities
operator|.
name|getSparkSession
argument_list|(
name|conf
argument_list|,
name|sparkSessionManager
argument_list|)
expr_stmt|;
name|SparkWork
name|sparkWork
init|=
name|getWork
argument_list|()
decl_stmt|;
name|sparkWork
operator|.
name|setRequiredCounterPrefix
argument_list|(
name|getOperatorCounters
argument_list|()
argument_list|)
expr_stmt|;
comment|// Submit the Spark job
name|perfLogger
operator|.
name|PerfLogBegin
argument_list|(
name|CLASS_NAME
argument_list|,
name|PerfLogger
operator|.
name|SPARK_SUBMIT_JOB
argument_list|)
expr_stmt|;
name|submitTime
operator|=
name|perfLogger
operator|.
name|getStartTime
argument_list|(
name|PerfLogger
operator|.
name|SPARK_SUBMIT_JOB
argument_list|)
expr_stmt|;
name|jobRef
operator|=
name|sparkSession
operator|.
name|submit
argument_list|(
name|driverContext
argument_list|,
name|sparkWork
argument_list|)
expr_stmt|;
name|perfLogger
operator|.
name|PerfLogEnd
argument_list|(
name|CLASS_NAME
argument_list|,
name|PerfLogger
operator|.
name|SPARK_SUBMIT_JOB
argument_list|)
expr_stmt|;
comment|// If the driver context has been shutdown (due to query cancellation) kill the Spark job
if|if
condition|(
name|driverContext
operator|.
name|isShutdown
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Killing Spark job"
argument_list|)
expr_stmt|;
name|killJob
argument_list|()
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Operation is cancelled."
argument_list|)
throw|;
block|}
comment|// Get the Job Handle id associated with the Spark job
name|sparkJobHandleId
operator|=
name|jobRef
operator|.
name|getJobId
argument_list|()
expr_stmt|;
comment|// Add Spark job handle id to the Hive History
name|addToHistory
argument_list|(
name|Keys
operator|.
name|SPARK_JOB_HANDLE_ID
argument_list|,
name|jobRef
operator|.
name|getJobId
argument_list|()
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Starting Spark job with job handle id "
operator|+
name|sparkJobHandleId
argument_list|)
expr_stmt|;
comment|// Get the application id of the Spark app
name|jobID
operator|=
name|jobRef
operator|.
name|getSparkJobStatus
argument_list|()
operator|.
name|getAppID
argument_list|()
expr_stmt|;
comment|// Start monitoring the Spark job, returns when the Spark job has completed / failed, or if
comment|// a timeout occurs
name|rc
operator|=
name|jobRef
operator|.
name|monitorJob
argument_list|()
expr_stmt|;
comment|// Get the id the Spark job that was launched, returns -1 if no Spark job was launched
name|sparkJobID
operator|=
name|jobRef
operator|.
name|getSparkJobStatus
argument_list|()
operator|.
name|getJobId
argument_list|()
expr_stmt|;
comment|// Add Spark job id to the Hive History
name|addToHistory
argument_list|(
name|Keys
operator|.
name|SPARK_JOB_ID
argument_list|,
name|Integer
operator|.
name|toString
argument_list|(
name|sparkJobID
argument_list|)
argument_list|)
expr_stmt|;
comment|// Get the final state of the Spark job and parses its job info
name|SparkJobStatus
name|sparkJobStatus
init|=
name|jobRef
operator|.
name|getSparkJobStatus
argument_list|()
decl_stmt|;
name|getSparkJobInfo
argument_list|(
name|sparkJobStatus
argument_list|)
expr_stmt|;
name|setSparkException
argument_list|(
name|sparkJobStatus
argument_list|,
name|rc
argument_list|)
expr_stmt|;
if|if
condition|(
name|rc
operator|==
literal|0
condition|)
block|{
name|sparkStatistics
operator|=
name|sparkJobStatus
operator|.
name|getSparkStatistics
argument_list|()
expr_stmt|;
name|printExcessiveGCWarning
argument_list|()
expr_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isInfoEnabled
argument_list|()
operator|&&
name|sparkStatistics
operator|!=
literal|null
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
name|sparkStatisticsToString
argument_list|(
name|sparkStatistics
argument_list|,
name|sparkJobID
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|LOG
operator|.
name|info
argument_list|(
literal|"Successfully completed Spark job["
operator|+
name|sparkJobID
operator|+
literal|"] with application ID "
operator|+
name|jobID
operator|+
literal|" and task ID "
operator|+
name|getId
argument_list|()
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|rc
operator|==
literal|2
condition|)
block|{
comment|// Cancel job if the monitor found job submission timeout.
comment|// TODO: If the timeout is because of lack of resources in the cluster, we should
comment|// ideally also cancel the app request here. But w/o facilities from Spark or YARN,
comment|// it's difficult to do it on hive side alone. See HIVE-12650.
name|LOG
operator|.
name|debug
argument_list|(
literal|"Failed to submit Spark job with job handle id "
operator|+
name|sparkJobHandleId
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Failed to submit Spark job for application id "
operator|+
operator|(
name|Strings
operator|.
name|isNullOrEmpty
argument_list|(
name|jobID
argument_list|)
condition|?
literal|"UNKNOWN"
else|:
name|jobID
operator|)
argument_list|)
expr_stmt|;
name|killJob
argument_list|()
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|rc
operator|==
literal|4
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"The spark job or one stage of it has too many tasks"
operator|+
literal|". Cancelling Spark job "
operator|+
name|sparkJobID
operator|+
literal|" with application ID "
operator|+
name|jobID
argument_list|)
expr_stmt|;
name|killJob
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
name|this
operator|.
name|jobID
operator|==
literal|null
condition|)
block|{
name|this
operator|.
name|jobID
operator|=
name|sparkJobStatus
operator|.
name|getAppID
argument_list|()
expr_stmt|;
block|}
name|sparkJobStatus
operator|.
name|cleanup
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|String
name|msg
init|=
literal|"Failed to execute spark task, with exception '"
operator|+
name|Utilities
operator|.
name|getNameMessage
argument_list|(
name|e
argument_list|)
operator|+
literal|"'"
decl_stmt|;
comment|// Has to use full name to make sure it does not conflict with
comment|// org.apache.commons.lang.StringUtils
name|console
operator|.
name|printError
argument_list|(
name|msg
argument_list|,
literal|"\n"
operator|+
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|error
argument_list|(
name|msg
argument_list|,
name|e
argument_list|)
expr_stmt|;
name|setException
argument_list|(
name|e
argument_list|)
expr_stmt|;
if|if
condition|(
name|e
operator|instanceof
name|HiveException
condition|)
block|{
name|HiveException
name|he
init|=
operator|(
name|HiveException
operator|)
name|e
decl_stmt|;
name|rc
operator|=
name|he
operator|.
name|getCanonicalErrorMsg
argument_list|()
operator|.
name|getErrorCode
argument_list|()
expr_stmt|;
block|}
else|else
block|{
name|rc
operator|=
literal|1
expr_stmt|;
block|}
block|}
finally|finally
block|{
name|startTime
operator|=
name|perfLogger
operator|.
name|getEndTime
argument_list|(
name|PerfLogger
operator|.
name|SPARK_SUBMIT_TO_RUNNING
argument_list|)
expr_stmt|;
comment|// The startTime may not be set if the sparkTask finished too fast,
comment|// because SparkJobMonitor will sleep for 1 second then check the state,
comment|// right after sleep, the spark job may be already completed.
comment|// In this case, set startTime the same as submitTime.
if|if
condition|(
name|startTime
operator|<
name|submitTime
condition|)
block|{
name|startTime
operator|=
name|submitTime
expr_stmt|;
block|}
name|finishTime
operator|=
name|perfLogger
operator|.
name|getEndTime
argument_list|(
name|PerfLogger
operator|.
name|SPARK_RUN_JOB
argument_list|)
expr_stmt|;
name|Utilities
operator|.
name|clearWork
argument_list|(
name|conf
argument_list|)
expr_stmt|;
if|if
condition|(
name|sparkSession
operator|!=
literal|null
operator|&&
name|sparkSessionManager
operator|!=
literal|null
condition|)
block|{
name|rc
operator|=
name|close
argument_list|(
name|rc
argument_list|)
expr_stmt|;
try|try
block|{
name|sparkSessionManager
operator|.
name|returnSession
argument_list|(
name|sparkSession
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|HiveException
name|ex
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Failed to return the session to SessionManager"
argument_list|,
name|ex
argument_list|)
expr_stmt|;
block|}
block|}
block|}
return|return
name|rc
return|;
block|}
comment|/**    * Use the Spark metrics and calculate how much task executione time was spent performing GC    * operations. If more than a defined threshold of time is spent, print out a warning on the    * console.    */
specifier|private
name|void
name|printExcessiveGCWarning
parameter_list|()
block|{
name|SparkStatisticGroup
name|sparkStatisticGroup
init|=
name|sparkStatistics
operator|.
name|getStatisticGroup
argument_list|(
name|SparkStatisticsNames
operator|.
name|SPARK_GROUP_NAME
argument_list|)
decl_stmt|;
if|if
condition|(
name|sparkStatisticGroup
operator|!=
literal|null
condition|)
block|{
name|long
name|taskDurationTime
init|=
name|Long
operator|.
name|parseLong
argument_list|(
name|sparkStatisticGroup
operator|.
name|getSparkStatistic
argument_list|(
name|SparkStatisticsNames
operator|.
name|TASK_DURATION_TIME
argument_list|)
operator|.
name|getValue
argument_list|()
argument_list|)
decl_stmt|;
name|long
name|jvmGCTime
init|=
name|Long
operator|.
name|parseLong
argument_list|(
name|sparkStatisticGroup
operator|.
name|getSparkStatistic
argument_list|(
name|SparkStatisticsNames
operator|.
name|JVM_GC_TIME
argument_list|)
operator|.
name|getValue
argument_list|()
argument_list|)
decl_stmt|;
comment|// Threshold percentage to trigger the GC warning
name|double
name|threshold
init|=
literal|0.1
decl_stmt|;
if|if
condition|(
name|jvmGCTime
operator|>
name|taskDurationTime
operator|*
name|threshold
condition|)
block|{
name|long
name|percentGcTime
init|=
name|Math
operator|.
name|round
argument_list|(
operator|(
name|double
operator|)
name|jvmGCTime
operator|/
name|taskDurationTime
operator|*
literal|100
argument_list|)
decl_stmt|;
name|String
name|gcWarning
init|=
name|String
operator|.
name|format
argument_list|(
literal|"WARNING: Spark Job[%s] Spent %s%% (%s ms / %s ms) of "
operator|+
literal|"task time in GC"
argument_list|,
name|sparkJobID
argument_list|,
name|percentGcTime
argument_list|,
name|jvmGCTime
argument_list|,
name|taskDurationTime
argument_list|)
decl_stmt|;
name|console
operator|.
name|printInfo
argument_list|(
name|gcWarning
argument_list|)
expr_stmt|;
block|}
block|}
block|}
specifier|private
name|void
name|addToHistory
parameter_list|(
name|Keys
name|key
parameter_list|,
name|String
name|value
parameter_list|)
block|{
if|if
condition|(
name|SessionState
operator|.
name|get
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|SessionState
operator|.
name|get
argument_list|()
operator|.
name|getHiveHistory
argument_list|()
operator|.
name|setQueryProperty
argument_list|(
name|queryState
operator|.
name|getQueryId
argument_list|()
argument_list|,
name|key
argument_list|,
name|value
argument_list|)
expr_stmt|;
block|}
block|}
annotation|@
name|VisibleForTesting
specifier|static
name|String
name|sparkStatisticsToString
parameter_list|(
name|SparkStatistics
name|sparkStatistic
parameter_list|,
name|int
name|sparkJobID
parameter_list|)
block|{
name|StringBuilder
name|sparkStatsString
init|=
operator|new
name|StringBuilder
argument_list|()
decl_stmt|;
name|sparkStatsString
operator|.
name|append
argument_list|(
literal|"\n\n"
argument_list|)
expr_stmt|;
name|sparkStatsString
operator|.
name|append
argument_list|(
name|String
operator|.
name|format
argument_list|(
literal|"=====Spark Job[%d] Statistics====="
argument_list|,
name|sparkJobID
argument_list|)
argument_list|)
expr_stmt|;
name|sparkStatsString
operator|.
name|append
argument_list|(
literal|"\n\n"
argument_list|)
expr_stmt|;
name|Iterator
argument_list|<
name|SparkStatisticGroup
argument_list|>
name|groupIterator
init|=
name|sparkStatistic
operator|.
name|getStatisticGroups
argument_list|()
decl_stmt|;
while|while
condition|(
name|groupIterator
operator|.
name|hasNext
argument_list|()
condition|)
block|{
name|SparkStatisticGroup
name|group
init|=
name|groupIterator
operator|.
name|next
argument_list|()
decl_stmt|;
name|sparkStatsString
operator|.
name|append
argument_list|(
name|group
operator|.
name|getGroupName
argument_list|()
argument_list|)
operator|.
name|append
argument_list|(
literal|"\n"
argument_list|)
expr_stmt|;
name|Iterator
argument_list|<
name|SparkStatistic
argument_list|>
name|statisticIterator
init|=
name|group
operator|.
name|getStatistics
argument_list|()
decl_stmt|;
while|while
condition|(
name|statisticIterator
operator|.
name|hasNext
argument_list|()
condition|)
block|{
name|SparkStatistic
name|statistic
init|=
name|statisticIterator
operator|.
name|next
argument_list|()
decl_stmt|;
name|sparkStatsString
operator|.
name|append
argument_list|(
literal|"\t"
argument_list|)
operator|.
name|append
argument_list|(
name|statistic
operator|.
name|getName
argument_list|()
argument_list|)
operator|.
name|append
argument_list|(
literal|": "
argument_list|)
operator|.
name|append
argument_list|(
name|statistic
operator|.
name|getValue
argument_list|()
argument_list|)
operator|.
name|append
argument_list|(
literal|"\n"
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|sparkStatsString
operator|.
name|toString
argument_list|()
return|;
block|}
comment|/**    * Close will move the temp files into the right place for the fetch    * task. If the job has failed it will clean up the files.    */
specifier|private
name|int
name|close
parameter_list|(
name|int
name|rc
parameter_list|)
block|{
try|try
block|{
name|List
argument_list|<
name|BaseWork
argument_list|>
name|ws
init|=
name|work
operator|.
name|getAllWork
argument_list|()
decl_stmt|;
for|for
control|(
name|BaseWork
name|w
range|:
name|ws
control|)
block|{
for|for
control|(
name|Operator
argument_list|<
name|?
argument_list|>
name|op
range|:
name|w
operator|.
name|getAllOperators
argument_list|()
control|)
block|{
name|op
operator|.
name|jobClose
argument_list|(
name|conf
argument_list|,
name|rc
operator|==
literal|0
argument_list|)
expr_stmt|;
block|}
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
comment|// jobClose needs to execute successfully otherwise fail task
if|if
condition|(
name|rc
operator|==
literal|0
condition|)
block|{
name|rc
operator|=
literal|3
expr_stmt|;
name|String
name|mesg
init|=
literal|"Job Commit failed with exception '"
operator|+
name|Utilities
operator|.
name|getNameMessage
argument_list|(
name|e
argument_list|)
operator|+
literal|"'"
decl_stmt|;
name|console
operator|.
name|printError
argument_list|(
name|mesg
argument_list|,
literal|"\n"
operator|+
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
name|setException
argument_list|(
name|e
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|rc
return|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|updateTaskMetrics
parameter_list|(
name|Metrics
name|metrics
parameter_list|)
block|{
name|metrics
operator|.
name|incrementCounter
argument_list|(
name|MetricsConstant
operator|.
name|HIVE_SPARK_TASKS
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|isMapRedTask
parameter_list|()
block|{
return|return
literal|true
return|;
block|}
annotation|@
name|Override
specifier|public
name|StageType
name|getType
parameter_list|()
block|{
return|return
name|StageType
operator|.
name|MAPRED
return|;
block|}
annotation|@
name|Override
specifier|public
name|String
name|getName
parameter_list|()
block|{
return|return
literal|"SPARK"
return|;
block|}
annotation|@
name|Override
specifier|public
name|Collection
argument_list|<
name|MapWork
argument_list|>
name|getMapWork
parameter_list|()
block|{
name|List
argument_list|<
name|MapWork
argument_list|>
name|result
init|=
name|Lists
operator|.
name|newArrayList
argument_list|()
decl_stmt|;
for|for
control|(
name|BaseWork
name|w
range|:
name|getWork
argument_list|()
operator|.
name|getRoots
argument_list|()
control|)
block|{
name|result
operator|.
name|add
argument_list|(
operator|(
name|MapWork
operator|)
name|w
argument_list|)
expr_stmt|;
block|}
return|return
name|result
return|;
block|}
annotation|@
name|Override
specifier|public
name|Operator
argument_list|<
name|?
extends|extends
name|OperatorDesc
argument_list|>
name|getReducer
parameter_list|(
name|MapWork
name|mapWork
parameter_list|)
block|{
name|List
argument_list|<
name|BaseWork
argument_list|>
name|children
init|=
name|getWork
argument_list|()
operator|.
name|getChildren
argument_list|(
name|mapWork
argument_list|)
decl_stmt|;
if|if
condition|(
name|children
operator|.
name|size
argument_list|()
operator|!=
literal|1
condition|)
block|{
return|return
literal|null
return|;
block|}
if|if
condition|(
operator|!
operator|(
name|children
operator|.
name|get
argument_list|(
literal|0
argument_list|)
operator|instanceof
name|ReduceWork
operator|)
condition|)
block|{
return|return
literal|null
return|;
block|}
return|return
operator|(
operator|(
name|ReduceWork
operator|)
name|children
operator|.
name|get
argument_list|(
literal|0
argument_list|)
operator|)
operator|.
name|getReducer
argument_list|()
return|;
block|}
specifier|public
name|int
name|getSparkJobID
parameter_list|()
block|{
return|return
name|sparkJobID
return|;
block|}
specifier|public
name|SparkStatistics
name|getSparkStatistics
parameter_list|()
block|{
return|return
name|sparkStatistics
return|;
block|}
specifier|public
name|int
name|getSucceededTaskCount
parameter_list|()
block|{
return|return
name|succeededTaskCount
return|;
block|}
specifier|public
name|int
name|getTotalTaskCount
parameter_list|()
block|{
return|return
name|totalTaskCount
return|;
block|}
specifier|public
name|int
name|getFailedTaskCount
parameter_list|()
block|{
return|return
name|failedTaskCount
return|;
block|}
specifier|public
name|List
argument_list|<
name|Integer
argument_list|>
name|getStageIds
parameter_list|()
block|{
return|return
name|stageIds
return|;
block|}
specifier|public
name|long
name|getStartTime
parameter_list|()
block|{
return|return
name|startTime
return|;
block|}
specifier|public
name|long
name|getSubmitTime
parameter_list|()
block|{
return|return
name|submitTime
return|;
block|}
specifier|public
name|long
name|getFinishTime
parameter_list|()
block|{
return|return
name|finishTime
return|;
block|}
specifier|public
name|boolean
name|isTaskShutdown
parameter_list|()
block|{
return|return
name|isShutdown
return|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|shutdown
parameter_list|()
block|{
name|super
operator|.
name|shutdown
argument_list|()
expr_stmt|;
name|killJob
argument_list|()
expr_stmt|;
name|isShutdown
operator|=
literal|true
expr_stmt|;
block|}
specifier|private
name|void
name|killJob
parameter_list|()
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Killing Spark job with job handle id "
operator|+
name|sparkJobHandleId
argument_list|)
expr_stmt|;
name|boolean
name|needToKillJob
init|=
literal|false
decl_stmt|;
if|if
condition|(
name|jobRef
operator|!=
literal|null
operator|&&
operator|!
name|jobKilled
condition|)
block|{
synchronized|synchronized
init|(
name|this
init|)
block|{
if|if
condition|(
operator|!
name|jobKilled
condition|)
block|{
name|jobKilled
operator|=
literal|true
expr_stmt|;
name|needToKillJob
operator|=
literal|true
expr_stmt|;
block|}
block|}
block|}
if|if
condition|(
name|needToKillJob
condition|)
block|{
try|try
block|{
name|jobRef
operator|.
name|cancelJob
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Failed to kill Spark job"
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
block|}
block|}
comment|/**    * Set the number of reducers for the spark work.    */
specifier|private
name|void
name|printConfigInfo
parameter_list|()
throws|throws
name|IOException
block|{
name|console
operator|.
name|printInfo
argument_list|(
literal|"In order to change the average load for a reducer (in bytes):"
argument_list|)
expr_stmt|;
name|console
operator|.
name|printInfo
argument_list|(
literal|"  set "
operator|+
name|HiveConf
operator|.
name|ConfVars
operator|.
name|BYTESPERREDUCER
operator|.
name|varname
operator|+
literal|"=<number>"
argument_list|)
expr_stmt|;
name|console
operator|.
name|printInfo
argument_list|(
literal|"In order to limit the maximum number of reducers:"
argument_list|)
expr_stmt|;
name|console
operator|.
name|printInfo
argument_list|(
literal|"  set "
operator|+
name|HiveConf
operator|.
name|ConfVars
operator|.
name|MAXREDUCERS
operator|.
name|varname
operator|+
literal|"=<number>"
argument_list|)
expr_stmt|;
name|console
operator|.
name|printInfo
argument_list|(
literal|"In order to set a constant number of reducers:"
argument_list|)
expr_stmt|;
name|console
operator|.
name|printInfo
argument_list|(
literal|"  set "
operator|+
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HADOOPNUMREDUCERS
operator|+
literal|"=<number>"
argument_list|)
expr_stmt|;
block|}
specifier|private
name|Map
argument_list|<
name|String
argument_list|,
name|List
argument_list|<
name|String
argument_list|>
argument_list|>
name|getOperatorCounters
parameter_list|()
block|{
name|String
name|groupName
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|conf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVECOUNTERGROUP
argument_list|)
decl_stmt|;
name|Map
argument_list|<
name|String
argument_list|,
name|List
argument_list|<
name|String
argument_list|>
argument_list|>
name|counters
init|=
operator|new
name|HashMap
argument_list|<
name|String
argument_list|,
name|List
argument_list|<
name|String
argument_list|>
argument_list|>
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|hiveCounters
init|=
operator|new
name|LinkedList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
name|counters
operator|.
name|put
argument_list|(
name|groupName
argument_list|,
name|hiveCounters
argument_list|)
expr_stmt|;
name|hiveCounters
operator|.
name|add
argument_list|(
name|Operator
operator|.
name|HIVE_COUNTER_CREATED_FILES
argument_list|)
expr_stmt|;
comment|// MapOperator is out of SparkWork, SparkMapRecordHandler use it to bridge
comment|// Spark transformation and Hive operators in SparkWork.
for|for
control|(
name|MapOperator
operator|.
name|Counter
name|counter
range|:
name|MapOperator
operator|.
name|Counter
operator|.
name|values
argument_list|()
control|)
block|{
name|hiveCounters
operator|.
name|add
argument_list|(
name|counter
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|SparkWork
name|sparkWork
init|=
name|this
operator|.
name|getWork
argument_list|()
decl_stmt|;
for|for
control|(
name|BaseWork
name|work
range|:
name|sparkWork
operator|.
name|getAllWork
argument_list|()
control|)
block|{
for|for
control|(
name|Operator
argument_list|<
name|?
extends|extends
name|OperatorDesc
argument_list|>
name|operator
range|:
name|work
operator|.
name|getAllOperators
argument_list|()
control|)
block|{
if|if
condition|(
name|operator
operator|instanceof
name|FileSinkOperator
condition|)
block|{
for|for
control|(
name|FileSinkOperator
operator|.
name|Counter
name|counter
range|:
name|FileSinkOperator
operator|.
name|Counter
operator|.
name|values
argument_list|()
control|)
block|{
name|hiveCounters
operator|.
name|add
argument_list|(
operator|(
operator|(
name|FileSinkOperator
operator|)
name|operator
operator|)
operator|.
name|getCounterName
argument_list|(
name|counter
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
elseif|else
if|if
condition|(
name|operator
operator|instanceof
name|ReduceSinkOperator
condition|)
block|{
specifier|final
name|String
name|contextName
init|=
name|conf
operator|.
name|get
argument_list|(
name|Operator
operator|.
name|CONTEXT_NAME_KEY
argument_list|,
literal|""
argument_list|)
decl_stmt|;
for|for
control|(
name|ReduceSinkOperator
operator|.
name|Counter
name|counter
range|:
name|ReduceSinkOperator
operator|.
name|Counter
operator|.
name|values
argument_list|()
control|)
block|{
name|hiveCounters
operator|.
name|add
argument_list|(
name|Utilities
operator|.
name|getVertexCounterName
argument_list|(
name|counter
operator|.
name|name
argument_list|()
argument_list|,
name|contextName
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
elseif|else
if|if
condition|(
name|operator
operator|instanceof
name|ScriptOperator
condition|)
block|{
for|for
control|(
name|ScriptOperator
operator|.
name|Counter
name|counter
range|:
name|ScriptOperator
operator|.
name|Counter
operator|.
name|values
argument_list|()
control|)
block|{
name|hiveCounters
operator|.
name|add
argument_list|(
name|counter
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
elseif|else
if|if
condition|(
name|operator
operator|instanceof
name|JoinOperator
condition|)
block|{
for|for
control|(
name|JoinOperator
operator|.
name|SkewkeyTableCounter
name|counter
range|:
name|JoinOperator
operator|.
name|SkewkeyTableCounter
operator|.
name|values
argument_list|()
control|)
block|{
name|hiveCounters
operator|.
name|add
argument_list|(
name|counter
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
return|return
name|counters
return|;
block|}
specifier|private
name|void
name|getSparkJobInfo
parameter_list|(
name|SparkJobStatus
name|sparkJobStatus
parameter_list|)
block|{
try|try
block|{
name|stageIds
operator|=
operator|new
name|ArrayList
argument_list|<
name|Integer
argument_list|>
argument_list|()
expr_stmt|;
name|int
index|[]
name|ids
init|=
name|sparkJobStatus
operator|.
name|getStageIds
argument_list|()
decl_stmt|;
if|if
condition|(
name|ids
operator|!=
literal|null
condition|)
block|{
for|for
control|(
name|int
name|stageId
range|:
name|ids
control|)
block|{
name|stageIds
operator|.
name|add
argument_list|(
name|stageId
argument_list|)
expr_stmt|;
block|}
block|}
name|Map
argument_list|<
name|String
argument_list|,
name|SparkStageProgress
argument_list|>
name|progressMap
init|=
name|sparkJobStatus
operator|.
name|getSparkStageProgress
argument_list|()
decl_stmt|;
name|int
name|sumTotal
init|=
literal|0
decl_stmt|;
name|int
name|sumComplete
init|=
literal|0
decl_stmt|;
name|int
name|sumFailed
init|=
literal|0
decl_stmt|;
for|for
control|(
name|String
name|s
range|:
name|progressMap
operator|.
name|keySet
argument_list|()
control|)
block|{
name|SparkStageProgress
name|progress
init|=
name|progressMap
operator|.
name|get
argument_list|(
name|s
argument_list|)
decl_stmt|;
specifier|final
name|int
name|complete
init|=
name|progress
operator|.
name|getSucceededTaskCount
argument_list|()
decl_stmt|;
specifier|final
name|int
name|total
init|=
name|progress
operator|.
name|getTotalTaskCount
argument_list|()
decl_stmt|;
specifier|final
name|int
name|failed
init|=
name|progress
operator|.
name|getFailedTaskCount
argument_list|()
decl_stmt|;
name|sumTotal
operator|+=
name|total
expr_stmt|;
name|sumComplete
operator|+=
name|complete
expr_stmt|;
name|sumFailed
operator|+=
name|failed
expr_stmt|;
block|}
name|succeededTaskCount
operator|=
name|sumComplete
expr_stmt|;
name|totalTaskCount
operator|=
name|sumTotal
expr_stmt|;
name|failedTaskCount
operator|=
name|sumFailed
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Failed to get Spark job information"
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
block|}
annotation|@
name|VisibleForTesting
name|void
name|setSparkException
parameter_list|(
name|SparkJobStatus
name|sparkJobStatus
parameter_list|,
name|int
name|rc
parameter_list|)
block|{
if|if
condition|(
name|rc
operator|!=
literal|0
condition|)
block|{
comment|// Set the Spark Job Exception
name|Throwable
name|sparkJobException
init|=
name|sparkJobStatus
operator|.
name|getSparkJobException
argument_list|()
decl_stmt|;
if|if
condition|(
name|sparkJobException
operator|!=
literal|null
condition|)
block|{
name|HiveException
name|he
decl_stmt|;
if|if
condition|(
name|isOOMError
argument_list|(
name|sparkJobException
argument_list|)
condition|)
block|{
name|he
operator|=
operator|new
name|HiveException
argument_list|(
name|sparkJobException
argument_list|,
name|ErrorMsg
operator|.
name|SPARK_RUNTIME_OOM
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|isTaskFailure
argument_list|(
name|sparkJobException
argument_list|)
condition|)
block|{
name|he
operator|=
operator|new
name|HiveException
argument_list|(
name|sparkJobException
argument_list|,
name|ErrorMsg
operator|.
name|SPARK_TASK_RUNTIME_ERROR
argument_list|,
name|Throwables
operator|.
name|getRootCause
argument_list|(
name|sparkJobException
argument_list|)
operator|.
name|getMessage
argument_list|()
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|he
operator|=
operator|new
name|HiveException
argument_list|(
name|sparkJobException
argument_list|,
name|ErrorMsg
operator|.
name|SPARK_JOB_RUNTIME_ERROR
argument_list|,
name|Throwables
operator|.
name|getRootCause
argument_list|(
name|sparkJobException
argument_list|)
operator|.
name|getMessage
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|setException
argument_list|(
name|he
argument_list|)
expr_stmt|;
block|}
comment|// Set the Monitor Error
name|Throwable
name|monitorError
init|=
name|sparkJobStatus
operator|.
name|getMonitorError
argument_list|()
decl_stmt|;
if|if
condition|(
name|monitorError
operator|!=
literal|null
condition|)
block|{
if|if
condition|(
operator|(
name|monitorError
operator|instanceof
name|InterruptedException
operator|)
operator|||
operator|(
name|monitorError
operator|instanceof
name|HiveException
operator|&&
name|monitorError
operator|.
name|getCause
argument_list|()
operator|instanceof
name|InterruptedException
operator|)
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Killing Spark job since query was interrupted"
argument_list|)
expr_stmt|;
name|killJob
argument_list|()
expr_stmt|;
block|}
comment|// Prefer to propagate errors from the Spark job rather than the monitor, as errors from
comment|// the Spark job are likely to be more relevant
if|if
condition|(
name|getException
argument_list|()
operator|==
literal|null
condition|)
block|{
name|setException
argument_list|(
name|monitorError
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
specifier|private
name|boolean
name|isTaskFailure
parameter_list|(
name|Throwable
name|error
parameter_list|)
block|{
name|Pattern
name|taskFailedPattern
init|=
name|Pattern
operator|.
name|compile
argument_list|(
literal|"Task.*in stage.*failed.*times"
argument_list|)
decl_stmt|;
while|while
condition|(
name|error
operator|!=
literal|null
condition|)
block|{
if|if
condition|(
name|taskFailedPattern
operator|.
name|matcher
argument_list|(
name|error
operator|.
name|getMessage
argument_list|()
argument_list|)
operator|.
name|find
argument_list|()
condition|)
block|{
return|return
literal|true
return|;
block|}
name|error
operator|=
name|error
operator|.
name|getCause
argument_list|()
expr_stmt|;
block|}
return|return
literal|false
return|;
block|}
specifier|private
name|boolean
name|isOOMError
parameter_list|(
name|Throwable
name|error
parameter_list|)
block|{
while|while
condition|(
name|error
operator|!=
literal|null
condition|)
block|{
if|if
condition|(
name|error
operator|instanceof
name|OutOfMemoryError
condition|)
block|{
return|return
literal|true
return|;
block|}
elseif|else
if|if
condition|(
name|error
operator|.
name|getMessage
argument_list|()
operator|.
name|contains
argument_list|(
literal|"Container killed by YARN for exceeding memory "
operator|+
literal|"limits"
argument_list|)
condition|)
block|{
return|return
literal|true
return|;
block|}
name|error
operator|=
name|error
operator|.
name|getCause
argument_list|()
expr_stmt|;
block|}
return|return
literal|false
return|;
block|}
block|}
end_class

end_unit

