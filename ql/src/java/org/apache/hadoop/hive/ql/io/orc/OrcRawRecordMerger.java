begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/*  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|orc
package|;
end_package

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Objects
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|TreeMap
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|hive_metastoreConstants
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|AbstractFileMergeOperator
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|Utilities
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|AcidOutputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|BucketCodec
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|shims
operator|.
name|HadoopShims
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|orc
operator|.
name|OrcUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|orc
operator|.
name|StripeInformation
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|orc
operator|.
name|TypeDescription
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|Logger
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|LoggerFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configuration
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|ValidWriteIdList
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|AcidInputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|AcidUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|RecordIdentifier
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|objectinspector
operator|.
name|ObjectInspector
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|IntWritable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|LongWritable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hive
operator|.
name|common
operator|.
name|util
operator|.
name|Ref
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|annotations
operator|.
name|VisibleForTesting
import|;
end_import

begin_comment
comment|/**  * Merges a base and a list of delta files together into a single stream of  * events.  */
end_comment

begin_class
specifier|public
class|class
name|OrcRawRecordMerger
implements|implements
name|AcidInputFormat
operator|.
name|RawReader
argument_list|<
name|OrcStruct
argument_list|>
block|{
specifier|private
specifier|static
specifier|final
name|Logger
name|LOG
init|=
name|LoggerFactory
operator|.
name|getLogger
argument_list|(
name|OrcRawRecordMerger
operator|.
name|class
argument_list|)
decl_stmt|;
specifier|private
specifier|final
name|boolean
name|collapse
decl_stmt|;
specifier|private
specifier|final
name|RecordReader
name|baseReader
decl_stmt|;
specifier|private
specifier|final
name|ObjectInspector
name|objectInspector
decl_stmt|;
specifier|private
specifier|final
name|long
name|offset
decl_stmt|;
specifier|private
specifier|final
name|long
name|length
decl_stmt|;
specifier|private
specifier|final
name|ValidWriteIdList
name|validWriteIdList
decl_stmt|;
specifier|private
specifier|final
name|int
name|columns
decl_stmt|;
specifier|private
specifier|final
name|ReaderKey
name|prevKey
init|=
operator|new
name|ReaderKey
argument_list|()
decl_stmt|;
comment|// this is the key less than the lowest key we need to process
specifier|private
specifier|final
name|RecordIdentifier
name|minKey
decl_stmt|;
comment|// this is the last key we need to process
specifier|private
specifier|final
name|RecordIdentifier
name|maxKey
decl_stmt|;
comment|// an extra value so that we can return it while reading ahead
specifier|private
name|OrcStruct
name|extraValue
decl_stmt|;
comment|/**    * A RecordIdentifier extended with the current write id. This is the    * key of our merge sort with the originalWriteId, bucket, and rowId    * ascending and the currentWriteId, statementId descending. This means that if the    * reader is collapsing events to just the last update, just the first    * instance of each record is required.    */
annotation|@
name|VisibleForTesting
specifier|public
specifier|final
specifier|static
class|class
name|ReaderKey
extends|extends
name|RecordIdentifier
block|{
specifier|private
name|long
name|currentWriteId
decl_stmt|;
specifier|private
name|boolean
name|isDeleteEvent
init|=
literal|false
decl_stmt|;
name|ReaderKey
parameter_list|()
block|{
name|this
argument_list|(
operator|-
literal|1
argument_list|,
operator|-
literal|1
argument_list|,
operator|-
literal|1
argument_list|,
operator|-
literal|1
argument_list|)
expr_stmt|;
block|}
specifier|public
name|ReaderKey
parameter_list|(
name|long
name|originalWriteId
parameter_list|,
name|int
name|bucket
parameter_list|,
name|long
name|rowId
parameter_list|,
name|long
name|currentWriteId
parameter_list|)
block|{
name|super
argument_list|(
name|originalWriteId
argument_list|,
name|bucket
argument_list|,
name|rowId
argument_list|)
expr_stmt|;
name|this
operator|.
name|currentWriteId
operator|=
name|currentWriteId
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|set
parameter_list|(
name|RecordIdentifier
name|other
parameter_list|)
block|{
name|super
operator|.
name|set
argument_list|(
name|other
argument_list|)
expr_stmt|;
name|currentWriteId
operator|=
operator|(
operator|(
name|ReaderKey
operator|)
name|other
operator|)
operator|.
name|currentWriteId
expr_stmt|;
name|isDeleteEvent
operator|=
operator|(
operator|(
name|ReaderKey
operator|)
name|other
operator|)
operator|.
name|isDeleteEvent
expr_stmt|;
block|}
specifier|public
name|void
name|setValues
parameter_list|(
name|long
name|originalWriteId
parameter_list|,
name|int
name|bucket
parameter_list|,
name|long
name|rowId
parameter_list|,
name|long
name|currentWriteId
parameter_list|,
name|boolean
name|isDelete
parameter_list|)
block|{
name|setValues
argument_list|(
name|originalWriteId
argument_list|,
name|bucket
argument_list|,
name|rowId
argument_list|)
expr_stmt|;
name|this
operator|.
name|currentWriteId
operator|=
name|currentWriteId
expr_stmt|;
name|this
operator|.
name|isDeleteEvent
operator|=
name|isDelete
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|equals
parameter_list|(
name|Object
name|other
parameter_list|)
block|{
return|return
name|super
operator|.
name|equals
argument_list|(
name|other
argument_list|)
operator|&&
name|currentWriteId
operator|==
operator|(
operator|(
name|ReaderKey
operator|)
name|other
operator|)
operator|.
name|currentWriteId
return|;
block|}
annotation|@
name|Override
specifier|public
name|int
name|hashCode
parameter_list|()
block|{
name|int
name|result
init|=
name|super
operator|.
name|hashCode
argument_list|()
decl_stmt|;
name|result
operator|=
literal|31
operator|*
name|result
operator|+
call|(
name|int
call|)
argument_list|(
name|currentWriteId
operator|^
operator|(
name|currentWriteId
operator|>>>
literal|32
operator|)
argument_list|)
expr_stmt|;
return|return
name|result
return|;
block|}
annotation|@
name|Override
specifier|public
name|int
name|compareTo
parameter_list|(
name|RecordIdentifier
name|other
parameter_list|)
block|{
name|int
name|sup
init|=
name|compareToInternal
argument_list|(
name|other
argument_list|)
decl_stmt|;
if|if
condition|(
name|sup
operator|==
literal|0
condition|)
block|{
if|if
condition|(
name|other
operator|.
name|getClass
argument_list|()
operator|==
name|ReaderKey
operator|.
name|class
condition|)
block|{
name|ReaderKey
name|oth
init|=
operator|(
name|ReaderKey
operator|)
name|other
decl_stmt|;
if|if
condition|(
name|currentWriteId
operator|!=
name|oth
operator|.
name|currentWriteId
condition|)
block|{
return|return
name|currentWriteId
operator|<
name|oth
operator|.
name|currentWriteId
condition|?
operator|+
literal|1
else|:
operator|-
literal|1
return|;
block|}
if|if
condition|(
name|isDeleteEvent
operator|!=
name|oth
operator|.
name|isDeleteEvent
condition|)
block|{
comment|//this is to break a tie if insert + delete of a given row is done within the same
comment|//txn (so that currentWriteId is the same for both events) and we want the
comment|//delete event to sort 1st since it needs to be sent up so that
comment|// OrcInputFormat.getReader(InputSplit inputSplit, Options options) can skip it.
return|return
name|isDeleteEvent
condition|?
operator|-
literal|1
else|:
operator|+
literal|1
return|;
block|}
block|}
else|else
block|{
return|return
operator|-
literal|1
return|;
block|}
block|}
return|return
name|sup
return|;
block|}
comment|/**      * This means 1 txn modified the same row more than once      */
specifier|private
name|boolean
name|isSameRow
parameter_list|(
name|ReaderKey
name|other
parameter_list|)
block|{
return|return
name|compareRow
argument_list|(
name|other
argument_list|)
operator|==
literal|0
operator|&&
name|currentWriteId
operator|==
name|other
operator|.
name|currentWriteId
return|;
block|}
name|long
name|getCurrentWriteId
parameter_list|()
block|{
return|return
name|currentWriteId
return|;
block|}
comment|/**      * Compare rows without considering the currentWriteId.      * @param other the value to compare to      * @return -1, 0, +1      */
name|int
name|compareRow
parameter_list|(
name|RecordIdentifier
name|other
parameter_list|)
block|{
return|return
name|compareToInternal
argument_list|(
name|other
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|public
name|String
name|toString
parameter_list|()
block|{
return|return
literal|"{originalWriteId: "
operator|+
name|getWriteId
argument_list|()
operator|+
literal|", "
operator|+
name|bucketToString
argument_list|(
name|getBucketProperty
argument_list|()
argument_list|)
operator|+
literal|", row: "
operator|+
name|getRowId
argument_list|()
operator|+
literal|", currentWriteId "
operator|+
name|currentWriteId
operator|+
literal|"}"
return|;
block|}
block|}
interface|interface
name|ReaderPair
block|{
name|OrcStruct
name|nextRecord
parameter_list|()
function_decl|;
name|int
name|getColumns
parameter_list|()
function_decl|;
name|RecordReader
name|getRecordReader
parameter_list|()
function_decl|;
name|Reader
name|getReader
parameter_list|()
function_decl|;
name|RecordIdentifier
name|getMinKey
parameter_list|()
function_decl|;
name|RecordIdentifier
name|getMaxKey
parameter_list|()
function_decl|;
name|ReaderKey
name|getKey
parameter_list|()
function_decl|;
name|void
name|next
parameter_list|(
name|OrcStruct
name|next
parameter_list|)
throws|throws
name|IOException
function_decl|;
block|}
comment|/**    * Used when base_x/bucket_N is missing - makes control flow a bit easier    */
specifier|private
class|class
name|EmptyReaderPair
implements|implements
name|ReaderPair
block|{
annotation|@
name|Override
specifier|public
name|OrcStruct
name|nextRecord
parameter_list|()
block|{
return|return
literal|null
return|;
block|}
annotation|@
name|Override
specifier|public
name|int
name|getColumns
parameter_list|()
block|{
return|return
literal|0
return|;
block|}
annotation|@
name|Override
specifier|public
name|RecordReader
name|getRecordReader
parameter_list|()
block|{
return|return
literal|null
return|;
block|}
annotation|@
name|Override
specifier|public
name|Reader
name|getReader
parameter_list|()
block|{
return|return
literal|null
return|;
block|}
annotation|@
name|Override
specifier|public
name|RecordIdentifier
name|getMinKey
parameter_list|()
block|{
return|return
literal|null
return|;
block|}
annotation|@
name|Override
specifier|public
name|RecordIdentifier
name|getMaxKey
parameter_list|()
block|{
return|return
literal|null
return|;
block|}
annotation|@
name|Override
specifier|public
name|ReaderKey
name|getKey
parameter_list|()
block|{
return|return
literal|null
return|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|next
parameter_list|(
name|OrcStruct
name|next
parameter_list|)
throws|throws
name|IOException
block|{     }
block|}
comment|/**    * A reader and the next record from that reader. The code reads ahead so that    * we can return the lowest ReaderKey from each of the readers. Thus, the    * next available row is nextRecord and only following records are still in    * the reader.    */
annotation|@
name|VisibleForTesting
specifier|final
specifier|static
class|class
name|ReaderPairAcid
implements|implements
name|ReaderPair
block|{
specifier|private
name|OrcStruct
name|nextRecord
decl_stmt|;
specifier|private
specifier|final
name|Reader
name|reader
decl_stmt|;
specifier|private
specifier|final
name|RecordReader
name|recordReader
decl_stmt|;
specifier|private
specifier|final
name|ReaderKey
name|key
decl_stmt|;
specifier|private
specifier|final
name|RecordIdentifier
name|minKey
decl_stmt|;
specifier|private
specifier|final
name|RecordIdentifier
name|maxKey
decl_stmt|;
comment|/**      * Create a reader that reads from the first key larger than minKey to any      * keys equal to maxKey.      * @param key the key to read into      * @param reader the ORC file reader      * @param minKey only return keys larger than minKey if it is non-null      * @param maxKey only return keys less than or equal to maxKey if it is      *               non-null      * @param options options to provide to read the rows.      * @param conf      * @throws IOException      */
annotation|@
name|VisibleForTesting
name|ReaderPairAcid
parameter_list|(
name|ReaderKey
name|key
parameter_list|,
name|Reader
name|reader
parameter_list|,
name|RecordIdentifier
name|minKey
parameter_list|,
name|RecordIdentifier
name|maxKey
parameter_list|,
name|ReaderImpl
operator|.
name|Options
name|options
parameter_list|,
specifier|final
name|Configuration
name|conf
parameter_list|)
throws|throws
name|IOException
block|{
name|this
operator|.
name|reader
operator|=
name|reader
expr_stmt|;
name|this
operator|.
name|key
operator|=
name|key
expr_stmt|;
comment|// TODO use stripe statistics to jump over stripes
name|recordReader
operator|=
name|reader
operator|.
name|rowsOptions
argument_list|(
name|options
argument_list|,
name|conf
argument_list|)
expr_stmt|;
name|this
operator|.
name|minKey
operator|=
name|minKey
expr_stmt|;
name|this
operator|.
name|maxKey
operator|=
name|maxKey
expr_stmt|;
comment|// advance the reader until we reach the minimum key
do|do
block|{
name|next
argument_list|(
name|nextRecord
argument_list|()
argument_list|)
expr_stmt|;
block|}
do|while
condition|(
name|nextRecord
argument_list|()
operator|!=
literal|null
operator|&&
operator|(
name|minKey
operator|!=
literal|null
operator|&&
name|key
operator|.
name|compareRow
argument_list|(
name|getMinKey
argument_list|()
argument_list|)
operator|<=
literal|0
operator|)
condition|)
do|;
block|}
annotation|@
name|Override
specifier|public
name|String
name|toString
parameter_list|()
block|{
return|return
literal|"[key="
operator|+
name|key
operator|+
literal|", nextRecord="
operator|+
name|nextRecord
operator|+
literal|", reader="
operator|+
name|reader
operator|+
literal|"]"
return|;
block|}
annotation|@
name|Override
specifier|public
specifier|final
name|OrcStruct
name|nextRecord
parameter_list|()
block|{
return|return
name|nextRecord
return|;
block|}
annotation|@
name|Override
specifier|public
specifier|final
name|int
name|getColumns
parameter_list|()
block|{
return|return
name|getReader
argument_list|()
operator|.
name|getTypes
argument_list|()
operator|.
name|get
argument_list|(
name|OrcRecordUpdater
operator|.
name|ROW
operator|+
literal|1
argument_list|)
operator|.
name|getSubtypesCount
argument_list|()
return|;
block|}
annotation|@
name|Override
specifier|public
name|RecordReader
name|getRecordReader
parameter_list|()
block|{
return|return
name|recordReader
return|;
block|}
annotation|@
name|Override
specifier|public
name|Reader
name|getReader
parameter_list|()
block|{
return|return
name|reader
return|;
block|}
annotation|@
name|Override
specifier|public
name|RecordIdentifier
name|getMinKey
parameter_list|()
block|{
return|return
name|minKey
return|;
block|}
annotation|@
name|Override
specifier|public
name|RecordIdentifier
name|getMaxKey
parameter_list|()
block|{
return|return
name|maxKey
return|;
block|}
annotation|@
name|Override
specifier|public
name|ReaderKey
name|getKey
parameter_list|()
block|{
return|return
name|key
return|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|next
parameter_list|(
name|OrcStruct
name|next
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|getRecordReader
argument_list|()
operator|.
name|hasNext
argument_list|()
condition|)
block|{
name|nextRecord
operator|=
operator|(
name|OrcStruct
operator|)
name|getRecordReader
argument_list|()
operator|.
name|next
argument_list|(
name|next
argument_list|)
expr_stmt|;
comment|// set the key
name|getKey
argument_list|()
operator|.
name|setValues
argument_list|(
name|OrcRecordUpdater
operator|.
name|getOriginalTransaction
argument_list|(
name|nextRecord
argument_list|()
argument_list|)
argument_list|,
name|OrcRecordUpdater
operator|.
name|getBucket
argument_list|(
name|nextRecord
argument_list|()
argument_list|)
argument_list|,
name|OrcRecordUpdater
operator|.
name|getRowId
argument_list|(
name|nextRecord
argument_list|()
argument_list|)
argument_list|,
name|OrcRecordUpdater
operator|.
name|getCurrentTransaction
argument_list|(
name|nextRecord
argument_list|()
argument_list|)
argument_list|,
name|OrcRecordUpdater
operator|.
name|getOperation
argument_list|(
name|nextRecord
argument_list|()
argument_list|)
operator|==
name|OrcRecordUpdater
operator|.
name|DELETE_OPERATION
argument_list|)
expr_stmt|;
comment|// if this record is larger than maxKey, we need to stop
if|if
condition|(
name|getMaxKey
argument_list|()
operator|!=
literal|null
operator|&&
name|getKey
argument_list|()
operator|.
name|compareRow
argument_list|(
name|getMaxKey
argument_list|()
argument_list|)
operator|>
literal|0
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"key "
operator|+
name|getKey
argument_list|()
operator|+
literal|"> maxkey "
operator|+
name|getMaxKey
argument_list|()
argument_list|)
expr_stmt|;
name|nextRecord
operator|=
literal|null
expr_stmt|;
name|getRecordReader
argument_list|()
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
else|else
block|{
name|nextRecord
operator|=
literal|null
expr_stmt|;
name|getRecordReader
argument_list|()
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
block|}
comment|/**    * A reader that pretends an original base file is a new versioned base file.    * It wraps the underlying reader's row with an ACID event object and    * makes the relevant translations.    *     * Running multiple Insert statements on the same partition (of non acid table) creates files    * like so: 00000_0, 00000_0_copy1, 00000_0_copy2, etc.  So the OriginalReaderPair must treat all    * of these files as part of a single logical bucket file.    *    * Also, for unbucketed (non acid) tables, there are no guarantees where data files may be placed.    * For example, CTAS+Tez+Union creates subdirs    * {@link AbstractFileMergeOperator#UNION_SUDBIR_PREFIX}_1/,    * {@link AbstractFileMergeOperator#UNION_SUDBIR_PREFIX}_2/, etc for each leg of the Union.  Thus    * the data file need not be an immediate child of partition dir.  All files for a given writerId    * are treated as one logical unit to assign {@link RecordIdentifier}s to them consistently.    *     * For Compaction, where each split includes the whole bucket, this means reading over all the    * files in order to assign ROW__ID.rowid in one sequence for the entire logical bucket.    * For unbucketed tables, a Compaction split is all files written by a given writerId.    *    * For a read after the table is marked transactional but before it's rewritten into a base/    * by compaction, each of the original files may be split into many pieces.  For each split we    * must make sure to include only the relevant part of each delta file.    * {@link OrcRawRecordMerger#minKey} and {@link OrcRawRecordMerger#maxKey} are computed for each    * split of the original file and used to filter rows from all the deltas.  The ROW__ID.rowid for    * the rows of the 'original' file of course, must be assigned from the beginning of logical    * bucket.  The last split of the logical bucket, i.e. the split that has the end of last file,    * should include all insert events from deltas (last sentence is obsolete for Acid 2: HIVE-17320)    */
specifier|private
specifier|static
specifier|abstract
class|class
name|OriginalReaderPair
implements|implements
name|ReaderPair
block|{
name|OrcStruct
name|nextRecord
decl_stmt|;
specifier|private
specifier|final
name|ReaderKey
name|key
decl_stmt|;
specifier|final
name|int
name|bucketId
decl_stmt|;
specifier|final
name|int
name|bucketProperty
decl_stmt|;
comment|/**      * Write Id to use when generating synthetic ROW_IDs      */
specifier|final
name|long
name|writeId
decl_stmt|;
comment|/**      * @param statementId - this should be from delta_x_y_stmtId file name.  Imagine 2 load data      *                    statements in 1 txn.  The stmtId will be embedded in      *                    {@link RecordIdentifier#bucketId} via {@link BucketCodec} below      */
name|OriginalReaderPair
parameter_list|(
name|ReaderKey
name|key
parameter_list|,
name|int
name|bucketId
parameter_list|,
name|Configuration
name|conf
parameter_list|,
name|Options
name|mergeOptions
parameter_list|,
name|int
name|statementId
parameter_list|)
throws|throws
name|IOException
block|{
name|this
operator|.
name|key
operator|=
name|key
expr_stmt|;
name|this
operator|.
name|bucketId
operator|=
name|bucketId
expr_stmt|;
assert|assert
name|bucketId
operator|>=
literal|0
operator|:
literal|"don't support non-bucketed tables yet"
assert|;
name|this
operator|.
name|bucketProperty
operator|=
name|encodeBucketId
argument_list|(
name|conf
argument_list|,
name|bucketId
argument_list|,
name|statementId
argument_list|)
expr_stmt|;
name|writeId
operator|=
name|mergeOptions
operator|.
name|getWriteId
argument_list|()
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
specifier|final
name|OrcStruct
name|nextRecord
parameter_list|()
block|{
return|return
name|nextRecord
return|;
block|}
annotation|@
name|Override
specifier|public
name|int
name|getColumns
parameter_list|()
block|{
return|return
name|getReader
argument_list|()
operator|.
name|getTypes
argument_list|()
operator|.
name|get
argument_list|(
literal|0
argument_list|)
operator|.
name|getSubtypesCount
argument_list|()
return|;
block|}
annotation|@
name|Override
specifier|public
specifier|final
name|ReaderKey
name|getKey
parameter_list|()
block|{
return|return
name|key
return|;
block|}
comment|/**      * The cumulative number of rows in all files of the logical bucket that precede the file      * represented by {@link #getRecordReader()}      */
specifier|abstract
name|long
name|getRowIdOffset
parameter_list|()
function_decl|;
specifier|final
name|boolean
name|nextFromCurrentFile
parameter_list|(
name|OrcStruct
name|next
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|getRecordReader
argument_list|()
operator|.
name|hasNext
argument_list|()
condition|)
block|{
comment|//RecordReader.getRowNumber() produces a file-global row number even with PPD
name|long
name|nextRowId
init|=
name|getRecordReader
argument_list|()
operator|.
name|getRowNumber
argument_list|()
operator|+
name|getRowIdOffset
argument_list|()
decl_stmt|;
comment|// have to do initialization here, because the super's constructor
comment|// calls next and thus we need to initialize before our constructor
comment|// runs
if|if
condition|(
name|next
operator|==
literal|null
condition|)
block|{
name|nextRecord
operator|=
operator|new
name|OrcStruct
argument_list|(
name|OrcRecordUpdater
operator|.
name|FIELDS
argument_list|)
expr_stmt|;
name|IntWritable
name|operation
init|=
operator|new
name|IntWritable
argument_list|(
name|OrcRecordUpdater
operator|.
name|INSERT_OPERATION
argument_list|)
decl_stmt|;
name|nextRecord
argument_list|()
operator|.
name|setFieldValue
argument_list|(
name|OrcRecordUpdater
operator|.
name|OPERATION
argument_list|,
name|operation
argument_list|)
expr_stmt|;
name|nextRecord
argument_list|()
operator|.
name|setFieldValue
argument_list|(
name|OrcRecordUpdater
operator|.
name|CURRENT_WRITEID
argument_list|,
operator|new
name|LongWritable
argument_list|(
name|writeId
argument_list|)
argument_list|)
expr_stmt|;
name|nextRecord
argument_list|()
operator|.
name|setFieldValue
argument_list|(
name|OrcRecordUpdater
operator|.
name|ORIGINAL_WRITEID
argument_list|,
operator|new
name|LongWritable
argument_list|(
name|writeId
argument_list|)
argument_list|)
expr_stmt|;
name|nextRecord
argument_list|()
operator|.
name|setFieldValue
argument_list|(
name|OrcRecordUpdater
operator|.
name|BUCKET
argument_list|,
operator|new
name|IntWritable
argument_list|(
name|bucketProperty
argument_list|)
argument_list|)
expr_stmt|;
name|nextRecord
argument_list|()
operator|.
name|setFieldValue
argument_list|(
name|OrcRecordUpdater
operator|.
name|ROW_ID
argument_list|,
operator|new
name|LongWritable
argument_list|(
name|nextRowId
argument_list|)
argument_list|)
expr_stmt|;
name|nextRecord
argument_list|()
operator|.
name|setFieldValue
argument_list|(
name|OrcRecordUpdater
operator|.
name|ROW
argument_list|,
name|getRecordReader
argument_list|()
operator|.
name|next
argument_list|(
literal|null
argument_list|)
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|nextRecord
operator|=
name|next
expr_stmt|;
operator|(
operator|(
name|IntWritable
operator|)
name|next
operator|.
name|getFieldValue
argument_list|(
name|OrcRecordUpdater
operator|.
name|OPERATION
argument_list|)
operator|)
operator|.
name|set
argument_list|(
name|OrcRecordUpdater
operator|.
name|INSERT_OPERATION
argument_list|)
expr_stmt|;
operator|(
operator|(
name|LongWritable
operator|)
name|next
operator|.
name|getFieldValue
argument_list|(
name|OrcRecordUpdater
operator|.
name|ORIGINAL_WRITEID
argument_list|)
operator|)
operator|.
name|set
argument_list|(
name|writeId
argument_list|)
expr_stmt|;
operator|(
operator|(
name|IntWritable
operator|)
name|next
operator|.
name|getFieldValue
argument_list|(
name|OrcRecordUpdater
operator|.
name|BUCKET
argument_list|)
operator|)
operator|.
name|set
argument_list|(
name|bucketProperty
argument_list|)
expr_stmt|;
operator|(
operator|(
name|LongWritable
operator|)
name|next
operator|.
name|getFieldValue
argument_list|(
name|OrcRecordUpdater
operator|.
name|CURRENT_WRITEID
argument_list|)
operator|)
operator|.
name|set
argument_list|(
name|writeId
argument_list|)
expr_stmt|;
operator|(
operator|(
name|LongWritable
operator|)
name|next
operator|.
name|getFieldValue
argument_list|(
name|OrcRecordUpdater
operator|.
name|ROW_ID
argument_list|)
operator|)
operator|.
name|set
argument_list|(
name|nextRowId
argument_list|)
expr_stmt|;
name|nextRecord
argument_list|()
operator|.
name|setFieldValue
argument_list|(
name|OrcRecordUpdater
operator|.
name|ROW
argument_list|,
name|getRecordReader
argument_list|()
operator|.
name|next
argument_list|(
name|OrcRecordUpdater
operator|.
name|getRow
argument_list|(
name|next
argument_list|)
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|key
operator|.
name|setValues
argument_list|(
name|writeId
argument_list|,
name|bucketProperty
argument_list|,
name|nextRowId
argument_list|,
name|writeId
argument_list|,
literal|false
argument_list|)
expr_stmt|;
if|if
condition|(
name|getMaxKey
argument_list|()
operator|!=
literal|null
operator|&&
name|key
operator|.
name|compareRow
argument_list|(
name|getMaxKey
argument_list|()
argument_list|)
operator|>
literal|0
condition|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"key "
operator|+
name|key
operator|+
literal|"> maxkey "
operator|+
name|getMaxKey
argument_list|()
argument_list|)
expr_stmt|;
block|}
return|return
literal|false
return|;
comment|//reached End Of Split
block|}
return|return
literal|true
return|;
block|}
return|return
literal|false
return|;
comment|//reached EndOfFile
block|}
block|}
specifier|static
name|int
name|encodeBucketId
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|int
name|bucketId
parameter_list|,
name|int
name|statementId
parameter_list|)
block|{
return|return
name|BucketCodec
operator|.
name|V1
operator|.
name|encode
argument_list|(
operator|new
name|AcidOutputFormat
operator|.
name|Options
argument_list|(
name|conf
argument_list|)
operator|.
name|bucket
argument_list|(
name|bucketId
argument_list|)
operator|.
name|statementId
argument_list|(
name|statementId
argument_list|)
argument_list|)
return|;
block|}
comment|/**    * This handles normal read (as opposed to Compaction) of a {@link AcidUtils.AcidBaseFileType#ORIGINAL_BASE}    * file.  These may be a result of Load Data or it may be a file that was written to the table    * before it was converted to acid.    */
annotation|@
name|VisibleForTesting
specifier|final
specifier|static
class|class
name|OriginalReaderPairToRead
extends|extends
name|OriginalReaderPair
block|{
specifier|private
specifier|final
name|long
name|rowIdOffset
decl_stmt|;
specifier|private
specifier|final
name|Reader
name|reader
decl_stmt|;
specifier|private
specifier|final
name|RecordReader
name|recordReader
decl_stmt|;
specifier|private
specifier|final
name|RecordIdentifier
name|minKey
decl_stmt|;
specifier|private
specifier|final
name|RecordIdentifier
name|maxKey
decl_stmt|;
name|OriginalReaderPairToRead
parameter_list|(
name|ReaderKey
name|key
parameter_list|,
name|Reader
name|reader
parameter_list|,
name|int
name|bucketId
parameter_list|,
specifier|final
name|RecordIdentifier
name|minKey
parameter_list|,
specifier|final
name|RecordIdentifier
name|maxKey
parameter_list|,
name|Reader
operator|.
name|Options
name|options
parameter_list|,
name|Options
name|mergerOptions
parameter_list|,
name|Configuration
name|conf
parameter_list|,
name|ValidWriteIdList
name|validWriteIdList
parameter_list|,
name|int
name|statementId
parameter_list|)
throws|throws
name|IOException
block|{
name|super
argument_list|(
name|key
argument_list|,
name|bucketId
argument_list|,
name|conf
argument_list|,
name|mergerOptions
argument_list|,
name|statementId
argument_list|)
expr_stmt|;
name|this
operator|.
name|reader
operator|=
name|reader
expr_stmt|;
assert|assert
operator|!
name|mergerOptions
operator|.
name|isCompacting
argument_list|()
assert|;
assert|assert
name|mergerOptions
operator|.
name|getRootPath
argument_list|()
operator|!=
literal|null
operator|:
literal|"Since we have original files"
assert|;
name|RecordIdentifier
name|newMinKey
init|=
name|minKey
decl_stmt|;
name|RecordIdentifier
name|newMaxKey
init|=
name|maxKey
decl_stmt|;
name|recordReader
operator|=
name|reader
operator|.
name|rowsOptions
argument_list|(
name|options
argument_list|,
name|conf
argument_list|)
expr_stmt|;
comment|/**        * Logically each bucket consists of 0000_0, 0000_0_copy_1... 0000_0_copy_N. etc  We don't        * know N a priori so if this is true, then the current split is from 0000_0_copy_N file.        * It's needed to correctly set maxKey.  In particular, set maxKey==null if this split        * is the tail of the last file for this logical bucket to include all deltas written after        * non-acid to acid table conversion (todo: HIVE-17320).        * Also, see comments at {@link OriginalReaderPair} about unbucketed tables.        */
name|boolean
name|isLastFileForThisBucket
init|=
literal|true
decl_stmt|;
name|boolean
name|haveSeenCurrentFile
init|=
literal|false
decl_stmt|;
name|long
name|rowIdOffsetTmp
init|=
literal|0
decl_stmt|;
block|{
comment|/**          * Note that for reading base_x/ or delta_x_x/ with non-acid schema,          * {@link Options#getRootPath()} is set to base_x/ or delta_x_x/ which causes all it's          * contents to be in {@link org.apache.hadoop.hive.ql.io.AcidUtils.Directory#getOriginalFiles()}          */
comment|//the split is from something other than the 1st file of the logical bucket - compute offset
name|AcidUtils
operator|.
name|Directory
name|directoryState
init|=
name|AcidUtils
operator|.
name|getAcidState
argument_list|(
literal|null
argument_list|,
name|mergerOptions
operator|.
name|getRootPath
argument_list|()
argument_list|,
name|conf
argument_list|,
name|validWriteIdList
argument_list|,
name|Ref
operator|.
name|from
argument_list|(
literal|false
argument_list|)
argument_list|,
literal|true
argument_list|,
literal|null
argument_list|,
literal|true
argument_list|)
decl_stmt|;
for|for
control|(
name|HadoopShims
operator|.
name|HdfsFileStatusWithId
name|f
range|:
name|directoryState
operator|.
name|getOriginalFiles
argument_list|()
control|)
block|{
name|int
name|bucketIdFromPath
init|=
name|AcidUtils
operator|.
name|parseBucketId
argument_list|(
name|f
operator|.
name|getFileStatus
argument_list|()
operator|.
name|getPath
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|bucketIdFromPath
operator|!=
name|bucketId
condition|)
block|{
continue|continue;
comment|//todo: HIVE-16952
block|}
if|if
condition|(
name|haveSeenCurrentFile
condition|)
block|{
comment|//if here we already saw current file and now found another file for the same bucket
comment|//so the current file is not the last file of the logical bucket
name|isLastFileForThisBucket
operator|=
literal|false
expr_stmt|;
break|break;
block|}
if|if
condition|(
name|f
operator|.
name|getFileStatus
argument_list|()
operator|.
name|getPath
argument_list|()
operator|.
name|equals
argument_list|(
name|mergerOptions
operator|.
name|getBucketPath
argument_list|()
argument_list|)
condition|)
block|{
comment|/**              * found the file whence the current split is from so we're done              * counting {@link rowIdOffset}              */
name|haveSeenCurrentFile
operator|=
literal|true
expr_stmt|;
name|isLastFileForThisBucket
operator|=
literal|true
expr_stmt|;
continue|continue;
block|}
name|Reader
name|copyReader
init|=
name|OrcFile
operator|.
name|createReader
argument_list|(
name|f
operator|.
name|getFileStatus
argument_list|()
operator|.
name|getPath
argument_list|()
argument_list|,
name|OrcFile
operator|.
name|readerOptions
argument_list|(
name|conf
argument_list|)
argument_list|)
decl_stmt|;
name|rowIdOffsetTmp
operator|+=
name|copyReader
operator|.
name|getNumberOfRows
argument_list|()
expr_stmt|;
block|}
name|this
operator|.
name|rowIdOffset
operator|=
name|rowIdOffsetTmp
expr_stmt|;
if|if
condition|(
name|rowIdOffset
operator|>
literal|0
condition|)
block|{
comment|//rowIdOffset could be 0 if all files before current one are empty
comment|/**            * Since we already done {@link OrcRawRecordMerger#discoverOriginalKeyBounds(Reader, int, Reader.Options, Configuration, Options)}            * need to fix min/max key since these are used by            * {@link #next(OrcStruct)} which uses {@link #rowIdOffset} to generate rowId for            * the key.  Clear?  */
if|if
condition|(
name|minKey
operator|!=
literal|null
condition|)
block|{
name|minKey
operator|.
name|setRowId
argument_list|(
name|minKey
operator|.
name|getRowId
argument_list|()
operator|+
name|rowIdOffset
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|/**              *  If this is not the 1st file, set minKey 1 less than the start of current file              * (Would not need to set minKey if we knew that there are no delta files)              * {@link #advanceToMinKey()} needs this */
name|newMinKey
operator|=
operator|new
name|RecordIdentifier
argument_list|(
name|writeId
argument_list|,
name|bucketProperty
argument_list|,
name|rowIdOffset
operator|-
literal|1
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|maxKey
operator|!=
literal|null
condition|)
block|{
name|maxKey
operator|.
name|setRowId
argument_list|(
name|maxKey
operator|.
name|getRowId
argument_list|()
operator|+
name|rowIdOffset
argument_list|)
expr_stmt|;
block|}
block|}
block|}
if|if
condition|(
operator|!
name|isLastFileForThisBucket
operator|&&
name|maxKey
operator|==
literal|null
condition|)
block|{
comment|/*            * If this is the last file for this bucket, maxKey == null means the split is the tail            * of the file so we want to leave it blank to make sure any insert events in delta            * files are included; Conversely, if it's not the last file, set the maxKey so that            * events from deltas that don't modify anything in the current split are excluded*/
name|newMaxKey
operator|=
operator|new
name|RecordIdentifier
argument_list|(
name|writeId
argument_list|,
name|bucketProperty
argument_list|,
name|rowIdOffset
operator|+
name|reader
operator|.
name|getNumberOfRows
argument_list|()
operator|-
literal|1
argument_list|)
expr_stmt|;
block|}
name|this
operator|.
name|minKey
operator|=
name|newMinKey
expr_stmt|;
name|this
operator|.
name|maxKey
operator|=
name|newMaxKey
expr_stmt|;
comment|// advance the reader until we reach the minimum key
do|do
block|{
name|next
argument_list|(
name|nextRecord
argument_list|()
argument_list|)
expr_stmt|;
block|}
do|while
condition|(
name|nextRecord
argument_list|()
operator|!=
literal|null
operator|&&
operator|(
name|getMinKey
argument_list|()
operator|!=
literal|null
operator|&&
name|this
operator|.
name|getKey
argument_list|()
operator|.
name|compareRow
argument_list|(
name|getMinKey
argument_list|()
argument_list|)
operator|<=
literal|0
operator|)
condition|)
do|;
block|}
annotation|@
name|Override
specifier|public
name|RecordReader
name|getRecordReader
parameter_list|()
block|{
return|return
name|recordReader
return|;
block|}
annotation|@
name|Override
specifier|public
name|Reader
name|getReader
parameter_list|()
block|{
return|return
name|reader
return|;
block|}
annotation|@
name|Override
specifier|public
name|RecordIdentifier
name|getMinKey
parameter_list|()
block|{
return|return
name|minKey
return|;
block|}
annotation|@
name|Override
specifier|public
name|RecordIdentifier
name|getMaxKey
parameter_list|()
block|{
return|return
name|maxKey
return|;
block|}
annotation|@
name|Override
specifier|public
name|long
name|getRowIdOffset
parameter_list|()
block|{
return|return
name|rowIdOffset
return|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|next
parameter_list|(
name|OrcStruct
name|next
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
operator|!
name|nextFromCurrentFile
argument_list|(
name|next
argument_list|)
condition|)
block|{
comment|//only have 1 file so done
name|nextRecord
operator|=
literal|null
expr_stmt|;
name|getRecordReader
argument_list|()
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
block|}
annotation|@
name|VisibleForTesting
specifier|final
specifier|static
class|class
name|OriginalReaderPairToCompact
extends|extends
name|OriginalReaderPair
block|{
comment|/**      * See {@link AcidUtils.Directory#getOriginalFiles()}.  This list has a fixed sort order.      * It includes all original files (for all buckets).        */
specifier|private
specifier|final
name|List
argument_list|<
name|HadoopShims
operator|.
name|HdfsFileStatusWithId
argument_list|>
name|originalFiles
decl_stmt|;
comment|/**      * index into {@link #originalFiles}      */
specifier|private
name|int
name|nextFileIndex
init|=
literal|0
decl_stmt|;
specifier|private
name|Reader
name|reader
decl_stmt|;
specifier|private
name|RecordReader
name|recordReader
init|=
literal|null
decl_stmt|;
specifier|private
specifier|final
name|Configuration
name|conf
decl_stmt|;
specifier|private
specifier|final
name|Reader
operator|.
name|Options
name|options
decl_stmt|;
specifier|private
name|long
name|rowIdOffset
init|=
literal|0
decl_stmt|;
name|OriginalReaderPairToCompact
parameter_list|(
name|ReaderKey
name|key
parameter_list|,
name|int
name|bucketId
parameter_list|,
name|Reader
operator|.
name|Options
name|options
parameter_list|,
name|Options
name|mergerOptions
parameter_list|,
name|Configuration
name|conf
parameter_list|,
name|ValidWriteIdList
name|validWriteIdList
parameter_list|,
name|int
name|statementId
parameter_list|)
throws|throws
name|IOException
block|{
name|super
argument_list|(
name|key
argument_list|,
name|bucketId
argument_list|,
name|conf
argument_list|,
name|mergerOptions
argument_list|,
name|statementId
argument_list|)
expr_stmt|;
assert|assert
name|mergerOptions
operator|.
name|isCompacting
argument_list|()
operator|:
literal|"Should only be used for Compaction"
assert|;
name|this
operator|.
name|conf
operator|=
name|conf
expr_stmt|;
name|this
operator|.
name|options
operator|=
name|options
expr_stmt|;
assert|assert
name|mergerOptions
operator|.
name|getRootPath
argument_list|()
operator|!=
literal|null
operator|:
literal|"Since we have original files"
assert|;
assert|assert
name|this
operator|.
name|bucketId
operator|>=
literal|0
operator|:
literal|"don't support non-bucketed tables yet"
assert|;
comment|//when compacting each split needs to process the whole logical bucket
assert|assert
name|options
operator|.
name|getOffset
argument_list|()
operator|==
literal|0
assert|;
assert|assert
name|options
operator|.
name|getMaxOffset
argument_list|()
operator|==
name|Long
operator|.
name|MAX_VALUE
assert|;
name|AcidUtils
operator|.
name|Directory
name|directoryState
init|=
name|AcidUtils
operator|.
name|getAcidState
argument_list|(
literal|null
argument_list|,
name|mergerOptions
operator|.
name|getRootPath
argument_list|()
argument_list|,
name|conf
argument_list|,
name|validWriteIdList
argument_list|,
name|Ref
operator|.
name|from
argument_list|(
literal|false
argument_list|)
argument_list|,
literal|true
argument_list|,
literal|null
argument_list|,
literal|true
argument_list|)
decl_stmt|;
comment|/**        * Note that for reading base_x/ or delta_x_x/ with non-acid schema,        * {@link Options#getRootPath()} is set to base_x/ or delta_x_x/ which causes all it's        * contents to be in {@link org.apache.hadoop.hive.ql.io.AcidUtils.Directory#getOriginalFiles()}        */
name|originalFiles
operator|=
name|directoryState
operator|.
name|getOriginalFiles
argument_list|()
expr_stmt|;
assert|assert
name|originalFiles
operator|.
name|size
argument_list|()
operator|>
literal|0
assert|;
comment|//in case of Compaction, this is the 1st file of the current bucket
name|this
operator|.
name|reader
operator|=
name|advanceToNextFile
argument_list|()
expr_stmt|;
if|if
condition|(
name|reader
operator|==
literal|null
condition|)
block|{
comment|//Compactor generated a split for a bucket that has no data?
throw|throw
operator|new
name|IllegalStateException
argument_list|(
literal|"No 'original' files found for bucketId="
operator|+
name|this
operator|.
name|bucketId
operator|+
literal|" in "
operator|+
name|mergerOptions
operator|.
name|getRootPath
argument_list|()
argument_list|)
throw|;
block|}
name|recordReader
operator|=
name|getReader
argument_list|()
operator|.
name|rowsOptions
argument_list|(
name|options
argument_list|,
name|conf
argument_list|)
expr_stmt|;
name|next
argument_list|(
name|nextRecord
argument_list|()
argument_list|)
expr_stmt|;
comment|//load 1st row
block|}
annotation|@
name|Override
specifier|public
name|RecordReader
name|getRecordReader
parameter_list|()
block|{
return|return
name|recordReader
return|;
block|}
annotation|@
name|Override
specifier|public
name|Reader
name|getReader
parameter_list|()
block|{
return|return
name|reader
return|;
block|}
annotation|@
name|Override
specifier|public
name|RecordIdentifier
name|getMinKey
parameter_list|()
block|{
return|return
literal|null
return|;
block|}
annotation|@
name|Override
specifier|public
name|RecordIdentifier
name|getMaxKey
parameter_list|()
block|{
return|return
literal|null
return|;
block|}
annotation|@
name|Override
specifier|public
name|long
name|getRowIdOffset
parameter_list|()
block|{
return|return
name|rowIdOffset
return|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|next
parameter_list|(
name|OrcStruct
name|next
parameter_list|)
throws|throws
name|IOException
block|{
while|while
condition|(
literal|true
condition|)
block|{
if|if
condition|(
name|nextFromCurrentFile
argument_list|(
name|next
argument_list|)
condition|)
block|{
return|return;
block|}
else|else
block|{
if|if
condition|(
name|originalFiles
operator|.
name|size
argument_list|()
operator|<=
name|nextFileIndex
condition|)
block|{
comment|//no more original files to read
name|nextRecord
operator|=
literal|null
expr_stmt|;
name|recordReader
operator|.
name|close
argument_list|()
expr_stmt|;
return|return;
block|}
else|else
block|{
name|rowIdOffset
operator|+=
name|reader
operator|.
name|getNumberOfRows
argument_list|()
expr_stmt|;
name|recordReader
operator|.
name|close
argument_list|()
expr_stmt|;
name|reader
operator|=
name|advanceToNextFile
argument_list|()
expr_stmt|;
if|if
condition|(
name|reader
operator|==
literal|null
condition|)
block|{
name|nextRecord
operator|=
literal|null
expr_stmt|;
return|return;
block|}
name|recordReader
operator|=
name|reader
operator|.
name|rowsOptions
argument_list|(
name|options
argument_list|,
name|conf
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
comment|/**      * Finds the next file of the logical bucket      * @return {@code null} if there are no more files      */
specifier|private
name|Reader
name|advanceToNextFile
parameter_list|()
throws|throws
name|IOException
block|{
while|while
condition|(
name|nextFileIndex
operator|<
name|originalFiles
operator|.
name|size
argument_list|()
condition|)
block|{
name|int
name|bucketIdFromPath
init|=
name|AcidUtils
operator|.
name|parseBucketId
argument_list|(
name|originalFiles
operator|.
name|get
argument_list|(
name|nextFileIndex
argument_list|)
operator|.
name|getFileStatus
argument_list|()
operator|.
name|getPath
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|bucketIdFromPath
operator|==
name|bucketId
condition|)
block|{
break|break;
block|}
comment|//the the bucket we care about here
name|nextFileIndex
operator|++
expr_stmt|;
block|}
if|if
condition|(
name|originalFiles
operator|.
name|size
argument_list|()
operator|<=
name|nextFileIndex
condition|)
block|{
return|return
literal|null
return|;
comment|//no more files for current bucket
block|}
return|return
name|OrcFile
operator|.
name|createReader
argument_list|(
name|originalFiles
operator|.
name|get
argument_list|(
name|nextFileIndex
operator|++
argument_list|)
operator|.
name|getFileStatus
argument_list|()
operator|.
name|getPath
argument_list|()
argument_list|,
name|OrcFile
operator|.
name|readerOptions
argument_list|(
name|conf
argument_list|)
argument_list|)
return|;
block|}
block|}
comment|/**    * The process here reads several (base + some deltas) files each of which is sorted on     * {@link ReaderKey} ascending.  The output of this Reader should a global order across these    * files.  The root of this tree is always the next 'file' to read from.    */
specifier|private
specifier|final
name|TreeMap
argument_list|<
name|ReaderKey
argument_list|,
name|ReaderPair
argument_list|>
name|readers
init|=
operator|new
name|TreeMap
argument_list|<>
argument_list|()
decl_stmt|;
comment|// The reader that currently has the lowest key.
specifier|private
name|ReaderPair
name|primary
decl_stmt|;
comment|// The key of the next lowest reader.
specifier|private
name|ReaderKey
name|secondaryKey
init|=
literal|null
decl_stmt|;
specifier|static
specifier|final
class|class
name|KeyInterval
block|{
specifier|private
specifier|final
name|RecordIdentifier
name|minKey
decl_stmt|;
specifier|private
specifier|final
name|RecordIdentifier
name|maxKey
decl_stmt|;
name|KeyInterval
parameter_list|(
name|RecordIdentifier
name|minKey
parameter_list|,
name|RecordIdentifier
name|maxKey
parameter_list|)
block|{
name|this
operator|.
name|minKey
operator|=
name|minKey
expr_stmt|;
name|this
operator|.
name|maxKey
operator|=
name|maxKey
expr_stmt|;
block|}
name|RecordIdentifier
name|getMinKey
parameter_list|()
block|{
return|return
name|minKey
return|;
block|}
name|RecordIdentifier
name|getMaxKey
parameter_list|()
block|{
return|return
name|maxKey
return|;
block|}
empty_stmt|;
annotation|@
name|Override
specifier|public
name|String
name|toString
parameter_list|()
block|{
return|return
literal|"KeyInterval["
operator|+
name|minKey
operator|+
literal|","
operator|+
name|maxKey
operator|+
literal|"]"
return|;
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|equals
parameter_list|(
name|Object
name|other
parameter_list|)
block|{
if|if
condition|(
operator|!
operator|(
name|other
operator|instanceof
name|KeyInterval
operator|)
condition|)
block|{
return|return
literal|false
return|;
block|}
name|KeyInterval
name|otherInterval
init|=
operator|(
name|KeyInterval
operator|)
name|other
decl_stmt|;
return|return
name|Objects
operator|.
name|equals
argument_list|(
name|minKey
argument_list|,
name|otherInterval
operator|.
name|getMinKey
argument_list|()
argument_list|)
operator|&&
name|Objects
operator|.
name|equals
argument_list|(
name|maxKey
argument_list|,
name|otherInterval
operator|.
name|getMaxKey
argument_list|()
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|public
name|int
name|hashCode
parameter_list|()
block|{
return|return
name|Objects
operator|.
name|hash
argument_list|(
name|minKey
argument_list|,
name|maxKey
argument_list|)
return|;
block|}
block|}
comment|/**    * Find the key range for original bucket files.    * For unbucketed tables the insert event data is still written to bucket_N file except that    * N is just a writer ID - it still matches {@link RecordIdentifier#getBucketProperty()}.  For    * 'original' files (ubucketed) the same applies.  A file 000000_0 encodes a taskId/wirterId and    * at read time we synthesize {@link RecordIdentifier#getBucketProperty()} to match the file name    * and so the same bucketProperty is used here to create minKey/maxKey, i.e. these keys are valid    * to filter data from delete_delta files even for unbucketed tables.    * @param reader the reader    * @param bucket the bucket number we are reading    * @param options the options for reading with    * @throws IOException    */
specifier|private
name|KeyInterval
name|discoverOriginalKeyBounds
parameter_list|(
name|Reader
name|reader
parameter_list|,
name|int
name|bucket
parameter_list|,
name|Reader
operator|.
name|Options
name|options
parameter_list|,
name|Configuration
name|conf
parameter_list|,
name|Options
name|mergerOptions
parameter_list|)
throws|throws
name|IOException
block|{
name|long
name|rowLength
init|=
literal|0
decl_stmt|;
name|long
name|rowOffset
init|=
literal|0
decl_stmt|;
name|long
name|offset
init|=
name|options
operator|.
name|getOffset
argument_list|()
decl_stmt|;
comment|//this would usually be at block boundary
name|long
name|maxOffset
init|=
name|options
operator|.
name|getMaxOffset
argument_list|()
decl_stmt|;
comment|//this would usually be at block boundary
name|boolean
name|isTail
init|=
literal|true
decl_stmt|;
name|RecordIdentifier
name|minKey
init|=
literal|null
decl_stmt|;
name|RecordIdentifier
name|maxKey
init|=
literal|null
decl_stmt|;
name|TransactionMetaData
name|tfp
init|=
name|TransactionMetaData
operator|.
name|findWriteIDForSynthetcRowIDs
argument_list|(
name|mergerOptions
operator|.
name|getBucketPath
argument_list|()
argument_list|,
name|mergerOptions
operator|.
name|getRootPath
argument_list|()
argument_list|,
name|conf
argument_list|)
decl_stmt|;
name|int
name|bucketProperty
init|=
name|encodeBucketId
argument_list|(
name|conf
argument_list|,
name|bucket
argument_list|,
name|tfp
operator|.
name|statementId
argument_list|)
decl_stmt|;
comment|/**     * options.getOffset() and getMaxOffset() would usually be at block boundary which doesn't     * necessarily match stripe boundary.  So we want to come up with minKey to be one before the 1st     * row of the first stripe that starts after getOffset() and maxKey to be the last row of the     * stripe that contains getMaxOffset().  This breaks if getOffset() and getMaxOffset() are inside     * the sames tripe - in this case we have minKey& isTail=false but rowLength is never set.     * (HIVE-16953)     */
for|for
control|(
name|StripeInformation
name|stripe
range|:
name|reader
operator|.
name|getStripes
argument_list|()
control|)
block|{
if|if
condition|(
name|offset
operator|>
name|stripe
operator|.
name|getOffset
argument_list|()
condition|)
block|{
name|rowOffset
operator|+=
name|stripe
operator|.
name|getNumberOfRows
argument_list|()
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|maxOffset
operator|>
name|stripe
operator|.
name|getOffset
argument_list|()
condition|)
block|{
name|rowLength
operator|+=
name|stripe
operator|.
name|getNumberOfRows
argument_list|()
expr_stmt|;
block|}
else|else
block|{
name|isTail
operator|=
literal|false
expr_stmt|;
break|break;
block|}
block|}
if|if
condition|(
name|rowOffset
operator|>
literal|0
condition|)
block|{
name|minKey
operator|=
operator|new
name|RecordIdentifier
argument_list|(
name|tfp
operator|.
name|syntheticWriteId
argument_list|,
name|bucketProperty
argument_list|,
name|rowOffset
operator|-
literal|1
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
operator|!
name|isTail
condition|)
block|{
name|maxKey
operator|=
operator|new
name|RecordIdentifier
argument_list|(
name|tfp
operator|.
name|syntheticWriteId
argument_list|,
name|bucketProperty
argument_list|,
name|rowOffset
operator|+
name|rowLength
operator|-
literal|1
argument_list|)
expr_stmt|;
block|}
return|return
operator|new
name|KeyInterval
argument_list|(
name|minKey
argument_list|,
name|maxKey
argument_list|)
return|;
block|}
comment|/**    * Find the key range for the split (of the base).  These are used to filter delta files since    * both are sorted by key.    * @param reader the reader    * @param options the options for reading with    * @throws IOException    */
specifier|private
name|KeyInterval
name|discoverKeyBounds
parameter_list|(
name|Reader
name|reader
parameter_list|,
name|Reader
operator|.
name|Options
name|options
parameter_list|)
throws|throws
name|IOException
block|{
name|RecordIdentifier
index|[]
name|keyIndex
init|=
name|OrcRecordUpdater
operator|.
name|parseKeyIndex
argument_list|(
name|reader
argument_list|)
decl_stmt|;
name|long
name|offset
init|=
name|options
operator|.
name|getOffset
argument_list|()
decl_stmt|;
name|long
name|maxOffset
init|=
name|options
operator|.
name|getMaxOffset
argument_list|()
decl_stmt|;
name|int
name|firstStripe
init|=
literal|0
decl_stmt|;
name|int
name|stripeCount
init|=
literal|0
decl_stmt|;
name|boolean
name|isTail
init|=
literal|true
decl_stmt|;
name|RecordIdentifier
name|minKey
init|=
literal|null
decl_stmt|;
name|RecordIdentifier
name|maxKey
init|=
literal|null
decl_stmt|;
name|List
argument_list|<
name|StripeInformation
argument_list|>
name|stripes
init|=
name|reader
operator|.
name|getStripes
argument_list|()
decl_stmt|;
for|for
control|(
name|StripeInformation
name|stripe
range|:
name|stripes
control|)
block|{
if|if
condition|(
name|offset
operator|>
name|stripe
operator|.
name|getOffset
argument_list|()
condition|)
block|{
name|firstStripe
operator|+=
literal|1
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|maxOffset
operator|>
name|stripe
operator|.
name|getOffset
argument_list|()
condition|)
block|{
name|stripeCount
operator|+=
literal|1
expr_stmt|;
block|}
else|else
block|{
name|isTail
operator|=
literal|false
expr_stmt|;
break|break;
block|}
block|}
if|if
condition|(
name|firstStripe
operator|!=
literal|0
condition|)
block|{
name|minKey
operator|=
name|keyIndex
index|[
name|firstStripe
operator|-
literal|1
index|]
expr_stmt|;
block|}
if|if
condition|(
operator|!
name|isTail
condition|)
block|{
name|maxKey
operator|=
name|keyIndex
index|[
name|firstStripe
operator|+
name|stripeCount
operator|-
literal|1
index|]
expr_stmt|;
block|}
return|return
operator|new
name|KeyInterval
argument_list|(
name|minKey
argument_list|,
name|maxKey
argument_list|)
return|;
block|}
comment|/**    * Convert from the row include/sarg/columnNames to the event equivalent    * for the underlying file.    * @param options options for the row reader    * @param rowSchema schema of the row, excluding ACID columns    * @return a cloned options object that is modified for the event reader    */
specifier|static
name|Reader
operator|.
name|Options
name|createEventOptions
parameter_list|(
name|Reader
operator|.
name|Options
name|options
parameter_list|,
name|TypeDescription
name|rowSchema
parameter_list|)
block|{
name|Reader
operator|.
name|Options
name|result
init|=
name|options
operator|.
name|clone
argument_list|()
decl_stmt|;
name|result
operator|.
name|include
argument_list|(
name|options
operator|.
name|getInclude
argument_list|()
argument_list|)
expr_stmt|;
comment|// slide the column names down by 6 for the name array
if|if
condition|(
name|options
operator|.
name|getColumnNames
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|String
index|[]
name|orig
init|=
name|options
operator|.
name|getColumnNames
argument_list|()
decl_stmt|;
name|String
index|[]
name|cols
init|=
operator|new
name|String
index|[
name|orig
operator|.
name|length
operator|+
name|OrcRecordUpdater
operator|.
name|FIELDS
index|]
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|orig
operator|.
name|length
condition|;
operator|++
name|i
control|)
block|{
name|cols
index|[
name|i
operator|+
name|OrcRecordUpdater
operator|.
name|FIELDS
index|]
operator|=
name|orig
index|[
name|i
index|]
expr_stmt|;
block|}
name|result
operator|.
name|searchArgument
argument_list|(
name|options
operator|.
name|getSearchArgument
argument_list|()
argument_list|,
name|cols
argument_list|)
expr_stmt|;
block|}
comment|// schema evolution will insert the acid columns to row schema for ACID read
name|result
operator|.
name|schema
argument_list|(
name|rowSchema
argument_list|)
expr_stmt|;
return|return
name|result
return|;
block|}
comment|/**    * {@link OrcRawRecordMerger} Acid reader is used slightly differently in various contexts.    * This makes the "context" explicit.    */
specifier|static
class|class
name|Options
implements|implements
name|Cloneable
block|{
specifier|private
name|int
name|copyIndex
init|=
literal|0
decl_stmt|;
specifier|private
name|boolean
name|isCompacting
init|=
literal|false
decl_stmt|;
specifier|private
name|Path
name|bucketPath
decl_stmt|;
specifier|private
name|Path
name|rootPath
decl_stmt|;
specifier|private
name|Path
name|baseDir
decl_stmt|;
specifier|private
name|boolean
name|isMajorCompaction
init|=
literal|false
decl_stmt|;
specifier|private
name|boolean
name|isDeleteReader
init|=
literal|false
decl_stmt|;
specifier|private
name|long
name|writeId
init|=
literal|0
decl_stmt|;
name|Options
name|copyIndex
parameter_list|(
name|int
name|copyIndex
parameter_list|)
block|{
assert|assert
name|copyIndex
operator|>=
literal|0
assert|;
name|this
operator|.
name|copyIndex
operator|=
name|copyIndex
expr_stmt|;
return|return
name|this
return|;
block|}
name|Options
name|isCompacting
parameter_list|(
name|boolean
name|isCompacting
parameter_list|)
block|{
name|this
operator|.
name|isCompacting
operator|=
name|isCompacting
expr_stmt|;
assert|assert
operator|!
name|isDeleteReader
assert|;
return|return
name|this
return|;
block|}
name|Options
name|bucketPath
parameter_list|(
name|Path
name|bucketPath
parameter_list|)
block|{
name|this
operator|.
name|bucketPath
operator|=
name|bucketPath
expr_stmt|;
return|return
name|this
return|;
block|}
name|Options
name|rootPath
parameter_list|(
name|Path
name|rootPath
parameter_list|)
block|{
name|this
operator|.
name|rootPath
operator|=
name|rootPath
expr_stmt|;
return|return
name|this
return|;
block|}
name|Options
name|isMajorCompaction
parameter_list|(
name|boolean
name|isMajor
parameter_list|)
block|{
name|this
operator|.
name|isMajorCompaction
operator|=
name|isMajor
expr_stmt|;
assert|assert
operator|!
name|isDeleteReader
assert|;
return|return
name|this
return|;
block|}
name|Options
name|isDeleteReader
parameter_list|(
name|boolean
name|isDeleteReader
parameter_list|)
block|{
name|this
operator|.
name|isDeleteReader
operator|=
name|isDeleteReader
expr_stmt|;
assert|assert
operator|!
name|isCompacting
assert|;
return|return
name|this
return|;
block|}
name|Options
name|writeId
parameter_list|(
name|long
name|writeId
parameter_list|)
block|{
name|this
operator|.
name|writeId
operator|=
name|writeId
expr_stmt|;
return|return
name|this
return|;
block|}
name|Options
name|baseDir
parameter_list|(
name|Path
name|baseDir
parameter_list|)
block|{
name|this
operator|.
name|baseDir
operator|=
name|baseDir
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**      * 0 means it's the original file, without {@link Utilities#COPY_KEYWORD} suffix      */
name|int
name|getCopyIndex
parameter_list|()
block|{
return|return
name|copyIndex
return|;
block|}
name|boolean
name|isCompacting
parameter_list|()
block|{
return|return
name|isCompacting
return|;
block|}
comment|/**      * Full path to the data file      */
name|Path
name|getBucketPath
parameter_list|()
block|{
return|return
name|bucketPath
return|;
block|}
comment|/**      * Partition folder (Table folder if not partitioned)      */
name|Path
name|getRootPath
parameter_list|()
block|{
return|return
name|rootPath
return|;
block|}
comment|/**      * @return true if major compaction, false if minor      */
name|boolean
name|isMajorCompaction
parameter_list|()
block|{
return|return
name|isMajorCompaction
operator|&&
name|isCompacting
return|;
block|}
name|boolean
name|isMinorCompaction
parameter_list|()
block|{
return|return
operator|!
name|isMajorCompaction
operator|&&
name|isCompacting
return|;
block|}
comment|/**      * true if this is only processing delete deltas to load in-memory table for      * vectorized reader      */
name|boolean
name|isDeleteReader
parameter_list|()
block|{
return|return
name|isDeleteReader
return|;
block|}
comment|/**      * for reading "original" files - i.e. not native acid schema.  Default value of 0 is      * appropriate for files that existed in a table before it was made transactional.  0 is the      * primordial transaction.  For non-native files resulting from Load Data command, they      * are located and base_x or delta_x_x and then writeId == x.      */
name|long
name|getWriteId
parameter_list|()
block|{
return|return
name|writeId
return|;
block|}
comment|/**      * In case of isMajorCompaction() this is the base dir from the Compactor, i.e. either a base_x      * or {@link #rootPath} if it's the 1st major compaction after non-acid2acid conversion      */
name|Path
name|getBaseDir
parameter_list|()
block|{
return|return
name|baseDir
return|;
block|}
comment|/**      * shallow clone      */
specifier|public
name|Options
name|clone
parameter_list|()
block|{
try|try
block|{
return|return
operator|(
name|Options
operator|)
name|super
operator|.
name|clone
argument_list|()
return|;
block|}
catch|catch
parameter_list|(
name|CloneNotSupportedException
name|ex
parameter_list|)
block|{
throw|throw
operator|new
name|AssertionError
argument_list|()
throw|;
block|}
block|}
block|}
name|OrcRawRecordMerger
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|boolean
name|collapseEvents
parameter_list|,
name|Reader
name|reader
parameter_list|,
name|boolean
name|isOriginal
parameter_list|,
name|int
name|bucket
parameter_list|,
name|ValidWriteIdList
name|validWriteIdList
parameter_list|,
name|Reader
operator|.
name|Options
name|options
parameter_list|,
name|Path
index|[]
name|deltaDirectory
parameter_list|,
name|Options
name|mergerOptions
parameter_list|)
throws|throws
name|IOException
block|{
name|this
argument_list|(
name|conf
argument_list|,
name|collapseEvents
argument_list|,
name|reader
argument_list|,
name|isOriginal
argument_list|,
name|bucket
argument_list|,
name|validWriteIdList
argument_list|,
name|options
argument_list|,
name|deltaDirectory
argument_list|,
name|mergerOptions
argument_list|,
literal|null
argument_list|)
expr_stmt|;
block|}
comment|/**    * Create a reader that merge sorts the ACID events together.  This handles    * 1. 'normal' reads on behalf of a query (non vectorized)    * 2. Compaction reads (major/minor)    * 3. Delete event reads - to create a sorted view of all delete events for vectorized read    *    * This makes the logic in the constructor confusing and needs to be refactored.  Liberal use of    * asserts below is primarily for documentation purposes.    *    * @param conf the configuration    * @param collapseEvents should the events on the same row be collapsed    * @param isOriginal if reading filws w/o acid schema - {@link AcidUtils.AcidBaseFileType#ORIGINAL_BASE}    * @param bucket the bucket/writer id of the file we are reading    * @param options the options to read with    * @param deltaDirectory the list of delta directories to include    * @throws IOException    */
name|OrcRawRecordMerger
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|boolean
name|collapseEvents
parameter_list|,
name|Reader
name|reader
parameter_list|,
name|boolean
name|isOriginal
parameter_list|,
name|int
name|bucket
parameter_list|,
name|ValidWriteIdList
name|validWriteIdList
parameter_list|,
name|Reader
operator|.
name|Options
name|options
parameter_list|,
name|Path
index|[]
name|deltaDirectory
parameter_list|,
name|Options
name|mergerOptions
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|deltasToAttemptId
parameter_list|)
throws|throws
name|IOException
block|{
name|this
operator|.
name|collapse
operator|=
name|collapseEvents
expr_stmt|;
name|this
operator|.
name|offset
operator|=
name|options
operator|.
name|getOffset
argument_list|()
expr_stmt|;
name|this
operator|.
name|length
operator|=
name|options
operator|.
name|getLength
argument_list|()
expr_stmt|;
name|this
operator|.
name|validWriteIdList
operator|=
name|validWriteIdList
expr_stmt|;
comment|/**      * @since Hive 3.0      * With split update (HIVE-14035) we have base/, delta/ and delete_delta/ - the latter only      * has Delete events and the others only have Insert events.  Thus {@link #baseReader} is      * a split of a file in base/ or delta/.      *      * For Compaction, each split (for now) is a logical bucket, i.e. all files from base/ + delta(s)/      * for a given bucket ID and delete_delta(s)/      *      * For bucketed tables, the data files are named bucket_N and all rows in this file are such      * that {@link org.apache.hadoop.hive.ql.io.BucketCodec#decodeWriterId(int)} of      * {@link RecordIdentifier#getBucketProperty()} is N.  This is currently true for all types of      * files but may not be true for for delete_delta/ files in the future.      *      * For un-bucketed tables, the system is designed so that it works when there is no relationship      * between delete_delta file name (bucket_N) and the value of {@link RecordIdentifier#getBucketProperty()}.      * (Later we this maybe optimized to take advantage of situations where it is known that      * bucket_N matches bucketProperty().)  This implies that for a given {@link baseReader} all      * files in delete_delta/ have to be opened ({@link ReaderPair} created).  Insert events are      * still written such that N in file name (writerId) matches what's in bucketProperty().      *      * Compactor for un-bucketed tables works exactly the same as for bucketed ones though it      * should be optimized (see HIVE-17206).  In particular, each split is a set of files      * created by a writer with the same writerId, i.e. all bucket_N files across base/&      * deleta/ for the same N. Unlike bucketed tables, there is no relationship between      * any values in user columns to file name.      * The maximum N is determined by the number of writers the system chose for the the "largest"      * write into a given partition.      *      * In both cases, Compactor should be changed so that Minor compaction is run very often and      * only compacts delete_delta/.  Major compaction can do what it does now.      */
name|boolean
name|isBucketed
init|=
name|conf
operator|.
name|getInt
argument_list|(
name|hive_metastoreConstants
operator|.
name|BUCKET_COUNT
argument_list|,
literal|0
argument_list|)
operator|>
literal|0
decl_stmt|;
name|TypeDescription
name|typeDescr
init|=
name|OrcInputFormat
operator|.
name|getDesiredRowTypeDescr
argument_list|(
name|conf
argument_list|,
literal|true
argument_list|,
name|Integer
operator|.
name|MAX_VALUE
argument_list|)
decl_stmt|;
name|objectInspector
operator|=
name|OrcRecordUpdater
operator|.
name|createEventObjectInspector
argument_list|(
name|OrcStruct
operator|.
name|createObjectInspector
argument_list|(
literal|0
argument_list|,
name|OrcUtils
operator|.
name|getOrcTypes
argument_list|(
name|typeDescr
argument_list|)
argument_list|)
argument_list|)
expr_stmt|;
assert|assert
operator|!
operator|(
name|mergerOptions
operator|.
name|isCompacting
argument_list|()
operator|&&
name|reader
operator|!=
literal|null
operator|)
operator|:
literal|"don't need a reader for compaction"
assert|;
comment|// modify the options to reflect the event instead of the base row
name|Reader
operator|.
name|Options
name|eventOptions
init|=
name|createEventOptions
argument_list|(
name|options
argument_list|,
name|typeDescr
argument_list|)
decl_stmt|;
comment|//suppose it's the first Major compaction so we only have deltas
name|boolean
name|isMajorNoBase
init|=
name|mergerOptions
operator|.
name|isCompacting
argument_list|()
operator|&&
name|mergerOptions
operator|.
name|isMajorCompaction
argument_list|()
operator|&&
name|mergerOptions
operator|.
name|getBaseDir
argument_list|()
operator|==
literal|null
decl_stmt|;
if|if
condition|(
operator|(
name|mergerOptions
operator|.
name|isCompacting
argument_list|()
operator|&&
name|mergerOptions
operator|.
name|isMinorCompaction
argument_list|()
operator|)
operator|||
name|mergerOptions
operator|.
name|isDeleteReader
argument_list|()
operator|||
name|isMajorNoBase
condition|)
block|{
comment|//for minor compaction, there is no progress report and we don't filter deltas
name|baseReader
operator|=
literal|null
expr_stmt|;
name|minKey
operator|=
name|maxKey
operator|=
literal|null
expr_stmt|;
assert|assert
name|reader
operator|==
literal|null
operator|:
literal|"unexpected input reader during minor compaction: "
operator|+
name|mergerOptions
operator|.
name|getRootPath
argument_list|()
assert|;
block|}
else|else
block|{
name|KeyInterval
name|keyInterval
decl_stmt|;
if|if
condition|(
name|mergerOptions
operator|.
name|isCompacting
argument_list|()
condition|)
block|{
assert|assert
name|mergerOptions
operator|.
name|isMajorCompaction
argument_list|()
assert|;
comment|//compaction doesn't filter deltas but *may* have a reader for 'base'
name|keyInterval
operator|=
operator|new
name|KeyInterval
argument_list|(
literal|null
argument_list|,
literal|null
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// find the min/max based on the offset and length (and more for 'original')
if|if
condition|(
name|isOriginal
condition|)
block|{
comment|//note that this KeyInterval may be adjusted later due to copy_N files
name|keyInterval
operator|=
name|discoverOriginalKeyBounds
argument_list|(
name|reader
argument_list|,
name|bucket
argument_list|,
name|options
argument_list|,
name|conf
argument_list|,
name|mergerOptions
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|keyInterval
operator|=
name|discoverKeyBounds
argument_list|(
name|reader
argument_list|,
name|options
argument_list|)
expr_stmt|;
block|}
block|}
name|LOG
operator|.
name|info
argument_list|(
literal|"min key = "
operator|+
name|keyInterval
operator|.
name|getMinKey
argument_list|()
operator|+
literal|", max key = "
operator|+
name|keyInterval
operator|.
name|getMaxKey
argument_list|()
argument_list|)
expr_stmt|;
comment|// use the min/max instead of the byte range
name|ReaderPair
name|pair
init|=
literal|null
decl_stmt|;
name|ReaderKey
name|baseKey
init|=
operator|new
name|ReaderKey
argument_list|()
decl_stmt|;
if|if
condition|(
name|isOriginal
condition|)
block|{
name|options
operator|=
name|options
operator|.
name|clone
argument_list|()
expr_stmt|;
if|if
condition|(
name|mergerOptions
operator|.
name|isCompacting
argument_list|()
condition|)
block|{
assert|assert
name|mergerOptions
operator|.
name|isMajorCompaction
argument_list|()
assert|;
name|Options
name|readerPairOptions
init|=
name|mergerOptions
decl_stmt|;
if|if
condition|(
name|mergerOptions
operator|.
name|getBaseDir
argument_list|()
operator|.
name|getName
argument_list|()
operator|.
name|startsWith
argument_list|(
name|AcidUtils
operator|.
name|BASE_PREFIX
argument_list|)
condition|)
block|{
name|readerPairOptions
operator|=
name|modifyForNonAcidSchemaRead
argument_list|(
name|mergerOptions
argument_list|,
name|AcidUtils
operator|.
name|ParsedBase
operator|.
name|parseBase
argument_list|(
name|mergerOptions
operator|.
name|getBaseDir
argument_list|()
argument_list|)
operator|.
name|getWriteId
argument_list|()
argument_list|,
name|mergerOptions
operator|.
name|getBaseDir
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|pair
operator|=
operator|new
name|OriginalReaderPairToCompact
argument_list|(
name|baseKey
argument_list|,
name|bucket
argument_list|,
name|options
argument_list|,
name|readerPairOptions
argument_list|,
name|conf
argument_list|,
name|validWriteIdList
argument_list|,
literal|0
argument_list|)
expr_stmt|;
comment|//0 since base_x doesn't have a suffix (neither does pre acid write)
block|}
else|else
block|{
assert|assert
name|mergerOptions
operator|.
name|getBucketPath
argument_list|()
operator|!=
literal|null
operator|:
literal|" since this is not compaction: "
operator|+
name|mergerOptions
operator|.
name|getRootPath
argument_list|()
assert|;
comment|//if here it's a non-acid schema file - check if from before table was marked transactional
comment|//or in base_x/delta_x_x from Load Data
name|Options
name|readerPairOptions
init|=
name|mergerOptions
decl_stmt|;
name|TransactionMetaData
name|tfp
init|=
name|TransactionMetaData
operator|.
name|findWriteIDForSynthetcRowIDs
argument_list|(
name|mergerOptions
operator|.
name|getBucketPath
argument_list|()
argument_list|,
name|mergerOptions
operator|.
name|getRootPath
argument_list|()
argument_list|,
name|conf
argument_list|)
decl_stmt|;
if|if
condition|(
name|tfp
operator|.
name|syntheticWriteId
operator|>
literal|0
condition|)
block|{
name|readerPairOptions
operator|=
name|modifyForNonAcidSchemaRead
argument_list|(
name|mergerOptions
argument_list|,
name|tfp
operator|.
name|syntheticWriteId
argument_list|,
name|tfp
operator|.
name|folder
argument_list|)
expr_stmt|;
block|}
name|pair
operator|=
operator|new
name|OriginalReaderPairToRead
argument_list|(
name|baseKey
argument_list|,
name|reader
argument_list|,
name|bucket
argument_list|,
name|keyInterval
operator|.
name|getMinKey
argument_list|()
argument_list|,
name|keyInterval
operator|.
name|getMaxKey
argument_list|()
argument_list|,
name|options
argument_list|,
name|readerPairOptions
argument_list|,
name|conf
argument_list|,
name|validWriteIdList
argument_list|,
name|tfp
operator|.
name|statementId
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
if|if
condition|(
name|mergerOptions
operator|.
name|isCompacting
argument_list|()
condition|)
block|{
assert|assert
name|mergerOptions
operator|.
name|isMajorCompaction
argument_list|()
operator|:
literal|"expected major compaction: "
operator|+
name|mergerOptions
operator|.
name|getBaseDir
argument_list|()
operator|+
literal|":"
operator|+
name|bucket
assert|;
assert|assert
name|mergerOptions
operator|.
name|getBaseDir
argument_list|()
operator|!=
literal|null
operator|:
literal|"no baseDir?: "
operator|+
name|mergerOptions
operator|.
name|getRootPath
argument_list|()
assert|;
comment|//we are compacting and it's acid schema so create a reader for the 1st bucket file that is not empty
name|FileSystem
name|fs
init|=
name|mergerOptions
operator|.
name|getBaseDir
argument_list|()
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|Path
name|bucketPath
init|=
name|AcidUtils
operator|.
name|createBucketFile
argument_list|(
name|mergerOptions
operator|.
name|getBaseDir
argument_list|()
argument_list|,
name|bucket
argument_list|)
decl_stmt|;
if|if
condition|(
name|fs
operator|.
name|exists
argument_list|(
name|bucketPath
argument_list|)
operator|&&
name|fs
operator|.
name|getFileStatus
argument_list|(
name|bucketPath
argument_list|)
operator|.
name|getLen
argument_list|()
operator|>
literal|0
condition|)
block|{
comment|//doing major compaction - it's possible where full compliment of bucket files is not
comment|//required (on Tez) that base_x/ doesn't have a file for 'bucket'
name|reader
operator|=
name|OrcFile
operator|.
name|createReader
argument_list|(
name|bucketPath
argument_list|,
name|OrcFile
operator|.
name|readerOptions
argument_list|(
name|conf
argument_list|)
argument_list|)
expr_stmt|;
name|pair
operator|=
operator|new
name|ReaderPairAcid
argument_list|(
name|baseKey
argument_list|,
name|reader
argument_list|,
name|keyInterval
operator|.
name|getMinKey
argument_list|()
argument_list|,
name|keyInterval
operator|.
name|getMaxKey
argument_list|()
argument_list|,
name|eventOptions
argument_list|,
name|conf
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|pair
operator|=
operator|new
name|EmptyReaderPair
argument_list|()
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"No non-empty "
operator|+
name|bucketPath
operator|+
literal|" was found for Major compaction"
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
assert|assert
name|reader
operator|!=
literal|null
operator|:
literal|"no reader? "
operator|+
name|mergerOptions
operator|.
name|getRootPath
argument_list|()
assert|;
name|pair
operator|=
operator|new
name|ReaderPairAcid
argument_list|(
name|baseKey
argument_list|,
name|reader
argument_list|,
name|keyInterval
operator|.
name|getMinKey
argument_list|()
argument_list|,
name|keyInterval
operator|.
name|getMaxKey
argument_list|()
argument_list|,
name|eventOptions
argument_list|,
name|conf
argument_list|)
expr_stmt|;
block|}
block|}
name|minKey
operator|=
name|pair
operator|.
name|getMinKey
argument_list|()
expr_stmt|;
name|maxKey
operator|=
name|pair
operator|.
name|getMaxKey
argument_list|()
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"updated min key = "
operator|+
name|keyInterval
operator|.
name|getMinKey
argument_list|()
operator|+
literal|", max key = "
operator|+
name|keyInterval
operator|.
name|getMaxKey
argument_list|()
argument_list|)
expr_stmt|;
comment|// if there is at least one record, put it in the map
if|if
condition|(
name|pair
operator|.
name|nextRecord
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|ensurePutReader
argument_list|(
name|baseKey
argument_list|,
name|pair
argument_list|)
expr_stmt|;
name|baseKey
operator|=
literal|null
expr_stmt|;
block|}
name|baseReader
operator|=
name|pair
operator|.
name|getRecordReader
argument_list|()
expr_stmt|;
block|}
comment|/*now process the delta files.  For normal read these should only be delete deltas.  For     * Compaction these may be any delta_x_y/.  The files inside any delta_x_y/ may be in Acid     * format (i.e. with Acid metadata columns) or 'original'.*/
if|if
condition|(
name|deltaDirectory
operator|!=
literal|null
operator|&&
name|deltaDirectory
operator|.
name|length
operator|>
literal|0
condition|)
block|{
comment|/*For reads, whatever SARG maybe applicable to base it's not applicable to delete_delta since it has no       * user columns.  For Compaction there is never a SARG.       * */
name|Reader
operator|.
name|Options
name|deltaEventOptions
init|=
name|eventOptions
operator|.
name|clone
argument_list|()
operator|.
name|searchArgument
argument_list|(
literal|null
argument_list|,
literal|null
argument_list|)
operator|.
name|range
argument_list|(
literal|0
argument_list|,
name|Long
operator|.
name|MAX_VALUE
argument_list|)
decl_stmt|;
for|for
control|(
name|Path
name|delta
range|:
name|deltaDirectory
control|)
block|{
if|if
condition|(
operator|!
name|mergerOptions
operator|.
name|isCompacting
argument_list|()
operator|&&
operator|!
name|AcidUtils
operator|.
name|isDeleteDelta
argument_list|(
name|delta
argument_list|)
condition|)
block|{
comment|//all inserts should be in baseReader for normal read so this should always be delete delta if not compacting
throw|throw
operator|new
name|IllegalStateException
argument_list|(
name|delta
operator|+
literal|" is not delete delta and is not compacting."
argument_list|)
throw|;
block|}
name|ReaderKey
name|key
init|=
operator|new
name|ReaderKey
argument_list|()
decl_stmt|;
comment|//todo: only need to know isRawFormat if compacting for acid V2 and V2 should normally run
comment|//in vectorized mode - i.e. this is not a significant perf overhead vs ParsedDeltaLight
name|AcidUtils
operator|.
name|ParsedDelta
name|deltaDir
init|=
name|AcidUtils
operator|.
name|parsedDelta
argument_list|(
name|delta
argument_list|,
name|delta
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
argument_list|)
decl_stmt|;
if|if
condition|(
name|deltaDir
operator|.
name|isRawFormat
argument_list|()
condition|)
block|{
assert|assert
operator|!
name|deltaDir
operator|.
name|isDeleteDelta
argument_list|()
operator|:
name|delta
operator|.
name|toString
argument_list|()
assert|;
assert|assert
name|mergerOptions
operator|.
name|isCompacting
argument_list|()
operator|:
literal|"during regular read anything which is not a"
operator|+
literal|" delete_delta is treated like base: "
operator|+
name|delta
assert|;
name|Options
name|rawCompactOptions
init|=
name|modifyForNonAcidSchemaRead
argument_list|(
name|mergerOptions
argument_list|,
name|deltaDir
operator|.
name|getMinWriteId
argument_list|()
argument_list|,
name|delta
argument_list|)
decl_stmt|;
comment|//this will also handle copy_N files if any
name|ReaderPair
name|deltaPair
init|=
operator|new
name|OriginalReaderPairToCompact
argument_list|(
name|key
argument_list|,
name|bucket
argument_list|,
name|options
argument_list|,
name|rawCompactOptions
argument_list|,
name|conf
argument_list|,
name|validWriteIdList
argument_list|,
name|deltaDir
operator|.
name|getStatementId
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|deltaPair
operator|.
name|nextRecord
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|ensurePutReader
argument_list|(
name|key
argument_list|,
name|deltaPair
argument_list|)
expr_stmt|;
name|key
operator|=
operator|new
name|ReaderKey
argument_list|()
expr_stmt|;
block|}
continue|continue;
block|}
name|String
name|attemptId
init|=
literal|null
decl_stmt|;
if|if
condition|(
name|deltasToAttemptId
operator|!=
literal|null
condition|)
block|{
name|attemptId
operator|=
name|deltasToAttemptId
operator|.
name|get
argument_list|(
name|delta
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
for|for
control|(
name|Path
name|deltaFile
range|:
name|getDeltaFiles
argument_list|(
name|delta
argument_list|,
name|bucket
argument_list|,
name|mergerOptions
argument_list|,
name|attemptId
argument_list|)
control|)
block|{
name|FileSystem
name|fs
init|=
name|deltaFile
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|fs
operator|.
name|exists
argument_list|(
name|deltaFile
argument_list|)
condition|)
block|{
comment|/**              * it's possible that the file for a specific {@link bucket} doesn't exist in any given              * delta since since no rows hashed to it (and not configured to create empty buckets)              */
continue|continue;
block|}
name|LOG
operator|.
name|debug
argument_list|(
literal|"Looking at delta file {}"
argument_list|,
name|deltaFile
argument_list|)
expr_stmt|;
if|if
condition|(
name|deltaDir
operator|.
name|isDeleteDelta
argument_list|()
condition|)
block|{
comment|//if here it maybe compaction or regular read or Delete event sorter
comment|//in the later 2 cases we should do:
comment|//HIVE-17320: we should compute a SARG to push down min/max key to delete_delta
name|Reader
name|deltaReader
init|=
name|OrcFile
operator|.
name|createReader
argument_list|(
name|deltaFile
argument_list|,
name|OrcFile
operator|.
name|readerOptions
argument_list|(
name|conf
argument_list|)
argument_list|)
decl_stmt|;
name|ReaderPair
name|deltaPair
init|=
operator|new
name|ReaderPairAcid
argument_list|(
name|key
argument_list|,
name|deltaReader
argument_list|,
name|minKey
argument_list|,
name|maxKey
argument_list|,
name|deltaEventOptions
argument_list|,
name|conf
argument_list|)
decl_stmt|;
if|if
condition|(
name|deltaPair
operator|.
name|nextRecord
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|ensurePutReader
argument_list|(
name|key
argument_list|,
name|deltaPair
argument_list|)
expr_stmt|;
name|key
operator|=
operator|new
name|ReaderKey
argument_list|()
expr_stmt|;
block|}
continue|continue;
block|}
comment|//if here then we must be compacting
assert|assert
name|mergerOptions
operator|.
name|isCompacting
argument_list|()
operator|:
literal|"not compacting and not delete delta : "
operator|+
name|delta
assert|;
comment|/* side files are only created by streaming ingest.  If this is a compaction, we may           * have an insert delta/ here with side files there because the original writer died.*/
name|long
name|length
init|=
name|AcidUtils
operator|.
name|getLogicalLength
argument_list|(
name|fs
argument_list|,
name|fs
operator|.
name|getFileStatus
argument_list|(
name|deltaFile
argument_list|)
argument_list|)
decl_stmt|;
assert|assert
name|length
operator|>=
literal|0
assert|;
name|Reader
name|deltaReader
init|=
name|OrcFile
operator|.
name|createReader
argument_list|(
name|deltaFile
argument_list|,
name|OrcFile
operator|.
name|readerOptions
argument_list|(
name|conf
argument_list|)
operator|.
name|maxLength
argument_list|(
name|length
argument_list|)
argument_list|)
decl_stmt|;
comment|//must get statementId from file name since Acid 1.0 doesn't write it into bucketProperty
name|ReaderPairAcid
name|deltaPair
init|=
operator|new
name|ReaderPairAcid
argument_list|(
name|key
argument_list|,
name|deltaReader
argument_list|,
name|minKey
argument_list|,
name|maxKey
argument_list|,
name|deltaEventOptions
argument_list|,
name|conf
argument_list|)
decl_stmt|;
if|if
condition|(
name|deltaPair
operator|.
name|nextRecord
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|ensurePutReader
argument_list|(
name|key
argument_list|,
name|deltaPair
argument_list|)
expr_stmt|;
name|key
operator|=
operator|new
name|ReaderKey
argument_list|()
expr_stmt|;
block|}
block|}
block|}
block|}
comment|// get the first record
name|LOG
operator|.
name|debug
argument_list|(
literal|"Final reader map {}"
argument_list|,
name|readers
argument_list|)
expr_stmt|;
name|Map
operator|.
name|Entry
argument_list|<
name|ReaderKey
argument_list|,
name|ReaderPair
argument_list|>
name|entry
init|=
name|readers
operator|.
name|pollFirstEntry
argument_list|()
decl_stmt|;
if|if
condition|(
name|entry
operator|==
literal|null
condition|)
block|{
name|columns
operator|=
literal|0
expr_stmt|;
name|primary
operator|=
literal|null
expr_stmt|;
block|}
else|else
block|{
name|primary
operator|=
name|entry
operator|.
name|getValue
argument_list|()
expr_stmt|;
if|if
condition|(
name|readers
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|secondaryKey
operator|=
literal|null
expr_stmt|;
block|}
else|else
block|{
name|secondaryKey
operator|=
name|readers
operator|.
name|firstKey
argument_list|()
expr_stmt|;
block|}
comment|// get the number of columns in the user's rows
name|columns
operator|=
name|primary
operator|.
name|getColumns
argument_list|()
expr_stmt|;
block|}
block|}
specifier|private
name|void
name|ensurePutReader
parameter_list|(
name|ReaderKey
name|key
parameter_list|,
name|ReaderPair
name|deltaPair
parameter_list|)
throws|throws
name|IOException
block|{
name|ReaderPair
name|oldPair
init|=
name|readers
operator|.
name|put
argument_list|(
name|key
argument_list|,
name|deltaPair
argument_list|)
decl_stmt|;
if|if
condition|(
name|oldPair
operator|==
literal|null
condition|)
return|return;
name|String
name|error
init|=
literal|"Two readers for "
operator|+
name|key
operator|+
literal|": new "
operator|+
name|deltaPair
operator|+
literal|", old "
operator|+
name|oldPair
decl_stmt|;
name|LOG
operator|.
name|error
argument_list|(
name|error
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|IOException
argument_list|(
name|error
argument_list|)
throw|;
block|}
comment|/**    * For use with Load Data statement which places {@link AcidUtils.AcidBaseFileType#ORIGINAL_BASE}    * type files into a base_x/ or delta_x_x.  The data in these are then assigned ROW_IDs at read    * time and made permanent at compaction time.  This is identical to how 'original' files (i.e.    * those that existed in the table before it was converted to an Acid table) except that the    * write ID to use in the ROW_ID should be that of the transaction that ran the Load Data.    */
specifier|static
specifier|final
class|class
name|TransactionMetaData
block|{
specifier|final
name|long
name|syntheticWriteId
decl_stmt|;
comment|/**      * folder which determines the write id to use in synthetic ROW_IDs      */
specifier|final
name|Path
name|folder
decl_stmt|;
specifier|final
name|int
name|statementId
decl_stmt|;
name|TransactionMetaData
parameter_list|(
name|long
name|syntheticWriteId
parameter_list|,
name|Path
name|folder
parameter_list|)
block|{
name|this
argument_list|(
name|syntheticWriteId
argument_list|,
name|folder
argument_list|,
literal|0
argument_list|)
expr_stmt|;
block|}
name|TransactionMetaData
parameter_list|(
name|long
name|syntheticWriteId
parameter_list|,
name|Path
name|folder
parameter_list|,
name|int
name|statementId
parameter_list|)
block|{
name|this
operator|.
name|syntheticWriteId
operator|=
name|syntheticWriteId
expr_stmt|;
name|this
operator|.
name|folder
operator|=
name|folder
expr_stmt|;
name|this
operator|.
name|statementId
operator|=
name|statementId
expr_stmt|;
block|}
specifier|static
name|TransactionMetaData
name|findWriteIDForSynthetcRowIDs
parameter_list|(
name|Path
name|splitPath
parameter_list|,
name|Path
name|rootPath
parameter_list|,
name|Configuration
name|conf
parameter_list|)
throws|throws
name|IOException
block|{
name|Path
name|parent
init|=
name|splitPath
operator|.
name|getParent
argument_list|()
decl_stmt|;
if|if
condition|(
name|rootPath
operator|.
name|equals
argument_list|(
name|parent
argument_list|)
condition|)
block|{
comment|//the 'isOriginal' file is at the root of the partition (or table) thus it is
comment|//from a pre-acid conversion write and belongs to primordial writeid:0.
return|return
operator|new
name|TransactionMetaData
argument_list|(
literal|0
argument_list|,
name|parent
argument_list|)
return|;
block|}
while|while
condition|(
name|parent
operator|!=
literal|null
operator|&&
operator|!
name|rootPath
operator|.
name|equals
argument_list|(
name|parent
argument_list|)
condition|)
block|{
name|boolean
name|isBase
init|=
name|parent
operator|.
name|getName
argument_list|()
operator|.
name|startsWith
argument_list|(
name|AcidUtils
operator|.
name|BASE_PREFIX
argument_list|)
decl_stmt|;
name|boolean
name|isDelta
init|=
name|parent
operator|.
name|getName
argument_list|()
operator|.
name|startsWith
argument_list|(
name|AcidUtils
operator|.
name|DELTA_PREFIX
argument_list|)
operator|||
name|parent
operator|.
name|getName
argument_list|()
operator|.
name|startsWith
argument_list|(
name|AcidUtils
operator|.
name|DELETE_DELTA_PREFIX
argument_list|)
decl_stmt|;
if|if
condition|(
name|isBase
operator|||
name|isDelta
condition|)
block|{
if|if
condition|(
name|isBase
condition|)
block|{
return|return
operator|new
name|TransactionMetaData
argument_list|(
name|AcidUtils
operator|.
name|ParsedBase
operator|.
name|parseBase
argument_list|(
name|parent
argument_list|)
operator|.
name|getWriteId
argument_list|()
argument_list|,
name|parent
argument_list|)
return|;
block|}
else|else
block|{
name|AcidUtils
operator|.
name|ParsedDeltaLight
name|pd
init|=
name|AcidUtils
operator|.
name|ParsedDeltaLight
operator|.
name|parse
argument_list|(
name|parent
argument_list|)
decl_stmt|;
return|return
operator|new
name|TransactionMetaData
argument_list|(
name|pd
operator|.
name|getMinWriteId
argument_list|()
argument_list|,
name|parent
argument_list|,
name|pd
operator|.
name|getStatementId
argument_list|()
argument_list|)
return|;
block|}
block|}
name|parent
operator|=
name|parent
operator|.
name|getParent
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
name|parent
operator|==
literal|null
condition|)
block|{
comment|//spit is marked isOriginal but it's not an immediate child of a partition nor is it in a
comment|//base/ or delta/ - this should never happen
throw|throw
operator|new
name|IllegalStateException
argument_list|(
literal|"Cannot determine write id for original file "
operator|+
name|splitPath
operator|+
literal|" in "
operator|+
name|rootPath
argument_list|)
throw|;
block|}
comment|//"warehouse/t/HIVE_UNION_SUBDIR_15/000000_0" is a meaningful path for nonAcid2acid
comment|// converted table
return|return
operator|new
name|TransactionMetaData
argument_list|(
literal|0
argument_list|,
name|rootPath
argument_list|)
return|;
block|}
block|}
comment|/**    * This is done to read non-acid schema files ("original") located in base_x/ or delta_x_x/ which    * happens as a result of Load Data statement.  Setting {@code rootPath} to base_x/ or delta_x_x    * causes {@link AcidUtils#getAcidState(Path, Configuration, ValidWriteIdList)} in subsequent    * {@link OriginalReaderPair} object to return the files in this dir    * in {@link AcidUtils.Directory#getOriginalFiles()}    * @return modified clone of {@code baseOptions}    */
specifier|private
name|Options
name|modifyForNonAcidSchemaRead
parameter_list|(
name|Options
name|baseOptions
parameter_list|,
name|long
name|writeId
parameter_list|,
name|Path
name|rootPath
parameter_list|)
block|{
return|return
name|baseOptions
operator|.
name|clone
argument_list|()
operator|.
name|writeId
argument_list|(
name|writeId
argument_list|)
operator|.
name|rootPath
argument_list|(
name|rootPath
argument_list|)
return|;
block|}
comment|/**    * This determines the set of {@link ReaderPairAcid} to create for a given delta/.    * For unbucketed tables {@code bucket} can be thought of as a write tranche.    */
specifier|static
name|Path
index|[]
name|getDeltaFiles
parameter_list|(
name|Path
name|deltaDirectory
parameter_list|,
name|int
name|bucket
parameter_list|,
name|Options
name|mergerOptions
parameter_list|,
name|String
name|attemptId
parameter_list|)
block|{
assert|assert
operator|(
operator|!
name|mergerOptions
operator|.
name|isCompacting
operator|&&
name|deltaDirectory
operator|.
name|getName
argument_list|()
operator|.
name|startsWith
argument_list|(
name|AcidUtils
operator|.
name|DELETE_DELTA_PREFIX
argument_list|)
operator|)
operator|||
name|mergerOptions
operator|.
name|isCompacting
operator|:
literal|"Unexpected delta: "
operator|+
name|deltaDirectory
operator|+
literal|"(isCompacting="
operator|+
name|mergerOptions
operator|.
name|isCompacting
argument_list|()
operator|+
literal|")"
assert|;
return|return
operator|new
name|Path
index|[]
block|{
name|AcidUtils
operator|.
name|createBucketFile
argument_list|(
name|deltaDirectory
argument_list|,
name|bucket
argument_list|,
name|attemptId
argument_list|)
block|}
return|;
block|}
annotation|@
name|VisibleForTesting
name|RecordIdentifier
name|getMinKey
parameter_list|()
block|{
return|return
name|minKey
return|;
block|}
annotation|@
name|VisibleForTesting
name|RecordIdentifier
name|getMaxKey
parameter_list|()
block|{
return|return
name|maxKey
return|;
block|}
annotation|@
name|VisibleForTesting
name|ReaderPair
name|getCurrentReader
parameter_list|()
block|{
return|return
name|primary
return|;
block|}
annotation|@
name|VisibleForTesting
name|Map
argument_list|<
name|ReaderKey
argument_list|,
name|ReaderPair
argument_list|>
name|getOtherReaders
parameter_list|()
block|{
return|return
name|readers
return|;
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|next
parameter_list|(
name|RecordIdentifier
name|recordIdentifier
parameter_list|,
name|OrcStruct
name|prev
parameter_list|)
throws|throws
name|IOException
block|{
name|boolean
name|keysSame
init|=
literal|true
decl_stmt|;
while|while
condition|(
name|keysSame
operator|&&
name|primary
operator|!=
literal|null
condition|)
block|{
comment|// The primary's nextRecord is the next value to return
name|OrcStruct
name|current
init|=
name|primary
operator|.
name|nextRecord
argument_list|()
decl_stmt|;
name|recordIdentifier
operator|.
name|set
argument_list|(
name|primary
operator|.
name|getKey
argument_list|()
argument_list|)
expr_stmt|;
comment|// Advance the primary reader to the next record
name|primary
operator|.
name|next
argument_list|(
name|extraValue
argument_list|)
expr_stmt|;
comment|// Save the current record as the new extraValue for next time so that
comment|// we minimize allocations
name|extraValue
operator|=
name|current
expr_stmt|;
comment|// now that the primary reader has advanced, we need to see if we
comment|// continue to read it or move to the secondary.
if|if
condition|(
name|primary
operator|.
name|nextRecord
argument_list|()
operator|==
literal|null
operator|||
name|primary
operator|.
name|getKey
argument_list|()
operator|.
name|compareTo
argument_list|(
name|secondaryKey
argument_list|)
operator|>
literal|0
condition|)
block|{
comment|// if the primary isn't done, push it back into the readers
if|if
condition|(
name|primary
operator|.
name|nextRecord
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|readers
operator|.
name|put
argument_list|(
name|primary
operator|.
name|getKey
argument_list|()
argument_list|,
name|primary
argument_list|)
expr_stmt|;
block|}
comment|// update primary and secondaryKey
name|Map
operator|.
name|Entry
argument_list|<
name|ReaderKey
argument_list|,
name|ReaderPair
argument_list|>
name|entry
init|=
name|readers
operator|.
name|pollFirstEntry
argument_list|()
decl_stmt|;
if|if
condition|(
name|entry
operator|!=
literal|null
condition|)
block|{
name|primary
operator|=
name|entry
operator|.
name|getValue
argument_list|()
expr_stmt|;
if|if
condition|(
name|readers
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|secondaryKey
operator|=
literal|null
expr_stmt|;
block|}
else|else
block|{
name|secondaryKey
operator|=
name|readers
operator|.
name|firstKey
argument_list|()
expr_stmt|;
block|}
block|}
else|else
block|{
name|primary
operator|=
literal|null
expr_stmt|;
block|}
block|}
comment|// if this transaction isn't ok, skip over it
if|if
condition|(
operator|!
name|validWriteIdList
operator|.
name|isWriteIdValid
argument_list|(
operator|(
operator|(
name|ReaderKey
operator|)
name|recordIdentifier
operator|)
operator|.
name|getCurrentWriteId
argument_list|()
argument_list|)
condition|)
block|{
continue|continue;
block|}
comment|/*for multi-statement txns, you may have multiple events for the same       * row in the same (current) transaction.  We want to collapse these to just the last one       * regardless whether we are minor compacting.  Consider INSERT/UPDATE/UPDATE of the       * same row in the same txn.  There is no benefit passing along anything except the last       * event.  If we did want to pass it along, we'd have to include statementId in the row       * returned so that compaction could write it out or make minor minor compaction understand       * how to write out delta files in delta_xxx_yyy_stid format.  There doesn't seem to be any       * value in this.       *       * todo: this could be simplified since in Acid2 even if you update the same row 2 times in 1       * txn, it will have different ROW__IDs, i.e. there is no such thing as multiple versions of       * the same physical row.  Leave it for now since this Acid reader should go away altogether       * and org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader will be used.*/
name|boolean
name|isSameRow
init|=
name|prevKey
operator|.
name|isSameRow
argument_list|(
operator|(
name|ReaderKey
operator|)
name|recordIdentifier
argument_list|)
decl_stmt|;
comment|// if we are collapsing, figure out if this is a new row
if|if
condition|(
name|collapse
operator|||
name|isSameRow
condition|)
block|{
comment|// Note: for collapse == false, this just sets keysSame.
name|keysSame
operator|=
operator|(
name|collapse
operator|&&
name|prevKey
operator|.
name|compareRow
argument_list|(
name|recordIdentifier
argument_list|)
operator|==
literal|0
operator|)
operator|||
operator|(
name|isSameRow
operator|)
expr_stmt|;
if|if
condition|(
operator|!
name|keysSame
condition|)
block|{
name|prevKey
operator|.
name|set
argument_list|(
name|recordIdentifier
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
name|keysSame
operator|=
literal|false
expr_stmt|;
block|}
comment|// set the output record by fiddling with the pointers so that we can
comment|// avoid a copy.
name|prev
operator|.
name|linkFields
argument_list|(
name|current
argument_list|)
expr_stmt|;
block|}
return|return
operator|!
name|keysSame
return|;
block|}
annotation|@
name|Override
specifier|public
name|RecordIdentifier
name|createKey
parameter_list|()
block|{
return|return
operator|new
name|ReaderKey
argument_list|()
return|;
block|}
annotation|@
name|Override
specifier|public
name|OrcStruct
name|createValue
parameter_list|()
block|{
return|return
operator|new
name|OrcStruct
argument_list|(
name|OrcRecordUpdater
operator|.
name|FIELDS
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|public
name|long
name|getPos
parameter_list|()
throws|throws
name|IOException
block|{
return|return
name|offset
operator|+
call|(
name|long
call|)
argument_list|(
name|getProgress
argument_list|()
operator|*
name|length
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|close
parameter_list|()
throws|throws
name|IOException
block|{
if|if
condition|(
name|primary
operator|!=
literal|null
condition|)
block|{
name|primary
operator|.
name|getRecordReader
argument_list|()
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
for|for
control|(
name|ReaderPair
name|pair
range|:
name|readers
operator|.
name|values
argument_list|()
control|)
block|{
name|pair
operator|.
name|getRecordReader
argument_list|()
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
annotation|@
name|Override
specifier|public
name|float
name|getProgress
parameter_list|()
throws|throws
name|IOException
block|{
comment|//this is not likely to do the right thing for Compaction of "original" files when there are copyN files
return|return
name|baseReader
operator|==
literal|null
condition|?
literal|1
else|:
name|baseReader
operator|.
name|getProgress
argument_list|()
return|;
block|}
annotation|@
name|Override
specifier|public
name|ObjectInspector
name|getObjectInspector
parameter_list|()
block|{
return|return
name|objectInspector
return|;
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|isDelete
parameter_list|(
name|OrcStruct
name|value
parameter_list|)
block|{
return|return
name|OrcRecordUpdater
operator|.
name|getOperation
argument_list|(
name|value
argument_list|)
operator|==
name|OrcRecordUpdater
operator|.
name|DELETE_OPERATION
return|;
block|}
comment|/**    * Get the number of columns in the underlying rows.    * @return 0 if there are no base and no deltas.    */
specifier|public
name|int
name|getColumns
parameter_list|()
block|{
return|return
name|columns
return|;
block|}
block|}
end_class

end_unit

