begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/**  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|orc
package|;
end_package

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Collections
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|TreeMap
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|Utilities
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|AcidOutputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|shims
operator|.
name|HadoopShims
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|orc
operator|.
name|OrcUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|orc
operator|.
name|StripeInformation
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|orc
operator|.
name|TypeDescription
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|orc
operator|.
name|impl
operator|.
name|AcidStats
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|orc
operator|.
name|impl
operator|.
name|OrcAcidUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|Logger
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|LoggerFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configuration
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|ValidTxnList
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|AcidInputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|AcidUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|RecordIdentifier
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|objectinspector
operator|.
name|ObjectInspector
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|IntWritable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|LongWritable
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|annotations
operator|.
name|VisibleForTesting
import|;
end_import

begin_comment
comment|/**  * Merges a base and a list of delta files together into a single stream of  * events.  */
end_comment

begin_class
specifier|public
class|class
name|OrcRawRecordMerger
implements|implements
name|AcidInputFormat
operator|.
name|RawReader
argument_list|<
name|OrcStruct
argument_list|>
block|{
specifier|private
specifier|static
specifier|final
name|Logger
name|LOG
init|=
name|LoggerFactory
operator|.
name|getLogger
argument_list|(
name|OrcRawRecordMerger
operator|.
name|class
argument_list|)
decl_stmt|;
specifier|private
specifier|final
name|Configuration
name|conf
decl_stmt|;
specifier|private
specifier|final
name|boolean
name|collapse
decl_stmt|;
specifier|private
specifier|final
name|RecordReader
name|baseReader
decl_stmt|;
specifier|private
specifier|final
name|ObjectInspector
name|objectInspector
decl_stmt|;
specifier|private
specifier|final
name|long
name|offset
decl_stmt|;
specifier|private
specifier|final
name|long
name|length
decl_stmt|;
specifier|private
specifier|final
name|ValidTxnList
name|validTxnList
decl_stmt|;
specifier|private
specifier|final
name|int
name|columns
decl_stmt|;
specifier|private
specifier|final
name|ReaderKey
name|prevKey
init|=
operator|new
name|ReaderKey
argument_list|()
decl_stmt|;
comment|// this is the key less than the lowest key we need to process
specifier|private
specifier|final
name|RecordIdentifier
name|minKey
decl_stmt|;
comment|// this is the last key we need to process
specifier|private
specifier|final
name|RecordIdentifier
name|maxKey
decl_stmt|;
comment|// an extra value so that we can return it while reading ahead
specifier|private
name|OrcStruct
name|extraValue
decl_stmt|;
comment|/**    * A RecordIdentifier extended with the current transaction id. This is the    * key of our merge sort with the originalTransaction, bucket, and rowId    * ascending and the currentTransaction, statementId descending. This means that if the    * reader is collapsing events to just the last update, just the first    * instance of each record is required.    */
annotation|@
name|VisibleForTesting
specifier|public
specifier|final
specifier|static
class|class
name|ReaderKey
extends|extends
name|RecordIdentifier
block|{
specifier|private
name|long
name|currentTransactionId
decl_stmt|;
comment|/**      * This is the value from delta file name which may be different from value encode in       * {@link RecordIdentifier#getBucketProperty()} in case of Update/Delete.      * So for Acid 1.0 + multi-stmt txn, if {@code isSameRow() == true}, then it must be an update      * or delete event.  For Acid 2.0 + multi-stmt txn, it must be a delete event.      * No 2 Insert events from can ever agree on {@link RecordIdentifier}      */
specifier|private
name|int
name|statementId
decl_stmt|;
comment|//sort on this descending, like currentTransactionId
specifier|public
name|ReaderKey
parameter_list|()
block|{
name|this
argument_list|(
operator|-
literal|1
argument_list|,
operator|-
literal|1
argument_list|,
operator|-
literal|1
argument_list|,
operator|-
literal|1
argument_list|,
literal|0
argument_list|)
expr_stmt|;
block|}
specifier|public
name|ReaderKey
parameter_list|(
name|long
name|originalTransaction
parameter_list|,
name|int
name|bucket
parameter_list|,
name|long
name|rowId
parameter_list|,
name|long
name|currentTransactionId
parameter_list|)
block|{
name|this
argument_list|(
name|originalTransaction
argument_list|,
name|bucket
argument_list|,
name|rowId
argument_list|,
name|currentTransactionId
argument_list|,
literal|0
argument_list|)
expr_stmt|;
block|}
comment|/**      * @param statementId - set this to 0 if N/A      */
specifier|public
name|ReaderKey
parameter_list|(
name|long
name|originalTransaction
parameter_list|,
name|int
name|bucket
parameter_list|,
name|long
name|rowId
parameter_list|,
name|long
name|currentTransactionId
parameter_list|,
name|int
name|statementId
parameter_list|)
block|{
name|super
argument_list|(
name|originalTransaction
argument_list|,
name|bucket
argument_list|,
name|rowId
argument_list|)
expr_stmt|;
name|this
operator|.
name|currentTransactionId
operator|=
name|currentTransactionId
expr_stmt|;
name|this
operator|.
name|statementId
operator|=
name|statementId
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|set
parameter_list|(
name|RecordIdentifier
name|other
parameter_list|)
block|{
name|super
operator|.
name|set
argument_list|(
name|other
argument_list|)
expr_stmt|;
name|currentTransactionId
operator|=
operator|(
operator|(
name|ReaderKey
operator|)
name|other
operator|)
operator|.
name|currentTransactionId
expr_stmt|;
name|statementId
operator|=
operator|(
operator|(
name|ReaderKey
operator|)
name|other
operator|)
operator|.
name|statementId
expr_stmt|;
block|}
specifier|public
name|void
name|setValues
parameter_list|(
name|long
name|originalTransactionId
parameter_list|,
name|int
name|bucket
parameter_list|,
name|long
name|rowId
parameter_list|,
name|long
name|currentTransactionId
parameter_list|,
name|int
name|statementId
parameter_list|)
block|{
name|setValues
argument_list|(
name|originalTransactionId
argument_list|,
name|bucket
argument_list|,
name|rowId
argument_list|)
expr_stmt|;
name|this
operator|.
name|currentTransactionId
operator|=
name|currentTransactionId
expr_stmt|;
name|this
operator|.
name|statementId
operator|=
name|statementId
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|equals
parameter_list|(
name|Object
name|other
parameter_list|)
block|{
return|return
name|super
operator|.
name|equals
argument_list|(
name|other
argument_list|)
operator|&&
name|currentTransactionId
operator|==
operator|(
operator|(
name|ReaderKey
operator|)
name|other
operator|)
operator|.
name|currentTransactionId
operator|&&
name|statementId
operator|==
operator|(
operator|(
name|ReaderKey
operator|)
name|other
operator|)
operator|.
name|statementId
comment|//consistent with compareTo()
return|;
block|}
annotation|@
name|Override
specifier|public
name|int
name|hashCode
parameter_list|()
block|{
name|int
name|result
init|=
name|super
operator|.
name|hashCode
argument_list|()
decl_stmt|;
name|result
operator|=
literal|31
operator|*
name|result
operator|+
call|(
name|int
call|)
argument_list|(
name|currentTransactionId
operator|^
operator|(
name|currentTransactionId
operator|>>>
literal|32
operator|)
argument_list|)
expr_stmt|;
name|result
operator|=
literal|31
operator|*
name|result
operator|+
name|statementId
expr_stmt|;
return|return
name|result
return|;
block|}
annotation|@
name|Override
specifier|public
name|int
name|compareTo
parameter_list|(
name|RecordIdentifier
name|other
parameter_list|)
block|{
name|int
name|sup
init|=
name|compareToInternal
argument_list|(
name|other
argument_list|)
decl_stmt|;
if|if
condition|(
name|sup
operator|==
literal|0
condition|)
block|{
if|if
condition|(
name|other
operator|.
name|getClass
argument_list|()
operator|==
name|ReaderKey
operator|.
name|class
condition|)
block|{
name|ReaderKey
name|oth
init|=
operator|(
name|ReaderKey
operator|)
name|other
decl_stmt|;
if|if
condition|(
name|currentTransactionId
operator|!=
name|oth
operator|.
name|currentTransactionId
condition|)
block|{
return|return
name|currentTransactionId
operator|<
name|oth
operator|.
name|currentTransactionId
condition|?
operator|+
literal|1
else|:
operator|-
literal|1
return|;
block|}
if|if
condition|(
name|statementId
operator|!=
name|oth
operator|.
name|statementId
condition|)
block|{
return|return
name|statementId
operator|<
name|oth
operator|.
name|statementId
condition|?
operator|+
literal|1
else|:
operator|-
literal|1
return|;
block|}
block|}
else|else
block|{
return|return
operator|-
literal|1
return|;
block|}
block|}
return|return
name|sup
return|;
block|}
comment|/**      * This means 1 txn modified the same row more than once      */
specifier|private
name|boolean
name|isSameRow
parameter_list|(
name|ReaderKey
name|other
parameter_list|)
block|{
return|return
name|compareRow
argument_list|(
name|other
argument_list|)
operator|==
literal|0
operator|&&
name|currentTransactionId
operator|==
name|other
operator|.
name|currentTransactionId
return|;
block|}
name|long
name|getCurrentTransactionId
parameter_list|()
block|{
return|return
name|currentTransactionId
return|;
block|}
comment|/**      * Compare rows without considering the currentTransactionId.      * @param other the value to compare to      * @return -1, 0, +1      */
name|int
name|compareRow
parameter_list|(
name|RecordIdentifier
name|other
parameter_list|)
block|{
return|return
name|compareToInternal
argument_list|(
name|other
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|public
name|String
name|toString
parameter_list|()
block|{
return|return
literal|"{originalTxn: "
operator|+
name|getTransactionId
argument_list|()
operator|+
literal|", "
operator|+
name|bucketToString
argument_list|()
operator|+
literal|", row: "
operator|+
name|getRowId
argument_list|()
operator|+
literal|", currentTxn: "
operator|+
name|currentTransactionId
operator|+
literal|", statementId: "
operator|+
name|statementId
operator|+
literal|"}"
return|;
block|}
block|}
comment|/**    * A reader and the next record from that reader. The code reads ahead so that    * we can return the lowest ReaderKey from each of the readers. Thus, the    * next available row is nextRecord and only following records are still in    * the reader.    */
specifier|static
class|class
name|ReaderPair
block|{
name|OrcStruct
name|nextRecord
decl_stmt|;
specifier|final
name|Reader
name|reader
decl_stmt|;
specifier|final
name|RecordReader
name|recordReader
decl_stmt|;
specifier|final
name|ReaderKey
name|key
decl_stmt|;
specifier|private
specifier|final
name|RecordIdentifier
name|minKey
decl_stmt|;
specifier|private
specifier|final
name|RecordIdentifier
name|maxKey
decl_stmt|;
specifier|final
name|int
name|bucket
decl_stmt|;
specifier|private
specifier|final
name|int
name|statementId
decl_stmt|;
name|boolean
name|advancedToMinKey
init|=
literal|false
decl_stmt|;
comment|/**      * Create a reader that reads from the first key larger than minKey to any      * keys equal to maxKey.      * @param key the key to read into      * @param reader the ORC file reader      * @param bucket the bucket number for the file      * @param minKey only return keys larger than minKey if it is non-null      * @param maxKey only return keys less than or equal to maxKey if it is      *               non-null      * @param options options to provide to read the rows.      * @param statementId id of SQL statement within a transaction      * @throws IOException      */
name|ReaderPair
parameter_list|(
name|ReaderKey
name|key
parameter_list|,
name|Reader
name|reader
parameter_list|,
name|int
name|bucket
parameter_list|,
name|RecordIdentifier
name|minKey
parameter_list|,
name|RecordIdentifier
name|maxKey
parameter_list|,
name|ReaderImpl
operator|.
name|Options
name|options
parameter_list|,
name|int
name|statementId
parameter_list|)
throws|throws
name|IOException
block|{
name|this
operator|.
name|reader
operator|=
name|reader
expr_stmt|;
name|this
operator|.
name|key
operator|=
name|key
expr_stmt|;
name|this
operator|.
name|minKey
operator|=
name|minKey
expr_stmt|;
name|this
operator|.
name|maxKey
operator|=
name|maxKey
expr_stmt|;
name|this
operator|.
name|bucket
operator|=
name|bucket
expr_stmt|;
comment|// TODO use stripe statistics to jump over stripes
name|recordReader
operator|=
name|reader
operator|.
name|rowsOptions
argument_list|(
name|options
argument_list|)
expr_stmt|;
name|this
operator|.
name|statementId
operator|=
name|statementId
expr_stmt|;
block|}
name|RecordReader
name|getRecordReader
parameter_list|()
block|{
return|return
name|recordReader
return|;
block|}
comment|/**      * This must be called right after the constructor but not in the constructor to make sure      * sub-classes are fully initialized before their {@link #next(OrcStruct)} is called      */
name|void
name|advnaceToMinKey
parameter_list|()
throws|throws
name|IOException
block|{
name|advancedToMinKey
operator|=
literal|true
expr_stmt|;
comment|// advance the reader until we reach the minimum key
do|do
block|{
name|next
argument_list|(
name|nextRecord
argument_list|)
expr_stmt|;
block|}
do|while
condition|(
name|nextRecord
operator|!=
literal|null
operator|&&
operator|(
name|getMinKey
argument_list|()
operator|!=
literal|null
operator|&&
name|key
operator|.
name|compareRow
argument_list|(
name|getMinKey
argument_list|()
argument_list|)
operator|<=
literal|0
operator|)
condition|)
do|;
block|}
name|void
name|next
parameter_list|(
name|OrcStruct
name|next
parameter_list|)
throws|throws
name|IOException
block|{
assert|assert
name|advancedToMinKey
operator|:
literal|"advnaceToMinKey() was not called"
assert|;
if|if
condition|(
name|getRecordReader
argument_list|()
operator|.
name|hasNext
argument_list|()
condition|)
block|{
name|nextRecord
operator|=
operator|(
name|OrcStruct
operator|)
name|getRecordReader
argument_list|()
operator|.
name|next
argument_list|(
name|next
argument_list|)
expr_stmt|;
comment|// set the key
name|key
operator|.
name|setValues
argument_list|(
name|OrcRecordUpdater
operator|.
name|getOriginalTransaction
argument_list|(
name|nextRecord
argument_list|)
argument_list|,
name|OrcRecordUpdater
operator|.
name|getBucket
argument_list|(
name|nextRecord
argument_list|)
argument_list|,
name|OrcRecordUpdater
operator|.
name|getRowId
argument_list|(
name|nextRecord
argument_list|)
argument_list|,
name|OrcRecordUpdater
operator|.
name|getCurrentTransaction
argument_list|(
name|nextRecord
argument_list|)
argument_list|,
name|statementId
argument_list|)
expr_stmt|;
comment|// if this record is larger than maxKey, we need to stop
if|if
condition|(
name|getMaxKey
argument_list|()
operator|!=
literal|null
operator|&&
name|key
operator|.
name|compareRow
argument_list|(
name|getMaxKey
argument_list|()
argument_list|)
operator|>
literal|0
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"key "
operator|+
name|key
operator|+
literal|"> maxkey "
operator|+
name|getMaxKey
argument_list|()
argument_list|)
expr_stmt|;
name|nextRecord
operator|=
literal|null
expr_stmt|;
name|getRecordReader
argument_list|()
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
else|else
block|{
name|nextRecord
operator|=
literal|null
expr_stmt|;
name|recordReader
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
name|RecordIdentifier
name|getMinKey
parameter_list|()
block|{
return|return
name|minKey
return|;
block|}
name|RecordIdentifier
name|getMaxKey
parameter_list|()
block|{
return|return
name|maxKey
return|;
block|}
name|int
name|getColumns
parameter_list|()
block|{
return|return
name|reader
operator|.
name|getTypes
argument_list|()
operator|.
name|get
argument_list|(
name|OrcRecordUpdater
operator|.
name|ROW
operator|+
literal|1
argument_list|)
operator|.
name|getSubtypesCount
argument_list|()
return|;
block|}
block|}
comment|/**    * A reader that pretends an original base file is a new version base file.    * It wraps the underlying reader's row with an ACID event object and    * makes the relevant translations.    *     * Running multiple Insert statements on the same partition (of non acid table) creates files    * like so: 00000_0, 00000_0_copy1, 00000_0_copy2, etc.  So the OriginalReaderPair must treat all    * of these files as part of a single logical bucket file.    *     * For Compaction, where each split includes the whole bucket, this means reading over all the    * files in order to assign ROW__ID.rowid in one sequence for the entire logical bucket.    *    * For a read after the table is marked transactional but before it's rewritten into a base/    * by compaction, each of the original files may be split into many pieces.  For each split we    * must make sure to include only the relevant part of each delta file.    * {@link OrcRawRecordMerger#minKey} and {@link OrcRawRecordMerger#maxKey} are computed for each    * split of the original file and used to filter rows from all the deltas.  The ROW__ID.rowid for    * the rows of the 'original' file of course, must be assigned from the beginning of logical    * bucket.    */
specifier|static
specifier|final
class|class
name|OriginalReaderPair
extends|extends
name|ReaderPair
block|{
specifier|private
specifier|final
name|Options
name|mergerOptions
decl_stmt|;
comment|/**      * Sum total of all rows in all the files before the 'current' one in {@link #originalFiles} list      */
specifier|private
name|long
name|rowIdOffset
init|=
literal|0
decl_stmt|;
comment|/**      * See {@link AcidUtils.Directory#getOriginalFiles()}.  This list has a fixed sort order. This      * is the full list when compacting and empty when doing a simple read.  The later is because we      * only need to read the current split from 1 file for simple read.      */
specifier|private
specifier|final
name|List
argument_list|<
name|HadoopShims
operator|.
name|HdfsFileStatusWithId
argument_list|>
name|originalFiles
decl_stmt|;
comment|/**      * index into {@link #originalFiles}      */
specifier|private
name|int
name|nextFileIndex
init|=
literal|0
decl_stmt|;
specifier|private
name|long
name|numRowsInCurrentFile
init|=
literal|0
decl_stmt|;
specifier|private
name|RecordReader
name|originalFileRecordReader
init|=
literal|null
decl_stmt|;
specifier|private
specifier|final
name|Configuration
name|conf
decl_stmt|;
specifier|private
specifier|final
name|Reader
operator|.
name|Options
name|options
decl_stmt|;
specifier|private
specifier|final
name|RecordIdentifier
name|minKey
decl_stmt|;
comment|//shadow parent minKey to make final
specifier|private
specifier|final
name|RecordIdentifier
name|maxKey
decl_stmt|;
comment|//shadow parent maxKey to make final
name|OriginalReaderPair
parameter_list|(
name|ReaderKey
name|key
parameter_list|,
name|Reader
name|reader
parameter_list|,
name|int
name|bucket
parameter_list|,
specifier|final
name|RecordIdentifier
name|minKey
parameter_list|,
specifier|final
name|RecordIdentifier
name|maxKey
parameter_list|,
name|Reader
operator|.
name|Options
name|options
parameter_list|,
name|Options
name|mergerOptions
parameter_list|,
name|Configuration
name|conf
parameter_list|,
name|ValidTxnList
name|validTxnList
parameter_list|)
throws|throws
name|IOException
block|{
name|super
argument_list|(
name|key
argument_list|,
name|reader
argument_list|,
name|bucket
argument_list|,
name|minKey
argument_list|,
name|maxKey
argument_list|,
name|options
argument_list|,
literal|0
argument_list|)
expr_stmt|;
name|this
operator|.
name|mergerOptions
operator|=
name|mergerOptions
expr_stmt|;
name|this
operator|.
name|conf
operator|=
name|conf
expr_stmt|;
name|this
operator|.
name|options
operator|=
name|options
expr_stmt|;
assert|assert
name|mergerOptions
operator|.
name|getRootPath
argument_list|()
operator|!=
literal|null
operator|:
literal|"Since we have original files"
assert|;
assert|assert
name|bucket
operator|>=
literal|0
operator|:
literal|"don't support non-bucketed tables yet"
assert|;
name|RecordIdentifier
name|newMinKey
init|=
name|minKey
decl_stmt|;
name|RecordIdentifier
name|newMaxKey
init|=
name|maxKey
decl_stmt|;
if|if
condition|(
name|mergerOptions
operator|.
name|isCompacting
argument_list|()
condition|)
block|{
block|{
comment|//when compacting each split needs to process the whole logical bucket
assert|assert
name|options
operator|.
name|getOffset
argument_list|()
operator|==
literal|0
assert|;
assert|assert
name|options
operator|.
name|getMaxOffset
argument_list|()
operator|==
name|Long
operator|.
name|MAX_VALUE
assert|;
assert|assert
name|minKey
operator|==
literal|null
assert|;
assert|assert
name|maxKey
operator|==
literal|null
assert|;
block|}
name|AcidUtils
operator|.
name|Directory
name|directoryState
init|=
name|AcidUtils
operator|.
name|getAcidState
argument_list|(
name|mergerOptions
operator|.
name|getRootPath
argument_list|()
argument_list|,
name|conf
argument_list|,
name|validTxnList
argument_list|,
literal|false
argument_list|,
literal|true
argument_list|)
decl_stmt|;
name|originalFiles
operator|=
name|directoryState
operator|.
name|getOriginalFiles
argument_list|()
expr_stmt|;
assert|assert
name|originalFiles
operator|.
name|size
argument_list|()
operator|>
literal|0
assert|;
comment|/**          * when there are no copyN files, the {@link #recordReader} will be the the one and only          * file for for 'bucket' but closing here makes flow cleaner and only happens once in the          * life of the table.  With copyN files, the caller may pass in any one of the copyN files.          * This is less prone to bugs than expecting the reader to pass in a Reader for the 1st file          * of a logical bucket.*/
name|recordReader
operator|.
name|close
argument_list|()
expr_stmt|;
name|reader
operator|=
name|advanceToNextFile
argument_list|()
expr_stmt|;
comment|//in case of Compaction, this is the 1st file of the current bucket
if|if
condition|(
name|reader
operator|==
literal|null
condition|)
block|{
comment|//Compactor generated a split for a bucket that has no data?
throw|throw
operator|new
name|IllegalStateException
argument_list|(
literal|"No 'original' files found for bucketId="
operator|+
name|bucket
operator|+
literal|" in "
operator|+
name|mergerOptions
operator|.
name|getRootPath
argument_list|()
argument_list|)
throw|;
block|}
name|numRowsInCurrentFile
operator|=
name|reader
operator|.
name|getNumberOfRows
argument_list|()
expr_stmt|;
name|originalFileRecordReader
operator|=
name|reader
operator|.
name|rowsOptions
argument_list|(
name|options
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|/**          * Logically each bucket consists of 0000_0, 0000_0_copy_1... 0000_0_copyN. etc  We don't          * know N a priori so if this is true, then the current split is from 0000_0_copyN file.          * It's needed to correctly set maxKey.  In particular, set maxKey==null if this split          * is the tail of the last file for this logical bucket to include all deltas written after          * non-acid to acid table conversion.          */
name|boolean
name|isLastFileForThisBucket
init|=
literal|false
decl_stmt|;
name|boolean
name|haveSeenCurrentFile
init|=
literal|false
decl_stmt|;
name|originalFiles
operator|=
name|Collections
operator|.
name|emptyList
argument_list|()
expr_stmt|;
if|if
condition|(
name|mergerOptions
operator|.
name|getCopyIndex
argument_list|()
operator|>
literal|0
condition|)
block|{
comment|//the split is from something other than the 1st file of the logical bucket - compute offset
name|AcidUtils
operator|.
name|Directory
name|directoryState
init|=
name|AcidUtils
operator|.
name|getAcidState
argument_list|(
name|mergerOptions
operator|.
name|getRootPath
argument_list|()
argument_list|,
name|conf
argument_list|,
name|validTxnList
argument_list|,
literal|false
argument_list|,
literal|true
argument_list|)
decl_stmt|;
for|for
control|(
name|HadoopShims
operator|.
name|HdfsFileStatusWithId
name|f
range|:
name|directoryState
operator|.
name|getOriginalFiles
argument_list|()
control|)
block|{
name|AcidOutputFormat
operator|.
name|Options
name|bucketOptions
init|=
name|AcidUtils
operator|.
name|parseBaseOrDeltaBucketFilename
argument_list|(
name|f
operator|.
name|getFileStatus
argument_list|()
operator|.
name|getPath
argument_list|()
argument_list|,
name|conf
argument_list|)
decl_stmt|;
if|if
condition|(
name|bucketOptions
operator|.
name|getBucketId
argument_list|()
operator|!=
name|bucket
condition|)
block|{
continue|continue;
block|}
if|if
condition|(
name|haveSeenCurrentFile
condition|)
block|{
comment|//if here we already saw current file and now found another file for the same bucket
comment|//so the current file is not the last file of the logical bucket
name|isLastFileForThisBucket
operator|=
literal|false
expr_stmt|;
break|break;
block|}
if|if
condition|(
name|f
operator|.
name|getFileStatus
argument_list|()
operator|.
name|getPath
argument_list|()
operator|.
name|equals
argument_list|(
name|mergerOptions
operator|.
name|getBucketPath
argument_list|()
argument_list|)
condition|)
block|{
comment|/**                * found the file whence the current split is from so we're done                * counting {@link rowIdOffset}                */
name|haveSeenCurrentFile
operator|=
literal|true
expr_stmt|;
name|isLastFileForThisBucket
operator|=
literal|true
expr_stmt|;
continue|continue;
block|}
name|Reader
name|copyReader
init|=
name|OrcFile
operator|.
name|createReader
argument_list|(
name|f
operator|.
name|getFileStatus
argument_list|()
operator|.
name|getPath
argument_list|()
argument_list|,
name|OrcFile
operator|.
name|readerOptions
argument_list|(
name|conf
argument_list|)
argument_list|)
decl_stmt|;
name|rowIdOffset
operator|+=
name|copyReader
operator|.
name|getNumberOfRows
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
name|rowIdOffset
operator|>
literal|0
condition|)
block|{
comment|//rowIdOffset could be 0 if all files before current one are empty
comment|/**              * Since we already done {@link OrcRawRecordMerger#discoverOriginalKeyBounds(Reader,              * int, Reader.Options)} need to fix min/max key since these are used by              * {@link #next(OrcStruct)} which uses {@link #rowIdOffset} to generate rowId for              * the key.  Clear?  */
if|if
condition|(
name|minKey
operator|!=
literal|null
condition|)
block|{
name|minKey
operator|.
name|setRowId
argument_list|(
name|minKey
operator|.
name|getRowId
argument_list|()
operator|+
name|rowIdOffset
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|/**                *  If this is not the 1st file, set minKey 1 less than the start of current file                * (Would not need to set minKey if we knew that there are no delta files)                * {@link #advanceToMinKey()} needs this */
name|newMinKey
operator|=
operator|new
name|RecordIdentifier
argument_list|(
literal|0
argument_list|,
name|bucket
argument_list|,
name|rowIdOffset
operator|-
literal|1
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|maxKey
operator|!=
literal|null
condition|)
block|{
name|maxKey
operator|.
name|setRowId
argument_list|(
name|maxKey
operator|.
name|getRowId
argument_list|()
operator|+
name|rowIdOffset
argument_list|)
expr_stmt|;
block|}
block|}
block|}
else|else
block|{
name|isLastFileForThisBucket
operator|=
literal|true
expr_stmt|;
name|AcidUtils
operator|.
name|Directory
name|directoryState
init|=
name|AcidUtils
operator|.
name|getAcidState
argument_list|(
name|mergerOptions
operator|.
name|getRootPath
argument_list|()
argument_list|,
name|conf
argument_list|,
name|validTxnList
argument_list|,
literal|false
argument_list|,
literal|true
argument_list|)
decl_stmt|;
name|int
name|numFilesInBucket
init|=
literal|0
decl_stmt|;
for|for
control|(
name|HadoopShims
operator|.
name|HdfsFileStatusWithId
name|f
range|:
name|directoryState
operator|.
name|getOriginalFiles
argument_list|()
control|)
block|{
name|AcidOutputFormat
operator|.
name|Options
name|bucketOptions
init|=
name|AcidUtils
operator|.
name|parseBaseOrDeltaBucketFilename
argument_list|(
name|f
operator|.
name|getFileStatus
argument_list|()
operator|.
name|getPath
argument_list|()
argument_list|,
name|conf
argument_list|)
decl_stmt|;
if|if
condition|(
name|bucketOptions
operator|.
name|getBucketId
argument_list|()
operator|==
name|bucket
condition|)
block|{
name|numFilesInBucket
operator|++
expr_stmt|;
if|if
condition|(
name|numFilesInBucket
operator|>
literal|1
condition|)
block|{
name|isLastFileForThisBucket
operator|=
literal|false
expr_stmt|;
break|break;
block|}
block|}
block|}
block|}
name|originalFileRecordReader
operator|=
name|recordReader
expr_stmt|;
if|if
condition|(
operator|!
name|isLastFileForThisBucket
operator|&&
name|maxKey
operator|==
literal|null
condition|)
block|{
comment|/*            * If this is the last file for this bucket, maxKey == null means the split is the tail            * of the file so we want to leave it blank to make sure any insert events in delta            * files are included; Conversely, if it's not the last file, set the maxKey so that            * events from deltas that don't modify anything in the current split are excluded*/
name|newMaxKey
operator|=
operator|new
name|RecordIdentifier
argument_list|(
literal|0
argument_list|,
name|bucket
argument_list|,
name|rowIdOffset
operator|+
name|reader
operator|.
name|getNumberOfRows
argument_list|()
operator|-
literal|1
argument_list|)
expr_stmt|;
block|}
block|}
name|this
operator|.
name|minKey
operator|=
name|newMinKey
expr_stmt|;
name|this
operator|.
name|maxKey
operator|=
name|newMaxKey
expr_stmt|;
block|}
annotation|@
name|Override
name|RecordReader
name|getRecordReader
parameter_list|()
block|{
return|return
name|originalFileRecordReader
return|;
block|}
annotation|@
name|Override
name|RecordIdentifier
name|getMinKey
parameter_list|()
block|{
return|return
name|minKey
return|;
block|}
annotation|@
name|Override
name|RecordIdentifier
name|getMaxKey
parameter_list|()
block|{
return|return
name|maxKey
return|;
block|}
specifier|private
name|boolean
name|nextFromCurrentFile
parameter_list|(
name|OrcStruct
name|next
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|originalFileRecordReader
operator|.
name|hasNext
argument_list|()
condition|)
block|{
comment|//RecordReader.getRowNumber() produces a file-global row number even with PPD
name|long
name|nextRowId
init|=
name|originalFileRecordReader
operator|.
name|getRowNumber
argument_list|()
operator|+
name|rowIdOffset
decl_stmt|;
comment|// have to do initialization here, because the super's constructor
comment|// calls next and thus we need to initialize before our constructor
comment|// runs
if|if
condition|(
name|next
operator|==
literal|null
condition|)
block|{
name|nextRecord
operator|=
operator|new
name|OrcStruct
argument_list|(
name|OrcRecordUpdater
operator|.
name|FIELDS
argument_list|)
expr_stmt|;
name|IntWritable
name|operation
init|=
operator|new
name|IntWritable
argument_list|(
name|OrcRecordUpdater
operator|.
name|INSERT_OPERATION
argument_list|)
decl_stmt|;
name|nextRecord
operator|.
name|setFieldValue
argument_list|(
name|OrcRecordUpdater
operator|.
name|OPERATION
argument_list|,
name|operation
argument_list|)
expr_stmt|;
name|nextRecord
operator|.
name|setFieldValue
argument_list|(
name|OrcRecordUpdater
operator|.
name|CURRENT_TRANSACTION
argument_list|,
operator|new
name|LongWritable
argument_list|(
literal|0
argument_list|)
argument_list|)
expr_stmt|;
name|nextRecord
operator|.
name|setFieldValue
argument_list|(
name|OrcRecordUpdater
operator|.
name|ORIGINAL_TRANSACTION
argument_list|,
operator|new
name|LongWritable
argument_list|(
literal|0
argument_list|)
argument_list|)
expr_stmt|;
name|nextRecord
operator|.
name|setFieldValue
argument_list|(
name|OrcRecordUpdater
operator|.
name|BUCKET
argument_list|,
operator|new
name|IntWritable
argument_list|(
name|bucket
argument_list|)
argument_list|)
expr_stmt|;
name|nextRecord
operator|.
name|setFieldValue
argument_list|(
name|OrcRecordUpdater
operator|.
name|ROW_ID
argument_list|,
operator|new
name|LongWritable
argument_list|(
name|nextRowId
argument_list|)
argument_list|)
expr_stmt|;
name|nextRecord
operator|.
name|setFieldValue
argument_list|(
name|OrcRecordUpdater
operator|.
name|ROW
argument_list|,
name|originalFileRecordReader
operator|.
name|next
argument_list|(
literal|null
argument_list|)
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|nextRecord
operator|=
name|next
expr_stmt|;
operator|(
operator|(
name|IntWritable
operator|)
name|next
operator|.
name|getFieldValue
argument_list|(
name|OrcRecordUpdater
operator|.
name|OPERATION
argument_list|)
operator|)
operator|.
name|set
argument_list|(
name|OrcRecordUpdater
operator|.
name|INSERT_OPERATION
argument_list|)
expr_stmt|;
operator|(
operator|(
name|LongWritable
operator|)
name|next
operator|.
name|getFieldValue
argument_list|(
name|OrcRecordUpdater
operator|.
name|ORIGINAL_TRANSACTION
argument_list|)
operator|)
operator|.
name|set
argument_list|(
literal|0
argument_list|)
expr_stmt|;
operator|(
operator|(
name|IntWritable
operator|)
name|next
operator|.
name|getFieldValue
argument_list|(
name|OrcRecordUpdater
operator|.
name|BUCKET
argument_list|)
operator|)
operator|.
name|set
argument_list|(
name|bucket
argument_list|)
expr_stmt|;
operator|(
operator|(
name|LongWritable
operator|)
name|next
operator|.
name|getFieldValue
argument_list|(
name|OrcRecordUpdater
operator|.
name|CURRENT_TRANSACTION
argument_list|)
operator|)
operator|.
name|set
argument_list|(
literal|0
argument_list|)
expr_stmt|;
operator|(
operator|(
name|LongWritable
operator|)
name|next
operator|.
name|getFieldValue
argument_list|(
name|OrcRecordUpdater
operator|.
name|ROW_ID
argument_list|)
operator|)
operator|.
name|set
argument_list|(
name|nextRowId
argument_list|)
expr_stmt|;
name|nextRecord
operator|.
name|setFieldValue
argument_list|(
name|OrcRecordUpdater
operator|.
name|ROW
argument_list|,
name|originalFileRecordReader
operator|.
name|next
argument_list|(
name|OrcRecordUpdater
operator|.
name|getRow
argument_list|(
name|next
argument_list|)
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|key
operator|.
name|setValues
argument_list|(
literal|0L
argument_list|,
name|bucket
argument_list|,
name|nextRowId
argument_list|,
literal|0L
argument_list|,
literal|0
argument_list|)
expr_stmt|;
if|if
condition|(
name|maxKey
operator|!=
literal|null
operator|&&
name|key
operator|.
name|compareRow
argument_list|(
name|maxKey
argument_list|)
operator|>
literal|0
condition|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"key "
operator|+
name|key
operator|+
literal|"> maxkey "
operator|+
name|maxKey
argument_list|)
expr_stmt|;
block|}
return|return
literal|false
return|;
comment|//reached End Of Split
block|}
return|return
literal|true
return|;
block|}
return|return
literal|false
return|;
comment|//reached EndOfFile
block|}
annotation|@
name|Override
name|void
name|next
parameter_list|(
name|OrcStruct
name|next
parameter_list|)
throws|throws
name|IOException
block|{
assert|assert
name|advancedToMinKey
operator|:
literal|"advnaceToMinKey() was not called"
assert|;
while|while
condition|(
literal|true
condition|)
block|{
if|if
condition|(
name|nextFromCurrentFile
argument_list|(
name|next
argument_list|)
condition|)
block|{
return|return;
block|}
else|else
block|{
if|if
condition|(
name|originalFiles
operator|.
name|size
argument_list|()
operator|<=
name|nextFileIndex
condition|)
block|{
comment|//no more original files to read
name|nextRecord
operator|=
literal|null
expr_stmt|;
name|originalFileRecordReader
operator|.
name|close
argument_list|()
expr_stmt|;
return|return;
block|}
else|else
block|{
assert|assert
name|mergerOptions
operator|.
name|isCompacting
argument_list|()
operator|:
literal|"originalFiles.size() should be 0 when not compacting"
assert|;
name|rowIdOffset
operator|+=
name|numRowsInCurrentFile
expr_stmt|;
name|originalFileRecordReader
operator|.
name|close
argument_list|()
expr_stmt|;
name|Reader
name|reader
init|=
name|advanceToNextFile
argument_list|()
decl_stmt|;
if|if
condition|(
name|reader
operator|==
literal|null
condition|)
block|{
name|nextRecord
operator|=
literal|null
expr_stmt|;
return|return;
block|}
name|numRowsInCurrentFile
operator|=
name|reader
operator|.
name|getNumberOfRows
argument_list|()
expr_stmt|;
name|originalFileRecordReader
operator|=
name|reader
operator|.
name|rowsOptions
argument_list|(
name|options
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
comment|/**      * Finds the next file of the logical bucket      * @return {@code null} if there are no more files      */
specifier|private
name|Reader
name|advanceToNextFile
parameter_list|()
throws|throws
name|IOException
block|{
while|while
condition|(
name|nextFileIndex
operator|<
name|originalFiles
operator|.
name|size
argument_list|()
condition|)
block|{
name|AcidOutputFormat
operator|.
name|Options
name|bucketOptions
init|=
name|AcidUtils
operator|.
name|parseBaseOrDeltaBucketFilename
argument_list|(
name|originalFiles
operator|.
name|get
argument_list|(
name|nextFileIndex
argument_list|)
operator|.
name|getFileStatus
argument_list|()
operator|.
name|getPath
argument_list|()
argument_list|,
name|conf
argument_list|)
decl_stmt|;
if|if
condition|(
name|bucketOptions
operator|.
name|getBucketId
argument_list|()
operator|==
name|bucket
condition|)
block|{
break|break;
block|}
name|nextFileIndex
operator|++
expr_stmt|;
block|}
if|if
condition|(
name|originalFiles
operator|.
name|size
argument_list|()
operator|<=
name|nextFileIndex
condition|)
block|{
return|return
literal|null
return|;
comment|//no more files for current bucket
block|}
return|return
name|OrcFile
operator|.
name|createReader
argument_list|(
name|originalFiles
operator|.
name|get
argument_list|(
name|nextFileIndex
operator|++
argument_list|)
operator|.
name|getFileStatus
argument_list|()
operator|.
name|getPath
argument_list|()
argument_list|,
name|OrcFile
operator|.
name|readerOptions
argument_list|(
name|conf
argument_list|)
argument_list|)
return|;
block|}
annotation|@
name|Override
name|int
name|getColumns
parameter_list|()
block|{
return|return
name|reader
operator|.
name|getTypes
argument_list|()
operator|.
name|get
argument_list|(
literal|0
argument_list|)
operator|.
name|getSubtypesCount
argument_list|()
return|;
block|}
block|}
comment|/**    * The process here reads several (base + some deltas) files each of which is sorted on     * {@link ReaderKey} ascending.  The output of this Reader should a global order across these    * files.  The root of this tree is always the next 'file' to read from.    */
specifier|private
specifier|final
name|TreeMap
argument_list|<
name|ReaderKey
argument_list|,
name|ReaderPair
argument_list|>
name|readers
init|=
operator|new
name|TreeMap
argument_list|<
name|ReaderKey
argument_list|,
name|ReaderPair
argument_list|>
argument_list|()
decl_stmt|;
comment|// The reader that currently has the lowest key.
specifier|private
name|ReaderPair
name|primary
decl_stmt|;
comment|// The key of the next lowest reader.
specifier|private
name|ReaderKey
name|secondaryKey
init|=
literal|null
decl_stmt|;
specifier|private
specifier|static
specifier|final
class|class
name|KeyInterval
block|{
specifier|private
specifier|final
name|RecordIdentifier
name|minKey
decl_stmt|;
specifier|private
specifier|final
name|RecordIdentifier
name|maxKey
decl_stmt|;
specifier|private
name|KeyInterval
parameter_list|(
name|RecordIdentifier
name|minKey
parameter_list|,
name|RecordIdentifier
name|maxKey
parameter_list|)
block|{
name|this
operator|.
name|minKey
operator|=
name|minKey
expr_stmt|;
name|this
operator|.
name|maxKey
operator|=
name|maxKey
expr_stmt|;
block|}
specifier|private
name|RecordIdentifier
name|getMinKey
parameter_list|()
block|{
return|return
name|minKey
return|;
block|}
specifier|private
name|RecordIdentifier
name|getMaxKey
parameter_list|()
block|{
return|return
name|maxKey
return|;
block|}
block|}
comment|/**    * Find the key range for original bucket files.    * @param reader the reader    * @param bucket the bucket number we are reading    * @param options the options for reading with    * @throws IOException    */
specifier|private
name|KeyInterval
name|discoverOriginalKeyBounds
parameter_list|(
name|Reader
name|reader
parameter_list|,
name|int
name|bucket
parameter_list|,
name|Reader
operator|.
name|Options
name|options
parameter_list|)
throws|throws
name|IOException
block|{
name|long
name|rowLength
init|=
literal|0
decl_stmt|;
name|long
name|rowOffset
init|=
literal|0
decl_stmt|;
name|long
name|offset
init|=
name|options
operator|.
name|getOffset
argument_list|()
decl_stmt|;
comment|//this would usually be at block boundary
name|long
name|maxOffset
init|=
name|options
operator|.
name|getMaxOffset
argument_list|()
decl_stmt|;
comment|//this would usually be at block boundary
name|boolean
name|isTail
init|=
literal|true
decl_stmt|;
name|RecordIdentifier
name|minKey
init|=
literal|null
decl_stmt|;
name|RecordIdentifier
name|maxKey
init|=
literal|null
decl_stmt|;
comment|/**     * options.getOffset() and getMaxOffset() would usually be at block boundary which doesn't     * necessarily match stripe boundary.  So we want to come up with minKey to be one before the 1st     * row of the first stripe that starts after getOffset() and maxKey to be the last row of the     * stripe that contains getMaxOffset().  This breaks if getOffset() and getMaxOffset() are inside     * the sames tripe - in this case we have minKey& isTail=false but rowLength is never set.     * (HIVE-16953)     */
for|for
control|(
name|StripeInformation
name|stripe
range|:
name|reader
operator|.
name|getStripes
argument_list|()
control|)
block|{
if|if
condition|(
name|offset
operator|>
name|stripe
operator|.
name|getOffset
argument_list|()
condition|)
block|{
name|rowOffset
operator|+=
name|stripe
operator|.
name|getNumberOfRows
argument_list|()
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|maxOffset
operator|>
name|stripe
operator|.
name|getOffset
argument_list|()
condition|)
block|{
name|rowLength
operator|+=
name|stripe
operator|.
name|getNumberOfRows
argument_list|()
expr_stmt|;
block|}
else|else
block|{
name|isTail
operator|=
literal|false
expr_stmt|;
break|break;
block|}
block|}
if|if
condition|(
name|rowOffset
operator|>
literal|0
condition|)
block|{
name|minKey
operator|=
operator|new
name|RecordIdentifier
argument_list|(
literal|0
argument_list|,
name|bucket
argument_list|,
name|rowOffset
operator|-
literal|1
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
operator|!
name|isTail
condition|)
block|{
name|maxKey
operator|=
operator|new
name|RecordIdentifier
argument_list|(
literal|0
argument_list|,
name|bucket
argument_list|,
name|rowOffset
operator|+
name|rowLength
operator|-
literal|1
argument_list|)
expr_stmt|;
block|}
return|return
operator|new
name|KeyInterval
argument_list|(
name|minKey
argument_list|,
name|maxKey
argument_list|)
return|;
block|}
comment|/**    * Find the key range for bucket files.    * @param reader the reader    * @param options the options for reading with    * @throws IOException    */
specifier|private
name|KeyInterval
name|discoverKeyBounds
parameter_list|(
name|Reader
name|reader
parameter_list|,
name|Reader
operator|.
name|Options
name|options
parameter_list|)
throws|throws
name|IOException
block|{
name|RecordIdentifier
index|[]
name|keyIndex
init|=
name|OrcRecordUpdater
operator|.
name|parseKeyIndex
argument_list|(
name|reader
argument_list|)
decl_stmt|;
name|long
name|offset
init|=
name|options
operator|.
name|getOffset
argument_list|()
decl_stmt|;
name|long
name|maxOffset
init|=
name|options
operator|.
name|getMaxOffset
argument_list|()
decl_stmt|;
name|int
name|firstStripe
init|=
literal|0
decl_stmt|;
name|int
name|stripeCount
init|=
literal|0
decl_stmt|;
name|boolean
name|isTail
init|=
literal|true
decl_stmt|;
name|RecordIdentifier
name|minKey
init|=
literal|null
decl_stmt|;
name|RecordIdentifier
name|maxKey
init|=
literal|null
decl_stmt|;
name|List
argument_list|<
name|StripeInformation
argument_list|>
name|stripes
init|=
name|reader
operator|.
name|getStripes
argument_list|()
decl_stmt|;
for|for
control|(
name|StripeInformation
name|stripe
range|:
name|stripes
control|)
block|{
if|if
condition|(
name|offset
operator|>
name|stripe
operator|.
name|getOffset
argument_list|()
condition|)
block|{
name|firstStripe
operator|+=
literal|1
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|maxOffset
operator|>
name|stripe
operator|.
name|getOffset
argument_list|()
condition|)
block|{
name|stripeCount
operator|+=
literal|1
expr_stmt|;
block|}
else|else
block|{
name|isTail
operator|=
literal|false
expr_stmt|;
break|break;
block|}
block|}
if|if
condition|(
name|firstStripe
operator|!=
literal|0
condition|)
block|{
name|minKey
operator|=
name|keyIndex
index|[
name|firstStripe
operator|-
literal|1
index|]
expr_stmt|;
block|}
if|if
condition|(
operator|!
name|isTail
condition|)
block|{
name|maxKey
operator|=
name|keyIndex
index|[
name|firstStripe
operator|+
name|stripeCount
operator|-
literal|1
index|]
expr_stmt|;
block|}
return|return
operator|new
name|KeyInterval
argument_list|(
name|minKey
argument_list|,
name|maxKey
argument_list|)
return|;
block|}
comment|/**    * Convert from the row include/sarg/columnNames to the event equivalent    * for the underlying file.    * @param options options for the row reader    * @return a cloned options object that is modified for the event reader    */
specifier|static
name|Reader
operator|.
name|Options
name|createEventOptions
parameter_list|(
name|Reader
operator|.
name|Options
name|options
parameter_list|)
block|{
name|Reader
operator|.
name|Options
name|result
init|=
name|options
operator|.
name|clone
argument_list|()
decl_stmt|;
name|result
operator|.
name|range
argument_list|(
name|options
operator|.
name|getOffset
argument_list|()
argument_list|,
name|Long
operator|.
name|MAX_VALUE
argument_list|)
expr_stmt|;
name|result
operator|.
name|include
argument_list|(
name|options
operator|.
name|getInclude
argument_list|()
argument_list|)
expr_stmt|;
comment|// slide the column names down by 6 for the name array
if|if
condition|(
name|options
operator|.
name|getColumnNames
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|String
index|[]
name|orig
init|=
name|options
operator|.
name|getColumnNames
argument_list|()
decl_stmt|;
name|String
index|[]
name|cols
init|=
operator|new
name|String
index|[
name|orig
operator|.
name|length
operator|+
name|OrcRecordUpdater
operator|.
name|FIELDS
index|]
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|orig
operator|.
name|length
condition|;
operator|++
name|i
control|)
block|{
name|cols
index|[
name|i
operator|+
name|OrcRecordUpdater
operator|.
name|FIELDS
index|]
operator|=
name|orig
index|[
name|i
index|]
expr_stmt|;
block|}
name|result
operator|.
name|searchArgument
argument_list|(
name|options
operator|.
name|getSearchArgument
argument_list|()
argument_list|,
name|cols
argument_list|)
expr_stmt|;
block|}
return|return
name|result
return|;
block|}
specifier|static
class|class
name|Options
block|{
specifier|private
name|int
name|copyIndex
init|=
literal|0
decl_stmt|;
specifier|private
name|boolean
name|isCompacting
init|=
literal|false
decl_stmt|;
specifier|private
name|Path
name|bucketPath
decl_stmt|;
specifier|private
name|Path
name|rootPath
decl_stmt|;
name|Options
name|copyIndex
parameter_list|(
name|int
name|copyIndex
parameter_list|)
block|{
assert|assert
name|copyIndex
operator|>=
literal|0
assert|;
name|this
operator|.
name|copyIndex
operator|=
name|copyIndex
expr_stmt|;
return|return
name|this
return|;
block|}
name|Options
name|isCompacting
parameter_list|(
name|boolean
name|isCompacting
parameter_list|)
block|{
name|this
operator|.
name|isCompacting
operator|=
name|isCompacting
expr_stmt|;
return|return
name|this
return|;
block|}
name|Options
name|bucketPath
parameter_list|(
name|Path
name|bucketPath
parameter_list|)
block|{
name|this
operator|.
name|bucketPath
operator|=
name|bucketPath
expr_stmt|;
return|return
name|this
return|;
block|}
name|Options
name|rootPath
parameter_list|(
name|Path
name|rootPath
parameter_list|)
block|{
name|this
operator|.
name|rootPath
operator|=
name|rootPath
expr_stmt|;
return|return
name|this
return|;
block|}
comment|/**      * 0 means it's the original file, without {@link Utilities#COPY_KEYWORD} suffix      */
name|int
name|getCopyIndex
parameter_list|()
block|{
return|return
name|copyIndex
return|;
block|}
name|boolean
name|isCompacting
parameter_list|()
block|{
return|return
name|isCompacting
return|;
block|}
comment|/**      * Full path to the data file      * @return      */
name|Path
name|getBucketPath
parameter_list|()
block|{
return|return
name|bucketPath
return|;
block|}
comment|/**      * Partition folder (Table folder if not partitioned)      */
name|Path
name|getRootPath
parameter_list|()
block|{
return|return
name|rootPath
return|;
block|}
block|}
comment|/**    * Create a reader that merge sorts the ACID events together.    * @param conf the configuration    * @param collapseEvents should the events on the same row be collapsed    * @param isOriginal is the base file a pre-acid file    * @param bucket the bucket we are reading    * @param options the options to read with    * @param deltaDirectory the list of delta directories to include    * @throws IOException    */
name|OrcRawRecordMerger
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|boolean
name|collapseEvents
parameter_list|,
name|Reader
name|reader
parameter_list|,
name|boolean
name|isOriginal
parameter_list|,
name|int
name|bucket
parameter_list|,
name|ValidTxnList
name|validTxnList
parameter_list|,
name|Reader
operator|.
name|Options
name|options
parameter_list|,
name|Path
index|[]
name|deltaDirectory
parameter_list|,
name|Options
name|mergerOptions
parameter_list|)
throws|throws
name|IOException
block|{
name|this
operator|.
name|conf
operator|=
name|conf
expr_stmt|;
name|this
operator|.
name|collapse
operator|=
name|collapseEvents
expr_stmt|;
name|this
operator|.
name|offset
operator|=
name|options
operator|.
name|getOffset
argument_list|()
expr_stmt|;
name|this
operator|.
name|length
operator|=
name|options
operator|.
name|getLength
argument_list|()
expr_stmt|;
name|this
operator|.
name|validTxnList
operator|=
name|validTxnList
expr_stmt|;
name|TypeDescription
name|typeDescr
init|=
name|OrcInputFormat
operator|.
name|getDesiredRowTypeDescr
argument_list|(
name|conf
argument_list|,
literal|true
argument_list|,
name|Integer
operator|.
name|MAX_VALUE
argument_list|)
decl_stmt|;
name|objectInspector
operator|=
name|OrcRecordUpdater
operator|.
name|createEventSchema
argument_list|(
name|OrcStruct
operator|.
name|createObjectInspector
argument_list|(
literal|0
argument_list|,
name|OrcUtils
operator|.
name|getOrcTypes
argument_list|(
name|typeDescr
argument_list|)
argument_list|)
argument_list|)
expr_stmt|;
comment|// modify the options to reflect the event instead of the base row
name|Reader
operator|.
name|Options
name|eventOptions
init|=
name|createEventOptions
argument_list|(
name|options
argument_list|)
decl_stmt|;
if|if
condition|(
name|reader
operator|==
literal|null
condition|)
block|{
name|baseReader
operator|=
literal|null
expr_stmt|;
name|minKey
operator|=
name|maxKey
operator|=
literal|null
expr_stmt|;
block|}
else|else
block|{
name|KeyInterval
name|keyInterval
decl_stmt|;
comment|// find the min/max based on the offset and length (and more for 'original')
if|if
condition|(
name|isOriginal
condition|)
block|{
name|keyInterval
operator|=
name|discoverOriginalKeyBounds
argument_list|(
name|reader
argument_list|,
name|bucket
argument_list|,
name|options
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|keyInterval
operator|=
name|discoverKeyBounds
argument_list|(
name|reader
argument_list|,
name|options
argument_list|)
expr_stmt|;
block|}
name|LOG
operator|.
name|info
argument_list|(
literal|"min key = "
operator|+
name|keyInterval
operator|.
name|getMinKey
argument_list|()
operator|+
literal|", max key = "
operator|+
name|keyInterval
operator|.
name|getMaxKey
argument_list|()
argument_list|)
expr_stmt|;
comment|// use the min/max instead of the byte range
name|ReaderPair
name|pair
decl_stmt|;
name|ReaderKey
name|key
init|=
operator|new
name|ReaderKey
argument_list|()
decl_stmt|;
if|if
condition|(
name|isOriginal
condition|)
block|{
name|options
operator|=
name|options
operator|.
name|clone
argument_list|()
expr_stmt|;
name|pair
operator|=
operator|new
name|OriginalReaderPair
argument_list|(
name|key
argument_list|,
name|reader
argument_list|,
name|bucket
argument_list|,
name|keyInterval
operator|.
name|getMinKey
argument_list|()
argument_list|,
name|keyInterval
operator|.
name|getMaxKey
argument_list|()
argument_list|,
name|options
argument_list|,
name|mergerOptions
argument_list|,
name|conf
argument_list|,
name|validTxnList
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|pair
operator|=
operator|new
name|ReaderPair
argument_list|(
name|key
argument_list|,
name|reader
argument_list|,
name|bucket
argument_list|,
name|keyInterval
operator|.
name|getMinKey
argument_list|()
argument_list|,
name|keyInterval
operator|.
name|getMaxKey
argument_list|()
argument_list|,
name|eventOptions
argument_list|,
literal|0
argument_list|)
expr_stmt|;
block|}
name|minKey
operator|=
name|pair
operator|.
name|getMinKey
argument_list|()
expr_stmt|;
name|maxKey
operator|=
name|pair
operator|.
name|getMaxKey
argument_list|()
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"updated min key = "
operator|+
name|keyInterval
operator|.
name|getMinKey
argument_list|()
operator|+
literal|", max key = "
operator|+
name|keyInterval
operator|.
name|getMaxKey
argument_list|()
argument_list|)
expr_stmt|;
name|pair
operator|.
name|advnaceToMinKey
argument_list|()
expr_stmt|;
comment|// if there is at least one record, put it in the map
if|if
condition|(
name|pair
operator|.
name|nextRecord
operator|!=
literal|null
condition|)
block|{
name|readers
operator|.
name|put
argument_list|(
name|key
argument_list|,
name|pair
argument_list|)
expr_stmt|;
block|}
name|baseReader
operator|=
name|pair
operator|.
name|getRecordReader
argument_list|()
expr_stmt|;
block|}
comment|// we always want to read all of the deltas
name|eventOptions
operator|.
name|range
argument_list|(
literal|0
argument_list|,
name|Long
operator|.
name|MAX_VALUE
argument_list|)
expr_stmt|;
if|if
condition|(
name|deltaDirectory
operator|!=
literal|null
condition|)
block|{
for|for
control|(
name|Path
name|delta
range|:
name|deltaDirectory
control|)
block|{
name|ReaderKey
name|key
init|=
operator|new
name|ReaderKey
argument_list|()
decl_stmt|;
name|Path
name|deltaFile
init|=
name|AcidUtils
operator|.
name|createBucketFile
argument_list|(
name|delta
argument_list|,
name|bucket
argument_list|)
decl_stmt|;
name|AcidUtils
operator|.
name|ParsedDelta
name|deltaDir
init|=
name|AcidUtils
operator|.
name|parsedDelta
argument_list|(
name|delta
argument_list|)
decl_stmt|;
name|FileSystem
name|fs
init|=
name|deltaFile
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|long
name|length
init|=
name|OrcAcidUtils
operator|.
name|getLastFlushLength
argument_list|(
name|fs
argument_list|,
name|deltaFile
argument_list|)
decl_stmt|;
if|if
condition|(
name|length
operator|!=
operator|-
literal|1
operator|&&
name|fs
operator|.
name|exists
argument_list|(
name|deltaFile
argument_list|)
condition|)
block|{
name|Reader
name|deltaReader
init|=
name|OrcFile
operator|.
name|createReader
argument_list|(
name|deltaFile
argument_list|,
name|OrcFile
operator|.
name|readerOptions
argument_list|(
name|conf
argument_list|)
operator|.
name|maxLength
argument_list|(
name|length
argument_list|)
argument_list|)
decl_stmt|;
name|Reader
operator|.
name|Options
name|deltaEventOptions
init|=
literal|null
decl_stmt|;
if|if
condition|(
name|eventOptions
operator|.
name|getSearchArgument
argument_list|()
operator|!=
literal|null
condition|)
block|{
comment|// Turn off the sarg before pushing it to delta.  We never want to push a sarg to a delta as
comment|// it can produce wrong results (if the latest valid version of the record is filtered out by
comment|// the sarg) or ArrayOutOfBounds errors (when the sarg is applied to a delete record)
comment|// unless the delta only has insert events
name|AcidStats
name|acidStats
init|=
name|OrcAcidUtils
operator|.
name|parseAcidStats
argument_list|(
name|deltaReader
argument_list|)
decl_stmt|;
if|if
condition|(
name|acidStats
operator|.
name|deletes
operator|>
literal|0
operator|||
name|acidStats
operator|.
name|updates
operator|>
literal|0
condition|)
block|{
name|deltaEventOptions
operator|=
name|eventOptions
operator|.
name|clone
argument_list|()
operator|.
name|searchArgument
argument_list|(
literal|null
argument_list|,
literal|null
argument_list|)
expr_stmt|;
block|}
block|}
name|ReaderPair
name|deltaPair
decl_stmt|;
name|deltaPair
operator|=
operator|new
name|ReaderPair
argument_list|(
name|key
argument_list|,
name|deltaReader
argument_list|,
name|bucket
argument_list|,
name|minKey
argument_list|,
name|maxKey
argument_list|,
name|deltaEventOptions
operator|!=
literal|null
condition|?
name|deltaEventOptions
else|:
name|eventOptions
argument_list|,
name|deltaDir
operator|.
name|getStatementId
argument_list|()
argument_list|)
expr_stmt|;
name|deltaPair
operator|.
name|advnaceToMinKey
argument_list|()
expr_stmt|;
if|if
condition|(
name|deltaPair
operator|.
name|nextRecord
operator|!=
literal|null
condition|)
block|{
name|readers
operator|.
name|put
argument_list|(
name|key
argument_list|,
name|deltaPair
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
comment|// get the first record
name|Map
operator|.
name|Entry
argument_list|<
name|ReaderKey
argument_list|,
name|ReaderPair
argument_list|>
name|entry
init|=
name|readers
operator|.
name|pollFirstEntry
argument_list|()
decl_stmt|;
if|if
condition|(
name|entry
operator|==
literal|null
condition|)
block|{
name|columns
operator|=
literal|0
expr_stmt|;
name|primary
operator|=
literal|null
expr_stmt|;
block|}
else|else
block|{
name|primary
operator|=
name|entry
operator|.
name|getValue
argument_list|()
expr_stmt|;
if|if
condition|(
name|readers
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|secondaryKey
operator|=
literal|null
expr_stmt|;
block|}
else|else
block|{
name|secondaryKey
operator|=
name|readers
operator|.
name|firstKey
argument_list|()
expr_stmt|;
block|}
comment|// get the number of columns in the user's rows
name|columns
operator|=
name|primary
operator|.
name|getColumns
argument_list|()
expr_stmt|;
block|}
block|}
annotation|@
name|VisibleForTesting
name|RecordIdentifier
name|getMinKey
parameter_list|()
block|{
return|return
name|minKey
return|;
block|}
annotation|@
name|VisibleForTesting
name|RecordIdentifier
name|getMaxKey
parameter_list|()
block|{
return|return
name|maxKey
return|;
block|}
annotation|@
name|VisibleForTesting
name|ReaderPair
name|getCurrentReader
parameter_list|()
block|{
return|return
name|primary
return|;
block|}
annotation|@
name|VisibleForTesting
name|Map
argument_list|<
name|ReaderKey
argument_list|,
name|ReaderPair
argument_list|>
name|getOtherReaders
parameter_list|()
block|{
return|return
name|readers
return|;
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|next
parameter_list|(
name|RecordIdentifier
name|recordIdentifier
parameter_list|,
name|OrcStruct
name|prev
parameter_list|)
throws|throws
name|IOException
block|{
name|boolean
name|keysSame
init|=
literal|true
decl_stmt|;
while|while
condition|(
name|keysSame
operator|&&
name|primary
operator|!=
literal|null
condition|)
block|{
comment|// The primary's nextRecord is the next value to return
name|OrcStruct
name|current
init|=
name|primary
operator|.
name|nextRecord
decl_stmt|;
name|recordIdentifier
operator|.
name|set
argument_list|(
name|primary
operator|.
name|key
argument_list|)
expr_stmt|;
comment|// Advance the primary reader to the next record
name|primary
operator|.
name|next
argument_list|(
name|extraValue
argument_list|)
expr_stmt|;
comment|// Save the current record as the new extraValue for next time so that
comment|// we minimize allocations
name|extraValue
operator|=
name|current
expr_stmt|;
comment|// now that the primary reader has advanced, we need to see if we
comment|// continue to read it or move to the secondary.
if|if
condition|(
name|primary
operator|.
name|nextRecord
operator|==
literal|null
operator|||
name|primary
operator|.
name|key
operator|.
name|compareTo
argument_list|(
name|secondaryKey
argument_list|)
operator|>
literal|0
condition|)
block|{
comment|// if the primary isn't done, push it back into the readers
if|if
condition|(
name|primary
operator|.
name|nextRecord
operator|!=
literal|null
condition|)
block|{
name|readers
operator|.
name|put
argument_list|(
name|primary
operator|.
name|key
argument_list|,
name|primary
argument_list|)
expr_stmt|;
block|}
comment|// update primary and secondaryKey
name|Map
operator|.
name|Entry
argument_list|<
name|ReaderKey
argument_list|,
name|ReaderPair
argument_list|>
name|entry
init|=
name|readers
operator|.
name|pollFirstEntry
argument_list|()
decl_stmt|;
if|if
condition|(
name|entry
operator|!=
literal|null
condition|)
block|{
name|primary
operator|=
name|entry
operator|.
name|getValue
argument_list|()
expr_stmt|;
if|if
condition|(
name|readers
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|secondaryKey
operator|=
literal|null
expr_stmt|;
block|}
else|else
block|{
name|secondaryKey
operator|=
name|readers
operator|.
name|firstKey
argument_list|()
expr_stmt|;
block|}
block|}
else|else
block|{
name|primary
operator|=
literal|null
expr_stmt|;
block|}
block|}
comment|// if this transaction isn't ok, skip over it
if|if
condition|(
operator|!
name|validTxnList
operator|.
name|isTxnValid
argument_list|(
operator|(
operator|(
name|ReaderKey
operator|)
name|recordIdentifier
operator|)
operator|.
name|getCurrentTransactionId
argument_list|()
argument_list|)
condition|)
block|{
continue|continue;
block|}
comment|/*for multi-statement txns, you may have multiple events for the same       * row in the same (current) transaction.  We want to collapse these to just the last one       * regardless whether we are minor compacting.  Consider INSERT/UPDATE/UPDATE of the       * same row in the same txn.  There is no benefit passing along anything except the last       * event.  If we did want to pass it along, we'd have to include statementId in the row       * returned so that compaction could write it out or make minor minor compaction understand       * how to write out delta files in delta_xxx_yyy_stid format.  There doesn't seem to be any       * value in this.*/
name|boolean
name|isSameRow
init|=
name|prevKey
operator|.
name|isSameRow
argument_list|(
operator|(
name|ReaderKey
operator|)
name|recordIdentifier
argument_list|)
decl_stmt|;
comment|// if we are collapsing, figure out if this is a new row
if|if
condition|(
name|collapse
operator|||
name|isSameRow
condition|)
block|{
name|keysSame
operator|=
operator|(
name|collapse
operator|&&
name|prevKey
operator|.
name|compareRow
argument_list|(
name|recordIdentifier
argument_list|)
operator|==
literal|0
operator|)
operator|||
operator|(
name|isSameRow
operator|)
expr_stmt|;
if|if
condition|(
operator|!
name|keysSame
condition|)
block|{
name|prevKey
operator|.
name|set
argument_list|(
name|recordIdentifier
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
name|keysSame
operator|=
literal|false
expr_stmt|;
block|}
comment|// set the output record by fiddling with the pointers so that we can
comment|// avoid a copy.
name|prev
operator|.
name|linkFields
argument_list|(
name|current
argument_list|)
expr_stmt|;
block|}
return|return
operator|!
name|keysSame
return|;
block|}
annotation|@
name|Override
specifier|public
name|RecordIdentifier
name|createKey
parameter_list|()
block|{
return|return
operator|new
name|ReaderKey
argument_list|()
return|;
block|}
annotation|@
name|Override
specifier|public
name|OrcStruct
name|createValue
parameter_list|()
block|{
return|return
operator|new
name|OrcStruct
argument_list|(
name|OrcRecordUpdater
operator|.
name|FIELDS
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|public
name|long
name|getPos
parameter_list|()
throws|throws
name|IOException
block|{
return|return
name|offset
operator|+
call|(
name|long
call|)
argument_list|(
name|getProgress
argument_list|()
operator|*
name|length
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|close
parameter_list|()
throws|throws
name|IOException
block|{
if|if
condition|(
name|primary
operator|!=
literal|null
condition|)
block|{
name|primary
operator|.
name|recordReader
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
for|for
control|(
name|ReaderPair
name|pair
range|:
name|readers
operator|.
name|values
argument_list|()
control|)
block|{
name|pair
operator|.
name|recordReader
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
annotation|@
name|Override
specifier|public
name|float
name|getProgress
parameter_list|()
throws|throws
name|IOException
block|{
comment|//this is not likely to do the right thing for Compaction of "original" files when there are copyN files
return|return
name|baseReader
operator|==
literal|null
condition|?
literal|1
else|:
name|baseReader
operator|.
name|getProgress
argument_list|()
return|;
block|}
annotation|@
name|Override
specifier|public
name|ObjectInspector
name|getObjectInspector
parameter_list|()
block|{
return|return
name|objectInspector
return|;
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|isDelete
parameter_list|(
name|OrcStruct
name|value
parameter_list|)
block|{
return|return
name|OrcRecordUpdater
operator|.
name|getOperation
argument_list|(
name|value
argument_list|)
operator|==
name|OrcRecordUpdater
operator|.
name|DELETE_OPERATION
return|;
block|}
comment|/**    * Get the number of columns in the underlying rows.    * @return 0 if there are no base and no deltas.    */
specifier|public
name|int
name|getColumns
parameter_list|()
block|{
return|return
name|columns
return|;
block|}
block|}
end_class

end_unit

