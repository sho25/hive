begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/**  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|mr
package|;
end_package

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|InputStream
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|OutputStream
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|Serializable
import|;
end_import

begin_import
import|import
name|java
operator|.
name|lang
operator|.
name|management
operator|.
name|ManagementFactory
import|;
end_import

begin_import
import|import
name|java
operator|.
name|lang
operator|.
name|management
operator|.
name|MemoryMXBean
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Collection
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Collections
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Enumeration
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Properties
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|lang
operator|.
name|StringUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|Log
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|LogFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configuration
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|filecache
operator|.
name|DistributedCache
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileStatus
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|CompressionUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|LogUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|LogUtils
operator|.
name|LogInitializationException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
operator|.
name|ConfVars
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|Context
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|DriverContext
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|ErrorMsg
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|QueryPlan
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|FetchOperator
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|HiveTotalOrderPartitioner
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|Operator
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|OperatorUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|PartitionKeySampler
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|TableScanOperator
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|Task
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|Utilities
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|BucketizedHiveInputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|HiveKey
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|HiveOutputFormatImpl
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|IOPrepareCache
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|HiveException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|FetchWork
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|MapWork
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|MapredLocalWork
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|MapredWork
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|OperatorDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|PartitionDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|ReduceWork
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|api
operator|.
name|StageType
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|session
operator|.
name|SessionState
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|session
operator|.
name|SessionState
operator|.
name|LogHelper
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|stats
operator|.
name|StatsFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|stats
operator|.
name|StatsPublisher
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|objectinspector
operator|.
name|ObjectInspector
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|shims
operator|.
name|ShimLoader
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|BytesWritable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|Text
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|Counters
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|InputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|JobClient
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|JobConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|Partitioner
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|RunningJob
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|log4j
operator|.
name|Appender
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|log4j
operator|.
name|BasicConfigurator
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|log4j
operator|.
name|FileAppender
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|log4j
operator|.
name|LogManager
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|log4j
operator|.
name|varia
operator|.
name|NullAppender
import|;
end_import

begin_comment
comment|/**  * ExecDriver is the central class in co-ordinating execution of any map-reduce task.  * It's main responsabilities are:  *  * - Converting the plan (MapredWork) into a MR Job (JobConf)  * - Submitting a MR job to the cluster via JobClient and ExecHelper  * - Executing MR job in local execution mode (where applicable)  *  */
end_comment

begin_class
specifier|public
class|class
name|ExecDriver
extends|extends
name|Task
argument_list|<
name|MapredWork
argument_list|>
implements|implements
name|Serializable
implements|,
name|HadoopJobExecHook
block|{
specifier|private
specifier|static
specifier|final
name|long
name|serialVersionUID
init|=
literal|1L
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|String
name|JOBCONF_FILENAME
init|=
literal|"jobconf.xml"
decl_stmt|;
specifier|protected
specifier|transient
name|JobConf
name|job
decl_stmt|;
specifier|public
specifier|static
name|MemoryMXBean
name|memoryMXBean
decl_stmt|;
specifier|protected
name|HadoopJobExecHelper
name|jobExecHelper
decl_stmt|;
specifier|protected
specifier|static
specifier|transient
specifier|final
name|Log
name|LOG
init|=
name|LogFactory
operator|.
name|getLog
argument_list|(
name|ExecDriver
operator|.
name|class
argument_list|)
decl_stmt|;
specifier|private
name|RunningJob
name|rj
decl_stmt|;
comment|/**    * Constructor when invoked from QL.    */
specifier|public
name|ExecDriver
parameter_list|()
block|{
name|super
argument_list|()
expr_stmt|;
name|console
operator|=
operator|new
name|LogHelper
argument_list|(
name|LOG
argument_list|)
expr_stmt|;
name|this
operator|.
name|jobExecHelper
operator|=
operator|new
name|HadoopJobExecHelper
argument_list|(
name|job
argument_list|,
name|console
argument_list|,
name|this
argument_list|,
name|this
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|requireLock
parameter_list|()
block|{
return|return
literal|true
return|;
block|}
specifier|private
name|void
name|initializeFiles
parameter_list|(
name|String
name|prop
parameter_list|,
name|String
name|files
parameter_list|)
block|{
if|if
condition|(
name|files
operator|!=
literal|null
operator|&&
name|files
operator|.
name|length
argument_list|()
operator|>
literal|0
condition|)
block|{
name|job
operator|.
name|set
argument_list|(
name|prop
argument_list|,
name|files
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * Initialization when invoked from QL.    */
annotation|@
name|Override
specifier|public
name|void
name|initialize
parameter_list|(
name|HiveConf
name|conf
parameter_list|,
name|QueryPlan
name|queryPlan
parameter_list|,
name|DriverContext
name|driverContext
parameter_list|)
block|{
name|super
operator|.
name|initialize
argument_list|(
name|conf
argument_list|,
name|queryPlan
argument_list|,
name|driverContext
argument_list|)
expr_stmt|;
name|job
operator|=
operator|new
name|JobConf
argument_list|(
name|conf
argument_list|,
name|ExecDriver
operator|.
name|class
argument_list|)
expr_stmt|;
comment|// NOTE: initialize is only called if it is in non-local mode.
comment|// In case it's in non-local mode, we need to move the SessionState files
comment|// and jars to jobConf.
comment|// In case it's in local mode, MapRedTask will set the jobConf.
comment|//
comment|// "tmpfiles" and "tmpjars" are set by the method ExecDriver.execute(),
comment|// which will be called by both local and NON-local mode.
name|String
name|addedFiles
init|=
name|Utilities
operator|.
name|getResourceFiles
argument_list|(
name|job
argument_list|,
name|SessionState
operator|.
name|ResourceType
operator|.
name|FILE
argument_list|)
decl_stmt|;
if|if
condition|(
name|StringUtils
operator|.
name|isNotBlank
argument_list|(
name|addedFiles
argument_list|)
condition|)
block|{
name|HiveConf
operator|.
name|setVar
argument_list|(
name|job
argument_list|,
name|ConfVars
operator|.
name|HIVEADDEDFILES
argument_list|,
name|addedFiles
argument_list|)
expr_stmt|;
block|}
name|String
name|addedJars
init|=
name|Utilities
operator|.
name|getResourceFiles
argument_list|(
name|job
argument_list|,
name|SessionState
operator|.
name|ResourceType
operator|.
name|JAR
argument_list|)
decl_stmt|;
if|if
condition|(
name|StringUtils
operator|.
name|isNotBlank
argument_list|(
name|addedJars
argument_list|)
condition|)
block|{
name|HiveConf
operator|.
name|setVar
argument_list|(
name|job
argument_list|,
name|ConfVars
operator|.
name|HIVEADDEDJARS
argument_list|,
name|addedJars
argument_list|)
expr_stmt|;
block|}
name|String
name|addedArchives
init|=
name|Utilities
operator|.
name|getResourceFiles
argument_list|(
name|job
argument_list|,
name|SessionState
operator|.
name|ResourceType
operator|.
name|ARCHIVE
argument_list|)
decl_stmt|;
if|if
condition|(
name|StringUtils
operator|.
name|isNotBlank
argument_list|(
name|addedArchives
argument_list|)
condition|)
block|{
name|HiveConf
operator|.
name|setVar
argument_list|(
name|job
argument_list|,
name|ConfVars
operator|.
name|HIVEADDEDARCHIVES
argument_list|,
name|addedArchives
argument_list|)
expr_stmt|;
block|}
name|this
operator|.
name|jobExecHelper
operator|=
operator|new
name|HadoopJobExecHelper
argument_list|(
name|job
argument_list|,
name|console
argument_list|,
name|this
argument_list|,
name|this
argument_list|)
expr_stmt|;
block|}
comment|/**    * Constructor/Initialization for invocation as independent utility.    */
specifier|public
name|ExecDriver
parameter_list|(
name|MapredWork
name|plan
parameter_list|,
name|JobConf
name|job
parameter_list|,
name|boolean
name|isSilent
parameter_list|)
throws|throws
name|HiveException
block|{
name|setWork
argument_list|(
name|plan
argument_list|)
expr_stmt|;
name|this
operator|.
name|job
operator|=
name|job
expr_stmt|;
name|console
operator|=
operator|new
name|LogHelper
argument_list|(
name|LOG
argument_list|,
name|isSilent
argument_list|)
expr_stmt|;
name|this
operator|.
name|jobExecHelper
operator|=
operator|new
name|HadoopJobExecHelper
argument_list|(
name|job
argument_list|,
name|console
argument_list|,
name|this
argument_list|,
name|this
argument_list|)
expr_stmt|;
block|}
comment|/**    * Fatal errors are those errors that cannot be recovered by retries. These are application    * dependent. Examples of fatal errors include: - the small table in the map-side joins is too    * large to be feasible to be handled by one mapper. The job should fail and the user should be    * warned to use regular joins rather than map-side joins. Fatal errors are indicated by counters    * that are set at execution time. If the counter is non-zero, a fatal error occurred. The value    * of the counter indicates the error type.    *    * @return true if fatal errors happened during job execution, false otherwise.    */
specifier|public
name|boolean
name|checkFatalErrors
parameter_list|(
name|Counters
name|ctrs
parameter_list|,
name|StringBuilder
name|errMsg
parameter_list|)
block|{
name|Counters
operator|.
name|Counter
name|cntr
init|=
name|ctrs
operator|.
name|findCounter
argument_list|(
name|HiveConf
operator|.
name|getVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVECOUNTERGROUP
argument_list|)
argument_list|,
name|Operator
operator|.
name|HIVECOUNTERFATAL
argument_list|)
decl_stmt|;
return|return
name|cntr
operator|!=
literal|null
operator|&&
name|cntr
operator|.
name|getValue
argument_list|()
operator|>
literal|0
return|;
block|}
comment|/**    * Execute a query plan using Hadoop.    */
annotation|@
name|SuppressWarnings
argument_list|(
block|{
literal|"deprecation"
block|,
literal|"unchecked"
block|}
argument_list|)
annotation|@
name|Override
specifier|public
name|int
name|execute
parameter_list|(
name|DriverContext
name|driverContext
parameter_list|)
block|{
name|IOPrepareCache
name|ioPrepareCache
init|=
name|IOPrepareCache
operator|.
name|get
argument_list|()
decl_stmt|;
name|ioPrepareCache
operator|.
name|clear
argument_list|()
expr_stmt|;
name|boolean
name|success
init|=
literal|true
decl_stmt|;
name|Context
name|ctx
init|=
name|driverContext
operator|.
name|getCtx
argument_list|()
decl_stmt|;
name|boolean
name|ctxCreated
init|=
literal|false
decl_stmt|;
name|String
name|emptyScratchDirStr
decl_stmt|;
name|Path
name|emptyScratchDir
decl_stmt|;
name|MapWork
name|mWork
init|=
name|work
operator|.
name|getMapWork
argument_list|()
decl_stmt|;
name|ReduceWork
name|rWork
init|=
name|work
operator|.
name|getReduceWork
argument_list|()
decl_stmt|;
try|try
block|{
if|if
condition|(
name|ctx
operator|==
literal|null
condition|)
block|{
name|ctx
operator|=
operator|new
name|Context
argument_list|(
name|job
argument_list|)
expr_stmt|;
name|ctxCreated
operator|=
literal|true
expr_stmt|;
block|}
name|emptyScratchDirStr
operator|=
name|ctx
operator|.
name|getMRTmpFileURI
argument_list|()
expr_stmt|;
name|emptyScratchDir
operator|=
operator|new
name|Path
argument_list|(
name|emptyScratchDirStr
argument_list|)
expr_stmt|;
name|FileSystem
name|fs
init|=
name|emptyScratchDir
operator|.
name|getFileSystem
argument_list|(
name|job
argument_list|)
decl_stmt|;
name|fs
operator|.
name|mkdirs
argument_list|(
name|emptyScratchDir
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|e
operator|.
name|printStackTrace
argument_list|()
expr_stmt|;
name|console
operator|.
name|printError
argument_list|(
literal|"Error launching map-reduce job"
argument_list|,
literal|"\n"
operator|+
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
return|return
literal|5
return|;
block|}
name|ShimLoader
operator|.
name|getHadoopShims
argument_list|()
operator|.
name|prepareJobOutput
argument_list|(
name|job
argument_list|)
expr_stmt|;
comment|//See the javadoc on HiveOutputFormatImpl and HadoopShims.prepareJobOutput()
name|job
operator|.
name|setOutputFormat
argument_list|(
name|HiveOutputFormatImpl
operator|.
name|class
argument_list|)
expr_stmt|;
name|job
operator|.
name|setMapperClass
argument_list|(
name|ExecMapper
operator|.
name|class
argument_list|)
expr_stmt|;
name|job
operator|.
name|setMapOutputKeyClass
argument_list|(
name|HiveKey
operator|.
name|class
argument_list|)
expr_stmt|;
name|job
operator|.
name|setMapOutputValueClass
argument_list|(
name|BytesWritable
operator|.
name|class
argument_list|)
expr_stmt|;
try|try
block|{
name|job
operator|.
name|setPartitionerClass
argument_list|(
call|(
name|Class
argument_list|<
name|?
extends|extends
name|Partitioner
argument_list|>
call|)
argument_list|(
name|Class
operator|.
name|forName
argument_list|(
name|HiveConf
operator|.
name|getVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEPARTITIONER
argument_list|)
argument_list|)
argument_list|)
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|ClassNotFoundException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
name|e
operator|.
name|getMessage
argument_list|()
argument_list|)
throw|;
block|}
if|if
condition|(
name|mWork
operator|.
name|getNumMapTasks
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|job
operator|.
name|setNumMapTasks
argument_list|(
name|mWork
operator|.
name|getNumMapTasks
argument_list|()
operator|.
name|intValue
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|mWork
operator|.
name|getMaxSplitSize
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|HiveConf
operator|.
name|setLongVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|MAPREDMAXSPLITSIZE
argument_list|,
name|mWork
operator|.
name|getMaxSplitSize
argument_list|()
operator|.
name|longValue
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|mWork
operator|.
name|getMinSplitSize
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|HiveConf
operator|.
name|setLongVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|MAPREDMINSPLITSIZE
argument_list|,
name|mWork
operator|.
name|getMinSplitSize
argument_list|()
operator|.
name|longValue
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|mWork
operator|.
name|getMinSplitSizePerNode
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|HiveConf
operator|.
name|setLongVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|MAPREDMINSPLITSIZEPERNODE
argument_list|,
name|mWork
operator|.
name|getMinSplitSizePerNode
argument_list|()
operator|.
name|longValue
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|mWork
operator|.
name|getMinSplitSizePerRack
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|HiveConf
operator|.
name|setLongVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|MAPREDMINSPLITSIZEPERRACK
argument_list|,
name|mWork
operator|.
name|getMinSplitSizePerRack
argument_list|()
operator|.
name|longValue
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|job
operator|.
name|setNumReduceTasks
argument_list|(
name|rWork
operator|!=
literal|null
condition|?
name|rWork
operator|.
name|getNumReduceTasks
argument_list|()
operator|.
name|intValue
argument_list|()
else|:
literal|0
argument_list|)
expr_stmt|;
name|job
operator|.
name|setReducerClass
argument_list|(
name|ExecReducer
operator|.
name|class
argument_list|)
expr_stmt|;
comment|// set input format information if necessary
name|setInputAttributes
argument_list|(
name|job
argument_list|)
expr_stmt|;
comment|// Turn on speculative execution for reducers
name|boolean
name|useSpeculativeExecReducers
init|=
name|HiveConf
operator|.
name|getBoolVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVESPECULATIVEEXECREDUCERS
argument_list|)
decl_stmt|;
name|HiveConf
operator|.
name|setBoolVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HADOOPSPECULATIVEEXECREDUCERS
argument_list|,
name|useSpeculativeExecReducers
argument_list|)
expr_stmt|;
name|String
name|inpFormat
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEINPUTFORMAT
argument_list|)
decl_stmt|;
if|if
condition|(
operator|(
name|inpFormat
operator|==
literal|null
operator|)
operator|||
operator|(
operator|!
name|StringUtils
operator|.
name|isNotBlank
argument_list|(
name|inpFormat
argument_list|)
operator|)
condition|)
block|{
name|inpFormat
operator|=
name|ShimLoader
operator|.
name|getHadoopShims
argument_list|()
operator|.
name|getInputFormatClassName
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
name|mWork
operator|.
name|isUseBucketizedHiveInputFormat
argument_list|()
condition|)
block|{
name|inpFormat
operator|=
name|BucketizedHiveInputFormat
operator|.
name|class
operator|.
name|getName
argument_list|()
expr_stmt|;
block|}
name|LOG
operator|.
name|info
argument_list|(
literal|"Using "
operator|+
name|inpFormat
argument_list|)
expr_stmt|;
try|try
block|{
name|job
operator|.
name|setInputFormat
argument_list|(
call|(
name|Class
argument_list|<
name|?
extends|extends
name|InputFormat
argument_list|>
call|)
argument_list|(
name|Class
operator|.
name|forName
argument_list|(
name|inpFormat
argument_list|)
argument_list|)
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|ClassNotFoundException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
name|e
operator|.
name|getMessage
argument_list|()
argument_list|)
throw|;
block|}
comment|// No-Op - we don't really write anything here ..
name|job
operator|.
name|setOutputKeyClass
argument_list|(
name|Text
operator|.
name|class
argument_list|)
expr_stmt|;
name|job
operator|.
name|setOutputValueClass
argument_list|(
name|Text
operator|.
name|class
argument_list|)
expr_stmt|;
comment|// Transfer HIVEAUXJARS and HIVEADDEDJARS to "tmpjars" so hadoop understands
comment|// it
name|String
name|auxJars
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEAUXJARS
argument_list|)
decl_stmt|;
name|String
name|addedJars
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEADDEDJARS
argument_list|)
decl_stmt|;
if|if
condition|(
name|StringUtils
operator|.
name|isNotBlank
argument_list|(
name|auxJars
argument_list|)
operator|||
name|StringUtils
operator|.
name|isNotBlank
argument_list|(
name|addedJars
argument_list|)
condition|)
block|{
name|String
name|allJars
init|=
name|StringUtils
operator|.
name|isNotBlank
argument_list|(
name|auxJars
argument_list|)
condition|?
operator|(
name|StringUtils
operator|.
name|isNotBlank
argument_list|(
name|addedJars
argument_list|)
condition|?
name|addedJars
operator|+
literal|","
operator|+
name|auxJars
else|:
name|auxJars
operator|)
else|:
name|addedJars
decl_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"adding libjars: "
operator|+
name|allJars
argument_list|)
expr_stmt|;
name|initializeFiles
argument_list|(
literal|"tmpjars"
argument_list|,
name|allJars
argument_list|)
expr_stmt|;
block|}
comment|// Transfer HIVEADDEDFILES to "tmpfiles" so hadoop understands it
name|String
name|addedFiles
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEADDEDFILES
argument_list|)
decl_stmt|;
if|if
condition|(
name|StringUtils
operator|.
name|isNotBlank
argument_list|(
name|addedFiles
argument_list|)
condition|)
block|{
name|initializeFiles
argument_list|(
literal|"tmpfiles"
argument_list|,
name|addedFiles
argument_list|)
expr_stmt|;
block|}
name|int
name|returnVal
init|=
literal|0
decl_stmt|;
name|boolean
name|noName
init|=
name|StringUtils
operator|.
name|isEmpty
argument_list|(
name|HiveConf
operator|.
name|getVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HADOOPJOBNAME
argument_list|)
argument_list|)
decl_stmt|;
if|if
condition|(
name|noName
condition|)
block|{
comment|// This is for a special case to ensure unit tests pass
name|HiveConf
operator|.
name|setVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HADOOPJOBNAME
argument_list|,
literal|"JOB"
operator|+
name|Utilities
operator|.
name|randGen
operator|.
name|nextInt
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|String
name|addedArchives
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEADDEDARCHIVES
argument_list|)
decl_stmt|;
comment|// Transfer HIVEADDEDARCHIVES to "tmparchives" so hadoop understands it
if|if
condition|(
name|StringUtils
operator|.
name|isNotBlank
argument_list|(
name|addedArchives
argument_list|)
condition|)
block|{
name|initializeFiles
argument_list|(
literal|"tmparchives"
argument_list|,
name|addedArchives
argument_list|)
expr_stmt|;
block|}
try|try
block|{
name|MapredLocalWork
name|localwork
init|=
name|mWork
operator|.
name|getMapLocalWork
argument_list|()
decl_stmt|;
if|if
condition|(
name|localwork
operator|!=
literal|null
condition|)
block|{
if|if
condition|(
operator|!
name|ShimLoader
operator|.
name|getHadoopShims
argument_list|()
operator|.
name|isLocalMode
argument_list|(
name|job
argument_list|)
condition|)
block|{
name|Path
name|localPath
init|=
operator|new
name|Path
argument_list|(
name|localwork
operator|.
name|getTmpFileURI
argument_list|()
argument_list|)
decl_stmt|;
name|Path
name|hdfsPath
init|=
operator|new
name|Path
argument_list|(
name|mWork
operator|.
name|getTmpHDFSFileURI
argument_list|()
argument_list|)
decl_stmt|;
name|FileSystem
name|hdfs
init|=
name|hdfsPath
operator|.
name|getFileSystem
argument_list|(
name|job
argument_list|)
decl_stmt|;
name|FileSystem
name|localFS
init|=
name|localPath
operator|.
name|getFileSystem
argument_list|(
name|job
argument_list|)
decl_stmt|;
name|FileStatus
index|[]
name|hashtableFiles
init|=
name|localFS
operator|.
name|listStatus
argument_list|(
name|localPath
argument_list|)
decl_stmt|;
name|int
name|fileNumber
init|=
name|hashtableFiles
operator|.
name|length
decl_stmt|;
name|String
index|[]
name|fileNames
init|=
operator|new
name|String
index|[
name|fileNumber
index|]
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|fileNumber
condition|;
name|i
operator|++
control|)
block|{
name|fileNames
index|[
name|i
index|]
operator|=
name|hashtableFiles
index|[
name|i
index|]
operator|.
name|getPath
argument_list|()
operator|.
name|getName
argument_list|()
expr_stmt|;
block|}
comment|//package and compress all the hashtable files to an archive file
name|String
name|parentDir
init|=
name|localPath
operator|.
name|toUri
argument_list|()
operator|.
name|getPath
argument_list|()
decl_stmt|;
name|String
name|stageId
init|=
name|this
operator|.
name|getId
argument_list|()
decl_stmt|;
name|String
name|archiveFileURI
init|=
name|Utilities
operator|.
name|generateTarURI
argument_list|(
name|parentDir
argument_list|,
name|stageId
argument_list|)
decl_stmt|;
name|String
name|archiveFileName
init|=
name|Utilities
operator|.
name|generateTarFileName
argument_list|(
name|stageId
argument_list|)
decl_stmt|;
name|localwork
operator|.
name|setStageID
argument_list|(
name|stageId
argument_list|)
expr_stmt|;
name|CompressionUtils
operator|.
name|tar
argument_list|(
name|parentDir
argument_list|,
name|fileNames
argument_list|,
name|archiveFileName
argument_list|)
expr_stmt|;
name|Path
name|archivePath
init|=
operator|new
name|Path
argument_list|(
name|archiveFileURI
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Archive "
operator|+
name|hashtableFiles
operator|.
name|length
operator|+
literal|" hash table files to "
operator|+
name|archiveFileURI
argument_list|)
expr_stmt|;
comment|//upload archive file to hdfs
name|String
name|hdfsFile
init|=
name|Utilities
operator|.
name|generateTarURI
argument_list|(
name|hdfsPath
argument_list|,
name|stageId
argument_list|)
decl_stmt|;
name|Path
name|hdfsFilePath
init|=
operator|new
name|Path
argument_list|(
name|hdfsFile
argument_list|)
decl_stmt|;
name|short
name|replication
init|=
operator|(
name|short
operator|)
name|job
operator|.
name|getInt
argument_list|(
literal|"mapred.submit.replication"
argument_list|,
literal|10
argument_list|)
decl_stmt|;
name|hdfs
operator|.
name|setReplication
argument_list|(
name|hdfsFilePath
argument_list|,
name|replication
argument_list|)
expr_stmt|;
name|hdfs
operator|.
name|copyFromLocalFile
argument_list|(
name|archivePath
argument_list|,
name|hdfsFilePath
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Upload 1 archive file  from"
operator|+
name|archivePath
operator|+
literal|" to: "
operator|+
name|hdfsFilePath
argument_list|)
expr_stmt|;
comment|//add the archive file to distributed cache
name|DistributedCache
operator|.
name|createSymlink
argument_list|(
name|job
argument_list|)
expr_stmt|;
name|DistributedCache
operator|.
name|addCacheArchive
argument_list|(
name|hdfsFilePath
operator|.
name|toUri
argument_list|()
argument_list|,
name|job
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Add 1 archive file to distributed cache. Archive file: "
operator|+
name|hdfsFilePath
operator|.
name|toUri
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
name|work
operator|.
name|configureJobConf
argument_list|(
name|job
argument_list|)
expr_stmt|;
name|List
argument_list|<
name|Path
argument_list|>
name|inputPaths
init|=
name|Utilities
operator|.
name|getInputPaths
argument_list|(
name|job
argument_list|,
name|mWork
argument_list|,
name|emptyScratchDirStr
argument_list|,
name|ctx
argument_list|)
decl_stmt|;
name|Utilities
operator|.
name|setInputPaths
argument_list|(
name|job
argument_list|,
name|inputPaths
argument_list|)
expr_stmt|;
name|Utilities
operator|.
name|setMapRedWork
argument_list|(
name|job
argument_list|,
name|work
argument_list|,
name|ctx
operator|.
name|getMRTmpFileURI
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|mWork
operator|.
name|getSamplingType
argument_list|()
operator|>
literal|0
operator|&&
name|rWork
operator|!=
literal|null
operator|&&
name|rWork
operator|.
name|getNumReduceTasks
argument_list|()
operator|>
literal|1
condition|)
block|{
try|try
block|{
name|handleSampling
argument_list|(
name|driverContext
argument_list|,
name|mWork
argument_list|,
name|job
argument_list|,
name|conf
argument_list|)
expr_stmt|;
name|job
operator|.
name|setPartitionerClass
argument_list|(
name|HiveTotalOrderPartitioner
operator|.
name|class
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IllegalStateException
name|e
parameter_list|)
block|{
name|console
operator|.
name|printInfo
argument_list|(
literal|"Not enough sampling data.. Rolling back to single reducer task"
argument_list|)
expr_stmt|;
name|rWork
operator|.
name|setNumReduceTasks
argument_list|(
literal|1
argument_list|)
expr_stmt|;
name|job
operator|.
name|setNumReduceTasks
argument_list|(
literal|1
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Sampling error"
argument_list|,
name|e
argument_list|)
expr_stmt|;
name|console
operator|.
name|printError
argument_list|(
name|e
operator|.
name|toString
argument_list|()
argument_list|,
literal|"\n"
operator|+
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
name|rWork
operator|.
name|setNumReduceTasks
argument_list|(
literal|1
argument_list|)
expr_stmt|;
name|job
operator|.
name|setNumReduceTasks
argument_list|(
literal|1
argument_list|)
expr_stmt|;
block|}
block|}
comment|// remove the pwd from conf file so that job tracker doesn't show this
comment|// logs
name|String
name|pwd
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|METASTOREPWD
argument_list|)
decl_stmt|;
if|if
condition|(
name|pwd
operator|!=
literal|null
condition|)
block|{
name|HiveConf
operator|.
name|setVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|METASTOREPWD
argument_list|,
literal|"HIVE"
argument_list|)
expr_stmt|;
block|}
name|JobClient
name|jc
init|=
operator|new
name|JobClient
argument_list|(
name|job
argument_list|)
decl_stmt|;
comment|// make this client wait if job trcker is not behaving well.
name|Throttle
operator|.
name|checkJobTracker
argument_list|(
name|job
argument_list|,
name|LOG
argument_list|)
expr_stmt|;
if|if
condition|(
name|mWork
operator|.
name|isGatheringStats
argument_list|()
operator|||
operator|(
name|rWork
operator|!=
literal|null
operator|&&
name|rWork
operator|.
name|isGatheringStats
argument_list|()
operator|)
condition|)
block|{
comment|// initialize stats publishing table
name|StatsPublisher
name|statsPublisher
decl_stmt|;
name|StatsFactory
name|factory
init|=
name|StatsFactory
operator|.
name|newFactory
argument_list|(
name|job
argument_list|)
decl_stmt|;
if|if
condition|(
name|factory
operator|!=
literal|null
condition|)
block|{
name|statsPublisher
operator|=
name|factory
operator|.
name|getStatsPublisher
argument_list|()
expr_stmt|;
if|if
condition|(
operator|!
name|statsPublisher
operator|.
name|init
argument_list|(
name|job
argument_list|)
condition|)
block|{
comment|// creating stats table if not exists
if|if
condition|(
name|HiveConf
operator|.
name|getBoolVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_STATS_RELIABLE
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|ErrorMsg
operator|.
name|STATSPUBLISHER_INITIALIZATION_ERROR
operator|.
name|getErrorCodedMsg
argument_list|()
argument_list|)
throw|;
block|}
block|}
block|}
block|}
name|Utilities
operator|.
name|createTmpDirs
argument_list|(
name|job
argument_list|,
name|mWork
argument_list|)
expr_stmt|;
name|Utilities
operator|.
name|createTmpDirs
argument_list|(
name|job
argument_list|,
name|rWork
argument_list|)
expr_stmt|;
comment|// Finally SUBMIT the JOB!
name|rj
operator|=
name|jc
operator|.
name|submitJob
argument_list|(
name|job
argument_list|)
expr_stmt|;
comment|// replace it back
if|if
condition|(
name|pwd
operator|!=
literal|null
condition|)
block|{
name|HiveConf
operator|.
name|setVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|METASTOREPWD
argument_list|,
name|pwd
argument_list|)
expr_stmt|;
block|}
name|returnVal
operator|=
name|jobExecHelper
operator|.
name|progress
argument_list|(
name|rj
argument_list|,
name|jc
argument_list|)
expr_stmt|;
name|success
operator|=
operator|(
name|returnVal
operator|==
literal|0
operator|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|e
operator|.
name|printStackTrace
argument_list|()
expr_stmt|;
name|String
name|mesg
init|=
literal|" with exception '"
operator|+
name|Utilities
operator|.
name|getNameMessage
argument_list|(
name|e
argument_list|)
operator|+
literal|"'"
decl_stmt|;
if|if
condition|(
name|rj
operator|!=
literal|null
condition|)
block|{
name|mesg
operator|=
literal|"Ended Job = "
operator|+
name|rj
operator|.
name|getJobID
argument_list|()
operator|+
name|mesg
expr_stmt|;
block|}
else|else
block|{
name|mesg
operator|=
literal|"Job Submission failed"
operator|+
name|mesg
expr_stmt|;
block|}
comment|// Has to use full name to make sure it does not conflict with
comment|// org.apache.commons.lang.StringUtils
name|console
operator|.
name|printError
argument_list|(
name|mesg
argument_list|,
literal|"\n"
operator|+
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
name|success
operator|=
literal|false
expr_stmt|;
name|returnVal
operator|=
literal|1
expr_stmt|;
block|}
finally|finally
block|{
name|Utilities
operator|.
name|clearWork
argument_list|(
name|job
argument_list|)
expr_stmt|;
try|try
block|{
if|if
condition|(
name|ctxCreated
condition|)
block|{
name|ctx
operator|.
name|clear
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
name|rj
operator|!=
literal|null
condition|)
block|{
if|if
condition|(
name|returnVal
operator|!=
literal|0
condition|)
block|{
name|rj
operator|.
name|killJob
argument_list|()
expr_stmt|;
block|}
name|HadoopJobExecHelper
operator|.
name|runningJobKillURIs
operator|.
name|remove
argument_list|(
name|rj
operator|.
name|getJobID
argument_list|()
argument_list|)
expr_stmt|;
name|jobID
operator|=
name|rj
operator|.
name|getID
argument_list|()
operator|.
name|toString
argument_list|()
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{       }
block|}
comment|// get the list of Dynamic partition paths
try|try
block|{
if|if
condition|(
name|rj
operator|!=
literal|null
condition|)
block|{
if|if
condition|(
name|mWork
operator|.
name|getAliasToWork
argument_list|()
operator|!=
literal|null
condition|)
block|{
for|for
control|(
name|Operator
argument_list|<
name|?
extends|extends
name|OperatorDesc
argument_list|>
name|op
range|:
name|mWork
operator|.
name|getAliasToWork
argument_list|()
operator|.
name|values
argument_list|()
control|)
block|{
name|op
operator|.
name|jobClose
argument_list|(
name|job
argument_list|,
name|success
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
name|rWork
operator|!=
literal|null
condition|)
block|{
name|rWork
operator|.
name|getReducer
argument_list|()
operator|.
name|jobClose
argument_list|(
name|job
argument_list|,
name|success
argument_list|)
expr_stmt|;
block|}
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
comment|// jobClose needs to execute successfully otherwise fail task
if|if
condition|(
name|success
condition|)
block|{
name|success
operator|=
literal|false
expr_stmt|;
name|returnVal
operator|=
literal|3
expr_stmt|;
name|String
name|mesg
init|=
literal|"Job Commit failed with exception '"
operator|+
name|Utilities
operator|.
name|getNameMessage
argument_list|(
name|e
argument_list|)
operator|+
literal|"'"
decl_stmt|;
name|console
operator|.
name|printError
argument_list|(
name|mesg
argument_list|,
literal|"\n"
operator|+
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
return|return
operator|(
name|returnVal
operator|)
return|;
block|}
specifier|private
name|void
name|handleSampling
parameter_list|(
name|DriverContext
name|context
parameter_list|,
name|MapWork
name|mWork
parameter_list|,
name|JobConf
name|job
parameter_list|,
name|HiveConf
name|conf
parameter_list|)
throws|throws
name|Exception
block|{
assert|assert
name|mWork
operator|.
name|getAliasToWork
argument_list|()
operator|.
name|keySet
argument_list|()
operator|.
name|size
argument_list|()
operator|==
literal|1
assert|;
name|String
name|alias
init|=
name|mWork
operator|.
name|getAliases
argument_list|()
operator|.
name|get
argument_list|(
literal|0
argument_list|)
decl_stmt|;
name|Operator
argument_list|<
name|?
argument_list|>
name|topOp
init|=
name|mWork
operator|.
name|getAliasToWork
argument_list|()
operator|.
name|get
argument_list|(
name|alias
argument_list|)
decl_stmt|;
name|PartitionDesc
name|partDesc
init|=
name|mWork
operator|.
name|getAliasToPartnInfo
argument_list|()
operator|.
name|get
argument_list|(
name|alias
argument_list|)
decl_stmt|;
name|ArrayList
argument_list|<
name|String
argument_list|>
name|paths
init|=
name|mWork
operator|.
name|getPaths
argument_list|()
decl_stmt|;
name|ArrayList
argument_list|<
name|PartitionDesc
argument_list|>
name|parts
init|=
name|mWork
operator|.
name|getPartitionDescs
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|Path
argument_list|>
name|inputPaths
init|=
operator|new
name|ArrayList
argument_list|<
name|Path
argument_list|>
argument_list|(
name|paths
operator|.
name|size
argument_list|()
argument_list|)
decl_stmt|;
for|for
control|(
name|String
name|path
range|:
name|paths
control|)
block|{
name|inputPaths
operator|.
name|add
argument_list|(
operator|new
name|Path
argument_list|(
name|path
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|Path
name|tmpPath
init|=
name|context
operator|.
name|getCtx
argument_list|()
operator|.
name|getExternalTmpPath
argument_list|(
name|inputPaths
operator|.
name|get
argument_list|(
literal|0
argument_list|)
operator|.
name|toUri
argument_list|()
argument_list|)
decl_stmt|;
name|Path
name|partitionFile
init|=
operator|new
name|Path
argument_list|(
name|tmpPath
argument_list|,
literal|".partitions"
argument_list|)
decl_stmt|;
name|ShimLoader
operator|.
name|getHadoopShims
argument_list|()
operator|.
name|setTotalOrderPartitionFile
argument_list|(
name|job
argument_list|,
name|partitionFile
argument_list|)
expr_stmt|;
name|PartitionKeySampler
name|sampler
init|=
operator|new
name|PartitionKeySampler
argument_list|()
decl_stmt|;
if|if
condition|(
name|mWork
operator|.
name|getSamplingType
argument_list|()
operator|==
name|MapWork
operator|.
name|SAMPLING_ON_PREV_MR
condition|)
block|{
name|console
operator|.
name|printInfo
argument_list|(
literal|"Use sampling data created in previous MR"
argument_list|)
expr_stmt|;
comment|// merges sampling data from previous MR and make paritition keys for total sort
for|for
control|(
name|Path
name|path
range|:
name|inputPaths
control|)
block|{
name|FileSystem
name|fs
init|=
name|path
operator|.
name|getFileSystem
argument_list|(
name|job
argument_list|)
decl_stmt|;
for|for
control|(
name|FileStatus
name|status
range|:
name|fs
operator|.
name|globStatus
argument_list|(
operator|new
name|Path
argument_list|(
name|path
argument_list|,
literal|".sampling*"
argument_list|)
argument_list|)
control|)
block|{
name|sampler
operator|.
name|addSampleFile
argument_list|(
name|status
operator|.
name|getPath
argument_list|()
argument_list|,
name|job
argument_list|)
expr_stmt|;
block|}
block|}
block|}
elseif|else
if|if
condition|(
name|mWork
operator|.
name|getSamplingType
argument_list|()
operator|==
name|MapWork
operator|.
name|SAMPLING_ON_START
condition|)
block|{
name|console
operator|.
name|printInfo
argument_list|(
literal|"Creating sampling data.."
argument_list|)
expr_stmt|;
assert|assert
name|topOp
operator|instanceof
name|TableScanOperator
assert|;
name|TableScanOperator
name|ts
init|=
operator|(
name|TableScanOperator
operator|)
name|topOp
decl_stmt|;
name|FetchWork
name|fetchWork
decl_stmt|;
if|if
condition|(
operator|!
name|partDesc
operator|.
name|isPartitioned
argument_list|()
condition|)
block|{
assert|assert
name|paths
operator|.
name|size
argument_list|()
operator|==
literal|1
assert|;
name|fetchWork
operator|=
operator|new
name|FetchWork
argument_list|(
name|inputPaths
operator|.
name|get
argument_list|(
literal|0
argument_list|)
argument_list|,
name|partDesc
operator|.
name|getTableDesc
argument_list|()
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|fetchWork
operator|=
operator|new
name|FetchWork
argument_list|(
name|inputPaths
argument_list|,
name|parts
argument_list|,
name|partDesc
operator|.
name|getTableDesc
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|fetchWork
operator|.
name|setSource
argument_list|(
name|ts
argument_list|)
expr_stmt|;
comment|// random sampling
name|FetchOperator
name|fetcher
init|=
name|PartitionKeySampler
operator|.
name|createSampler
argument_list|(
name|fetchWork
argument_list|,
name|conf
argument_list|,
name|job
argument_list|,
name|ts
argument_list|)
decl_stmt|;
try|try
block|{
name|ts
operator|.
name|initialize
argument_list|(
name|conf
argument_list|,
operator|new
name|ObjectInspector
index|[]
block|{
name|fetcher
operator|.
name|getOutputObjectInspector
argument_list|()
block|}
argument_list|)
expr_stmt|;
name|OperatorUtils
operator|.
name|setChildrenCollector
argument_list|(
name|ts
operator|.
name|getChildOperators
argument_list|()
argument_list|,
name|sampler
argument_list|)
expr_stmt|;
while|while
condition|(
name|fetcher
operator|.
name|pushRow
argument_list|()
condition|)
block|{ }
block|}
finally|finally
block|{
name|fetcher
operator|.
name|clearFetchContext
argument_list|()
expr_stmt|;
block|}
block|}
else|else
block|{
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
literal|"Invalid sampling type "
operator|+
name|mWork
operator|.
name|getSamplingType
argument_list|()
argument_list|)
throw|;
block|}
name|sampler
operator|.
name|writePartitionKeys
argument_list|(
name|partitionFile
argument_list|,
name|job
argument_list|)
expr_stmt|;
block|}
comment|/**    * Set hive input format, and input format file if necessary.    */
specifier|protected
name|void
name|setInputAttributes
parameter_list|(
name|Configuration
name|conf
parameter_list|)
block|{
name|MapWork
name|mWork
init|=
name|work
operator|.
name|getMapWork
argument_list|()
decl_stmt|;
if|if
condition|(
name|mWork
operator|.
name|getInputformat
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|HiveConf
operator|.
name|setVar
argument_list|(
name|conf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEINPUTFORMAT
argument_list|,
name|mWork
operator|.
name|getInputformat
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|mWork
operator|.
name|getIndexIntermediateFile
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|conf
operator|.
name|set
argument_list|(
literal|"hive.index.compact.file"
argument_list|,
name|mWork
operator|.
name|getIndexIntermediateFile
argument_list|()
argument_list|)
expr_stmt|;
name|conf
operator|.
name|set
argument_list|(
literal|"hive.index.blockfilter.file"
argument_list|,
name|mWork
operator|.
name|getIndexIntermediateFile
argument_list|()
argument_list|)
expr_stmt|;
block|}
comment|// Intentionally overwrites anything the user may have put here
name|conf
operator|.
name|setBoolean
argument_list|(
literal|"hive.input.format.sorted"
argument_list|,
name|mWork
operator|.
name|isInputFormatSorted
argument_list|()
argument_list|)
expr_stmt|;
block|}
specifier|public
name|boolean
name|mapStarted
parameter_list|()
block|{
return|return
name|this
operator|.
name|jobExecHelper
operator|.
name|mapStarted
argument_list|()
return|;
block|}
specifier|public
name|boolean
name|reduceStarted
parameter_list|()
block|{
return|return
name|this
operator|.
name|jobExecHelper
operator|.
name|reduceStarted
argument_list|()
return|;
block|}
specifier|public
name|boolean
name|mapDone
parameter_list|()
block|{
return|return
name|this
operator|.
name|jobExecHelper
operator|.
name|mapDone
argument_list|()
return|;
block|}
specifier|public
name|boolean
name|reduceDone
parameter_list|()
block|{
return|return
name|this
operator|.
name|jobExecHelper
operator|.
name|reduceDone
argument_list|()
return|;
block|}
specifier|private
specifier|static
name|void
name|printUsage
parameter_list|()
block|{
name|System
operator|.
name|err
operator|.
name|println
argument_list|(
literal|"ExecDriver -plan<plan-file> [-jobconffile<job conf file>]"
operator|+
literal|"[-files<file1>[,<file2>] ...]"
argument_list|)
expr_stmt|;
name|System
operator|.
name|exit
argument_list|(
literal|1
argument_list|)
expr_stmt|;
block|}
comment|/**    * we are running the hadoop job via a sub-command. this typically happens when we are running    * jobs in local mode. the log4j in this mode is controlled as follows: 1. if the admin provides a    * log4j properties file especially for execution mode - then we pick that up 2. otherwise - we    * default to the regular hive log4j properties if one is supplied 3. if none of the above two    * apply - we don't do anything - the log4j properties would likely be determined by hadoop.    *    * The intention behind providing a separate option #1 is to be able to collect hive run time logs    * generated in local mode in a separate (centralized) location if desired. This mimics the    * behavior of hive run time logs when running against a hadoop cluster where they are available    * on the tasktracker nodes.    */
specifier|private
specifier|static
name|void
name|setupChildLog4j
parameter_list|(
name|Configuration
name|conf
parameter_list|)
block|{
try|try
block|{
name|LogUtils
operator|.
name|initHiveExecLog4j
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|LogInitializationException
name|e
parameter_list|)
block|{
name|System
operator|.
name|err
operator|.
name|println
argument_list|(
name|e
operator|.
name|getMessage
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
annotation|@
name|SuppressWarnings
argument_list|(
literal|"unchecked"
argument_list|)
specifier|public
specifier|static
name|void
name|main
parameter_list|(
name|String
index|[]
name|args
parameter_list|)
throws|throws
name|IOException
throws|,
name|HiveException
block|{
name|String
name|planFileName
init|=
literal|null
decl_stmt|;
name|String
name|jobConfFileName
init|=
literal|null
decl_stmt|;
name|boolean
name|noLog
init|=
literal|false
decl_stmt|;
name|String
name|files
init|=
literal|null
decl_stmt|;
name|boolean
name|localtask
init|=
literal|false
decl_stmt|;
try|try
block|{
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|args
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
if|if
condition|(
name|args
index|[
name|i
index|]
operator|.
name|equals
argument_list|(
literal|"-plan"
argument_list|)
condition|)
block|{
name|planFileName
operator|=
name|args
index|[
operator|++
name|i
index|]
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|args
index|[
name|i
index|]
operator|.
name|equals
argument_list|(
literal|"-jobconffile"
argument_list|)
condition|)
block|{
name|jobConfFileName
operator|=
name|args
index|[
operator|++
name|i
index|]
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|args
index|[
name|i
index|]
operator|.
name|equals
argument_list|(
literal|"-nolog"
argument_list|)
condition|)
block|{
name|noLog
operator|=
literal|true
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|args
index|[
name|i
index|]
operator|.
name|equals
argument_list|(
literal|"-files"
argument_list|)
condition|)
block|{
name|files
operator|=
name|args
index|[
operator|++
name|i
index|]
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|args
index|[
name|i
index|]
operator|.
name|equals
argument_list|(
literal|"-localtask"
argument_list|)
condition|)
block|{
name|localtask
operator|=
literal|true
expr_stmt|;
block|}
block|}
block|}
catch|catch
parameter_list|(
name|IndexOutOfBoundsException
name|e
parameter_list|)
block|{
name|System
operator|.
name|err
operator|.
name|println
argument_list|(
literal|"Missing argument to option"
argument_list|)
expr_stmt|;
name|printUsage
argument_list|()
expr_stmt|;
block|}
name|JobConf
name|conf
decl_stmt|;
if|if
condition|(
name|localtask
condition|)
block|{
name|conf
operator|=
operator|new
name|JobConf
argument_list|(
name|MapredLocalTask
operator|.
name|class
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|conf
operator|=
operator|new
name|JobConf
argument_list|(
name|ExecDriver
operator|.
name|class
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|jobConfFileName
operator|!=
literal|null
condition|)
block|{
name|conf
operator|.
name|addResource
argument_list|(
operator|new
name|Path
argument_list|(
name|jobConfFileName
argument_list|)
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|files
operator|!=
literal|null
condition|)
block|{
name|conf
operator|.
name|set
argument_list|(
literal|"tmpfiles"
argument_list|,
name|files
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|ShimLoader
operator|.
name|getHadoopShims
argument_list|()
operator|.
name|isSecurityEnabled
argument_list|()
condition|)
block|{
name|String
name|hadoopAuthToken
init|=
name|System
operator|.
name|getenv
argument_list|(
name|ShimLoader
operator|.
name|getHadoopShims
argument_list|()
operator|.
name|getTokenFileLocEnvName
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|hadoopAuthToken
operator|!=
literal|null
condition|)
block|{
name|conf
operator|.
name|set
argument_list|(
literal|"mapreduce.job.credentials.binary"
argument_list|,
name|hadoopAuthToken
argument_list|)
expr_stmt|;
block|}
block|}
name|boolean
name|isSilent
init|=
name|HiveConf
operator|.
name|getBoolVar
argument_list|(
name|conf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVESESSIONSILENT
argument_list|)
decl_stmt|;
name|String
name|queryId
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|conf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEQUERYID
argument_list|,
literal|""
argument_list|)
operator|.
name|trim
argument_list|()
decl_stmt|;
if|if
condition|(
name|queryId
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|queryId
operator|=
literal|"unknown-"
operator|+
name|System
operator|.
name|currentTimeMillis
argument_list|()
expr_stmt|;
block|}
name|System
operator|.
name|setProperty
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEQUERYID
operator|.
name|toString
argument_list|()
argument_list|,
name|queryId
argument_list|)
expr_stmt|;
if|if
condition|(
name|noLog
condition|)
block|{
comment|// If started from main(), and noLog is on, we should not output
comment|// any logs. To turn the log on, please set -Dtest.silent=false
name|BasicConfigurator
operator|.
name|resetConfiguration
argument_list|()
expr_stmt|;
name|BasicConfigurator
operator|.
name|configure
argument_list|(
operator|new
name|NullAppender
argument_list|()
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|setupChildLog4j
argument_list|(
name|conf
argument_list|)
expr_stmt|;
block|}
name|Log
name|LOG
init|=
name|LogFactory
operator|.
name|getLog
argument_list|(
name|ExecDriver
operator|.
name|class
operator|.
name|getName
argument_list|()
argument_list|)
decl_stmt|;
name|LogHelper
name|console
init|=
operator|new
name|LogHelper
argument_list|(
name|LOG
argument_list|,
name|isSilent
argument_list|)
decl_stmt|;
if|if
condition|(
name|planFileName
operator|==
literal|null
condition|)
block|{
name|console
operator|.
name|printError
argument_list|(
literal|"Must specify Plan File Name"
argument_list|)
expr_stmt|;
name|printUsage
argument_list|()
expr_stmt|;
block|}
comment|// print out the location of the log file for the user so
comment|// that it's easy to find reason for local mode execution failures
for|for
control|(
name|Appender
name|appender
range|:
name|Collections
operator|.
name|list
argument_list|(
operator|(
name|Enumeration
argument_list|<
name|Appender
argument_list|>
operator|)
name|LogManager
operator|.
name|getRootLogger
argument_list|()
operator|.
name|getAllAppenders
argument_list|()
argument_list|)
control|)
block|{
if|if
condition|(
name|appender
operator|instanceof
name|FileAppender
condition|)
block|{
name|console
operator|.
name|printInfo
argument_list|(
literal|"Execution log at: "
operator|+
operator|(
operator|(
name|FileAppender
operator|)
name|appender
operator|)
operator|.
name|getFile
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
comment|// the plan file should always be in local directory
name|Path
name|p
init|=
operator|new
name|Path
argument_list|(
name|planFileName
argument_list|)
decl_stmt|;
name|FileSystem
name|fs
init|=
name|FileSystem
operator|.
name|getLocal
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|InputStream
name|pathData
init|=
name|fs
operator|.
name|open
argument_list|(
name|p
argument_list|)
decl_stmt|;
comment|// this is workaround for hadoop-17 - libjars are not added to classpath of the
comment|// child process. so we add it here explicitly
name|String
name|auxJars
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|conf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEAUXJARS
argument_list|)
decl_stmt|;
name|String
name|addedJars
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|conf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEADDEDJARS
argument_list|)
decl_stmt|;
try|try
block|{
comment|// see also - code in CliDriver.java
name|ClassLoader
name|loader
init|=
name|conf
operator|.
name|getClassLoader
argument_list|()
decl_stmt|;
if|if
condition|(
name|StringUtils
operator|.
name|isNotBlank
argument_list|(
name|auxJars
argument_list|)
condition|)
block|{
name|loader
operator|=
name|Utilities
operator|.
name|addToClassPath
argument_list|(
name|loader
argument_list|,
name|StringUtils
operator|.
name|split
argument_list|(
name|auxJars
argument_list|,
literal|","
argument_list|)
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|StringUtils
operator|.
name|isNotBlank
argument_list|(
name|addedJars
argument_list|)
condition|)
block|{
name|loader
operator|=
name|Utilities
operator|.
name|addToClassPath
argument_list|(
name|loader
argument_list|,
name|StringUtils
operator|.
name|split
argument_list|(
name|addedJars
argument_list|,
literal|","
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|conf
operator|.
name|setClassLoader
argument_list|(
name|loader
argument_list|)
expr_stmt|;
comment|// Also set this to the Thread ContextClassLoader, so new threads will
comment|// inherit
comment|// this class loader, and propagate into newly created Configurations by
comment|// those
comment|// new threads.
name|Thread
operator|.
name|currentThread
argument_list|()
operator|.
name|setContextClassLoader
argument_list|(
name|loader
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
name|int
name|ret
decl_stmt|;
if|if
condition|(
name|localtask
condition|)
block|{
name|memoryMXBean
operator|=
name|ManagementFactory
operator|.
name|getMemoryMXBean
argument_list|()
expr_stmt|;
name|MapredLocalWork
name|plan
init|=
name|Utilities
operator|.
name|deserializePlan
argument_list|(
name|pathData
argument_list|,
name|MapredLocalWork
operator|.
name|class
argument_list|,
name|conf
argument_list|)
decl_stmt|;
name|MapredLocalTask
name|ed
init|=
operator|new
name|MapredLocalTask
argument_list|(
name|plan
argument_list|,
name|conf
argument_list|,
name|isSilent
argument_list|)
decl_stmt|;
name|ret
operator|=
name|ed
operator|.
name|executeFromChildJVM
argument_list|(
operator|new
name|DriverContext
argument_list|()
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|MapredWork
name|plan
init|=
name|Utilities
operator|.
name|deserializePlan
argument_list|(
name|pathData
argument_list|,
name|MapredWork
operator|.
name|class
argument_list|,
name|conf
argument_list|)
decl_stmt|;
name|ExecDriver
name|ed
init|=
operator|new
name|ExecDriver
argument_list|(
name|plan
argument_list|,
name|conf
argument_list|,
name|isSilent
argument_list|)
decl_stmt|;
name|ret
operator|=
name|ed
operator|.
name|execute
argument_list|(
operator|new
name|DriverContext
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|ret
operator|!=
literal|0
condition|)
block|{
name|System
operator|.
name|exit
argument_list|(
name|ret
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * Given a Hive Configuration object - generate a command line fragment for passing such    * configuration information to ExecDriver.    */
specifier|public
specifier|static
name|String
name|generateCmdLine
parameter_list|(
name|HiveConf
name|hconf
parameter_list|,
name|Context
name|ctx
parameter_list|)
throws|throws
name|IOException
block|{
name|HiveConf
name|tempConf
init|=
operator|new
name|HiveConf
argument_list|()
decl_stmt|;
name|Path
name|hConfFilePath
init|=
operator|new
name|Path
argument_list|(
name|ctx
operator|.
name|getLocalTmpFileURI
argument_list|()
argument_list|,
name|JOBCONF_FILENAME
argument_list|)
decl_stmt|;
name|OutputStream
name|out
init|=
literal|null
decl_stmt|;
name|Properties
name|deltaP
init|=
name|hconf
operator|.
name|getChangedProperties
argument_list|()
decl_stmt|;
name|boolean
name|hadoopLocalMode
init|=
name|ShimLoader
operator|.
name|getHadoopShims
argument_list|()
operator|.
name|isLocalMode
argument_list|(
name|hconf
argument_list|)
decl_stmt|;
name|String
name|hadoopSysDir
init|=
literal|"mapred.system.dir"
decl_stmt|;
name|String
name|hadoopWorkDir
init|=
literal|"mapred.local.dir"
decl_stmt|;
for|for
control|(
name|Object
name|one
range|:
name|deltaP
operator|.
name|keySet
argument_list|()
control|)
block|{
name|String
name|oneProp
init|=
operator|(
name|String
operator|)
name|one
decl_stmt|;
if|if
condition|(
name|hadoopLocalMode
operator|&&
operator|(
name|oneProp
operator|.
name|equals
argument_list|(
name|hadoopSysDir
argument_list|)
operator|||
name|oneProp
operator|.
name|equals
argument_list|(
name|hadoopWorkDir
argument_list|)
operator|)
condition|)
block|{
continue|continue;
block|}
name|tempConf
operator|.
name|set
argument_list|(
name|oneProp
argument_list|,
name|hconf
operator|.
name|get
argument_list|(
name|oneProp
argument_list|)
argument_list|)
expr_stmt|;
block|}
comment|// Multiple concurrent local mode job submissions can cause collisions in
comment|// working dirs and system dirs
comment|// Workaround is to rename map red working dir to a temp dir in such cases
if|if
condition|(
name|hadoopLocalMode
condition|)
block|{
name|tempConf
operator|.
name|set
argument_list|(
name|hadoopSysDir
argument_list|,
name|hconf
operator|.
name|get
argument_list|(
name|hadoopSysDir
argument_list|)
operator|+
literal|"/"
operator|+
name|Utilities
operator|.
name|randGen
operator|.
name|nextInt
argument_list|()
argument_list|)
expr_stmt|;
name|tempConf
operator|.
name|set
argument_list|(
name|hadoopWorkDir
argument_list|,
name|hconf
operator|.
name|get
argument_list|(
name|hadoopWorkDir
argument_list|)
operator|+
literal|"/"
operator|+
name|Utilities
operator|.
name|randGen
operator|.
name|nextInt
argument_list|()
argument_list|)
expr_stmt|;
block|}
try|try
block|{
name|out
operator|=
name|FileSystem
operator|.
name|getLocal
argument_list|(
name|hconf
argument_list|)
operator|.
name|create
argument_list|(
name|hConfFilePath
argument_list|)
expr_stmt|;
name|tempConf
operator|.
name|writeXml
argument_list|(
name|out
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
if|if
condition|(
name|out
operator|!=
literal|null
condition|)
block|{
name|out
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
return|return
literal|" -jobconffile "
operator|+
name|hConfFilePath
operator|.
name|toString
argument_list|()
return|;
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|isMapRedTask
parameter_list|()
block|{
return|return
literal|true
return|;
block|}
annotation|@
name|Override
specifier|public
name|Collection
argument_list|<
name|Operator
argument_list|<
name|?
extends|extends
name|OperatorDesc
argument_list|>
argument_list|>
name|getTopOperators
parameter_list|()
block|{
return|return
name|getWork
argument_list|()
operator|.
name|getMapWork
argument_list|()
operator|.
name|getAliasToWork
argument_list|()
operator|.
name|values
argument_list|()
return|;
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|hasReduce
parameter_list|()
block|{
name|MapredWork
name|w
init|=
name|getWork
argument_list|()
decl_stmt|;
return|return
name|w
operator|.
name|getReduceWork
argument_list|()
operator|!=
literal|null
return|;
block|}
annotation|@
name|Override
specifier|public
name|StageType
name|getType
parameter_list|()
block|{
return|return
name|StageType
operator|.
name|MAPRED
return|;
block|}
annotation|@
name|Override
specifier|public
name|String
name|getName
parameter_list|()
block|{
return|return
literal|"MAPRED"
return|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|logPlanProgress
parameter_list|(
name|SessionState
name|ss
parameter_list|)
throws|throws
name|IOException
block|{
name|ss
operator|.
name|getHiveHistory
argument_list|()
operator|.
name|logPlanProgress
argument_list|(
name|queryPlan
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|shutdown
parameter_list|()
block|{
name|super
operator|.
name|shutdown
argument_list|()
expr_stmt|;
if|if
condition|(
name|rj
operator|!=
literal|null
condition|)
block|{
try|try
block|{
name|rj
operator|.
name|killJob
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"failed to kill job "
operator|+
name|rj
operator|.
name|getID
argument_list|()
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
name|rj
operator|=
literal|null
expr_stmt|;
block|}
block|}
block|}
end_class

end_unit

