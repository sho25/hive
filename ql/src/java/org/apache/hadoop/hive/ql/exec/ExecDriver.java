begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/**  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
package|;
end_package

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|File
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|InputStream
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|OutputStream
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|Serializable
import|;
end_import

begin_import
import|import
name|java
operator|.
name|lang
operator|.
name|management
operator|.
name|ManagementFactory
import|;
end_import

begin_import
import|import
name|java
operator|.
name|lang
operator|.
name|management
operator|.
name|MemoryMXBean
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Collection
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Collections
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Enumeration
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashSet
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|LinkedHashMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|LinkedList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Properties
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Set
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|lang
operator|.
name|StringUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|Log
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|LogFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configuration
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|filecache
operator|.
name|DistributedCache
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileStatus
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|CompressionUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|LogUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|LogUtils
operator|.
name|LogInitializationException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
operator|.
name|ConfVars
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|Context
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|DriverContext
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|ErrorMsg
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|QueryPlan
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|FileSinkOperator
operator|.
name|RecordWriter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|vector
operator|.
name|VectorExecMapper
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|BucketizedHiveInputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|HiveKey
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|HiveOutputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|HiveOutputFormatImpl
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|IOPrepareCache
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|OneNullRowInputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|HiveException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|FetchWork
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|FileSinkDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|MapredLocalWork
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|MapredWork
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|OperatorDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|PartitionDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|TableDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|api
operator|.
name|StageType
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|session
operator|.
name|SessionState
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|session
operator|.
name|SessionState
operator|.
name|LogHelper
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|stats
operator|.
name|StatsFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|stats
operator|.
name|StatsPublisher
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|shims
operator|.
name|ShimLoader
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|BytesWritable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|Text
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|Counters
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|FileInputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|InputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|JobClient
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|JobConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|Partitioner
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|RunningJob
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|log4j
operator|.
name|Appender
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|log4j
operator|.
name|BasicConfigurator
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|log4j
operator|.
name|FileAppender
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|log4j
operator|.
name|LogManager
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|log4j
operator|.
name|varia
operator|.
name|NullAppender
import|;
end_import

begin_comment
comment|/**  * ExecDriver.  *  */
end_comment

begin_class
specifier|public
class|class
name|ExecDriver
extends|extends
name|Task
argument_list|<
name|MapredWork
argument_list|>
implements|implements
name|Serializable
implements|,
name|HadoopJobExecHook
block|{
specifier|private
specifier|static
specifier|final
name|long
name|serialVersionUID
init|=
literal|1L
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|String
name|JOBCONF_FILENAME
init|=
literal|"jobconf.xml"
decl_stmt|;
specifier|protected
specifier|transient
name|JobConf
name|job
decl_stmt|;
specifier|public
specifier|static
name|MemoryMXBean
name|memoryMXBean
decl_stmt|;
specifier|protected
name|HadoopJobExecHelper
name|jobExecHelper
decl_stmt|;
specifier|protected
specifier|static
specifier|transient
specifier|final
name|Log
name|LOG
init|=
name|LogFactory
operator|.
name|getLog
argument_list|(
name|ExecDriver
operator|.
name|class
argument_list|)
decl_stmt|;
specifier|private
name|RunningJob
name|rj
decl_stmt|;
comment|/**    * Constructor when invoked from QL.    */
specifier|public
name|ExecDriver
parameter_list|()
block|{
name|super
argument_list|()
expr_stmt|;
name|console
operator|=
operator|new
name|LogHelper
argument_list|(
name|LOG
argument_list|)
expr_stmt|;
name|this
operator|.
name|jobExecHelper
operator|=
operator|new
name|HadoopJobExecHelper
argument_list|(
name|job
argument_list|,
name|console
argument_list|,
name|this
argument_list|,
name|this
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|requireLock
parameter_list|()
block|{
return|return
literal|true
return|;
block|}
specifier|private
name|void
name|initializeFiles
parameter_list|(
name|String
name|prop
parameter_list|,
name|String
name|files
parameter_list|)
block|{
if|if
condition|(
name|files
operator|!=
literal|null
operator|&&
name|files
operator|.
name|length
argument_list|()
operator|>
literal|0
condition|)
block|{
name|job
operator|.
name|set
argument_list|(
name|prop
argument_list|,
name|files
argument_list|)
expr_stmt|;
name|ShimLoader
operator|.
name|getHadoopShims
argument_list|()
operator|.
name|setTmpFiles
argument_list|(
name|prop
argument_list|,
name|files
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * Initialization when invoked from QL.    */
annotation|@
name|Override
specifier|public
name|void
name|initialize
parameter_list|(
name|HiveConf
name|conf
parameter_list|,
name|QueryPlan
name|queryPlan
parameter_list|,
name|DriverContext
name|driverContext
parameter_list|)
block|{
name|super
operator|.
name|initialize
argument_list|(
name|conf
argument_list|,
name|queryPlan
argument_list|,
name|driverContext
argument_list|)
expr_stmt|;
name|job
operator|=
operator|new
name|JobConf
argument_list|(
name|conf
argument_list|,
name|ExecDriver
operator|.
name|class
argument_list|)
expr_stmt|;
comment|// NOTE: initialize is only called if it is in non-local mode.
comment|// In case it's in non-local mode, we need to move the SessionState files
comment|// and jars to jobConf.
comment|// In case it's in local mode, MapRedTask will set the jobConf.
comment|//
comment|// "tmpfiles" and "tmpjars" are set by the method ExecDriver.execute(),
comment|// which will be called by both local and NON-local mode.
name|String
name|addedFiles
init|=
name|Utilities
operator|.
name|getResourceFiles
argument_list|(
name|job
argument_list|,
name|SessionState
operator|.
name|ResourceType
operator|.
name|FILE
argument_list|)
decl_stmt|;
if|if
condition|(
name|StringUtils
operator|.
name|isNotBlank
argument_list|(
name|addedFiles
argument_list|)
condition|)
block|{
name|HiveConf
operator|.
name|setVar
argument_list|(
name|job
argument_list|,
name|ConfVars
operator|.
name|HIVEADDEDFILES
argument_list|,
name|addedFiles
argument_list|)
expr_stmt|;
block|}
name|String
name|addedJars
init|=
name|Utilities
operator|.
name|getResourceFiles
argument_list|(
name|job
argument_list|,
name|SessionState
operator|.
name|ResourceType
operator|.
name|JAR
argument_list|)
decl_stmt|;
if|if
condition|(
name|StringUtils
operator|.
name|isNotBlank
argument_list|(
name|addedJars
argument_list|)
condition|)
block|{
name|HiveConf
operator|.
name|setVar
argument_list|(
name|job
argument_list|,
name|ConfVars
operator|.
name|HIVEADDEDJARS
argument_list|,
name|addedJars
argument_list|)
expr_stmt|;
block|}
name|String
name|addedArchives
init|=
name|Utilities
operator|.
name|getResourceFiles
argument_list|(
name|job
argument_list|,
name|SessionState
operator|.
name|ResourceType
operator|.
name|ARCHIVE
argument_list|)
decl_stmt|;
if|if
condition|(
name|StringUtils
operator|.
name|isNotBlank
argument_list|(
name|addedArchives
argument_list|)
condition|)
block|{
name|HiveConf
operator|.
name|setVar
argument_list|(
name|job
argument_list|,
name|ConfVars
operator|.
name|HIVEADDEDARCHIVES
argument_list|,
name|addedArchives
argument_list|)
expr_stmt|;
block|}
name|this
operator|.
name|jobExecHelper
operator|=
operator|new
name|HadoopJobExecHelper
argument_list|(
name|job
argument_list|,
name|console
argument_list|,
name|this
argument_list|,
name|this
argument_list|)
expr_stmt|;
block|}
comment|/**    * Constructor/Initialization for invocation as independent utility.    */
specifier|public
name|ExecDriver
parameter_list|(
name|MapredWork
name|plan
parameter_list|,
name|JobConf
name|job
parameter_list|,
name|boolean
name|isSilent
parameter_list|)
throws|throws
name|HiveException
block|{
name|setWork
argument_list|(
name|plan
argument_list|)
expr_stmt|;
name|this
operator|.
name|job
operator|=
name|job
expr_stmt|;
name|console
operator|=
operator|new
name|LogHelper
argument_list|(
name|LOG
argument_list|,
name|isSilent
argument_list|)
expr_stmt|;
name|this
operator|.
name|jobExecHelper
operator|=
operator|new
name|HadoopJobExecHelper
argument_list|(
name|job
argument_list|,
name|console
argument_list|,
name|this
argument_list|,
name|this
argument_list|)
expr_stmt|;
block|}
comment|/**    * Fatal errors are those errors that cannot be recovered by retries. These are application    * dependent. Examples of fatal errors include: - the small table in the map-side joins is too    * large to be feasible to be handled by one mapper. The job should fail and the user should be    * warned to use regular joins rather than map-side joins. Fatal errors are indicated by counters    * that are set at execution time. If the counter is non-zero, a fatal error occurred. The value    * of the counter indicates the error type.    *    * @return true if fatal errors happened during job execution, false otherwise.    */
specifier|public
name|boolean
name|checkFatalErrors
parameter_list|(
name|Counters
name|ctrs
parameter_list|,
name|StringBuilder
name|errMsg
parameter_list|)
block|{
for|for
control|(
name|Operator
argument_list|<
name|?
extends|extends
name|OperatorDesc
argument_list|>
name|op
range|:
name|work
operator|.
name|getAliasToWork
argument_list|()
operator|.
name|values
argument_list|()
control|)
block|{
if|if
condition|(
name|op
operator|.
name|checkFatalErrors
argument_list|(
name|ctrs
argument_list|,
name|errMsg
argument_list|)
condition|)
block|{
return|return
literal|true
return|;
block|}
block|}
if|if
condition|(
name|work
operator|.
name|getReducer
argument_list|()
operator|!=
literal|null
condition|)
block|{
if|if
condition|(
name|work
operator|.
name|getReducer
argument_list|()
operator|.
name|checkFatalErrors
argument_list|(
name|ctrs
argument_list|,
name|errMsg
argument_list|)
condition|)
block|{
return|return
literal|true
return|;
block|}
block|}
return|return
literal|false
return|;
block|}
specifier|protected
name|void
name|createTmpDirs
parameter_list|()
throws|throws
name|IOException
block|{
comment|// fix up outputs
name|Map
argument_list|<
name|String
argument_list|,
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|>
name|pa
init|=
name|work
operator|.
name|getPathToAliases
argument_list|()
decl_stmt|;
if|if
condition|(
name|pa
operator|!=
literal|null
condition|)
block|{
name|List
argument_list|<
name|Operator
argument_list|<
name|?
extends|extends
name|OperatorDesc
argument_list|>
argument_list|>
name|opList
init|=
operator|new
name|ArrayList
argument_list|<
name|Operator
argument_list|<
name|?
extends|extends
name|OperatorDesc
argument_list|>
argument_list|>
argument_list|()
decl_stmt|;
if|if
condition|(
name|work
operator|.
name|getReducer
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|opList
operator|.
name|add
argument_list|(
name|work
operator|.
name|getReducer
argument_list|()
argument_list|)
expr_stmt|;
block|}
for|for
control|(
name|List
argument_list|<
name|String
argument_list|>
name|ls
range|:
name|pa
operator|.
name|values
argument_list|()
control|)
block|{
for|for
control|(
name|String
name|a
range|:
name|ls
control|)
block|{
name|opList
operator|.
name|add
argument_list|(
name|work
operator|.
name|getAliasToWork
argument_list|()
operator|.
name|get
argument_list|(
name|a
argument_list|)
argument_list|)
expr_stmt|;
while|while
condition|(
operator|!
name|opList
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|Operator
argument_list|<
name|?
extends|extends
name|OperatorDesc
argument_list|>
name|op
init|=
name|opList
operator|.
name|remove
argument_list|(
literal|0
argument_list|)
decl_stmt|;
if|if
condition|(
name|op
operator|instanceof
name|FileSinkOperator
condition|)
block|{
name|FileSinkDesc
name|fdesc
init|=
operator|(
operator|(
name|FileSinkOperator
operator|)
name|op
operator|)
operator|.
name|getConf
argument_list|()
decl_stmt|;
name|String
name|tempDir
init|=
name|fdesc
operator|.
name|getDirName
argument_list|()
decl_stmt|;
if|if
condition|(
name|tempDir
operator|!=
literal|null
condition|)
block|{
name|Path
name|tempPath
init|=
name|Utilities
operator|.
name|toTempPath
argument_list|(
operator|new
name|Path
argument_list|(
name|tempDir
argument_list|)
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Making Temp Directory: "
operator|+
name|tempDir
argument_list|)
expr_stmt|;
name|FileSystem
name|fs
init|=
name|tempPath
operator|.
name|getFileSystem
argument_list|(
name|job
argument_list|)
decl_stmt|;
name|fs
operator|.
name|mkdirs
argument_list|(
name|tempPath
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
name|op
operator|.
name|getChildOperators
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|opList
operator|.
name|addAll
argument_list|(
name|op
operator|.
name|getChildOperators
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
block|}
block|}
comment|/**    * Execute a query plan using Hadoop.    */
annotation|@
name|Override
specifier|public
name|int
name|execute
parameter_list|(
name|DriverContext
name|driverContext
parameter_list|)
block|{
name|IOPrepareCache
name|ioPrepareCache
init|=
name|IOPrepareCache
operator|.
name|get
argument_list|()
decl_stmt|;
name|ioPrepareCache
operator|.
name|clear
argument_list|()
expr_stmt|;
name|boolean
name|success
init|=
literal|true
decl_stmt|;
name|String
name|invalidReason
init|=
name|work
operator|.
name|isInvalid
argument_list|()
decl_stmt|;
if|if
condition|(
name|invalidReason
operator|!=
literal|null
condition|)
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
literal|"Plan invalid, Reason: "
operator|+
name|invalidReason
argument_list|)
throw|;
block|}
name|Context
name|ctx
init|=
name|driverContext
operator|.
name|getCtx
argument_list|()
decl_stmt|;
name|boolean
name|ctxCreated
init|=
literal|false
decl_stmt|;
name|String
name|emptyScratchDirStr
decl_stmt|;
name|Path
name|emptyScratchDir
decl_stmt|;
try|try
block|{
if|if
condition|(
name|ctx
operator|==
literal|null
condition|)
block|{
name|ctx
operator|=
operator|new
name|Context
argument_list|(
name|job
argument_list|)
expr_stmt|;
name|ctxCreated
operator|=
literal|true
expr_stmt|;
block|}
name|emptyScratchDirStr
operator|=
name|ctx
operator|.
name|getMRTmpFileURI
argument_list|()
expr_stmt|;
name|emptyScratchDir
operator|=
operator|new
name|Path
argument_list|(
name|emptyScratchDirStr
argument_list|)
expr_stmt|;
name|FileSystem
name|fs
init|=
name|emptyScratchDir
operator|.
name|getFileSystem
argument_list|(
name|job
argument_list|)
decl_stmt|;
name|fs
operator|.
name|mkdirs
argument_list|(
name|emptyScratchDir
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|e
operator|.
name|printStackTrace
argument_list|()
expr_stmt|;
name|console
operator|.
name|printError
argument_list|(
literal|"Error launching map-reduce job"
argument_list|,
literal|"\n"
operator|+
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
return|return
literal|5
return|;
block|}
name|ShimLoader
operator|.
name|getHadoopShims
argument_list|()
operator|.
name|prepareJobOutput
argument_list|(
name|job
argument_list|)
expr_stmt|;
comment|//See the javadoc on HiveOutputFormatImpl and HadoopShims.prepareJobOutput()
name|job
operator|.
name|setOutputFormat
argument_list|(
name|HiveOutputFormatImpl
operator|.
name|class
argument_list|)
expr_stmt|;
name|boolean
name|vectorPath
init|=
name|HiveConf
operator|.
name|getBoolVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_VECTORIZATION_ENABLED
argument_list|)
decl_stmt|;
if|if
condition|(
name|vectorPath
condition|)
block|{
name|System
operator|.
name|out
operator|.
name|println
argument_list|(
literal|"Going down the vectorization path"
argument_list|)
expr_stmt|;
name|job
operator|.
name|setMapperClass
argument_list|(
name|VectorExecMapper
operator|.
name|class
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|job
operator|.
name|setMapperClass
argument_list|(
name|ExecMapper
operator|.
name|class
argument_list|)
expr_stmt|;
block|}
name|job
operator|.
name|setMapOutputKeyClass
argument_list|(
name|HiveKey
operator|.
name|class
argument_list|)
expr_stmt|;
name|job
operator|.
name|setMapOutputValueClass
argument_list|(
name|BytesWritable
operator|.
name|class
argument_list|)
expr_stmt|;
try|try
block|{
name|job
operator|.
name|setPartitionerClass
argument_list|(
call|(
name|Class
argument_list|<
name|?
extends|extends
name|Partitioner
argument_list|>
call|)
argument_list|(
name|Class
operator|.
name|forName
argument_list|(
name|HiveConf
operator|.
name|getVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEPARTITIONER
argument_list|)
argument_list|)
argument_list|)
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|ClassNotFoundException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
name|e
operator|.
name|getMessage
argument_list|()
argument_list|)
throw|;
block|}
if|if
condition|(
name|work
operator|.
name|getNumMapTasks
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|job
operator|.
name|setNumMapTasks
argument_list|(
name|work
operator|.
name|getNumMapTasks
argument_list|()
operator|.
name|intValue
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|work
operator|.
name|getMaxSplitSize
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|HiveConf
operator|.
name|setLongVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|MAPREDMAXSPLITSIZE
argument_list|,
name|work
operator|.
name|getMaxSplitSize
argument_list|()
operator|.
name|longValue
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|work
operator|.
name|getMinSplitSize
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|HiveConf
operator|.
name|setLongVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|MAPREDMINSPLITSIZE
argument_list|,
name|work
operator|.
name|getMinSplitSize
argument_list|()
operator|.
name|longValue
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|work
operator|.
name|getMinSplitSizePerNode
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|HiveConf
operator|.
name|setLongVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|MAPREDMINSPLITSIZEPERNODE
argument_list|,
name|work
operator|.
name|getMinSplitSizePerNode
argument_list|()
operator|.
name|longValue
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|work
operator|.
name|getMinSplitSizePerRack
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|HiveConf
operator|.
name|setLongVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|MAPREDMINSPLITSIZEPERRACK
argument_list|,
name|work
operator|.
name|getMinSplitSizePerRack
argument_list|()
operator|.
name|longValue
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|job
operator|.
name|setNumReduceTasks
argument_list|(
name|work
operator|.
name|getNumReduceTasks
argument_list|()
operator|.
name|intValue
argument_list|()
argument_list|)
expr_stmt|;
name|job
operator|.
name|setReducerClass
argument_list|(
name|ExecReducer
operator|.
name|class
argument_list|)
expr_stmt|;
comment|// set input format information if necessary
name|setInputAttributes
argument_list|(
name|job
argument_list|)
expr_stmt|;
comment|// Turn on speculative execution for reducers
name|boolean
name|useSpeculativeExecReducers
init|=
name|HiveConf
operator|.
name|getBoolVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVESPECULATIVEEXECREDUCERS
argument_list|)
decl_stmt|;
name|HiveConf
operator|.
name|setBoolVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HADOOPSPECULATIVEEXECREDUCERS
argument_list|,
name|useSpeculativeExecReducers
argument_list|)
expr_stmt|;
name|String
name|inpFormat
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEINPUTFORMAT
argument_list|)
decl_stmt|;
if|if
condition|(
operator|(
name|inpFormat
operator|==
literal|null
operator|)
operator|||
operator|(
operator|!
name|StringUtils
operator|.
name|isNotBlank
argument_list|(
name|inpFormat
argument_list|)
operator|)
condition|)
block|{
name|inpFormat
operator|=
name|ShimLoader
operator|.
name|getHadoopShims
argument_list|()
operator|.
name|getInputFormatClassName
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
name|getWork
argument_list|()
operator|.
name|isUseBucketizedHiveInputFormat
argument_list|()
condition|)
block|{
name|inpFormat
operator|=
name|BucketizedHiveInputFormat
operator|.
name|class
operator|.
name|getName
argument_list|()
expr_stmt|;
block|}
name|LOG
operator|.
name|info
argument_list|(
literal|"Using "
operator|+
name|inpFormat
argument_list|)
expr_stmt|;
try|try
block|{
name|job
operator|.
name|setInputFormat
argument_list|(
call|(
name|Class
argument_list|<
name|?
extends|extends
name|InputFormat
argument_list|>
call|)
argument_list|(
name|Class
operator|.
name|forName
argument_list|(
name|inpFormat
argument_list|)
argument_list|)
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|ClassNotFoundException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
name|e
operator|.
name|getMessage
argument_list|()
argument_list|)
throw|;
block|}
comment|// No-Op - we don't really write anything here ..
name|job
operator|.
name|setOutputKeyClass
argument_list|(
name|Text
operator|.
name|class
argument_list|)
expr_stmt|;
name|job
operator|.
name|setOutputValueClass
argument_list|(
name|Text
operator|.
name|class
argument_list|)
expr_stmt|;
comment|// Transfer HIVEAUXJARS and HIVEADDEDJARS to "tmpjars" so hadoop understands
comment|// it
name|String
name|auxJars
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEAUXJARS
argument_list|)
decl_stmt|;
name|String
name|addedJars
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEADDEDJARS
argument_list|)
decl_stmt|;
if|if
condition|(
name|StringUtils
operator|.
name|isNotBlank
argument_list|(
name|auxJars
argument_list|)
operator|||
name|StringUtils
operator|.
name|isNotBlank
argument_list|(
name|addedJars
argument_list|)
condition|)
block|{
name|String
name|allJars
init|=
name|StringUtils
operator|.
name|isNotBlank
argument_list|(
name|auxJars
argument_list|)
condition|?
operator|(
name|StringUtils
operator|.
name|isNotBlank
argument_list|(
name|addedJars
argument_list|)
condition|?
name|addedJars
operator|+
literal|","
operator|+
name|auxJars
else|:
name|auxJars
operator|)
else|:
name|addedJars
decl_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"adding libjars: "
operator|+
name|allJars
argument_list|)
expr_stmt|;
name|initializeFiles
argument_list|(
literal|"tmpjars"
argument_list|,
name|allJars
argument_list|)
expr_stmt|;
block|}
comment|// Transfer HIVEADDEDFILES to "tmpfiles" so hadoop understands it
name|String
name|addedFiles
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEADDEDFILES
argument_list|)
decl_stmt|;
if|if
condition|(
name|StringUtils
operator|.
name|isNotBlank
argument_list|(
name|addedFiles
argument_list|)
condition|)
block|{
name|initializeFiles
argument_list|(
literal|"tmpfiles"
argument_list|,
name|addedFiles
argument_list|)
expr_stmt|;
block|}
name|int
name|returnVal
init|=
literal|0
decl_stmt|;
name|boolean
name|noName
init|=
name|StringUtils
operator|.
name|isEmpty
argument_list|(
name|HiveConf
operator|.
name|getVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HADOOPJOBNAME
argument_list|)
argument_list|)
decl_stmt|;
if|if
condition|(
name|noName
condition|)
block|{
comment|// This is for a special case to ensure unit tests pass
name|HiveConf
operator|.
name|setVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HADOOPJOBNAME
argument_list|,
literal|"JOB"
operator|+
name|Utilities
operator|.
name|randGen
operator|.
name|nextInt
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|String
name|addedArchives
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEADDEDARCHIVES
argument_list|)
decl_stmt|;
comment|// Transfer HIVEADDEDARCHIVES to "tmparchives" so hadoop understands it
if|if
condition|(
name|StringUtils
operator|.
name|isNotBlank
argument_list|(
name|addedArchives
argument_list|)
condition|)
block|{
name|initializeFiles
argument_list|(
literal|"tmparchives"
argument_list|,
name|addedArchives
argument_list|)
expr_stmt|;
block|}
try|try
block|{
name|MapredLocalWork
name|localwork
init|=
name|work
operator|.
name|getMapLocalWork
argument_list|()
decl_stmt|;
if|if
condition|(
name|localwork
operator|!=
literal|null
condition|)
block|{
if|if
condition|(
operator|!
name|ShimLoader
operator|.
name|getHadoopShims
argument_list|()
operator|.
name|isLocalMode
argument_list|(
name|job
argument_list|)
condition|)
block|{
name|Path
name|localPath
init|=
operator|new
name|Path
argument_list|(
name|localwork
operator|.
name|getTmpFileURI
argument_list|()
argument_list|)
decl_stmt|;
name|Path
name|hdfsPath
init|=
operator|new
name|Path
argument_list|(
name|work
operator|.
name|getTmpHDFSFileURI
argument_list|()
argument_list|)
decl_stmt|;
name|FileSystem
name|hdfs
init|=
name|hdfsPath
operator|.
name|getFileSystem
argument_list|(
name|job
argument_list|)
decl_stmt|;
name|FileSystem
name|localFS
init|=
name|localPath
operator|.
name|getFileSystem
argument_list|(
name|job
argument_list|)
decl_stmt|;
name|FileStatus
index|[]
name|hashtableFiles
init|=
name|localFS
operator|.
name|listStatus
argument_list|(
name|localPath
argument_list|)
decl_stmt|;
name|int
name|fileNumber
init|=
name|hashtableFiles
operator|.
name|length
decl_stmt|;
name|String
index|[]
name|fileNames
init|=
operator|new
name|String
index|[
name|fileNumber
index|]
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|fileNumber
condition|;
name|i
operator|++
control|)
block|{
name|fileNames
index|[
name|i
index|]
operator|=
name|hashtableFiles
index|[
name|i
index|]
operator|.
name|getPath
argument_list|()
operator|.
name|getName
argument_list|()
expr_stmt|;
block|}
comment|//package and compress all the hashtable files to an archive file
name|String
name|parentDir
init|=
name|localPath
operator|.
name|toUri
argument_list|()
operator|.
name|getPath
argument_list|()
decl_stmt|;
name|String
name|stageId
init|=
name|this
operator|.
name|getId
argument_list|()
decl_stmt|;
name|String
name|archiveFileURI
init|=
name|Utilities
operator|.
name|generateTarURI
argument_list|(
name|parentDir
argument_list|,
name|stageId
argument_list|)
decl_stmt|;
name|String
name|archiveFileName
init|=
name|Utilities
operator|.
name|generateTarFileName
argument_list|(
name|stageId
argument_list|)
decl_stmt|;
name|localwork
operator|.
name|setStageID
argument_list|(
name|stageId
argument_list|)
expr_stmt|;
name|CompressionUtils
operator|.
name|tar
argument_list|(
name|parentDir
argument_list|,
name|fileNames
argument_list|,
name|archiveFileName
argument_list|)
expr_stmt|;
name|Path
name|archivePath
init|=
operator|new
name|Path
argument_list|(
name|archiveFileURI
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Archive "
operator|+
name|hashtableFiles
operator|.
name|length
operator|+
literal|" hash table files to "
operator|+
name|archiveFileURI
argument_list|)
expr_stmt|;
comment|//upload archive file to hdfs
name|String
name|hdfsFile
init|=
name|Utilities
operator|.
name|generateTarURI
argument_list|(
name|hdfsPath
argument_list|,
name|stageId
argument_list|)
decl_stmt|;
name|Path
name|hdfsFilePath
init|=
operator|new
name|Path
argument_list|(
name|hdfsFile
argument_list|)
decl_stmt|;
name|short
name|replication
init|=
operator|(
name|short
operator|)
name|job
operator|.
name|getInt
argument_list|(
literal|"mapred.submit.replication"
argument_list|,
literal|10
argument_list|)
decl_stmt|;
name|hdfs
operator|.
name|setReplication
argument_list|(
name|hdfsFilePath
argument_list|,
name|replication
argument_list|)
expr_stmt|;
name|hdfs
operator|.
name|copyFromLocalFile
argument_list|(
name|archivePath
argument_list|,
name|hdfsFilePath
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Upload 1 archive file  from"
operator|+
name|archivePath
operator|+
literal|" to: "
operator|+
name|hdfsFilePath
argument_list|)
expr_stmt|;
comment|//add the archive file to distributed cache
name|DistributedCache
operator|.
name|createSymlink
argument_list|(
name|job
argument_list|)
expr_stmt|;
name|DistributedCache
operator|.
name|addCacheArchive
argument_list|(
name|hdfsFilePath
operator|.
name|toUri
argument_list|()
argument_list|,
name|job
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Add 1 archive file to distributed cache. Archive file: "
operator|+
name|hdfsFilePath
operator|.
name|toUri
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
name|work
operator|.
name|configureJobConf
argument_list|(
name|job
argument_list|)
expr_stmt|;
name|addInputPaths
argument_list|(
name|job
argument_list|,
name|work
argument_list|,
name|emptyScratchDirStr
argument_list|,
name|ctx
argument_list|)
expr_stmt|;
name|Utilities
operator|.
name|setMapRedWork
argument_list|(
name|job
argument_list|,
name|work
argument_list|,
name|ctx
operator|.
name|getMRTmpFileURI
argument_list|()
argument_list|)
expr_stmt|;
comment|// remove the pwd from conf file so that job tracker doesn't show this
comment|// logs
name|String
name|pwd
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|METASTOREPWD
argument_list|)
decl_stmt|;
if|if
condition|(
name|pwd
operator|!=
literal|null
condition|)
block|{
name|HiveConf
operator|.
name|setVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|METASTOREPWD
argument_list|,
literal|"HIVE"
argument_list|)
expr_stmt|;
block|}
name|JobClient
name|jc
init|=
operator|new
name|JobClient
argument_list|(
name|job
argument_list|)
decl_stmt|;
comment|// make this client wait if job trcker is not behaving well.
name|Throttle
operator|.
name|checkJobTracker
argument_list|(
name|job
argument_list|,
name|LOG
argument_list|)
expr_stmt|;
if|if
condition|(
name|work
operator|.
name|isGatheringStats
argument_list|()
condition|)
block|{
comment|// initialize stats publishing table
name|StatsPublisher
name|statsPublisher
decl_stmt|;
name|String
name|statsImplementationClass
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVESTATSDBCLASS
argument_list|)
decl_stmt|;
if|if
condition|(
name|StatsFactory
operator|.
name|setImplementation
argument_list|(
name|statsImplementationClass
argument_list|,
name|job
argument_list|)
condition|)
block|{
name|statsPublisher
operator|=
name|StatsFactory
operator|.
name|getStatsPublisher
argument_list|()
expr_stmt|;
if|if
condition|(
operator|!
name|statsPublisher
operator|.
name|init
argument_list|(
name|job
argument_list|)
condition|)
block|{
comment|// creating stats table if not exists
if|if
condition|(
name|HiveConf
operator|.
name|getBoolVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_STATS_RELIABLE
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|ErrorMsg
operator|.
name|STATSPUBLISHER_INITIALIZATION_ERROR
operator|.
name|getErrorCodedMsg
argument_list|()
argument_list|)
throw|;
block|}
block|}
block|}
block|}
name|this
operator|.
name|createTmpDirs
argument_list|()
expr_stmt|;
comment|// Finally SUBMIT the JOB!
name|rj
operator|=
name|jc
operator|.
name|submitJob
argument_list|(
name|job
argument_list|)
expr_stmt|;
comment|// replace it back
if|if
condition|(
name|pwd
operator|!=
literal|null
condition|)
block|{
name|HiveConf
operator|.
name|setVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|METASTOREPWD
argument_list|,
name|pwd
argument_list|)
expr_stmt|;
block|}
name|returnVal
operator|=
name|jobExecHelper
operator|.
name|progress
argument_list|(
name|rj
argument_list|,
name|jc
argument_list|)
expr_stmt|;
name|success
operator|=
operator|(
name|returnVal
operator|==
literal|0
operator|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|e
operator|.
name|printStackTrace
argument_list|()
expr_stmt|;
name|String
name|mesg
init|=
literal|" with exception '"
operator|+
name|Utilities
operator|.
name|getNameMessage
argument_list|(
name|e
argument_list|)
operator|+
literal|"'"
decl_stmt|;
if|if
condition|(
name|rj
operator|!=
literal|null
condition|)
block|{
name|mesg
operator|=
literal|"Ended Job = "
operator|+
name|rj
operator|.
name|getJobID
argument_list|()
operator|+
name|mesg
expr_stmt|;
block|}
else|else
block|{
name|mesg
operator|=
literal|"Job Submission failed"
operator|+
name|mesg
expr_stmt|;
block|}
comment|// Has to use full name to make sure it does not conflict with
comment|// org.apache.commons.lang.StringUtils
name|console
operator|.
name|printError
argument_list|(
name|mesg
argument_list|,
literal|"\n"
operator|+
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
name|success
operator|=
literal|false
expr_stmt|;
name|returnVal
operator|=
literal|1
expr_stmt|;
block|}
finally|finally
block|{
name|Utilities
operator|.
name|clearMapRedWork
argument_list|(
name|job
argument_list|)
expr_stmt|;
try|try
block|{
if|if
condition|(
name|ctxCreated
condition|)
block|{
name|ctx
operator|.
name|clear
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
name|rj
operator|!=
literal|null
condition|)
block|{
if|if
condition|(
name|returnVal
operator|!=
literal|0
condition|)
block|{
name|rj
operator|.
name|killJob
argument_list|()
expr_stmt|;
block|}
name|HadoopJobExecHelper
operator|.
name|runningJobKillURIs
operator|.
name|remove
argument_list|(
name|rj
operator|.
name|getJobID
argument_list|()
argument_list|)
expr_stmt|;
name|jobID
operator|=
name|rj
operator|.
name|getID
argument_list|()
operator|.
name|toString
argument_list|()
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{       }
block|}
comment|// get the list of Dynamic partition paths
try|try
block|{
if|if
condition|(
name|rj
operator|!=
literal|null
condition|)
block|{
name|JobCloseFeedBack
name|feedBack
init|=
operator|new
name|JobCloseFeedBack
argument_list|()
decl_stmt|;
if|if
condition|(
name|work
operator|.
name|getAliasToWork
argument_list|()
operator|!=
literal|null
condition|)
block|{
for|for
control|(
name|Operator
argument_list|<
name|?
extends|extends
name|OperatorDesc
argument_list|>
name|op
range|:
name|work
operator|.
name|getAliasToWork
argument_list|()
operator|.
name|values
argument_list|()
control|)
block|{
name|op
operator|.
name|jobClose
argument_list|(
name|job
argument_list|,
name|success
argument_list|,
name|feedBack
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
name|work
operator|.
name|getReducer
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|work
operator|.
name|getReducer
argument_list|()
operator|.
name|jobClose
argument_list|(
name|job
argument_list|,
name|success
argument_list|,
name|feedBack
argument_list|)
expr_stmt|;
block|}
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
comment|// jobClose needs to execute successfully otherwise fail task
if|if
condition|(
name|success
condition|)
block|{
name|success
operator|=
literal|false
expr_stmt|;
name|returnVal
operator|=
literal|3
expr_stmt|;
name|String
name|mesg
init|=
literal|"Job Commit failed with exception '"
operator|+
name|Utilities
operator|.
name|getNameMessage
argument_list|(
name|e
argument_list|)
operator|+
literal|"'"
decl_stmt|;
name|console
operator|.
name|printError
argument_list|(
name|mesg
argument_list|,
literal|"\n"
operator|+
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
return|return
operator|(
name|returnVal
operator|)
return|;
block|}
comment|/**    * Set hive input format, and input format file if necessary.    */
specifier|protected
name|void
name|setInputAttributes
parameter_list|(
name|Configuration
name|conf
parameter_list|)
block|{
if|if
condition|(
name|work
operator|.
name|getInputformat
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|HiveConf
operator|.
name|setVar
argument_list|(
name|conf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEINPUTFORMAT
argument_list|,
name|work
operator|.
name|getInputformat
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|work
operator|.
name|getIndexIntermediateFile
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|conf
operator|.
name|set
argument_list|(
literal|"hive.index.compact.file"
argument_list|,
name|work
operator|.
name|getIndexIntermediateFile
argument_list|()
argument_list|)
expr_stmt|;
name|conf
operator|.
name|set
argument_list|(
literal|"hive.index.blockfilter.file"
argument_list|,
name|work
operator|.
name|getIndexIntermediateFile
argument_list|()
argument_list|)
expr_stmt|;
block|}
comment|// Intentionally overwrites anything the user may have put here
name|conf
operator|.
name|setBoolean
argument_list|(
literal|"hive.input.format.sorted"
argument_list|,
name|work
operator|.
name|isInputFormatSorted
argument_list|()
argument_list|)
expr_stmt|;
block|}
specifier|public
name|boolean
name|mapStarted
parameter_list|()
block|{
return|return
name|this
operator|.
name|jobExecHelper
operator|.
name|mapStarted
argument_list|()
return|;
block|}
specifier|public
name|boolean
name|reduceStarted
parameter_list|()
block|{
return|return
name|this
operator|.
name|jobExecHelper
operator|.
name|reduceStarted
argument_list|()
return|;
block|}
specifier|public
name|boolean
name|mapDone
parameter_list|()
block|{
return|return
name|this
operator|.
name|jobExecHelper
operator|.
name|mapDone
argument_list|()
return|;
block|}
specifier|public
name|boolean
name|reduceDone
parameter_list|()
block|{
return|return
name|this
operator|.
name|jobExecHelper
operator|.
name|reduceDone
argument_list|()
return|;
block|}
specifier|private
specifier|static
name|void
name|printUsage
parameter_list|()
block|{
name|System
operator|.
name|err
operator|.
name|println
argument_list|(
literal|"ExecDriver -plan<plan-file> [-jobconffile<job conf file>]"
operator|+
literal|"[-files<file1>[,<file2>] ...]"
argument_list|)
expr_stmt|;
name|System
operator|.
name|exit
argument_list|(
literal|1
argument_list|)
expr_stmt|;
block|}
comment|/**    * we are running the hadoop job via a sub-command. this typically happens when we are running    * jobs in local mode. the log4j in this mode is controlled as follows: 1. if the admin provides a    * log4j properties file especially for execution mode - then we pick that up 2. otherwise - we    * default to the regular hive log4j properties if one is supplied 3. if none of the above two    * apply - we don't do anything - the log4j properties would likely be determined by hadoop.    *    * The intention behind providing a separate option #1 is to be able to collect hive run time logs    * generated in local mode in a separate (centralized) location if desired. This mimics the    * behavior of hive run time logs when running against a hadoop cluster where they are available    * on the tasktracker nodes.    */
specifier|private
specifier|static
name|void
name|setupChildLog4j
parameter_list|(
name|Configuration
name|conf
parameter_list|)
block|{
try|try
block|{
name|LogUtils
operator|.
name|initHiveExecLog4j
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|LogInitializationException
name|e
parameter_list|)
block|{
name|System
operator|.
name|err
operator|.
name|println
argument_list|(
name|e
operator|.
name|getMessage
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
specifier|public
specifier|static
name|void
name|main
parameter_list|(
name|String
index|[]
name|args
parameter_list|)
throws|throws
name|IOException
throws|,
name|HiveException
block|{
name|String
name|planFileName
init|=
literal|null
decl_stmt|;
name|String
name|jobConfFileName
init|=
literal|null
decl_stmt|;
name|boolean
name|noLog
init|=
literal|false
decl_stmt|;
name|String
name|files
init|=
literal|null
decl_stmt|;
name|boolean
name|localtask
init|=
literal|false
decl_stmt|;
try|try
block|{
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|args
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
if|if
condition|(
name|args
index|[
name|i
index|]
operator|.
name|equals
argument_list|(
literal|"-plan"
argument_list|)
condition|)
block|{
name|planFileName
operator|=
name|args
index|[
operator|++
name|i
index|]
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|args
index|[
name|i
index|]
operator|.
name|equals
argument_list|(
literal|"-jobconffile"
argument_list|)
condition|)
block|{
name|jobConfFileName
operator|=
name|args
index|[
operator|++
name|i
index|]
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|args
index|[
name|i
index|]
operator|.
name|equals
argument_list|(
literal|"-nolog"
argument_list|)
condition|)
block|{
name|noLog
operator|=
literal|true
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|args
index|[
name|i
index|]
operator|.
name|equals
argument_list|(
literal|"-files"
argument_list|)
condition|)
block|{
name|files
operator|=
name|args
index|[
operator|++
name|i
index|]
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|args
index|[
name|i
index|]
operator|.
name|equals
argument_list|(
literal|"-localtask"
argument_list|)
condition|)
block|{
name|localtask
operator|=
literal|true
expr_stmt|;
block|}
block|}
block|}
catch|catch
parameter_list|(
name|IndexOutOfBoundsException
name|e
parameter_list|)
block|{
name|System
operator|.
name|err
operator|.
name|println
argument_list|(
literal|"Missing argument to option"
argument_list|)
expr_stmt|;
name|printUsage
argument_list|()
expr_stmt|;
block|}
name|JobConf
name|conf
decl_stmt|;
if|if
condition|(
name|localtask
condition|)
block|{
name|conf
operator|=
operator|new
name|JobConf
argument_list|(
name|MapredLocalTask
operator|.
name|class
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|conf
operator|=
operator|new
name|JobConf
argument_list|(
name|ExecDriver
operator|.
name|class
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|jobConfFileName
operator|!=
literal|null
condition|)
block|{
name|conf
operator|.
name|addResource
argument_list|(
operator|new
name|Path
argument_list|(
name|jobConfFileName
argument_list|)
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|files
operator|!=
literal|null
condition|)
block|{
name|conf
operator|.
name|set
argument_list|(
literal|"tmpfiles"
argument_list|,
name|files
argument_list|)
expr_stmt|;
block|}
name|boolean
name|isSilent
init|=
name|HiveConf
operator|.
name|getBoolVar
argument_list|(
name|conf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVESESSIONSILENT
argument_list|)
decl_stmt|;
if|if
condition|(
name|noLog
condition|)
block|{
comment|// If started from main(), and noLog is on, we should not output
comment|// any logs. To turn the log on, please set -Dtest.silent=false
name|BasicConfigurator
operator|.
name|resetConfiguration
argument_list|()
expr_stmt|;
name|BasicConfigurator
operator|.
name|configure
argument_list|(
operator|new
name|NullAppender
argument_list|()
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|setupChildLog4j
argument_list|(
name|conf
argument_list|)
expr_stmt|;
block|}
name|Log
name|LOG
init|=
name|LogFactory
operator|.
name|getLog
argument_list|(
name|ExecDriver
operator|.
name|class
operator|.
name|getName
argument_list|()
argument_list|)
decl_stmt|;
name|LogHelper
name|console
init|=
operator|new
name|LogHelper
argument_list|(
name|LOG
argument_list|,
name|isSilent
argument_list|)
decl_stmt|;
if|if
condition|(
name|planFileName
operator|==
literal|null
condition|)
block|{
name|console
operator|.
name|printError
argument_list|(
literal|"Must specify Plan File Name"
argument_list|)
expr_stmt|;
name|printUsage
argument_list|()
expr_stmt|;
block|}
comment|// print out the location of the log file for the user so
comment|// that it's easy to find reason for local mode execution failures
for|for
control|(
name|Appender
name|appender
range|:
name|Collections
operator|.
name|list
argument_list|(
operator|(
name|Enumeration
argument_list|<
name|Appender
argument_list|>
operator|)
name|LogManager
operator|.
name|getRootLogger
argument_list|()
operator|.
name|getAllAppenders
argument_list|()
argument_list|)
control|)
block|{
if|if
condition|(
name|appender
operator|instanceof
name|FileAppender
condition|)
block|{
name|console
operator|.
name|printInfo
argument_list|(
literal|"Execution log at: "
operator|+
operator|(
operator|(
name|FileAppender
operator|)
name|appender
operator|)
operator|.
name|getFile
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
comment|// the plan file should always be in local directory
name|Path
name|p
init|=
operator|new
name|Path
argument_list|(
name|planFileName
argument_list|)
decl_stmt|;
name|FileSystem
name|fs
init|=
name|FileSystem
operator|.
name|getLocal
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|InputStream
name|pathData
init|=
name|fs
operator|.
name|open
argument_list|(
name|p
argument_list|)
decl_stmt|;
comment|// this is workaround for hadoop-17 - libjars are not added to classpath of the
comment|// child process. so we add it here explicitly
name|String
name|auxJars
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|conf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEAUXJARS
argument_list|)
decl_stmt|;
name|String
name|addedJars
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|conf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEADDEDJARS
argument_list|)
decl_stmt|;
try|try
block|{
comment|// see also - code in CliDriver.java
name|ClassLoader
name|loader
init|=
name|conf
operator|.
name|getClassLoader
argument_list|()
decl_stmt|;
if|if
condition|(
name|StringUtils
operator|.
name|isNotBlank
argument_list|(
name|auxJars
argument_list|)
condition|)
block|{
name|loader
operator|=
name|Utilities
operator|.
name|addToClassPath
argument_list|(
name|loader
argument_list|,
name|StringUtils
operator|.
name|split
argument_list|(
name|auxJars
argument_list|,
literal|","
argument_list|)
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|StringUtils
operator|.
name|isNotBlank
argument_list|(
name|addedJars
argument_list|)
condition|)
block|{
name|loader
operator|=
name|Utilities
operator|.
name|addToClassPath
argument_list|(
name|loader
argument_list|,
name|StringUtils
operator|.
name|split
argument_list|(
name|addedJars
argument_list|,
literal|","
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|conf
operator|.
name|setClassLoader
argument_list|(
name|loader
argument_list|)
expr_stmt|;
comment|// Also set this to the Thread ContextClassLoader, so new threads will
comment|// inherit
comment|// this class loader, and propagate into newly created Configurations by
comment|// those
comment|// new threads.
name|Thread
operator|.
name|currentThread
argument_list|()
operator|.
name|setContextClassLoader
argument_list|(
name|loader
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
name|int
name|ret
decl_stmt|;
if|if
condition|(
name|localtask
condition|)
block|{
name|memoryMXBean
operator|=
name|ManagementFactory
operator|.
name|getMemoryMXBean
argument_list|()
expr_stmt|;
name|MapredLocalWork
name|plan
init|=
name|Utilities
operator|.
name|deserializeMapRedLocalWork
argument_list|(
name|pathData
argument_list|,
name|conf
argument_list|)
decl_stmt|;
name|MapredLocalTask
name|ed
init|=
operator|new
name|MapredLocalTask
argument_list|(
name|plan
argument_list|,
name|conf
argument_list|,
name|isSilent
argument_list|)
decl_stmt|;
name|ret
operator|=
name|ed
operator|.
name|executeFromChildJVM
argument_list|(
operator|new
name|DriverContext
argument_list|()
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|MapredWork
name|plan
init|=
name|Utilities
operator|.
name|deserializeMapRedWork
argument_list|(
name|pathData
argument_list|,
name|conf
argument_list|)
decl_stmt|;
name|ExecDriver
name|ed
init|=
operator|new
name|ExecDriver
argument_list|(
name|plan
argument_list|,
name|conf
argument_list|,
name|isSilent
argument_list|)
decl_stmt|;
name|ret
operator|=
name|ed
operator|.
name|execute
argument_list|(
operator|new
name|DriverContext
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|ret
operator|!=
literal|0
condition|)
block|{
name|System
operator|.
name|exit
argument_list|(
name|ret
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * Given a Hive Configuration object - generate a command line fragment for passing such    * configuration information to ExecDriver.    */
specifier|public
specifier|static
name|String
name|generateCmdLine
parameter_list|(
name|HiveConf
name|hconf
parameter_list|,
name|Context
name|ctx
parameter_list|)
throws|throws
name|IOException
block|{
name|HiveConf
name|tempConf
init|=
operator|new
name|HiveConf
argument_list|()
decl_stmt|;
name|Path
name|hConfFilePath
init|=
operator|new
name|Path
argument_list|(
name|ctx
operator|.
name|getLocalTmpFileURI
argument_list|()
argument_list|,
name|JOBCONF_FILENAME
argument_list|)
decl_stmt|;
name|OutputStream
name|out
init|=
literal|null
decl_stmt|;
name|Properties
name|deltaP
init|=
name|hconf
operator|.
name|getChangedProperties
argument_list|()
decl_stmt|;
name|boolean
name|hadoopLocalMode
init|=
name|ShimLoader
operator|.
name|getHadoopShims
argument_list|()
operator|.
name|isLocalMode
argument_list|(
name|hconf
argument_list|)
decl_stmt|;
name|String
name|hadoopSysDir
init|=
literal|"mapred.system.dir"
decl_stmt|;
name|String
name|hadoopWorkDir
init|=
literal|"mapred.local.dir"
decl_stmt|;
for|for
control|(
name|Object
name|one
range|:
name|deltaP
operator|.
name|keySet
argument_list|()
control|)
block|{
name|String
name|oneProp
init|=
operator|(
name|String
operator|)
name|one
decl_stmt|;
if|if
condition|(
name|hadoopLocalMode
operator|&&
operator|(
name|oneProp
operator|.
name|equals
argument_list|(
name|hadoopSysDir
argument_list|)
operator|||
name|oneProp
operator|.
name|equals
argument_list|(
name|hadoopWorkDir
argument_list|)
operator|)
condition|)
block|{
continue|continue;
block|}
name|tempConf
operator|.
name|set
argument_list|(
name|oneProp
argument_list|,
name|deltaP
operator|.
name|getProperty
argument_list|(
name|oneProp
argument_list|)
argument_list|)
expr_stmt|;
block|}
comment|// Multiple concurrent local mode job submissions can cause collisions in
comment|// working dirs and system dirs
comment|// Workaround is to rename map red working dir to a temp dir in such cases
if|if
condition|(
name|hadoopLocalMode
condition|)
block|{
name|tempConf
operator|.
name|set
argument_list|(
name|hadoopSysDir
argument_list|,
name|hconf
operator|.
name|get
argument_list|(
name|hadoopSysDir
argument_list|)
operator|+
literal|"/"
operator|+
name|Utilities
operator|.
name|randGen
operator|.
name|nextInt
argument_list|()
argument_list|)
expr_stmt|;
name|tempConf
operator|.
name|set
argument_list|(
name|hadoopWorkDir
argument_list|,
name|hconf
operator|.
name|get
argument_list|(
name|hadoopWorkDir
argument_list|)
operator|+
literal|"/"
operator|+
name|Utilities
operator|.
name|randGen
operator|.
name|nextInt
argument_list|()
argument_list|)
expr_stmt|;
block|}
try|try
block|{
name|out
operator|=
name|FileSystem
operator|.
name|getLocal
argument_list|(
name|hconf
argument_list|)
operator|.
name|create
argument_list|(
name|hConfFilePath
argument_list|)
expr_stmt|;
name|tempConf
operator|.
name|writeXml
argument_list|(
name|out
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
if|if
condition|(
name|out
operator|!=
literal|null
condition|)
block|{
name|out
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
return|return
literal|" -jobconffile "
operator|+
name|hConfFilePath
operator|.
name|toString
argument_list|()
return|;
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|isMapRedTask
parameter_list|()
block|{
return|return
literal|true
return|;
block|}
annotation|@
name|Override
specifier|public
name|Collection
argument_list|<
name|Operator
argument_list|<
name|?
extends|extends
name|OperatorDesc
argument_list|>
argument_list|>
name|getTopOperators
parameter_list|()
block|{
return|return
name|getWork
argument_list|()
operator|.
name|getAliasToWork
argument_list|()
operator|.
name|values
argument_list|()
return|;
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|hasReduce
parameter_list|()
block|{
name|MapredWork
name|w
init|=
name|getWork
argument_list|()
decl_stmt|;
return|return
name|w
operator|.
name|getReducer
argument_list|()
operator|!=
literal|null
return|;
block|}
comment|/**    * Handle a empty/null path for a given alias.    */
specifier|private
specifier|static
name|int
name|addInputPath
parameter_list|(
name|String
name|path
parameter_list|,
name|JobConf
name|job
parameter_list|,
name|MapredWork
name|work
parameter_list|,
name|String
name|hiveScratchDir
parameter_list|,
name|int
name|numEmptyPaths
parameter_list|,
name|boolean
name|isEmptyPath
parameter_list|,
name|String
name|alias
parameter_list|)
throws|throws
name|Exception
block|{
comment|// either the directory does not exist or it is empty
assert|assert
name|path
operator|==
literal|null
operator|||
name|isEmptyPath
assert|;
comment|// The input file does not exist, replace it by a empty file
name|Class
argument_list|<
name|?
extends|extends
name|HiveOutputFormat
argument_list|>
name|outFileFormat
init|=
literal|null
decl_stmt|;
name|boolean
name|nonNative
init|=
literal|true
decl_stmt|;
name|boolean
name|oneRow
init|=
literal|false
decl_stmt|;
name|Properties
name|props
decl_stmt|;
if|if
condition|(
name|isEmptyPath
condition|)
block|{
name|PartitionDesc
name|partDesc
init|=
name|work
operator|.
name|getPathToPartitionInfo
argument_list|()
operator|.
name|get
argument_list|(
name|path
argument_list|)
decl_stmt|;
name|props
operator|=
name|partDesc
operator|.
name|getProperties
argument_list|()
expr_stmt|;
name|outFileFormat
operator|=
name|partDesc
operator|.
name|getOutputFileFormatClass
argument_list|()
expr_stmt|;
name|nonNative
operator|=
name|partDesc
operator|.
name|getTableDesc
argument_list|()
operator|.
name|isNonNative
argument_list|()
expr_stmt|;
name|oneRow
operator|=
name|partDesc
operator|.
name|getInputFileFormatClass
argument_list|()
operator|==
name|OneNullRowInputFormat
operator|.
name|class
expr_stmt|;
block|}
else|else
block|{
name|TableDesc
name|tableDesc
init|=
name|work
operator|.
name|getAliasToPartnInfo
argument_list|()
operator|.
name|get
argument_list|(
name|alias
argument_list|)
operator|.
name|getTableDesc
argument_list|()
decl_stmt|;
name|props
operator|=
name|tableDesc
operator|.
name|getProperties
argument_list|()
expr_stmt|;
name|outFileFormat
operator|=
name|tableDesc
operator|.
name|getOutputFileFormatClass
argument_list|()
expr_stmt|;
name|nonNative
operator|=
name|tableDesc
operator|.
name|isNonNative
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
name|nonNative
condition|)
block|{
name|FileInputFormat
operator|.
name|addInputPaths
argument_list|(
name|job
argument_list|,
name|path
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Add a non-native table "
operator|+
name|path
argument_list|)
expr_stmt|;
return|return
name|numEmptyPaths
return|;
block|}
comment|// create a dummy empty file in a new directory
name|String
name|newDir
init|=
name|hiveScratchDir
operator|+
name|File
operator|.
name|separator
operator|+
operator|(
operator|++
name|numEmptyPaths
operator|)
decl_stmt|;
name|Path
name|newPath
init|=
operator|new
name|Path
argument_list|(
name|newDir
argument_list|)
decl_stmt|;
name|FileSystem
name|fs
init|=
name|newPath
operator|.
name|getFileSystem
argument_list|(
name|job
argument_list|)
decl_stmt|;
name|fs
operator|.
name|mkdirs
argument_list|(
name|newPath
argument_list|)
expr_stmt|;
comment|//Qualify the path against the filesystem. The user configured path might contain default port which is skipped
comment|//in the file status. This makes sure that all paths which goes into PathToPartitionInfo are always listed status
comment|//filepath.
name|newPath
operator|=
name|fs
operator|.
name|makeQualified
argument_list|(
name|newPath
argument_list|)
expr_stmt|;
name|String
name|newFile
init|=
name|newDir
operator|+
name|File
operator|.
name|separator
operator|+
literal|"emptyFile"
decl_stmt|;
name|Path
name|newFilePath
init|=
operator|new
name|Path
argument_list|(
name|newFile
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Changed input file to "
operator|+
name|newPath
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
comment|// toggle the work
name|LinkedHashMap
argument_list|<
name|String
argument_list|,
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|>
name|pathToAliases
init|=
name|work
operator|.
name|getPathToAliases
argument_list|()
decl_stmt|;
if|if
condition|(
name|isEmptyPath
condition|)
block|{
assert|assert
name|path
operator|!=
literal|null
assert|;
name|pathToAliases
operator|.
name|put
argument_list|(
name|newPath
operator|.
name|toUri
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|,
name|pathToAliases
operator|.
name|get
argument_list|(
name|path
argument_list|)
argument_list|)
expr_stmt|;
name|pathToAliases
operator|.
name|remove
argument_list|(
name|path
argument_list|)
expr_stmt|;
block|}
else|else
block|{
assert|assert
name|path
operator|==
literal|null
assert|;
name|ArrayList
argument_list|<
name|String
argument_list|>
name|newList
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
name|newList
operator|.
name|add
argument_list|(
name|alias
argument_list|)
expr_stmt|;
name|pathToAliases
operator|.
name|put
argument_list|(
name|newPath
operator|.
name|toUri
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|,
name|newList
argument_list|)
expr_stmt|;
block|}
name|work
operator|.
name|setPathToAliases
argument_list|(
name|pathToAliases
argument_list|)
expr_stmt|;
name|LinkedHashMap
argument_list|<
name|String
argument_list|,
name|PartitionDesc
argument_list|>
name|pathToPartitionInfo
init|=
name|work
operator|.
name|getPathToPartitionInfo
argument_list|()
decl_stmt|;
if|if
condition|(
name|isEmptyPath
condition|)
block|{
name|pathToPartitionInfo
operator|.
name|put
argument_list|(
name|newPath
operator|.
name|toUri
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|,
name|pathToPartitionInfo
operator|.
name|get
argument_list|(
name|path
argument_list|)
argument_list|)
expr_stmt|;
name|pathToPartitionInfo
operator|.
name|remove
argument_list|(
name|path
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|PartitionDesc
name|pDesc
init|=
name|work
operator|.
name|getAliasToPartnInfo
argument_list|()
operator|.
name|get
argument_list|(
name|alias
argument_list|)
operator|.
name|clone
argument_list|()
decl_stmt|;
name|pathToPartitionInfo
operator|.
name|put
argument_list|(
name|newPath
operator|.
name|toUri
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|,
name|pDesc
argument_list|)
expr_stmt|;
block|}
name|work
operator|.
name|setPathToPartitionInfo
argument_list|(
name|pathToPartitionInfo
argument_list|)
expr_stmt|;
name|String
name|onefile
init|=
name|newPath
operator|.
name|toString
argument_list|()
decl_stmt|;
name|RecordWriter
name|recWriter
init|=
name|outFileFormat
operator|.
name|newInstance
argument_list|()
operator|.
name|getHiveRecordWriter
argument_list|(
name|job
argument_list|,
name|newFilePath
argument_list|,
name|Text
operator|.
name|class
argument_list|,
literal|false
argument_list|,
name|props
argument_list|,
literal|null
argument_list|)
decl_stmt|;
if|if
condition|(
name|oneRow
condition|)
block|{
comment|// empty files are ommited at CombineHiveInputFormat.
comment|// for metadata only query, it effectively makes partition columns disappear..
comment|// this could be fixed by other methods, but this seemed to be the most easy (HIVEV-2955)
name|recWriter
operator|.
name|write
argument_list|(
operator|new
name|Text
argument_list|(
literal|"empty"
argument_list|)
argument_list|)
expr_stmt|;
comment|// written via HiveIgnoreKeyTextOutputFormat
block|}
name|recWriter
operator|.
name|close
argument_list|(
literal|false
argument_list|)
expr_stmt|;
name|FileInputFormat
operator|.
name|addInputPaths
argument_list|(
name|job
argument_list|,
name|onefile
argument_list|)
expr_stmt|;
return|return
name|numEmptyPaths
return|;
block|}
specifier|public
specifier|static
name|void
name|addInputPaths
parameter_list|(
name|JobConf
name|job
parameter_list|,
name|MapredWork
name|work
parameter_list|,
name|String
name|hiveScratchDir
parameter_list|,
name|Context
name|ctx
parameter_list|)
throws|throws
name|Exception
block|{
name|int
name|numEmptyPaths
init|=
literal|0
decl_stmt|;
name|Set
argument_list|<
name|String
argument_list|>
name|pathsProcessed
init|=
operator|new
name|HashSet
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|pathsToAdd
init|=
operator|new
name|LinkedList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
comment|// AliasToWork contains all the aliases
for|for
control|(
name|String
name|oneAlias
range|:
name|work
operator|.
name|getAliasToWork
argument_list|()
operator|.
name|keySet
argument_list|()
control|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Processing alias "
operator|+
name|oneAlias
argument_list|)
expr_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|emptyPaths
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
comment|// The alias may not have any path
name|String
name|path
init|=
literal|null
decl_stmt|;
for|for
control|(
name|String
name|onefile
range|:
name|work
operator|.
name|getPathToAliases
argument_list|()
operator|.
name|keySet
argument_list|()
control|)
block|{
name|List
argument_list|<
name|String
argument_list|>
name|aliases
init|=
name|work
operator|.
name|getPathToAliases
argument_list|()
operator|.
name|get
argument_list|(
name|onefile
argument_list|)
decl_stmt|;
if|if
condition|(
name|aliases
operator|.
name|contains
argument_list|(
name|oneAlias
argument_list|)
condition|)
block|{
name|path
operator|=
name|onefile
expr_stmt|;
comment|// Multiple aliases can point to the same path - it should be
comment|// processed only once
if|if
condition|(
name|pathsProcessed
operator|.
name|contains
argument_list|(
name|path
argument_list|)
condition|)
block|{
continue|continue;
block|}
name|pathsProcessed
operator|.
name|add
argument_list|(
name|path
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Adding input file "
operator|+
name|path
argument_list|)
expr_stmt|;
if|if
condition|(
name|Utilities
operator|.
name|isEmptyPath
argument_list|(
name|job
argument_list|,
name|path
argument_list|,
name|ctx
argument_list|)
condition|)
block|{
name|emptyPaths
operator|.
name|add
argument_list|(
name|path
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|pathsToAdd
operator|.
name|add
argument_list|(
name|path
argument_list|)
expr_stmt|;
block|}
block|}
block|}
comment|// Create a empty file if the directory is empty
for|for
control|(
name|String
name|emptyPath
range|:
name|emptyPaths
control|)
block|{
name|numEmptyPaths
operator|=
name|addInputPath
argument_list|(
name|emptyPath
argument_list|,
name|job
argument_list|,
name|work
argument_list|,
name|hiveScratchDir
argument_list|,
name|numEmptyPaths
argument_list|,
literal|true
argument_list|,
name|oneAlias
argument_list|)
expr_stmt|;
block|}
comment|// If the query references non-existent partitions
comment|// We need to add a empty file, it is not acceptable to change the
comment|// operator tree
comment|// Consider the query:
comment|// select * from (select count(1) from T union all select count(1) from
comment|// T2) x;
comment|// If T is empty and T2 contains 100 rows, the user expects: 0, 100 (2
comment|// rows)
if|if
condition|(
name|path
operator|==
literal|null
condition|)
block|{
name|numEmptyPaths
operator|=
name|addInputPath
argument_list|(
literal|null
argument_list|,
name|job
argument_list|,
name|work
argument_list|,
name|hiveScratchDir
argument_list|,
name|numEmptyPaths
argument_list|,
literal|false
argument_list|,
name|oneAlias
argument_list|)
expr_stmt|;
block|}
block|}
name|setInputPaths
argument_list|(
name|job
argument_list|,
name|pathsToAdd
argument_list|)
expr_stmt|;
block|}
specifier|private
specifier|static
name|void
name|setInputPaths
parameter_list|(
name|JobConf
name|job
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|pathsToAdd
parameter_list|)
block|{
name|Path
index|[]
name|addedPaths
init|=
name|FileInputFormat
operator|.
name|getInputPaths
argument_list|(
name|job
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|Path
argument_list|>
name|toAddPathList
init|=
operator|new
name|ArrayList
argument_list|<
name|Path
argument_list|>
argument_list|()
decl_stmt|;
if|if
condition|(
name|addedPaths
operator|!=
literal|null
condition|)
block|{
for|for
control|(
name|Path
name|added
range|:
name|addedPaths
control|)
block|{
name|toAddPathList
operator|.
name|add
argument_list|(
name|added
argument_list|)
expr_stmt|;
block|}
block|}
for|for
control|(
name|String
name|toAdd
range|:
name|pathsToAdd
control|)
block|{
name|toAddPathList
operator|.
name|add
argument_list|(
operator|new
name|Path
argument_list|(
name|toAdd
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|FileInputFormat
operator|.
name|setInputPaths
argument_list|(
name|job
argument_list|,
name|toAddPathList
operator|.
name|toArray
argument_list|(
operator|new
name|Path
index|[
literal|0
index|]
argument_list|)
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|StageType
name|getType
parameter_list|()
block|{
return|return
name|StageType
operator|.
name|MAPRED
return|;
block|}
annotation|@
name|Override
specifier|public
name|String
name|getName
parameter_list|()
block|{
return|return
literal|"MAPRED"
return|;
block|}
annotation|@
name|Override
specifier|protected
name|void
name|localizeMRTmpFilesImpl
parameter_list|(
name|Context
name|ctx
parameter_list|)
block|{
comment|// localize any map-reduce input paths
name|ctx
operator|.
name|localizeKeys
argument_list|(
call|(
name|Map
argument_list|<
name|String
argument_list|,
name|Object
argument_list|>
call|)
argument_list|(
operator|(
name|Object
operator|)
name|work
operator|.
name|getPathToAliases
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
name|ctx
operator|.
name|localizeKeys
argument_list|(
call|(
name|Map
argument_list|<
name|String
argument_list|,
name|Object
argument_list|>
call|)
argument_list|(
operator|(
name|Object
operator|)
name|work
operator|.
name|getPathToPartitionInfo
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
comment|// localize any input paths for maplocal work
name|MapredLocalWork
name|l
init|=
name|work
operator|.
name|getMapLocalWork
argument_list|()
decl_stmt|;
if|if
condition|(
name|l
operator|!=
literal|null
condition|)
block|{
name|Map
argument_list|<
name|String
argument_list|,
name|FetchWork
argument_list|>
name|m
init|=
name|l
operator|.
name|getAliasToFetchWork
argument_list|()
decl_stmt|;
if|if
condition|(
name|m
operator|!=
literal|null
condition|)
block|{
for|for
control|(
name|FetchWork
name|fw
range|:
name|m
operator|.
name|values
argument_list|()
control|)
block|{
name|String
name|s
init|=
name|fw
operator|.
name|getTblDir
argument_list|()
decl_stmt|;
if|if
condition|(
operator|(
name|s
operator|!=
literal|null
operator|)
operator|&&
name|ctx
operator|.
name|isMRTmpFileURI
argument_list|(
name|s
argument_list|)
condition|)
block|{
name|fw
operator|.
name|setTblDir
argument_list|(
name|ctx
operator|.
name|localizeMRTmpFileURI
argument_list|(
name|s
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
comment|// fix up outputs
name|Map
argument_list|<
name|String
argument_list|,
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|>
name|pa
init|=
name|work
operator|.
name|getPathToAliases
argument_list|()
decl_stmt|;
if|if
condition|(
name|pa
operator|!=
literal|null
condition|)
block|{
for|for
control|(
name|List
argument_list|<
name|String
argument_list|>
name|ls
range|:
name|pa
operator|.
name|values
argument_list|()
control|)
block|{
for|for
control|(
name|String
name|a
range|:
name|ls
control|)
block|{
name|ArrayList
argument_list|<
name|Operator
argument_list|<
name|?
extends|extends
name|OperatorDesc
argument_list|>
argument_list|>
name|opList
init|=
operator|new
name|ArrayList
argument_list|<
name|Operator
argument_list|<
name|?
extends|extends
name|OperatorDesc
argument_list|>
argument_list|>
argument_list|()
decl_stmt|;
name|opList
operator|.
name|add
argument_list|(
name|work
operator|.
name|getAliasToWork
argument_list|()
operator|.
name|get
argument_list|(
name|a
argument_list|)
argument_list|)
expr_stmt|;
while|while
condition|(
operator|!
name|opList
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|Operator
argument_list|<
name|?
extends|extends
name|OperatorDesc
argument_list|>
name|op
init|=
name|opList
operator|.
name|remove
argument_list|(
literal|0
argument_list|)
decl_stmt|;
if|if
condition|(
name|op
operator|instanceof
name|FileSinkOperator
condition|)
block|{
name|FileSinkDesc
name|fdesc
init|=
operator|(
operator|(
name|FileSinkOperator
operator|)
name|op
operator|)
operator|.
name|getConf
argument_list|()
decl_stmt|;
name|String
name|s
init|=
name|fdesc
operator|.
name|getDirName
argument_list|()
decl_stmt|;
if|if
condition|(
operator|(
name|s
operator|!=
literal|null
operator|)
operator|&&
name|ctx
operator|.
name|isMRTmpFileURI
argument_list|(
name|s
argument_list|)
condition|)
block|{
name|fdesc
operator|.
name|setDirName
argument_list|(
name|ctx
operator|.
name|localizeMRTmpFileURI
argument_list|(
name|s
argument_list|)
argument_list|)
expr_stmt|;
block|}
operator|(
operator|(
name|FileSinkOperator
operator|)
name|op
operator|)
operator|.
name|setConf
argument_list|(
name|fdesc
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|op
operator|.
name|getChildOperators
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|opList
operator|.
name|addAll
argument_list|(
name|op
operator|.
name|getChildOperators
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
block|}
block|}
annotation|@
name|Override
specifier|public
name|void
name|updateCounters
parameter_list|(
name|Counters
name|ctrs
parameter_list|,
name|RunningJob
name|rj
parameter_list|)
throws|throws
name|IOException
block|{
for|for
control|(
name|Operator
argument_list|<
name|?
extends|extends
name|OperatorDesc
argument_list|>
name|op
range|:
name|work
operator|.
name|getAliasToWork
argument_list|()
operator|.
name|values
argument_list|()
control|)
block|{
name|op
operator|.
name|updateCounters
argument_list|(
name|ctrs
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|work
operator|.
name|getReducer
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|work
operator|.
name|getReducer
argument_list|()
operator|.
name|updateCounters
argument_list|(
name|ctrs
argument_list|)
expr_stmt|;
block|}
block|}
annotation|@
name|Override
specifier|public
name|void
name|logPlanProgress
parameter_list|(
name|SessionState
name|ss
parameter_list|)
throws|throws
name|IOException
block|{
name|ss
operator|.
name|getHiveHistory
argument_list|()
operator|.
name|logPlanProgress
argument_list|(
name|queryPlan
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|shutdown
parameter_list|()
block|{
name|super
operator|.
name|shutdown
argument_list|()
expr_stmt|;
if|if
condition|(
name|rj
operator|!=
literal|null
condition|)
block|{
try|try
block|{
name|rj
operator|.
name|killJob
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"failed to kill job "
operator|+
name|rj
operator|.
name|getID
argument_list|()
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
name|rj
operator|=
literal|null
expr_stmt|;
block|}
block|}
block|}
end_class

end_unit

