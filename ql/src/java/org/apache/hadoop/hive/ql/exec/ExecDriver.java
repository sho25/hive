begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/**  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
package|;
end_package

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|File
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|FileInputStream
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|InputStream
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|Serializable
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|UnsupportedEncodingException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|net
operator|.
name|URI
import|;
end_import

begin_import
import|import
name|java
operator|.
name|net
operator|.
name|URLDecoder
import|;
end_import

begin_import
import|import
name|java
operator|.
name|net
operator|.
name|URL
import|;
end_import

begin_import
import|import
name|java
operator|.
name|net
operator|.
name|URLEncoder
import|;
end_import

begin_import
import|import
name|java
operator|.
name|text
operator|.
name|SimpleDateFormat
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Calendar
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Collections
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashSet
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|LinkedHashMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Properties
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Random
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Set
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|lang
operator|.
name|StringUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|LogFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|Log
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configuration
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|ContentSummary
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileStatus
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
operator|.
name|ConfVars
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|DriverContext
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|QueryPlan
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|FileSinkOperator
operator|.
name|RecordWriter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|errors
operator|.
name|ErrorAndSolution
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|errors
operator|.
name|TaskLogProcessor
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|history
operator|.
name|HiveHistory
operator|.
name|Keys
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|HiveKey
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|HiveOutputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|HiveException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|MapredWork
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|PartitionDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|TableDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|api
operator|.
name|StageType
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|session
operator|.
name|SessionState
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|session
operator|.
name|SessionState
operator|.
name|LogHelper
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|shims
operator|.
name|ShimLoader
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|BytesWritable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|Text
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|Counters
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|FileInputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|InputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|JobClient
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|JobConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|JobStatus
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|Partitioner
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|RunningJob
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|TaskCompletionEvent
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|log4j
operator|.
name|BasicConfigurator
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|log4j
operator|.
name|varia
operator|.
name|NullAppender
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|log4j
operator|.
name|LogManager
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|log4j
operator|.
name|PropertyConfigurator
import|;
end_import

begin_comment
comment|/**  * ExecDriver.  *  */
end_comment

begin_class
specifier|public
class|class
name|ExecDriver
extends|extends
name|Task
argument_list|<
name|MapredWork
argument_list|>
implements|implements
name|Serializable
block|{
specifier|private
specifier|static
specifier|final
name|long
name|serialVersionUID
init|=
literal|1L
decl_stmt|;
specifier|protected
specifier|transient
name|JobConf
name|job
decl_stmt|;
specifier|protected
specifier|transient
name|int
name|mapProgress
init|=
literal|0
decl_stmt|;
specifier|protected
specifier|transient
name|int
name|reduceProgress
init|=
literal|0
decl_stmt|;
specifier|protected
specifier|transient
name|boolean
name|success
init|=
literal|false
decl_stmt|;
comment|// if job execution is successful
specifier|public
specifier|static
name|Random
name|randGen
init|=
operator|new
name|Random
argument_list|()
decl_stmt|;
comment|/**    * Constructor when invoked from QL.    */
specifier|public
name|ExecDriver
parameter_list|()
block|{
name|super
argument_list|()
expr_stmt|;
block|}
specifier|public
specifier|static
name|String
name|getResourceFiles
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|SessionState
operator|.
name|ResourceType
name|t
parameter_list|)
block|{
comment|// fill in local files to be added to the task environment
name|SessionState
name|ss
init|=
name|SessionState
operator|.
name|get
argument_list|()
decl_stmt|;
name|Set
argument_list|<
name|String
argument_list|>
name|files
init|=
operator|(
name|ss
operator|==
literal|null
operator|)
condition|?
literal|null
else|:
name|ss
operator|.
name|list_resource
argument_list|(
name|t
argument_list|,
literal|null
argument_list|)
decl_stmt|;
if|if
condition|(
name|files
operator|!=
literal|null
condition|)
block|{
name|List
argument_list|<
name|String
argument_list|>
name|realFiles
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|(
name|files
operator|.
name|size
argument_list|()
argument_list|)
decl_stmt|;
for|for
control|(
name|String
name|one
range|:
name|files
control|)
block|{
try|try
block|{
name|realFiles
operator|.
name|add
argument_list|(
name|Utilities
operator|.
name|realFile
argument_list|(
name|one
argument_list|,
name|conf
argument_list|)
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
literal|"Cannot validate file "
operator|+
name|one
operator|+
literal|"due to exception: "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
return|return
name|StringUtils
operator|.
name|join
argument_list|(
name|realFiles
argument_list|,
literal|","
argument_list|)
return|;
block|}
else|else
block|{
return|return
literal|""
return|;
block|}
block|}
specifier|private
name|void
name|initializeFiles
parameter_list|(
name|String
name|prop
parameter_list|,
name|String
name|files
parameter_list|)
block|{
if|if
condition|(
name|files
operator|!=
literal|null
operator|&&
name|files
operator|.
name|length
argument_list|()
operator|>
literal|0
condition|)
block|{
name|job
operator|.
name|set
argument_list|(
name|prop
argument_list|,
name|files
argument_list|)
expr_stmt|;
name|ShimLoader
operator|.
name|getHadoopShims
argument_list|()
operator|.
name|setTmpFiles
argument_list|(
name|prop
argument_list|,
name|files
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * Initialization when invoked from QL.    */
annotation|@
name|Override
specifier|public
name|void
name|initialize
parameter_list|(
name|HiveConf
name|conf
parameter_list|,
name|QueryPlan
name|queryPlan
parameter_list|,
name|DriverContext
name|driverContext
parameter_list|)
block|{
name|super
operator|.
name|initialize
argument_list|(
name|conf
argument_list|,
name|queryPlan
argument_list|,
name|driverContext
argument_list|)
expr_stmt|;
name|job
operator|=
operator|new
name|JobConf
argument_list|(
name|conf
argument_list|,
name|ExecDriver
operator|.
name|class
argument_list|)
expr_stmt|;
comment|// NOTE: initialize is only called if it is in non-local mode.
comment|// In case it's in non-local mode, we need to move the SessionState files
comment|// and jars to jobConf.
comment|// In case it's in local mode, MapRedTask will set the jobConf.
comment|//
comment|// "tmpfiles" and "tmpjars" are set by the method ExecDriver.execute(),
comment|// which will be called by both local and NON-local mode.
name|String
name|addedFiles
init|=
name|getResourceFiles
argument_list|(
name|job
argument_list|,
name|SessionState
operator|.
name|ResourceType
operator|.
name|FILE
argument_list|)
decl_stmt|;
if|if
condition|(
name|StringUtils
operator|.
name|isNotBlank
argument_list|(
name|addedFiles
argument_list|)
condition|)
block|{
name|HiveConf
operator|.
name|setVar
argument_list|(
name|job
argument_list|,
name|ConfVars
operator|.
name|HIVEADDEDFILES
argument_list|,
name|addedFiles
argument_list|)
expr_stmt|;
block|}
name|String
name|addedJars
init|=
name|getResourceFiles
argument_list|(
name|job
argument_list|,
name|SessionState
operator|.
name|ResourceType
operator|.
name|JAR
argument_list|)
decl_stmt|;
if|if
condition|(
name|StringUtils
operator|.
name|isNotBlank
argument_list|(
name|addedJars
argument_list|)
condition|)
block|{
name|HiveConf
operator|.
name|setVar
argument_list|(
name|job
argument_list|,
name|ConfVars
operator|.
name|HIVEADDEDJARS
argument_list|,
name|addedJars
argument_list|)
expr_stmt|;
block|}
name|String
name|addedArchives
init|=
name|getResourceFiles
argument_list|(
name|job
argument_list|,
name|SessionState
operator|.
name|ResourceType
operator|.
name|ARCHIVE
argument_list|)
decl_stmt|;
if|if
condition|(
name|StringUtils
operator|.
name|isNotBlank
argument_list|(
name|addedArchives
argument_list|)
condition|)
block|{
name|HiveConf
operator|.
name|setVar
argument_list|(
name|job
argument_list|,
name|ConfVars
operator|.
name|HIVEADDEDARCHIVES
argument_list|,
name|addedArchives
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * Constructor/Initialization for invocation as independent utility.    */
specifier|public
name|ExecDriver
parameter_list|(
name|MapredWork
name|plan
parameter_list|,
name|JobConf
name|job
parameter_list|,
name|boolean
name|isSilent
parameter_list|)
throws|throws
name|HiveException
block|{
name|setWork
argument_list|(
name|plan
argument_list|)
expr_stmt|;
name|this
operator|.
name|job
operator|=
name|job
expr_stmt|;
name|LOG
operator|=
name|LogFactory
operator|.
name|getLog
argument_list|(
name|this
operator|.
name|getClass
argument_list|()
operator|.
name|getName
argument_list|()
argument_list|)
expr_stmt|;
name|console
operator|=
operator|new
name|LogHelper
argument_list|(
name|LOG
argument_list|,
name|isSilent
argument_list|)
expr_stmt|;
block|}
comment|/**    * A list of the currently running jobs spawned in this Hive instance that is    * used to kill all running jobs in the event of an unexpected shutdown -    * i.e., the JVM shuts down while there are still jobs running.    */
specifier|public
specifier|static
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|runningJobKillURIs
init|=
name|Collections
operator|.
name|synchronizedMap
argument_list|(
operator|new
name|HashMap
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
argument_list|()
argument_list|)
decl_stmt|;
comment|/**    * In Hive, when the user control-c's the command line, any running jobs    * spawned from that command line are best-effort killed.    *    * This static constructor registers a shutdown thread to iterate over all the    * running job kill URLs and do a get on them.    *    */
static|static
block|{
if|if
condition|(
operator|new
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configuration
argument_list|()
operator|.
name|getBoolean
argument_list|(
literal|"webinterface.private.actions"
argument_list|,
literal|false
argument_list|)
condition|)
block|{
name|Runtime
operator|.
name|getRuntime
argument_list|()
operator|.
name|addShutdownHook
argument_list|(
operator|new
name|Thread
argument_list|()
block|{
annotation|@
name|Override
specifier|public
name|void
name|run
parameter_list|()
block|{
synchronized|synchronized
init|(
name|runningJobKillURIs
init|)
block|{
for|for
control|(
name|String
name|uri
range|:
name|runningJobKillURIs
operator|.
name|values
argument_list|()
control|)
block|{
try|try
block|{
name|System
operator|.
name|err
operator|.
name|println
argument_list|(
literal|"killing job with: "
operator|+
name|uri
argument_list|)
expr_stmt|;
name|java
operator|.
name|net
operator|.
name|HttpURLConnection
name|conn
init|=
operator|(
name|java
operator|.
name|net
operator|.
name|HttpURLConnection
operator|)
operator|new
name|java
operator|.
name|net
operator|.
name|URL
argument_list|(
name|uri
argument_list|)
operator|.
name|openConnection
argument_list|()
decl_stmt|;
name|conn
operator|.
name|setRequestMethod
argument_list|(
literal|"POST"
argument_list|)
expr_stmt|;
name|int
name|retCode
init|=
name|conn
operator|.
name|getResponseCode
argument_list|()
decl_stmt|;
if|if
condition|(
name|retCode
operator|!=
literal|200
condition|)
block|{
name|System
operator|.
name|err
operator|.
name|println
argument_list|(
literal|"Got an error trying to kill job with URI: "
operator|+
name|uri
operator|+
literal|" = "
operator|+
name|retCode
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|System
operator|.
name|err
operator|.
name|println
argument_list|(
literal|"trying to kill job, caught: "
operator|+
name|e
argument_list|)
expr_stmt|;
comment|// do nothing
block|}
block|}
block|}
block|}
block|}
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * from StreamJob.java.    */
specifier|public
name|void
name|jobInfo
parameter_list|(
name|RunningJob
name|rj
parameter_list|)
block|{
if|if
condition|(
name|job
operator|.
name|get
argument_list|(
literal|"mapred.job.tracker"
argument_list|,
literal|"local"
argument_list|)
operator|.
name|equals
argument_list|(
literal|"local"
argument_list|)
condition|)
block|{
name|console
operator|.
name|printInfo
argument_list|(
literal|"Job running in-process (local Hadoop)"
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|String
name|hp
init|=
name|job
operator|.
name|get
argument_list|(
literal|"mapred.job.tracker"
argument_list|)
decl_stmt|;
if|if
condition|(
name|SessionState
operator|.
name|get
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|SessionState
operator|.
name|get
argument_list|()
operator|.
name|getHiveHistory
argument_list|()
operator|.
name|setTaskProperty
argument_list|(
name|SessionState
operator|.
name|get
argument_list|()
operator|.
name|getQueryId
argument_list|()
argument_list|,
name|getId
argument_list|()
argument_list|,
name|Keys
operator|.
name|TASK_HADOOP_ID
argument_list|,
name|rj
operator|.
name|getJobID
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|console
operator|.
name|printInfo
argument_list|(
name|ExecDriver
operator|.
name|getJobStartMsg
argument_list|(
name|rj
operator|.
name|getJobID
argument_list|()
argument_list|)
operator|+
literal|", Tracking URL = "
operator|+
name|rj
operator|.
name|getTrackingURL
argument_list|()
argument_list|)
expr_stmt|;
name|console
operator|.
name|printInfo
argument_list|(
literal|"Kill Command = "
operator|+
name|HiveConf
operator|.
name|getVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HADOOPBIN
argument_list|)
operator|+
literal|" job  -Dmapred.job.tracker="
operator|+
name|hp
operator|+
literal|" -kill "
operator|+
name|rj
operator|.
name|getJobID
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * This class contains the state of the running task Going forward, we will    * return this handle from execute and Driver can split execute into start,    * monitorProgess and postProcess.    */
specifier|public
specifier|static
class|class
name|ExecDriverTaskHandle
extends|extends
name|TaskHandle
block|{
name|JobClient
name|jc
decl_stmt|;
name|RunningJob
name|rj
decl_stmt|;
name|JobClient
name|getJobClient
parameter_list|()
block|{
return|return
name|jc
return|;
block|}
name|RunningJob
name|getRunningJob
parameter_list|()
block|{
return|return
name|rj
return|;
block|}
specifier|public
name|ExecDriverTaskHandle
parameter_list|(
name|JobClient
name|jc
parameter_list|,
name|RunningJob
name|rj
parameter_list|)
block|{
name|this
operator|.
name|jc
operator|=
name|jc
expr_stmt|;
name|this
operator|.
name|rj
operator|=
name|rj
expr_stmt|;
block|}
specifier|public
name|void
name|setRunningJob
parameter_list|(
name|RunningJob
name|job
parameter_list|)
block|{
name|rj
operator|=
name|job
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|Counters
name|getCounters
parameter_list|()
throws|throws
name|IOException
block|{
return|return
name|rj
operator|.
name|getCounters
argument_list|()
return|;
block|}
block|}
comment|/**    * Fatal errors are those errors that cannot be recovered by retries. These    * are application dependent. Examples of fatal errors include: - the small    * table in the map-side joins is too large to be feasible to be handled by    * one mapper. The job should fail and the user should be warned to use    * regular joins rather than map-side joins. Fatal errors are indicated by    * counters that are set at execution time. If the counter is non-zero, a    * fatal error occurred. The value of the counter indicates the error type.    *    * @return true if fatal errors happened during job execution, false    *         otherwise.    */
specifier|protected
name|boolean
name|checkFatalErrors
parameter_list|(
name|TaskHandle
name|t
parameter_list|,
name|StringBuilder
name|errMsg
parameter_list|)
block|{
name|ExecDriverTaskHandle
name|th
init|=
operator|(
name|ExecDriverTaskHandle
operator|)
name|t
decl_stmt|;
name|RunningJob
name|rj
init|=
name|th
operator|.
name|getRunningJob
argument_list|()
decl_stmt|;
try|try
block|{
name|Counters
name|ctrs
init|=
name|th
operator|.
name|getCounters
argument_list|()
decl_stmt|;
comment|// HIVE-1422
if|if
condition|(
name|ctrs
operator|==
literal|null
condition|)
block|{
return|return
literal|false
return|;
block|}
for|for
control|(
name|Operator
argument_list|<
name|?
extends|extends
name|Serializable
argument_list|>
name|op
range|:
name|work
operator|.
name|getAliasToWork
argument_list|()
operator|.
name|values
argument_list|()
control|)
block|{
if|if
condition|(
name|op
operator|.
name|checkFatalErrors
argument_list|(
name|ctrs
argument_list|,
name|errMsg
argument_list|)
condition|)
block|{
return|return
literal|true
return|;
block|}
block|}
if|if
condition|(
name|work
operator|.
name|getReducer
argument_list|()
operator|!=
literal|null
condition|)
block|{
if|if
condition|(
name|work
operator|.
name|getReducer
argument_list|()
operator|.
name|checkFatalErrors
argument_list|(
name|ctrs
argument_list|,
name|errMsg
argument_list|)
condition|)
block|{
return|return
literal|true
return|;
block|}
block|}
return|return
literal|false
return|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
comment|// this exception can be tolerated
name|e
operator|.
name|printStackTrace
argument_list|()
expr_stmt|;
return|return
literal|false
return|;
block|}
block|}
annotation|@
name|Override
specifier|public
name|void
name|progress
parameter_list|(
name|TaskHandle
name|taskHandle
parameter_list|)
throws|throws
name|IOException
block|{
name|ExecDriverTaskHandle
name|th
init|=
operator|(
name|ExecDriverTaskHandle
operator|)
name|taskHandle
decl_stmt|;
name|JobClient
name|jc
init|=
name|th
operator|.
name|getJobClient
argument_list|()
decl_stmt|;
name|RunningJob
name|rj
init|=
name|th
operator|.
name|getRunningJob
argument_list|()
decl_stmt|;
name|String
name|lastReport
init|=
literal|""
decl_stmt|;
name|SimpleDateFormat
name|dateFormat
init|=
operator|new
name|SimpleDateFormat
argument_list|(
literal|"yyyy-MM-dd HH:mm:ss,SSS"
argument_list|)
decl_stmt|;
name|long
name|reportTime
init|=
name|System
operator|.
name|currentTimeMillis
argument_list|()
decl_stmt|;
name|long
name|maxReportInterval
init|=
literal|60
operator|*
literal|1000
decl_stmt|;
comment|// One minute
name|boolean
name|fatal
init|=
literal|false
decl_stmt|;
name|StringBuilder
name|errMsg
init|=
operator|new
name|StringBuilder
argument_list|()
decl_stmt|;
name|long
name|pullInterval
init|=
name|HiveConf
operator|.
name|getLongVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVECOUNTERSPULLINTERVAL
argument_list|)
decl_stmt|;
name|boolean
name|initializing
init|=
literal|true
decl_stmt|;
while|while
condition|(
operator|!
name|rj
operator|.
name|isComplete
argument_list|()
condition|)
block|{
try|try
block|{
name|Thread
operator|.
name|sleep
argument_list|(
name|pullInterval
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|InterruptedException
name|e
parameter_list|)
block|{       }
if|if
condition|(
name|initializing
operator|&&
name|ShimLoader
operator|.
name|getHadoopShims
argument_list|()
operator|.
name|isJobPreparing
argument_list|(
name|rj
argument_list|)
condition|)
block|{
comment|// No reason to poll untill the job is initialized
continue|continue;
block|}
else|else
block|{
comment|// By now the job is initialized so no reason to do
comment|// rj.getJobState() again and we do not want to do an extra RPC call
name|initializing
operator|=
literal|false
expr_stmt|;
block|}
name|th
operator|.
name|setRunningJob
argument_list|(
name|jc
operator|.
name|getJob
argument_list|(
name|rj
operator|.
name|getJobID
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
comment|// If fatal errors happen we should kill the job immediately rather than
comment|// let the job retry several times, which eventually lead to failure.
if|if
condition|(
name|fatal
condition|)
block|{
continue|continue;
comment|// wait until rj.isComplete
block|}
if|if
condition|(
name|fatal
operator|=
name|checkFatalErrors
argument_list|(
name|th
argument_list|,
name|errMsg
argument_list|)
condition|)
block|{
name|success
operator|=
literal|false
expr_stmt|;
name|console
operator|.
name|printError
argument_list|(
literal|"[Fatal Error] "
operator|+
name|errMsg
operator|.
name|toString
argument_list|()
operator|+
literal|". Killing the job."
argument_list|)
expr_stmt|;
name|rj
operator|.
name|killJob
argument_list|()
expr_stmt|;
continue|continue;
block|}
name|errMsg
operator|.
name|setLength
argument_list|(
literal|0
argument_list|)
expr_stmt|;
name|updateCounters
argument_list|(
name|th
argument_list|)
expr_stmt|;
name|String
name|report
init|=
literal|" "
operator|+
name|getId
argument_list|()
operator|+
literal|" map = "
operator|+
name|mapProgress
operator|+
literal|"%,  reduce = "
operator|+
name|reduceProgress
operator|+
literal|"%"
decl_stmt|;
if|if
condition|(
operator|!
name|report
operator|.
name|equals
argument_list|(
name|lastReport
argument_list|)
operator|||
name|System
operator|.
name|currentTimeMillis
argument_list|()
operator|>=
name|reportTime
operator|+
name|maxReportInterval
condition|)
block|{
comment|// write out serialized plan with counters to log file
comment|// LOG.info(queryPlan);
name|String
name|output
init|=
name|dateFormat
operator|.
name|format
argument_list|(
name|Calendar
operator|.
name|getInstance
argument_list|()
operator|.
name|getTime
argument_list|()
argument_list|)
operator|+
name|report
decl_stmt|;
name|SessionState
name|ss
init|=
name|SessionState
operator|.
name|get
argument_list|()
decl_stmt|;
if|if
condition|(
name|ss
operator|!=
literal|null
condition|)
block|{
name|ss
operator|.
name|getHiveHistory
argument_list|()
operator|.
name|setTaskCounters
argument_list|(
name|SessionState
operator|.
name|get
argument_list|()
operator|.
name|getQueryId
argument_list|()
argument_list|,
name|getId
argument_list|()
argument_list|,
name|rj
argument_list|)
expr_stmt|;
name|ss
operator|.
name|getHiveHistory
argument_list|()
operator|.
name|setTaskProperty
argument_list|(
name|SessionState
operator|.
name|get
argument_list|()
operator|.
name|getQueryId
argument_list|()
argument_list|,
name|getId
argument_list|()
argument_list|,
name|Keys
operator|.
name|TASK_HADOOP_PROGRESS
argument_list|,
name|output
argument_list|)
expr_stmt|;
name|ss
operator|.
name|getHiveHistory
argument_list|()
operator|.
name|progressTask
argument_list|(
name|SessionState
operator|.
name|get
argument_list|()
operator|.
name|getQueryId
argument_list|()
argument_list|,
name|this
argument_list|)
expr_stmt|;
name|ss
operator|.
name|getHiveHistory
argument_list|()
operator|.
name|logPlanProgress
argument_list|(
name|queryPlan
argument_list|)
expr_stmt|;
block|}
name|console
operator|.
name|printInfo
argument_list|(
name|output
argument_list|)
expr_stmt|;
name|lastReport
operator|=
name|report
expr_stmt|;
name|reportTime
operator|=
name|System
operator|.
name|currentTimeMillis
argument_list|()
expr_stmt|;
block|}
block|}
comment|// check for fatal error again in case it occurred after the last check
comment|// before the job is completed
if|if
condition|(
operator|!
name|fatal
operator|&&
operator|(
name|fatal
operator|=
name|checkFatalErrors
argument_list|(
name|th
argument_list|,
name|errMsg
argument_list|)
operator|)
condition|)
block|{
name|console
operator|.
name|printError
argument_list|(
literal|"[Fatal Error] "
operator|+
name|errMsg
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
name|success
operator|=
literal|false
expr_stmt|;
block|}
else|else
block|{
name|success
operator|=
name|rj
operator|.
name|isSuccessful
argument_list|()
expr_stmt|;
block|}
name|setDone
argument_list|()
expr_stmt|;
name|th
operator|.
name|setRunningJob
argument_list|(
name|jc
operator|.
name|getJob
argument_list|(
name|rj
operator|.
name|getJobID
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
name|updateCounters
argument_list|(
name|th
argument_list|)
expr_stmt|;
name|SessionState
name|ss
init|=
name|SessionState
operator|.
name|get
argument_list|()
decl_stmt|;
if|if
condition|(
name|ss
operator|!=
literal|null
condition|)
block|{
name|ss
operator|.
name|getHiveHistory
argument_list|()
operator|.
name|logPlanProgress
argument_list|(
name|queryPlan
argument_list|)
expr_stmt|;
block|}
comment|// LOG.info(queryPlan);
block|}
comment|/**    * Estimate the number of reducers needed for this job, based on job input,    * and configuration parameters.    *    * @return the number of reducers.    */
specifier|public
name|int
name|estimateNumberOfReducers
parameter_list|(
name|HiveConf
name|hive
parameter_list|,
name|JobConf
name|job
parameter_list|,
name|MapredWork
name|work
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|hive
operator|==
literal|null
condition|)
block|{
name|hive
operator|=
operator|new
name|HiveConf
argument_list|()
expr_stmt|;
block|}
name|long
name|bytesPerReducer
init|=
name|hive
operator|.
name|getLongVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|BYTESPERREDUCER
argument_list|)
decl_stmt|;
name|int
name|maxReducers
init|=
name|hive
operator|.
name|getIntVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|MAXREDUCERS
argument_list|)
decl_stmt|;
name|long
name|totalInputFileSize
init|=
name|getTotalInputFileSize
argument_list|(
name|job
argument_list|,
name|work
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"BytesPerReducer="
operator|+
name|bytesPerReducer
operator|+
literal|" maxReducers="
operator|+
name|maxReducers
operator|+
literal|" totalInputFileSize="
operator|+
name|totalInputFileSize
argument_list|)
expr_stmt|;
name|int
name|reducers
init|=
call|(
name|int
call|)
argument_list|(
operator|(
name|totalInputFileSize
operator|+
name|bytesPerReducer
operator|-
literal|1
operator|)
operator|/
name|bytesPerReducer
argument_list|)
decl_stmt|;
name|reducers
operator|=
name|Math
operator|.
name|max
argument_list|(
literal|1
argument_list|,
name|reducers
argument_list|)
expr_stmt|;
name|reducers
operator|=
name|Math
operator|.
name|min
argument_list|(
name|maxReducers
argument_list|,
name|reducers
argument_list|)
expr_stmt|;
return|return
name|reducers
return|;
block|}
comment|/**    * Set the number of reducers for the mapred work.    */
specifier|protected
name|void
name|setNumberOfReducers
parameter_list|()
throws|throws
name|IOException
block|{
comment|// this is a temporary hack to fix things that are not fixed in the compiler
name|Integer
name|numReducersFromWork
init|=
name|work
operator|.
name|getNumReduceTasks
argument_list|()
decl_stmt|;
if|if
condition|(
name|work
operator|.
name|getReducer
argument_list|()
operator|==
literal|null
condition|)
block|{
name|console
operator|.
name|printInfo
argument_list|(
literal|"Number of reduce tasks is set to 0 since there's no reduce operator"
argument_list|)
expr_stmt|;
name|work
operator|.
name|setNumReduceTasks
argument_list|(
name|Integer
operator|.
name|valueOf
argument_list|(
literal|0
argument_list|)
argument_list|)
expr_stmt|;
block|}
else|else
block|{
if|if
condition|(
name|numReducersFromWork
operator|>=
literal|0
condition|)
block|{
name|console
operator|.
name|printInfo
argument_list|(
literal|"Number of reduce tasks determined at compile time: "
operator|+
name|work
operator|.
name|getNumReduceTasks
argument_list|()
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|job
operator|.
name|getNumReduceTasks
argument_list|()
operator|>
literal|0
condition|)
block|{
name|int
name|reducers
init|=
name|job
operator|.
name|getNumReduceTasks
argument_list|()
decl_stmt|;
name|work
operator|.
name|setNumReduceTasks
argument_list|(
name|reducers
argument_list|)
expr_stmt|;
name|console
operator|.
name|printInfo
argument_list|(
literal|"Number of reduce tasks not specified. Defaulting to jobconf value of: "
operator|+
name|reducers
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|int
name|reducers
init|=
name|estimateNumberOfReducers
argument_list|(
name|conf
argument_list|,
name|job
argument_list|,
name|work
argument_list|)
decl_stmt|;
name|work
operator|.
name|setNumReduceTasks
argument_list|(
name|reducers
argument_list|)
expr_stmt|;
name|console
operator|.
name|printInfo
argument_list|(
literal|"Number of reduce tasks not specified. Estimated from input data size: "
operator|+
name|reducers
argument_list|)
expr_stmt|;
block|}
name|console
operator|.
name|printInfo
argument_list|(
literal|"In order to change the average load for a reducer (in bytes):"
argument_list|)
expr_stmt|;
name|console
operator|.
name|printInfo
argument_list|(
literal|"  set "
operator|+
name|HiveConf
operator|.
name|ConfVars
operator|.
name|BYTESPERREDUCER
operator|.
name|varname
operator|+
literal|"=<number>"
argument_list|)
expr_stmt|;
name|console
operator|.
name|printInfo
argument_list|(
literal|"In order to limit the maximum number of reducers:"
argument_list|)
expr_stmt|;
name|console
operator|.
name|printInfo
argument_list|(
literal|"  set "
operator|+
name|HiveConf
operator|.
name|ConfVars
operator|.
name|MAXREDUCERS
operator|.
name|varname
operator|+
literal|"=<number>"
argument_list|)
expr_stmt|;
name|console
operator|.
name|printInfo
argument_list|(
literal|"In order to set a constant number of reducers:"
argument_list|)
expr_stmt|;
name|console
operator|.
name|printInfo
argument_list|(
literal|"  set "
operator|+
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HADOOPNUMREDUCERS
operator|+
literal|"=<number>"
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * Calculate the total size of input files.    *    * @param job    *          the hadoop job conf.    * @return the total size in bytes.    * @throws IOException    */
specifier|public
name|long
name|getTotalInputFileSize
parameter_list|(
name|JobConf
name|job
parameter_list|,
name|MapredWork
name|work
parameter_list|)
throws|throws
name|IOException
block|{
name|long
name|r
init|=
literal|0
decl_stmt|;
comment|// For each input path, calculate the total size.
for|for
control|(
name|String
name|path
range|:
name|work
operator|.
name|getPathToAliases
argument_list|()
operator|.
name|keySet
argument_list|()
control|)
block|{
try|try
block|{
name|Path
name|p
init|=
operator|new
name|Path
argument_list|(
name|path
argument_list|)
decl_stmt|;
name|FileSystem
name|fs
init|=
name|p
operator|.
name|getFileSystem
argument_list|(
name|job
argument_list|)
decl_stmt|;
name|ContentSummary
name|cs
init|=
name|fs
operator|.
name|getContentSummary
argument_list|(
name|p
argument_list|)
decl_stmt|;
name|r
operator|+=
name|cs
operator|.
name|getLength
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Cannot get size of "
operator|+
name|path
operator|+
literal|". Safely ignored."
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|r
return|;
block|}
comment|/**    * Update counters relevant to this task.    */
annotation|@
name|Override
specifier|public
name|void
name|updateCounters
parameter_list|(
name|TaskHandle
name|t
parameter_list|)
throws|throws
name|IOException
block|{
name|ExecDriverTaskHandle
name|th
init|=
operator|(
name|ExecDriverTaskHandle
operator|)
name|t
decl_stmt|;
name|RunningJob
name|rj
init|=
name|th
operator|.
name|getRunningJob
argument_list|()
decl_stmt|;
name|mapProgress
operator|=
name|Math
operator|.
name|round
argument_list|(
name|rj
operator|.
name|mapProgress
argument_list|()
operator|*
literal|100
argument_list|)
expr_stmt|;
name|reduceProgress
operator|=
name|Math
operator|.
name|round
argument_list|(
name|rj
operator|.
name|reduceProgress
argument_list|()
operator|*
literal|100
argument_list|)
expr_stmt|;
name|taskCounters
operator|.
name|put
argument_list|(
literal|"CNTR_NAME_"
operator|+
name|getId
argument_list|()
operator|+
literal|"_MAP_PROGRESS"
argument_list|,
name|Long
operator|.
name|valueOf
argument_list|(
name|mapProgress
argument_list|)
argument_list|)
expr_stmt|;
name|taskCounters
operator|.
name|put
argument_list|(
literal|"CNTR_NAME_"
operator|+
name|getId
argument_list|()
operator|+
literal|"_REDUCE_PROGRESS"
argument_list|,
name|Long
operator|.
name|valueOf
argument_list|(
name|reduceProgress
argument_list|)
argument_list|)
expr_stmt|;
name|Counters
name|ctrs
init|=
name|th
operator|.
name|getCounters
argument_list|()
decl_stmt|;
comment|// HIVE-1422
if|if
condition|(
name|ctrs
operator|==
literal|null
condition|)
block|{
return|return;
block|}
for|for
control|(
name|Operator
argument_list|<
name|?
extends|extends
name|Serializable
argument_list|>
name|op
range|:
name|work
operator|.
name|getAliasToWork
argument_list|()
operator|.
name|values
argument_list|()
control|)
block|{
name|op
operator|.
name|updateCounters
argument_list|(
name|ctrs
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|work
operator|.
name|getReducer
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|work
operator|.
name|getReducer
argument_list|()
operator|.
name|updateCounters
argument_list|(
name|ctrs
argument_list|)
expr_stmt|;
block|}
block|}
specifier|public
name|boolean
name|mapStarted
parameter_list|()
block|{
return|return
name|mapProgress
operator|>
literal|0
return|;
block|}
specifier|public
name|boolean
name|reduceStarted
parameter_list|()
block|{
return|return
name|reduceProgress
operator|>
literal|0
return|;
block|}
specifier|public
name|boolean
name|mapDone
parameter_list|()
block|{
return|return
name|mapProgress
operator|==
literal|100
return|;
block|}
specifier|public
name|boolean
name|reduceDone
parameter_list|()
block|{
return|return
name|reduceProgress
operator|==
literal|100
return|;
block|}
comment|/**    * Execute a query plan using Hadoop.    */
annotation|@
name|Override
specifier|public
name|int
name|execute
parameter_list|(
name|DriverContext
name|driverContext
parameter_list|)
block|{
name|success
operator|=
literal|true
expr_stmt|;
try|try
block|{
name|setNumberOfReducers
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|String
name|statusMesg
init|=
literal|"IOException while accessing HDFS to estimate the number of reducers: "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
decl_stmt|;
name|console
operator|.
name|printError
argument_list|(
name|statusMesg
argument_list|,
literal|"\n"
operator|+
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
return|return
literal|1
return|;
block|}
name|String
name|invalidReason
init|=
name|work
operator|.
name|isInvalid
argument_list|()
decl_stmt|;
if|if
condition|(
name|invalidReason
operator|!=
literal|null
condition|)
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
literal|"Plan invalid, Reason: "
operator|+
name|invalidReason
argument_list|)
throw|;
block|}
name|String
name|hiveScratchDir
decl_stmt|;
if|if
condition|(
name|driverContext
operator|.
name|getCtx
argument_list|()
operator|!=
literal|null
operator|&&
name|driverContext
operator|.
name|getCtx
argument_list|()
operator|.
name|getQueryPath
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|hiveScratchDir
operator|=
name|driverContext
operator|.
name|getCtx
argument_list|()
operator|.
name|getQueryPath
argument_list|()
operator|.
name|toString
argument_list|()
expr_stmt|;
block|}
else|else
block|{
name|hiveScratchDir
operator|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|SCRATCHDIR
argument_list|)
expr_stmt|;
block|}
name|String
name|emptyScratchDirStr
init|=
literal|null
decl_stmt|;
name|Path
name|emptyScratchDir
init|=
literal|null
decl_stmt|;
name|int
name|numTries
init|=
literal|3
decl_stmt|;
while|while
condition|(
name|numTries
operator|>
literal|0
condition|)
block|{
name|emptyScratchDirStr
operator|=
name|hiveScratchDir
operator|+
name|File
operator|.
name|separator
operator|+
name|Utilities
operator|.
name|randGen
operator|.
name|nextInt
argument_list|()
expr_stmt|;
name|emptyScratchDir
operator|=
operator|new
name|Path
argument_list|(
name|emptyScratchDirStr
argument_list|)
expr_stmt|;
try|try
block|{
name|FileSystem
name|fs
init|=
name|emptyScratchDir
operator|.
name|getFileSystem
argument_list|(
name|job
argument_list|)
decl_stmt|;
name|fs
operator|.
name|mkdirs
argument_list|(
name|emptyScratchDir
argument_list|)
expr_stmt|;
break|break;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
if|if
condition|(
name|numTries
operator|>
literal|0
condition|)
block|{
name|numTries
operator|--
expr_stmt|;
block|}
else|else
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
literal|"Failed to make dir "
operator|+
name|emptyScratchDir
operator|.
name|toString
argument_list|()
operator|+
literal|" : "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|)
throw|;
block|}
block|}
block|}
name|ShimLoader
operator|.
name|getHadoopShims
argument_list|()
operator|.
name|setNullOutputFormat
argument_list|(
name|job
argument_list|)
expr_stmt|;
name|job
operator|.
name|setMapperClass
argument_list|(
name|ExecMapper
operator|.
name|class
argument_list|)
expr_stmt|;
name|job
operator|.
name|setMapOutputKeyClass
argument_list|(
name|HiveKey
operator|.
name|class
argument_list|)
expr_stmt|;
name|job
operator|.
name|setMapOutputValueClass
argument_list|(
name|BytesWritable
operator|.
name|class
argument_list|)
expr_stmt|;
try|try
block|{
name|job
operator|.
name|setPartitionerClass
argument_list|(
call|(
name|Class
argument_list|<
name|?
extends|extends
name|Partitioner
argument_list|>
call|)
argument_list|(
name|Class
operator|.
name|forName
argument_list|(
name|HiveConf
operator|.
name|getVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEPARTITIONER
argument_list|)
argument_list|)
argument_list|)
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|ClassNotFoundException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
name|e
operator|.
name|getMessage
argument_list|()
argument_list|)
throw|;
block|}
if|if
condition|(
name|work
operator|.
name|getNumMapTasks
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|job
operator|.
name|setNumMapTasks
argument_list|(
name|work
operator|.
name|getNumMapTasks
argument_list|()
operator|.
name|intValue
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|work
operator|.
name|getMinSplitSize
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|HiveConf
operator|.
name|setIntVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|MAPREDMINSPLITSIZE
argument_list|,
name|work
operator|.
name|getMinSplitSize
argument_list|()
operator|.
name|intValue
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|job
operator|.
name|setNumReduceTasks
argument_list|(
name|work
operator|.
name|getNumReduceTasks
argument_list|()
operator|.
name|intValue
argument_list|()
argument_list|)
expr_stmt|;
name|job
operator|.
name|setReducerClass
argument_list|(
name|ExecReducer
operator|.
name|class
argument_list|)
expr_stmt|;
if|if
condition|(
name|work
operator|.
name|getInputformat
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|HiveConf
operator|.
name|setVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEINPUTFORMAT
argument_list|,
name|work
operator|.
name|getInputformat
argument_list|()
argument_list|)
expr_stmt|;
block|}
comment|// Turn on speculative execution for reducers
name|boolean
name|useSpeculativeExecReducers
init|=
name|HiveConf
operator|.
name|getBoolVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVESPECULATIVEEXECREDUCERS
argument_list|)
decl_stmt|;
name|HiveConf
operator|.
name|setBoolVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HADOOPSPECULATIVEEXECREDUCERS
argument_list|,
name|useSpeculativeExecReducers
argument_list|)
expr_stmt|;
name|String
name|inpFormat
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEINPUTFORMAT
argument_list|)
decl_stmt|;
if|if
condition|(
operator|(
name|inpFormat
operator|==
literal|null
operator|)
operator|||
operator|(
operator|!
name|StringUtils
operator|.
name|isNotBlank
argument_list|(
name|inpFormat
argument_list|)
operator|)
condition|)
block|{
name|inpFormat
operator|=
name|ShimLoader
operator|.
name|getHadoopShims
argument_list|()
operator|.
name|getInputFormatClassName
argument_list|()
expr_stmt|;
block|}
name|LOG
operator|.
name|info
argument_list|(
literal|"Using "
operator|+
name|inpFormat
argument_list|)
expr_stmt|;
try|try
block|{
name|job
operator|.
name|setInputFormat
argument_list|(
call|(
name|Class
argument_list|<
name|?
extends|extends
name|InputFormat
argument_list|>
call|)
argument_list|(
name|Class
operator|.
name|forName
argument_list|(
name|inpFormat
argument_list|)
argument_list|)
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|ClassNotFoundException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
name|e
operator|.
name|getMessage
argument_list|()
argument_list|)
throw|;
block|}
comment|// No-Op - we don't really write anything here ..
name|job
operator|.
name|setOutputKeyClass
argument_list|(
name|Text
operator|.
name|class
argument_list|)
expr_stmt|;
name|job
operator|.
name|setOutputValueClass
argument_list|(
name|Text
operator|.
name|class
argument_list|)
expr_stmt|;
comment|// Transfer HIVEAUXJARS and HIVEADDEDJARS to "tmpjars" so hadoop understands
comment|// it
name|String
name|auxJars
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEAUXJARS
argument_list|)
decl_stmt|;
name|String
name|addedJars
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEADDEDJARS
argument_list|)
decl_stmt|;
if|if
condition|(
name|StringUtils
operator|.
name|isNotBlank
argument_list|(
name|auxJars
argument_list|)
operator|||
name|StringUtils
operator|.
name|isNotBlank
argument_list|(
name|addedJars
argument_list|)
condition|)
block|{
name|String
name|allJars
init|=
name|StringUtils
operator|.
name|isNotBlank
argument_list|(
name|auxJars
argument_list|)
condition|?
operator|(
name|StringUtils
operator|.
name|isNotBlank
argument_list|(
name|addedJars
argument_list|)
condition|?
name|addedJars
operator|+
literal|","
operator|+
name|auxJars
else|:
name|auxJars
operator|)
else|:
name|addedJars
decl_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"adding libjars: "
operator|+
name|allJars
argument_list|)
expr_stmt|;
name|initializeFiles
argument_list|(
literal|"tmpjars"
argument_list|,
name|allJars
argument_list|)
expr_stmt|;
block|}
comment|// Transfer HIVEADDEDFILES to "tmpfiles" so hadoop understands it
name|String
name|addedFiles
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEADDEDFILES
argument_list|)
decl_stmt|;
if|if
condition|(
name|StringUtils
operator|.
name|isNotBlank
argument_list|(
name|addedFiles
argument_list|)
condition|)
block|{
name|initializeFiles
argument_list|(
literal|"tmpfiles"
argument_list|,
name|addedFiles
argument_list|)
expr_stmt|;
block|}
comment|// Transfer HIVEADDEDARCHIVES to "tmparchives" so hadoop understands it
name|String
name|addedArchives
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEADDEDARCHIVES
argument_list|)
decl_stmt|;
if|if
condition|(
name|StringUtils
operator|.
name|isNotBlank
argument_list|(
name|addedArchives
argument_list|)
condition|)
block|{
name|initializeFiles
argument_list|(
literal|"tmparchives"
argument_list|,
name|addedArchives
argument_list|)
expr_stmt|;
block|}
name|int
name|returnVal
init|=
literal|0
decl_stmt|;
name|RunningJob
name|rj
init|=
literal|null
decl_stmt|,
name|orig_rj
init|=
literal|null
decl_stmt|;
name|boolean
name|noName
init|=
name|StringUtils
operator|.
name|isEmpty
argument_list|(
name|HiveConf
operator|.
name|getVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HADOOPJOBNAME
argument_list|)
argument_list|)
decl_stmt|;
if|if
condition|(
name|noName
condition|)
block|{
comment|// This is for a special case to ensure unit tests pass
name|HiveConf
operator|.
name|setVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HADOOPJOBNAME
argument_list|,
literal|"JOB"
operator|+
name|randGen
operator|.
name|nextInt
argument_list|()
argument_list|)
expr_stmt|;
block|}
try|try
block|{
name|addInputPaths
argument_list|(
name|job
argument_list|,
name|work
argument_list|,
name|emptyScratchDirStr
argument_list|)
expr_stmt|;
name|Utilities
operator|.
name|setMapRedWork
argument_list|(
name|job
argument_list|,
name|work
argument_list|,
name|hiveScratchDir
argument_list|)
expr_stmt|;
comment|// remove the pwd from conf file so that job tracker doesn't show this
comment|// logs
name|String
name|pwd
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|METASTOREPWD
argument_list|)
decl_stmt|;
if|if
condition|(
name|pwd
operator|!=
literal|null
condition|)
block|{
name|HiveConf
operator|.
name|setVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|METASTOREPWD
argument_list|,
literal|"HIVE"
argument_list|)
expr_stmt|;
block|}
name|JobClient
name|jc
init|=
operator|new
name|JobClient
argument_list|(
name|job
argument_list|)
decl_stmt|;
comment|// make this client wait if job trcker is not behaving well.
name|Throttle
operator|.
name|checkJobTracker
argument_list|(
name|job
argument_list|,
name|LOG
argument_list|)
expr_stmt|;
name|orig_rj
operator|=
name|rj
operator|=
name|jc
operator|.
name|submitJob
argument_list|(
name|job
argument_list|)
expr_stmt|;
comment|// replace it back
if|if
condition|(
name|pwd
operator|!=
literal|null
condition|)
block|{
name|HiveConf
operator|.
name|setVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|METASTOREPWD
argument_list|,
name|pwd
argument_list|)
expr_stmt|;
block|}
comment|// add to list of running jobs so in case of abnormal shutdown can kill
comment|// it.
name|runningJobKillURIs
operator|.
name|put
argument_list|(
name|rj
operator|.
name|getJobID
argument_list|()
argument_list|,
name|rj
operator|.
name|getTrackingURL
argument_list|()
operator|+
literal|"&action=kill"
argument_list|)
expr_stmt|;
name|TaskHandle
name|th
init|=
operator|new
name|ExecDriverTaskHandle
argument_list|(
name|jc
argument_list|,
name|rj
argument_list|)
decl_stmt|;
name|jobInfo
argument_list|(
name|rj
argument_list|)
expr_stmt|;
name|progress
argument_list|(
name|th
argument_list|)
expr_stmt|;
comment|// success status will be setup inside progress
if|if
condition|(
name|rj
operator|==
literal|null
condition|)
block|{
comment|// in the corner case where the running job has disappeared from JT
comment|// memory
comment|// remember that we did actually submit the job.
name|rj
operator|=
name|orig_rj
expr_stmt|;
name|success
operator|=
literal|false
expr_stmt|;
block|}
name|String
name|statusMesg
init|=
name|getJobEndMsg
argument_list|(
name|rj
operator|.
name|getJobID
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|success
condition|)
block|{
name|statusMesg
operator|+=
literal|" with errors"
expr_stmt|;
name|returnVal
operator|=
literal|2
expr_stmt|;
name|console
operator|.
name|printError
argument_list|(
name|statusMesg
argument_list|)
expr_stmt|;
name|showJobFailDebugInfo
argument_list|(
name|job
argument_list|,
name|rj
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|console
operator|.
name|printInfo
argument_list|(
name|statusMesg
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|e
operator|.
name|printStackTrace
argument_list|()
expr_stmt|;
name|String
name|mesg
init|=
literal|" with exception '"
operator|+
name|Utilities
operator|.
name|getNameMessage
argument_list|(
name|e
argument_list|)
operator|+
literal|"'"
decl_stmt|;
if|if
condition|(
name|rj
operator|!=
literal|null
condition|)
block|{
name|mesg
operator|=
literal|"Ended Job = "
operator|+
name|rj
operator|.
name|getJobID
argument_list|()
operator|+
name|mesg
expr_stmt|;
block|}
else|else
block|{
name|mesg
operator|=
literal|"Job Submission failed"
operator|+
name|mesg
expr_stmt|;
block|}
comment|// Has to use full name to make sure it does not conflict with
comment|// org.apache.commons.lang.StringUtils
name|console
operator|.
name|printError
argument_list|(
name|mesg
argument_list|,
literal|"\n"
operator|+
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
name|success
operator|=
literal|false
expr_stmt|;
name|returnVal
operator|=
literal|1
expr_stmt|;
block|}
finally|finally
block|{
name|Utilities
operator|.
name|clearMapRedWork
argument_list|(
name|job
argument_list|)
expr_stmt|;
try|try
block|{
name|emptyScratchDir
operator|.
name|getFileSystem
argument_list|(
name|job
argument_list|)
operator|.
name|delete
argument_list|(
name|emptyScratchDir
argument_list|,
literal|true
argument_list|)
expr_stmt|;
if|if
condition|(
name|returnVal
operator|!=
literal|0
operator|&&
name|rj
operator|!=
literal|null
condition|)
block|{
name|rj
operator|.
name|killJob
argument_list|()
expr_stmt|;
block|}
name|runningJobKillURIs
operator|.
name|remove
argument_list|(
name|rj
operator|.
name|getJobID
argument_list|()
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{       }
block|}
comment|// get the list of Dynamic partition paths
name|ArrayList
argument_list|<
name|String
argument_list|>
name|dpPaths
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
try|try
block|{
if|if
condition|(
name|rj
operator|!=
literal|null
condition|)
block|{
name|JobCloseFeedBack
name|feedBack
init|=
operator|new
name|JobCloseFeedBack
argument_list|()
decl_stmt|;
if|if
condition|(
name|work
operator|.
name|getAliasToWork
argument_list|()
operator|!=
literal|null
condition|)
block|{
for|for
control|(
name|Operator
argument_list|<
name|?
extends|extends
name|Serializable
argument_list|>
name|op
range|:
name|work
operator|.
name|getAliasToWork
argument_list|()
operator|.
name|values
argument_list|()
control|)
block|{
name|op
operator|.
name|jobClose
argument_list|(
name|job
argument_list|,
name|success
argument_list|,
name|feedBack
argument_list|)
expr_stmt|;
name|ArrayList
argument_list|<
name|Object
argument_list|>
name|dirs
init|=
name|feedBack
operator|.
name|get
argument_list|(
name|JobCloseFeedBack
operator|.
name|FeedBackType
operator|.
name|DYNAMIC_PARTITIONS
argument_list|)
decl_stmt|;
if|if
condition|(
name|dirs
operator|!=
literal|null
condition|)
block|{
for|for
control|(
name|Object
name|o
range|:
name|dirs
control|)
block|{
if|if
condition|(
name|o
operator|instanceof
name|String
condition|)
block|{
name|dpPaths
operator|.
name|add
argument_list|(
operator|(
name|String
operator|)
name|o
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
block|}
if|if
condition|(
name|work
operator|.
name|getReducer
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|work
operator|.
name|getReducer
argument_list|()
operator|.
name|jobClose
argument_list|(
name|job
argument_list|,
name|success
argument_list|,
name|feedBack
argument_list|)
expr_stmt|;
block|}
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
comment|// jobClose needs to execute successfully otherwise fail task
if|if
condition|(
name|success
condition|)
block|{
name|success
operator|=
literal|false
expr_stmt|;
name|returnVal
operator|=
literal|3
expr_stmt|;
name|String
name|mesg
init|=
literal|"Job Commit failed with exception '"
operator|+
name|Utilities
operator|.
name|getNameMessage
argument_list|(
name|e
argument_list|)
operator|+
literal|"'"
decl_stmt|;
name|console
operator|.
name|printError
argument_list|(
name|mesg
argument_list|,
literal|"\n"
operator|+
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
return|return
operator|(
name|returnVal
operator|)
return|;
block|}
comment|/**    * This msg pattern is used to track when a job is started.    *    * @param jobId    * @return    */
specifier|public
specifier|static
name|String
name|getJobStartMsg
parameter_list|(
name|String
name|jobId
parameter_list|)
block|{
return|return
literal|"Starting Job = "
operator|+
name|jobId
return|;
block|}
comment|/**    * this msg pattern is used to track when a job is successfully done.    *    * @param jobId    * @return    */
specifier|public
specifier|static
name|String
name|getJobEndMsg
parameter_list|(
name|String
name|jobId
parameter_list|)
block|{
return|return
literal|"Ended Job = "
operator|+
name|jobId
return|;
block|}
specifier|private
name|String
name|getTaskAttemptLogUrl
parameter_list|(
name|String
name|taskTrackerHttpAddress
parameter_list|,
name|String
name|taskAttemptId
parameter_list|)
block|{
return|return
name|taskTrackerHttpAddress
operator|+
literal|"/tasklog?taskid="
operator|+
name|taskAttemptId
operator|+
literal|"&all=true"
return|;
block|}
comment|// Used for showJobFailDebugInfo
specifier|private
specifier|static
class|class
name|TaskInfo
block|{
name|String
name|jobId
decl_stmt|;
name|HashSet
argument_list|<
name|String
argument_list|>
name|logUrls
decl_stmt|;
specifier|public
name|TaskInfo
parameter_list|(
name|String
name|jobId
parameter_list|)
block|{
name|this
operator|.
name|jobId
operator|=
name|jobId
expr_stmt|;
name|logUrls
operator|=
operator|new
name|HashSet
argument_list|<
name|String
argument_list|>
argument_list|()
expr_stmt|;
block|}
specifier|public
name|void
name|addLogUrl
parameter_list|(
name|String
name|logUrl
parameter_list|)
block|{
name|logUrls
operator|.
name|add
argument_list|(
name|logUrl
argument_list|)
expr_stmt|;
block|}
specifier|public
name|HashSet
argument_list|<
name|String
argument_list|>
name|getLogUrls
parameter_list|()
block|{
return|return
name|logUrls
return|;
block|}
specifier|public
name|String
name|getJobId
parameter_list|()
block|{
return|return
name|jobId
return|;
block|}
block|}
annotation|@
name|SuppressWarnings
argument_list|(
literal|"deprecation"
argument_list|)
specifier|private
name|void
name|showJobFailDebugInfo
parameter_list|(
name|JobConf
name|conf
parameter_list|,
name|RunningJob
name|rj
parameter_list|)
throws|throws
name|IOException
block|{
comment|// Mapping from task ID to the number of failures
name|Map
argument_list|<
name|String
argument_list|,
name|Integer
argument_list|>
name|failures
init|=
operator|new
name|HashMap
argument_list|<
name|String
argument_list|,
name|Integer
argument_list|>
argument_list|()
decl_stmt|;
comment|// Successful task ID's
name|Set
argument_list|<
name|String
argument_list|>
name|successes
init|=
operator|new
name|HashSet
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
name|Map
argument_list|<
name|String
argument_list|,
name|TaskInfo
argument_list|>
name|taskIdToInfo
init|=
operator|new
name|HashMap
argument_list|<
name|String
argument_list|,
name|TaskInfo
argument_list|>
argument_list|()
decl_stmt|;
name|int
name|startIndex
init|=
literal|0
decl_stmt|;
comment|// Loop to get all task completion events because getTaskCompletionEvents
comment|// only returns a subset per call
while|while
condition|(
literal|true
condition|)
block|{
name|TaskCompletionEvent
index|[]
name|taskCompletions
init|=
name|rj
operator|.
name|getTaskCompletionEvents
argument_list|(
name|startIndex
argument_list|)
decl_stmt|;
if|if
condition|(
name|taskCompletions
operator|==
literal|null
operator|||
name|taskCompletions
operator|.
name|length
operator|==
literal|0
condition|)
block|{
break|break;
block|}
name|boolean
name|more
init|=
literal|true
decl_stmt|;
for|for
control|(
name|TaskCompletionEvent
name|t
range|:
name|taskCompletions
control|)
block|{
comment|// getTaskJobIDs returns Strings for compatibility with Hadoop versions
comment|// without TaskID or TaskAttemptID
name|String
index|[]
name|taskJobIds
init|=
name|ShimLoader
operator|.
name|getHadoopShims
argument_list|()
operator|.
name|getTaskJobIDs
argument_list|(
name|t
argument_list|)
decl_stmt|;
if|if
condition|(
name|taskJobIds
operator|==
literal|null
condition|)
block|{
name|console
operator|.
name|printError
argument_list|(
literal|"Task attempt info is unavailable in "
operator|+
literal|"this Hadoop version"
argument_list|)
expr_stmt|;
name|more
operator|=
literal|false
expr_stmt|;
break|break;
block|}
comment|// For each task completion event, get the associated task id, job id
comment|// and the logs
name|String
name|taskId
init|=
name|taskJobIds
index|[
literal|0
index|]
decl_stmt|;
name|String
name|jobId
init|=
name|taskJobIds
index|[
literal|1
index|]
decl_stmt|;
name|TaskInfo
name|ti
init|=
name|taskIdToInfo
operator|.
name|get
argument_list|(
name|taskId
argument_list|)
decl_stmt|;
if|if
condition|(
name|ti
operator|==
literal|null
condition|)
block|{
name|ti
operator|=
operator|new
name|TaskInfo
argument_list|(
name|jobId
argument_list|)
expr_stmt|;
name|taskIdToInfo
operator|.
name|put
argument_list|(
name|taskId
argument_list|,
name|ti
argument_list|)
expr_stmt|;
block|}
comment|// These tasks should have come from the same job.
assert|assert
operator|(
name|ti
operator|.
name|getJobId
argument_list|()
operator|==
name|jobId
operator|)
assert|;
name|ti
operator|.
name|getLogUrls
argument_list|()
operator|.
name|add
argument_list|(
name|getTaskAttemptLogUrl
argument_list|(
name|t
operator|.
name|getTaskTrackerHttp
argument_list|()
argument_list|,
name|t
operator|.
name|getTaskId
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
comment|// If a task failed, then keep track of the total number of failures
comment|// for that task (typically, a task gets re-run up to 4 times if it
comment|// fails
if|if
condition|(
name|t
operator|.
name|getTaskStatus
argument_list|()
operator|!=
name|TaskCompletionEvent
operator|.
name|Status
operator|.
name|SUCCEEDED
condition|)
block|{
name|Integer
name|failAttempts
init|=
name|failures
operator|.
name|get
argument_list|(
name|taskId
argument_list|)
decl_stmt|;
if|if
condition|(
name|failAttempts
operator|==
literal|null
condition|)
block|{
name|failAttempts
operator|=
name|Integer
operator|.
name|valueOf
argument_list|(
literal|0
argument_list|)
expr_stmt|;
block|}
name|failAttempts
operator|=
name|Integer
operator|.
name|valueOf
argument_list|(
name|failAttempts
operator|.
name|intValue
argument_list|()
operator|+
literal|1
argument_list|)
expr_stmt|;
name|failures
operator|.
name|put
argument_list|(
name|taskId
argument_list|,
name|failAttempts
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|successes
operator|.
name|add
argument_list|(
name|taskId
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
operator|!
name|more
condition|)
block|{
break|break;
block|}
name|startIndex
operator|+=
name|taskCompletions
operator|.
name|length
expr_stmt|;
block|}
comment|// Remove failures for tasks that succeeded
for|for
control|(
name|String
name|task
range|:
name|successes
control|)
block|{
name|failures
operator|.
name|remove
argument_list|(
name|task
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|failures
operator|.
name|keySet
argument_list|()
operator|.
name|size
argument_list|()
operator|==
literal|0
condition|)
block|{
return|return;
block|}
comment|// Find the highest failure count
name|int
name|maxFailures
init|=
literal|0
decl_stmt|;
for|for
control|(
name|Integer
name|failCount
range|:
name|failures
operator|.
name|values
argument_list|()
control|)
block|{
if|if
condition|(
name|maxFailures
operator|<
name|failCount
operator|.
name|intValue
argument_list|()
condition|)
block|{
name|maxFailures
operator|=
name|failCount
operator|.
name|intValue
argument_list|()
expr_stmt|;
block|}
block|}
comment|// Display Error Message for tasks with the highest failure count
name|String
name|jtUrl
init|=
name|JobTrackerURLResolver
operator|.
name|getURL
argument_list|(
name|conf
argument_list|)
decl_stmt|;
for|for
control|(
name|String
name|task
range|:
name|failures
operator|.
name|keySet
argument_list|()
control|)
block|{
if|if
condition|(
name|failures
operator|.
name|get
argument_list|(
name|task
argument_list|)
operator|.
name|intValue
argument_list|()
operator|==
name|maxFailures
condition|)
block|{
name|TaskInfo
name|ti
init|=
name|taskIdToInfo
operator|.
name|get
argument_list|(
name|task
argument_list|)
decl_stmt|;
name|String
name|jobId
init|=
name|ti
operator|.
name|getJobId
argument_list|()
decl_stmt|;
name|String
name|taskUrl
init|=
name|jtUrl
operator|+
literal|"/taskdetails.jsp?jobid="
operator|+
name|jobId
operator|+
literal|"&tipid="
operator|+
name|task
operator|.
name|toString
argument_list|()
decl_stmt|;
name|TaskLogProcessor
name|tlp
init|=
operator|new
name|TaskLogProcessor
argument_list|(
name|conf
argument_list|)
decl_stmt|;
for|for
control|(
name|String
name|logUrl
range|:
name|ti
operator|.
name|getLogUrls
argument_list|()
control|)
block|{
name|tlp
operator|.
name|addTaskAttemptLogUrl
argument_list|(
name|logUrl
argument_list|)
expr_stmt|;
block|}
name|List
argument_list|<
name|ErrorAndSolution
argument_list|>
name|errors
init|=
name|tlp
operator|.
name|getErrors
argument_list|()
decl_stmt|;
name|StringBuilder
name|sb
init|=
operator|new
name|StringBuilder
argument_list|()
decl_stmt|;
comment|// We use a StringBuilder and then call printError only once as
comment|// printError will write to both stderr and the error log file. In
comment|// situations where both the stderr and the log file output is
comment|// simultaneously output to a single stream, this will look cleaner.
name|sb
operator|.
name|append
argument_list|(
literal|"\n"
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
literal|"Task with the most failures("
operator|+
name|maxFailures
operator|+
literal|"): \n"
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
literal|"-----\n"
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
literal|"Task ID:\n  "
operator|+
name|task
operator|+
literal|"\n\n"
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
literal|"URL:\n  "
operator|+
name|taskUrl
operator|+
literal|"\n"
argument_list|)
expr_stmt|;
for|for
control|(
name|ErrorAndSolution
name|e
range|:
name|errors
control|)
block|{
name|sb
operator|.
name|append
argument_list|(
literal|"\n"
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
literal|"Possible error:\n  "
operator|+
name|e
operator|.
name|getError
argument_list|()
operator|+
literal|"\n\n"
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
literal|"Solution:\n  "
operator|+
name|e
operator|.
name|getSolution
argument_list|()
operator|+
literal|"\n"
argument_list|)
expr_stmt|;
block|}
name|sb
operator|.
name|append
argument_list|(
literal|"-----\n"
argument_list|)
expr_stmt|;
name|console
operator|.
name|printError
argument_list|(
name|sb
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
comment|// Only print out one task because that's good enough for debugging.
break|break;
block|}
block|}
return|return;
block|}
specifier|private
specifier|static
name|void
name|printUsage
parameter_list|()
block|{
name|System
operator|.
name|err
operator|.
name|println
argument_list|(
literal|"ExecDriver -plan<plan-file> [-jobconf k1=v1 [-jobconf k2=v2] ...] "
operator|+
literal|"[-files<file1>[,<file2>] ...]"
argument_list|)
expr_stmt|;
name|System
operator|.
name|exit
argument_list|(
literal|1
argument_list|)
expr_stmt|;
block|}
comment|/**    * we are running the hadoop job via a sub-command. this typically    * happens when we are running jobs in local mode. the log4j in this    * mode is controlled as follows:    * 1. if the admin provides a log4j properties file especially for    *    execution mode - then we pick that up    * 2. otherwise - we default to the regular hive log4j properties if    *    one is supplied    * 3. if none of the above two apply - we don't do anything - the log4j    *    properties would likely be determined by hadoop.    *    * The intention behind providing a separate option #1 is to be able to    * collect hive run time logs generated in local mode in a separate    * (centralized) location if desired. This mimics the behavior of hive    * run time logs when running against a hadoop cluster where they are available    * on the tasktracker nodes.    */
specifier|private
specifier|static
name|void
name|setupChildLog4j
parameter_list|(
name|Configuration
name|conf
parameter_list|)
block|{
name|URL
name|hive_l4j
init|=
name|ExecDriver
operator|.
name|class
operator|.
name|getClassLoader
argument_list|()
operator|.
name|getResource
argument_list|(
name|SessionState
operator|.
name|HIVE_EXEC_L4J
argument_list|)
decl_stmt|;
if|if
condition|(
name|hive_l4j
operator|==
literal|null
condition|)
name|hive_l4j
operator|=
name|ExecDriver
operator|.
name|class
operator|.
name|getClassLoader
argument_list|()
operator|.
name|getResource
argument_list|(
name|SessionState
operator|.
name|HIVE_L4J
argument_list|)
expr_stmt|;
if|if
condition|(
name|hive_l4j
operator|!=
literal|null
condition|)
block|{
comment|// setting queryid so that log4j configuration can use it to generate
comment|// per query log file
name|System
operator|.
name|setProperty
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEQUERYID
operator|.
name|toString
argument_list|()
argument_list|,
name|HiveConf
operator|.
name|getVar
argument_list|(
name|conf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEQUERYID
argument_list|)
argument_list|)
expr_stmt|;
name|LogManager
operator|.
name|resetConfiguration
argument_list|()
expr_stmt|;
name|PropertyConfigurator
operator|.
name|configure
argument_list|(
name|hive_l4j
argument_list|)
expr_stmt|;
block|}
block|}
specifier|public
specifier|static
name|void
name|main
parameter_list|(
name|String
index|[]
name|args
parameter_list|)
throws|throws
name|IOException
throws|,
name|HiveException
block|{
name|String
name|planFileName
init|=
literal|null
decl_stmt|;
name|ArrayList
argument_list|<
name|String
argument_list|>
name|jobConfArgs
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
name|boolean
name|noLog
init|=
literal|false
decl_stmt|;
name|String
name|files
init|=
literal|null
decl_stmt|;
try|try
block|{
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|args
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
if|if
condition|(
name|args
index|[
name|i
index|]
operator|.
name|equals
argument_list|(
literal|"-plan"
argument_list|)
condition|)
block|{
name|planFileName
operator|=
name|args
index|[
operator|++
name|i
index|]
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|args
index|[
name|i
index|]
operator|.
name|equals
argument_list|(
literal|"-jobconf"
argument_list|)
condition|)
block|{
name|jobConfArgs
operator|.
name|add
argument_list|(
name|args
index|[
operator|++
name|i
index|]
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|args
index|[
name|i
index|]
operator|.
name|equals
argument_list|(
literal|"-nolog"
argument_list|)
condition|)
block|{
name|noLog
operator|=
literal|true
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|args
index|[
name|i
index|]
operator|.
name|equals
argument_list|(
literal|"-files"
argument_list|)
condition|)
block|{
name|files
operator|=
name|args
index|[
operator|++
name|i
index|]
expr_stmt|;
block|}
block|}
block|}
catch|catch
parameter_list|(
name|IndexOutOfBoundsException
name|e
parameter_list|)
block|{
name|System
operator|.
name|err
operator|.
name|println
argument_list|(
literal|"Missing argument to option"
argument_list|)
expr_stmt|;
name|printUsage
argument_list|()
expr_stmt|;
block|}
name|JobConf
name|conf
init|=
operator|new
name|JobConf
argument_list|(
name|ExecDriver
operator|.
name|class
argument_list|)
decl_stmt|;
name|StringBuilder
name|sb
init|=
operator|new
name|StringBuilder
argument_list|(
literal|"JobConf:\n"
argument_list|)
decl_stmt|;
for|for
control|(
name|String
name|one
range|:
name|jobConfArgs
control|)
block|{
name|int
name|eqIndex
init|=
name|one
operator|.
name|indexOf
argument_list|(
literal|'='
argument_list|)
decl_stmt|;
if|if
condition|(
name|eqIndex
operator|!=
operator|-
literal|1
condition|)
block|{
try|try
block|{
name|String
name|key
init|=
name|one
operator|.
name|substring
argument_list|(
literal|0
argument_list|,
name|eqIndex
argument_list|)
decl_stmt|;
name|String
name|value
init|=
name|URLDecoder
operator|.
name|decode
argument_list|(
name|one
operator|.
name|substring
argument_list|(
name|eqIndex
operator|+
literal|1
argument_list|)
argument_list|,
literal|"UTF-8"
argument_list|)
decl_stmt|;
name|conf
operator|.
name|set
argument_list|(
name|key
argument_list|,
name|value
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
name|key
argument_list|)
operator|.
name|append
argument_list|(
literal|"="
argument_list|)
operator|.
name|append
argument_list|(
name|value
argument_list|)
operator|.
name|append
argument_list|(
literal|"\n"
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|UnsupportedEncodingException
name|e
parameter_list|)
block|{
name|System
operator|.
name|err
operator|.
name|println
argument_list|(
literal|"Unexpected error "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
operator|+
literal|" while encoding "
operator|+
name|one
operator|.
name|substring
argument_list|(
name|eqIndex
operator|+
literal|1
argument_list|)
argument_list|)
expr_stmt|;
name|System
operator|.
name|exit
argument_list|(
literal|3
argument_list|)
expr_stmt|;
block|}
block|}
block|}
if|if
condition|(
name|files
operator|!=
literal|null
condition|)
block|{
name|conf
operator|.
name|set
argument_list|(
literal|"tmpfiles"
argument_list|,
name|files
argument_list|)
expr_stmt|;
block|}
name|boolean
name|isSilent
init|=
name|HiveConf
operator|.
name|getBoolVar
argument_list|(
name|conf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVESESSIONSILENT
argument_list|)
decl_stmt|;
if|if
condition|(
name|noLog
condition|)
block|{
comment|// If started from main(), and noLog is on, we should not output
comment|// any logs. To turn the log on, please set -Dtest.silent=false
name|BasicConfigurator
operator|.
name|resetConfiguration
argument_list|()
expr_stmt|;
name|BasicConfigurator
operator|.
name|configure
argument_list|(
operator|new
name|NullAppender
argument_list|()
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|setupChildLog4j
argument_list|(
name|conf
argument_list|)
expr_stmt|;
block|}
name|Log
name|LOG
init|=
name|LogFactory
operator|.
name|getLog
argument_list|(
name|ExecDriver
operator|.
name|class
operator|.
name|getName
argument_list|()
argument_list|)
decl_stmt|;
name|LogHelper
name|console
init|=
operator|new
name|LogHelper
argument_list|(
name|LOG
argument_list|,
name|isSilent
argument_list|)
decl_stmt|;
if|if
condition|(
name|planFileName
operator|==
literal|null
condition|)
block|{
name|console
operator|.
name|printError
argument_list|(
literal|"Must specify Plan File Name"
argument_list|)
expr_stmt|;
name|printUsage
argument_list|()
expr_stmt|;
block|}
name|console
operator|.
name|printInfo
argument_list|(
literal|"plan = "
operator|+
name|planFileName
argument_list|)
expr_stmt|;
comment|// log the list of job conf parameters for reference
name|LOG
operator|.
name|info
argument_list|(
name|sb
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
name|URI
name|pathURI
init|=
operator|(
operator|new
name|Path
argument_list|(
name|planFileName
argument_list|)
operator|)
operator|.
name|toUri
argument_list|()
decl_stmt|;
name|InputStream
name|pathData
decl_stmt|;
if|if
condition|(
name|StringUtils
operator|.
name|isEmpty
argument_list|(
name|pathURI
operator|.
name|getScheme
argument_list|()
argument_list|)
condition|)
block|{
comment|// default to local file system
name|pathData
operator|=
operator|new
name|FileInputStream
argument_list|(
name|planFileName
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// otherwise may be in hadoop ..
name|FileSystem
name|fs
init|=
name|FileSystem
operator|.
name|get
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|pathData
operator|=
name|fs
operator|.
name|open
argument_list|(
operator|new
name|Path
argument_list|(
name|planFileName
argument_list|)
argument_list|)
expr_stmt|;
block|}
comment|// this is workaround for hadoop-17 - libjars are not added to classpath of the
comment|// child process. so we add it here explicitly
name|String
name|auxJars
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|conf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEAUXJARS
argument_list|)
decl_stmt|;
name|String
name|addedJars
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|conf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEADDEDJARS
argument_list|)
decl_stmt|;
try|try
block|{
comment|// see also - code in CliDriver.java
name|ClassLoader
name|loader
init|=
name|conf
operator|.
name|getClassLoader
argument_list|()
decl_stmt|;
if|if
condition|(
name|StringUtils
operator|.
name|isNotBlank
argument_list|(
name|auxJars
argument_list|)
condition|)
block|{
name|loader
operator|=
name|Utilities
operator|.
name|addToClassPath
argument_list|(
name|loader
argument_list|,
name|StringUtils
operator|.
name|split
argument_list|(
name|auxJars
argument_list|,
literal|","
argument_list|)
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|StringUtils
operator|.
name|isNotBlank
argument_list|(
name|addedJars
argument_list|)
condition|)
block|{
name|loader
operator|=
name|Utilities
operator|.
name|addToClassPath
argument_list|(
name|loader
argument_list|,
name|StringUtils
operator|.
name|split
argument_list|(
name|addedJars
argument_list|,
literal|","
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|conf
operator|.
name|setClassLoader
argument_list|(
name|loader
argument_list|)
expr_stmt|;
comment|// Also set this to the Thread ContextClassLoader, so new threads will
comment|// inherit
comment|// this class loader, and propagate into newly created Configurations by
comment|// those
comment|// new threads.
name|Thread
operator|.
name|currentThread
argument_list|()
operator|.
name|setContextClassLoader
argument_list|(
name|loader
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
name|MapredWork
name|plan
init|=
name|Utilities
operator|.
name|deserializeMapRedWork
argument_list|(
name|pathData
argument_list|,
name|conf
argument_list|)
decl_stmt|;
name|ExecDriver
name|ed
init|=
operator|new
name|ExecDriver
argument_list|(
name|plan
argument_list|,
name|conf
argument_list|,
name|isSilent
argument_list|)
decl_stmt|;
name|int
name|ret
init|=
name|ed
operator|.
name|execute
argument_list|(
operator|new
name|DriverContext
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|ret
operator|!=
literal|0
condition|)
block|{
name|System
operator|.
name|exit
argument_list|(
literal|2
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * Given a Hive Configuration object - generate a command line fragment for    * passing such configuration information to ExecDriver.    */
specifier|public
specifier|static
name|String
name|generateCmdLine
parameter_list|(
name|HiveConf
name|hconf
parameter_list|)
block|{
try|try
block|{
name|StringBuilder
name|sb
init|=
operator|new
name|StringBuilder
argument_list|()
decl_stmt|;
name|Properties
name|deltaP
init|=
name|hconf
operator|.
name|getChangedProperties
argument_list|()
decl_stmt|;
name|boolean
name|hadoopLocalMode
init|=
name|hconf
operator|.
name|getVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HADOOPJT
argument_list|)
operator|.
name|equals
argument_list|(
literal|"local"
argument_list|)
decl_stmt|;
name|String
name|hadoopSysDir
init|=
literal|"mapred.system.dir"
decl_stmt|;
name|String
name|hadoopWorkDir
init|=
literal|"mapred.local.dir"
decl_stmt|;
for|for
control|(
name|Object
name|one
range|:
name|deltaP
operator|.
name|keySet
argument_list|()
control|)
block|{
name|String
name|oneProp
init|=
operator|(
name|String
operator|)
name|one
decl_stmt|;
if|if
condition|(
name|hadoopLocalMode
operator|&&
operator|(
name|oneProp
operator|.
name|equals
argument_list|(
name|hadoopSysDir
argument_list|)
operator|||
name|oneProp
operator|.
name|equals
argument_list|(
name|hadoopWorkDir
argument_list|)
operator|)
condition|)
block|{
continue|continue;
block|}
name|String
name|oneValue
init|=
name|deltaP
operator|.
name|getProperty
argument_list|(
name|oneProp
argument_list|)
decl_stmt|;
name|sb
operator|.
name|append
argument_list|(
literal|"-jobconf "
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
name|oneProp
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
literal|"="
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
name|URLEncoder
operator|.
name|encode
argument_list|(
name|oneValue
argument_list|,
literal|"UTF-8"
argument_list|)
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
literal|" "
argument_list|)
expr_stmt|;
block|}
comment|// Multiple concurrent local mode job submissions can cause collisions in
comment|// working dirs
comment|// Workaround is to rename map red working dir to a temp dir in such cases
if|if
condition|(
name|hadoopLocalMode
condition|)
block|{
name|sb
operator|.
name|append
argument_list|(
literal|"-jobconf "
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
name|hadoopSysDir
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
literal|"="
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
name|URLEncoder
operator|.
name|encode
argument_list|(
name|hconf
operator|.
name|get
argument_list|(
name|hadoopSysDir
argument_list|)
operator|+
literal|"/"
operator|+
name|Utilities
operator|.
name|randGen
operator|.
name|nextInt
argument_list|()
argument_list|,
literal|"UTF-8"
argument_list|)
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
literal|" "
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
literal|"-jobconf "
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
name|hadoopWorkDir
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
literal|"="
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
name|URLEncoder
operator|.
name|encode
argument_list|(
name|hconf
operator|.
name|get
argument_list|(
name|hadoopWorkDir
argument_list|)
operator|+
literal|"/"
operator|+
name|Utilities
operator|.
name|randGen
operator|.
name|nextInt
argument_list|()
argument_list|,
literal|"UTF-8"
argument_list|)
argument_list|)
expr_stmt|;
block|}
return|return
name|sb
operator|.
name|toString
argument_list|()
return|;
block|}
catch|catch
parameter_list|(
name|UnsupportedEncodingException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|isMapRedTask
parameter_list|()
block|{
return|return
literal|true
return|;
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|hasReduce
parameter_list|()
block|{
name|MapredWork
name|w
init|=
name|getWork
argument_list|()
decl_stmt|;
return|return
name|w
operator|.
name|getReducer
argument_list|()
operator|!=
literal|null
return|;
block|}
specifier|private
name|boolean
name|isEmptyPath
parameter_list|(
name|JobConf
name|job
parameter_list|,
name|String
name|path
parameter_list|)
throws|throws
name|Exception
block|{
name|Path
name|dirPath
init|=
operator|new
name|Path
argument_list|(
name|path
argument_list|)
decl_stmt|;
name|FileSystem
name|inpFs
init|=
name|dirPath
operator|.
name|getFileSystem
argument_list|(
name|job
argument_list|)
decl_stmt|;
if|if
condition|(
name|inpFs
operator|.
name|exists
argument_list|(
name|dirPath
argument_list|)
condition|)
block|{
name|FileStatus
index|[]
name|fStats
init|=
name|inpFs
operator|.
name|listStatus
argument_list|(
name|dirPath
argument_list|)
decl_stmt|;
if|if
condition|(
name|fStats
operator|.
name|length
operator|>
literal|0
condition|)
block|{
return|return
literal|false
return|;
block|}
block|}
return|return
literal|true
return|;
block|}
comment|/**    * Handle a empty/null path for a given alias.    */
specifier|private
name|int
name|addInputPath
parameter_list|(
name|String
name|path
parameter_list|,
name|JobConf
name|job
parameter_list|,
name|MapredWork
name|work
parameter_list|,
name|String
name|hiveScratchDir
parameter_list|,
name|int
name|numEmptyPaths
parameter_list|,
name|boolean
name|isEmptyPath
parameter_list|,
name|String
name|alias
parameter_list|)
throws|throws
name|Exception
block|{
comment|// either the directory does not exist or it is empty
assert|assert
name|path
operator|==
literal|null
operator|||
name|isEmptyPath
assert|;
comment|// The input file does not exist, replace it by a empty file
name|Class
argument_list|<
name|?
extends|extends
name|HiveOutputFormat
argument_list|>
name|outFileFormat
init|=
literal|null
decl_stmt|;
name|boolean
name|nonNative
init|=
literal|true
decl_stmt|;
if|if
condition|(
name|isEmptyPath
condition|)
block|{
name|PartitionDesc
name|partDesc
init|=
name|work
operator|.
name|getPathToPartitionInfo
argument_list|()
operator|.
name|get
argument_list|(
name|path
argument_list|)
decl_stmt|;
name|outFileFormat
operator|=
name|partDesc
operator|.
name|getOutputFileFormatClass
argument_list|()
expr_stmt|;
name|nonNative
operator|=
name|partDesc
operator|.
name|getTableDesc
argument_list|()
operator|.
name|isNonNative
argument_list|()
expr_stmt|;
block|}
else|else
block|{
name|TableDesc
name|tableDesc
init|=
name|work
operator|.
name|getAliasToPartnInfo
argument_list|()
operator|.
name|get
argument_list|(
name|alias
argument_list|)
operator|.
name|getTableDesc
argument_list|()
decl_stmt|;
name|outFileFormat
operator|=
name|tableDesc
operator|.
name|getOutputFileFormatClass
argument_list|()
expr_stmt|;
name|nonNative
operator|=
name|tableDesc
operator|.
name|isNonNative
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
name|nonNative
condition|)
block|{
name|FileInputFormat
operator|.
name|addInputPaths
argument_list|(
name|job
argument_list|,
name|path
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Add a non-native table "
operator|+
name|path
argument_list|)
expr_stmt|;
return|return
name|numEmptyPaths
return|;
block|}
comment|// create a dummy empty file in a new directory
name|String
name|newDir
init|=
name|hiveScratchDir
operator|+
name|File
operator|.
name|separator
operator|+
operator|(
operator|++
name|numEmptyPaths
operator|)
decl_stmt|;
name|Path
name|newPath
init|=
operator|new
name|Path
argument_list|(
name|newDir
argument_list|)
decl_stmt|;
name|FileSystem
name|fs
init|=
name|newPath
operator|.
name|getFileSystem
argument_list|(
name|job
argument_list|)
decl_stmt|;
name|fs
operator|.
name|mkdirs
argument_list|(
name|newPath
argument_list|)
expr_stmt|;
name|String
name|newFile
init|=
name|newDir
operator|+
name|File
operator|.
name|separator
operator|+
literal|"emptyFile"
decl_stmt|;
name|Path
name|newFilePath
init|=
operator|new
name|Path
argument_list|(
name|newFile
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Changed input file to "
operator|+
name|newPath
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
comment|// toggle the work
name|LinkedHashMap
argument_list|<
name|String
argument_list|,
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|>
name|pathToAliases
init|=
name|work
operator|.
name|getPathToAliases
argument_list|()
decl_stmt|;
if|if
condition|(
name|isEmptyPath
condition|)
block|{
assert|assert
name|path
operator|!=
literal|null
assert|;
name|pathToAliases
operator|.
name|put
argument_list|(
name|newPath
operator|.
name|toUri
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|,
name|pathToAliases
operator|.
name|get
argument_list|(
name|path
argument_list|)
argument_list|)
expr_stmt|;
name|pathToAliases
operator|.
name|remove
argument_list|(
name|path
argument_list|)
expr_stmt|;
block|}
else|else
block|{
assert|assert
name|path
operator|==
literal|null
assert|;
name|ArrayList
argument_list|<
name|String
argument_list|>
name|newList
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
name|newList
operator|.
name|add
argument_list|(
name|alias
argument_list|)
expr_stmt|;
name|pathToAliases
operator|.
name|put
argument_list|(
name|newPath
operator|.
name|toUri
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|,
name|newList
argument_list|)
expr_stmt|;
block|}
name|work
operator|.
name|setPathToAliases
argument_list|(
name|pathToAliases
argument_list|)
expr_stmt|;
name|LinkedHashMap
argument_list|<
name|String
argument_list|,
name|PartitionDesc
argument_list|>
name|pathToPartitionInfo
init|=
name|work
operator|.
name|getPathToPartitionInfo
argument_list|()
decl_stmt|;
if|if
condition|(
name|isEmptyPath
condition|)
block|{
name|pathToPartitionInfo
operator|.
name|put
argument_list|(
name|newPath
operator|.
name|toUri
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|,
name|pathToPartitionInfo
operator|.
name|get
argument_list|(
name|path
argument_list|)
argument_list|)
expr_stmt|;
name|pathToPartitionInfo
operator|.
name|remove
argument_list|(
name|path
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|PartitionDesc
name|pDesc
init|=
name|work
operator|.
name|getAliasToPartnInfo
argument_list|()
operator|.
name|get
argument_list|(
name|alias
argument_list|)
operator|.
name|clone
argument_list|()
decl_stmt|;
name|pathToPartitionInfo
operator|.
name|put
argument_list|(
name|newPath
operator|.
name|toUri
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|,
name|pDesc
argument_list|)
expr_stmt|;
block|}
name|work
operator|.
name|setPathToPartitionInfo
argument_list|(
name|pathToPartitionInfo
argument_list|)
expr_stmt|;
name|String
name|onefile
init|=
name|newPath
operator|.
name|toString
argument_list|()
decl_stmt|;
name|RecordWriter
name|recWriter
init|=
name|outFileFormat
operator|.
name|newInstance
argument_list|()
operator|.
name|getHiveRecordWriter
argument_list|(
name|job
argument_list|,
name|newFilePath
argument_list|,
name|Text
operator|.
name|class
argument_list|,
literal|false
argument_list|,
operator|new
name|Properties
argument_list|()
argument_list|,
literal|null
argument_list|)
decl_stmt|;
name|recWriter
operator|.
name|close
argument_list|(
literal|false
argument_list|)
expr_stmt|;
name|FileInputFormat
operator|.
name|addInputPaths
argument_list|(
name|job
argument_list|,
name|onefile
argument_list|)
expr_stmt|;
return|return
name|numEmptyPaths
return|;
block|}
specifier|private
name|void
name|addInputPaths
parameter_list|(
name|JobConf
name|job
parameter_list|,
name|MapredWork
name|work
parameter_list|,
name|String
name|hiveScratchDir
parameter_list|)
throws|throws
name|Exception
block|{
name|int
name|numEmptyPaths
init|=
literal|0
decl_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|pathsProcessed
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
comment|// AliasToWork contains all the aliases
for|for
control|(
name|String
name|oneAlias
range|:
name|work
operator|.
name|getAliasToWork
argument_list|()
operator|.
name|keySet
argument_list|()
control|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Processing alias "
operator|+
name|oneAlias
argument_list|)
expr_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|emptyPaths
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
comment|// The alias may not have any path
name|String
name|path
init|=
literal|null
decl_stmt|;
for|for
control|(
name|String
name|onefile
range|:
name|work
operator|.
name|getPathToAliases
argument_list|()
operator|.
name|keySet
argument_list|()
control|)
block|{
name|List
argument_list|<
name|String
argument_list|>
name|aliases
init|=
name|work
operator|.
name|getPathToAliases
argument_list|()
operator|.
name|get
argument_list|(
name|onefile
argument_list|)
decl_stmt|;
if|if
condition|(
name|aliases
operator|.
name|contains
argument_list|(
name|oneAlias
argument_list|)
condition|)
block|{
name|path
operator|=
name|onefile
expr_stmt|;
comment|// Multiple aliases can point to the same path - it should be
comment|// processed only once
if|if
condition|(
name|pathsProcessed
operator|.
name|contains
argument_list|(
name|path
argument_list|)
condition|)
block|{
continue|continue;
block|}
name|pathsProcessed
operator|.
name|add
argument_list|(
name|path
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Adding input file "
operator|+
name|path
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|isEmptyPath
argument_list|(
name|job
argument_list|,
name|path
argument_list|)
condition|)
block|{
name|FileInputFormat
operator|.
name|addInputPaths
argument_list|(
name|job
argument_list|,
name|path
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|emptyPaths
operator|.
name|add
argument_list|(
name|path
argument_list|)
expr_stmt|;
block|}
block|}
block|}
comment|// Create a empty file if the directory is empty
for|for
control|(
name|String
name|emptyPath
range|:
name|emptyPaths
control|)
block|{
name|numEmptyPaths
operator|=
name|addInputPath
argument_list|(
name|emptyPath
argument_list|,
name|job
argument_list|,
name|work
argument_list|,
name|hiveScratchDir
argument_list|,
name|numEmptyPaths
argument_list|,
literal|true
argument_list|,
name|oneAlias
argument_list|)
expr_stmt|;
block|}
comment|// If the query references non-existent partitions
comment|// We need to add a empty file, it is not acceptable to change the
comment|// operator tree
comment|// Consider the query:
comment|// select * from (select count(1) from T union all select count(1) from
comment|// T2) x;
comment|// If T is empty and T2 contains 100 rows, the user expects: 0, 100 (2
comment|// rows)
if|if
condition|(
name|path
operator|==
literal|null
condition|)
block|{
name|numEmptyPaths
operator|=
name|addInputPath
argument_list|(
literal|null
argument_list|,
name|job
argument_list|,
name|work
argument_list|,
name|hiveScratchDir
argument_list|,
name|numEmptyPaths
argument_list|,
literal|false
argument_list|,
name|oneAlias
argument_list|)
expr_stmt|;
block|}
block|}
block|}
annotation|@
name|Override
specifier|public
name|int
name|getType
parameter_list|()
block|{
return|return
name|StageType
operator|.
name|MAPRED
return|;
block|}
annotation|@
name|Override
specifier|public
name|String
name|getName
parameter_list|()
block|{
return|return
literal|"EXEC"
return|;
block|}
block|}
end_class

end_unit

