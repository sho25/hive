begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/**  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
package|;
end_package

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|*
import|;
end_import

begin_import
import|import
name|java
operator|.
name|text
operator|.
name|SimpleDateFormat
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|*
import|;
end_import

begin_import
import|import
name|java
operator|.
name|net
operator|.
name|URI
import|;
end_import

begin_import
import|import
name|java
operator|.
name|net
operator|.
name|URLEncoder
import|;
end_import

begin_import
import|import
name|java
operator|.
name|net
operator|.
name|URLDecoder
import|;
end_import

begin_import
import|import
name|java
operator|.
name|net
operator|.
name|URL
import|;
end_import

begin_import
import|import
name|java
operator|.
name|net
operator|.
name|URLClassLoader
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|LogFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|lang
operator|.
name|StringUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|ContentSummary
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileStatus
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|*
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|*
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|JobConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|FileInputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configuration
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
operator|.
name|ConfVars
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|mapredWork
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|exprNodeDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|partitionDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|tableDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|HiveException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|FileSinkOperator
operator|.
name|RecordWriter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|history
operator|.
name|HiveHistory
operator|.
name|Keys
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|*
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|session
operator|.
name|SessionState
operator|.
name|LogHelper
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|session
operator|.
name|SessionState
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|log4j
operator|.
name|BasicConfigurator
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|log4j
operator|.
name|varia
operator|.
name|NullAppender
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|lazy
operator|.
name|LazySimpleSerDe
import|;
end_import

begin_class
specifier|public
class|class
name|ExecDriver
extends|extends
name|Task
argument_list|<
name|mapredWork
argument_list|>
implements|implements
name|Serializable
block|{
specifier|private
specifier|static
specifier|final
name|long
name|serialVersionUID
init|=
literal|1L
decl_stmt|;
specifier|transient
specifier|protected
name|JobConf
name|job
decl_stmt|;
comment|/**    * Constructor when invoked from QL    */
specifier|public
name|ExecDriver
parameter_list|()
block|{
name|super
argument_list|()
expr_stmt|;
block|}
specifier|public
specifier|static
name|String
name|getResourceFiles
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|SessionState
operator|.
name|ResourceType
name|t
parameter_list|)
block|{
comment|// fill in local files to be added to the task environment
name|SessionState
name|ss
init|=
name|SessionState
operator|.
name|get
argument_list|()
decl_stmt|;
name|Set
argument_list|<
name|String
argument_list|>
name|files
init|=
operator|(
name|ss
operator|==
literal|null
operator|)
condition|?
literal|null
else|:
name|ss
operator|.
name|list_resource
argument_list|(
name|t
argument_list|,
literal|null
argument_list|)
decl_stmt|;
if|if
condition|(
name|files
operator|!=
literal|null
condition|)
block|{
name|ArrayList
argument_list|<
name|String
argument_list|>
name|realFiles
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|(
name|files
operator|.
name|size
argument_list|()
argument_list|)
decl_stmt|;
for|for
control|(
name|String
name|one
range|:
name|files
control|)
block|{
try|try
block|{
name|realFiles
operator|.
name|add
argument_list|(
name|Utilities
operator|.
name|realFile
argument_list|(
name|one
argument_list|,
name|conf
argument_list|)
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
literal|"Cannot validate file "
operator|+
name|one
operator|+
literal|"due to exception: "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
return|return
name|StringUtils
operator|.
name|join
argument_list|(
name|realFiles
argument_list|,
literal|","
argument_list|)
return|;
block|}
else|else
block|{
return|return
literal|""
return|;
block|}
block|}
specifier|private
name|void
name|initializeFiles
parameter_list|(
name|String
name|prop
parameter_list|,
name|String
name|files
parameter_list|)
block|{
if|if
condition|(
name|files
operator|!=
literal|null
operator|&&
name|files
operator|.
name|length
argument_list|()
operator|>
literal|0
condition|)
block|{
name|job
operator|.
name|set
argument_list|(
name|prop
argument_list|,
name|files
argument_list|)
expr_stmt|;
comment|// workaround for hadoop-17 - jobclient only looks at commandlineconfig
name|Configuration
name|commandConf
init|=
name|JobClient
operator|.
name|getCommandLineConfig
argument_list|()
decl_stmt|;
if|if
condition|(
name|commandConf
operator|!=
literal|null
condition|)
block|{
name|commandConf
operator|.
name|set
argument_list|(
name|prop
argument_list|,
name|files
argument_list|)
expr_stmt|;
block|}
block|}
block|}
comment|/**    * Initialization when invoked from QL    */
specifier|public
name|void
name|initialize
parameter_list|(
name|HiveConf
name|conf
parameter_list|)
block|{
name|super
operator|.
name|initialize
argument_list|(
name|conf
argument_list|)
expr_stmt|;
name|job
operator|=
operator|new
name|JobConf
argument_list|(
name|conf
argument_list|,
name|ExecDriver
operator|.
name|class
argument_list|)
expr_stmt|;
comment|// NOTE: initialize is only called if it is in non-local mode.
comment|// In case it's in non-local mode, we need to move the SessionState files
comment|// and jars to jobConf.
comment|// In case it's in local mode, MapRedTask will set the jobConf.
comment|//
comment|// "tmpfiles" and "tmpjars" are set by the method ExecDriver.execute(),
comment|// which will be called by both local and NON-local mode.
name|String
name|addedFiles
init|=
name|getResourceFiles
argument_list|(
name|job
argument_list|,
name|SessionState
operator|.
name|ResourceType
operator|.
name|FILE
argument_list|)
decl_stmt|;
if|if
condition|(
name|StringUtils
operator|.
name|isNotBlank
argument_list|(
name|addedFiles
argument_list|)
condition|)
block|{
name|HiveConf
operator|.
name|setVar
argument_list|(
name|job
argument_list|,
name|ConfVars
operator|.
name|HIVEADDEDFILES
argument_list|,
name|addedFiles
argument_list|)
expr_stmt|;
block|}
name|String
name|addedJars
init|=
name|getResourceFiles
argument_list|(
name|job
argument_list|,
name|SessionState
operator|.
name|ResourceType
operator|.
name|JAR
argument_list|)
decl_stmt|;
if|if
condition|(
name|StringUtils
operator|.
name|isNotBlank
argument_list|(
name|addedJars
argument_list|)
condition|)
block|{
name|HiveConf
operator|.
name|setVar
argument_list|(
name|job
argument_list|,
name|ConfVars
operator|.
name|HIVEADDEDJARS
argument_list|,
name|addedJars
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * Constructor/Initialization for invocation as independent utility    */
specifier|public
name|ExecDriver
parameter_list|(
name|mapredWork
name|plan
parameter_list|,
name|JobConf
name|job
parameter_list|,
name|boolean
name|isSilent
parameter_list|)
throws|throws
name|HiveException
block|{
name|setWork
argument_list|(
name|plan
argument_list|)
expr_stmt|;
name|this
operator|.
name|job
operator|=
name|job
expr_stmt|;
name|LOG
operator|=
name|LogFactory
operator|.
name|getLog
argument_list|(
name|this
operator|.
name|getClass
argument_list|()
operator|.
name|getName
argument_list|()
argument_list|)
expr_stmt|;
name|console
operator|=
operator|new
name|LogHelper
argument_list|(
name|LOG
argument_list|,
name|isSilent
argument_list|)
expr_stmt|;
block|}
comment|/**    * A list of the currently running jobs spawned in this Hive instance that is    * used to kill all running jobs in the event of an unexpected shutdown -    * i.e., the JVM shuts down while there are still jobs running.    */
specifier|public
specifier|static
name|HashMap
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|runningJobKillURIs
init|=
operator|new
name|HashMap
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
argument_list|()
decl_stmt|;
comment|/**    * In Hive, when the user control-c's the command line, any running jobs    * spawned from that command line are best-effort killed.    *     * This static constructor registers a shutdown thread to iterate over all the    * running job kill URLs and do a get on them.    *     */
static|static
block|{
if|if
condition|(
operator|new
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configuration
argument_list|()
operator|.
name|getBoolean
argument_list|(
literal|"webinterface.private.actions"
argument_list|,
literal|false
argument_list|)
condition|)
block|{
name|Runtime
operator|.
name|getRuntime
argument_list|()
operator|.
name|addShutdownHook
argument_list|(
operator|new
name|Thread
argument_list|()
block|{
specifier|public
name|void
name|run
parameter_list|()
block|{
for|for
control|(
name|Iterator
argument_list|<
name|String
argument_list|>
name|elems
init|=
name|runningJobKillURIs
operator|.
name|values
argument_list|()
operator|.
name|iterator
argument_list|()
init|;
name|elems
operator|.
name|hasNext
argument_list|()
condition|;
control|)
block|{
name|String
name|uri
init|=
name|elems
operator|.
name|next
argument_list|()
decl_stmt|;
try|try
block|{
name|System
operator|.
name|err
operator|.
name|println
argument_list|(
literal|"killing job with: "
operator|+
name|uri
argument_list|)
expr_stmt|;
name|int
name|retCode
init|=
operator|(
operator|(
name|java
operator|.
name|net
operator|.
name|HttpURLConnection
operator|)
operator|new
name|java
operator|.
name|net
operator|.
name|URL
argument_list|(
name|uri
argument_list|)
operator|.
name|openConnection
argument_list|()
operator|)
operator|.
name|getResponseCode
argument_list|()
decl_stmt|;
if|if
condition|(
name|retCode
operator|!=
literal|200
condition|)
block|{
name|System
operator|.
name|err
operator|.
name|println
argument_list|(
literal|"Got an error trying to kill job with URI: "
operator|+
name|uri
operator|+
literal|" = "
operator|+
name|retCode
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|System
operator|.
name|err
operator|.
name|println
argument_list|(
literal|"trying to kill job, caught: "
operator|+
name|e
argument_list|)
expr_stmt|;
comment|// do nothing
block|}
block|}
block|}
block|}
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * from StreamJob.java    */
specifier|public
name|void
name|jobInfo
parameter_list|(
name|RunningJob
name|rj
parameter_list|)
block|{
if|if
condition|(
name|job
operator|.
name|get
argument_list|(
literal|"mapred.job.tracker"
argument_list|,
literal|"local"
argument_list|)
operator|.
name|equals
argument_list|(
literal|"local"
argument_list|)
condition|)
block|{
name|console
operator|.
name|printInfo
argument_list|(
literal|"Job running in-process (local Hadoop)"
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|String
name|hp
init|=
name|job
operator|.
name|get
argument_list|(
literal|"mapred.job.tracker"
argument_list|)
decl_stmt|;
if|if
condition|(
name|SessionState
operator|.
name|get
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|SessionState
operator|.
name|get
argument_list|()
operator|.
name|getHiveHistory
argument_list|()
operator|.
name|setTaskProperty
argument_list|(
name|SessionState
operator|.
name|get
argument_list|()
operator|.
name|getQueryId
argument_list|()
argument_list|,
name|getId
argument_list|()
argument_list|,
name|Keys
operator|.
name|TASK_HADOOP_ID
argument_list|,
name|rj
operator|.
name|getJobID
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|console
operator|.
name|printInfo
argument_list|(
literal|"Starting Job = "
operator|+
name|rj
operator|.
name|getJobID
argument_list|()
operator|+
literal|", Tracking URL = "
operator|+
name|rj
operator|.
name|getTrackingURL
argument_list|()
argument_list|)
expr_stmt|;
name|console
operator|.
name|printInfo
argument_list|(
literal|"Kill Command = "
operator|+
name|HiveConf
operator|.
name|getVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HADOOPBIN
argument_list|)
operator|+
literal|" job  -Dmapred.job.tracker="
operator|+
name|hp
operator|+
literal|" -kill "
operator|+
name|rj
operator|.
name|getJobID
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * from StreamJob.java    */
specifier|public
name|RunningJob
name|jobProgress
parameter_list|(
name|JobClient
name|jc
parameter_list|,
name|RunningJob
name|rj
parameter_list|)
throws|throws
name|IOException
block|{
name|String
name|lastReport
init|=
literal|""
decl_stmt|;
name|SimpleDateFormat
name|dateFormat
init|=
operator|new
name|SimpleDateFormat
argument_list|(
literal|"yyyy-MM-dd hh:mm:ss,SSS"
argument_list|)
decl_stmt|;
name|long
name|reportTime
init|=
name|System
operator|.
name|currentTimeMillis
argument_list|()
decl_stmt|;
name|long
name|maxReportInterval
init|=
literal|60
operator|*
literal|1000
decl_stmt|;
comment|// One minute
while|while
condition|(
operator|!
name|rj
operator|.
name|isComplete
argument_list|()
condition|)
block|{
try|try
block|{
name|Thread
operator|.
name|sleep
argument_list|(
literal|1000
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|InterruptedException
name|e
parameter_list|)
block|{       }
name|rj
operator|=
name|jc
operator|.
name|getJob
argument_list|(
name|rj
operator|.
name|getJobID
argument_list|()
argument_list|)
expr_stmt|;
name|String
name|report
init|=
literal|" map = "
operator|+
name|Math
operator|.
name|round
argument_list|(
name|rj
operator|.
name|mapProgress
argument_list|()
operator|*
literal|100
argument_list|)
operator|+
literal|"%,  reduce ="
operator|+
name|Math
operator|.
name|round
argument_list|(
name|rj
operator|.
name|reduceProgress
argument_list|()
operator|*
literal|100
argument_list|)
operator|+
literal|"%"
decl_stmt|;
if|if
condition|(
operator|!
name|report
operator|.
name|equals
argument_list|(
name|lastReport
argument_list|)
operator|||
name|System
operator|.
name|currentTimeMillis
argument_list|()
operator|>=
name|reportTime
operator|+
name|maxReportInterval
condition|)
block|{
name|String
name|output
init|=
name|dateFormat
operator|.
name|format
argument_list|(
name|Calendar
operator|.
name|getInstance
argument_list|()
operator|.
name|getTime
argument_list|()
argument_list|)
operator|+
name|report
decl_stmt|;
name|SessionState
name|ss
init|=
name|SessionState
operator|.
name|get
argument_list|()
decl_stmt|;
if|if
condition|(
name|ss
operator|!=
literal|null
condition|)
block|{
name|ss
operator|.
name|getHiveHistory
argument_list|()
operator|.
name|setTaskCounters
argument_list|(
name|SessionState
operator|.
name|get
argument_list|()
operator|.
name|getQueryId
argument_list|()
argument_list|,
name|getId
argument_list|()
argument_list|,
name|rj
argument_list|)
expr_stmt|;
name|ss
operator|.
name|getHiveHistory
argument_list|()
operator|.
name|setTaskProperty
argument_list|(
name|SessionState
operator|.
name|get
argument_list|()
operator|.
name|getQueryId
argument_list|()
argument_list|,
name|getId
argument_list|()
argument_list|,
name|Keys
operator|.
name|TASK_HADOOP_PROGRESS
argument_list|,
name|output
argument_list|)
expr_stmt|;
name|ss
operator|.
name|getHiveHistory
argument_list|()
operator|.
name|progressTask
argument_list|(
name|SessionState
operator|.
name|get
argument_list|()
operator|.
name|getQueryId
argument_list|()
argument_list|,
name|this
argument_list|)
expr_stmt|;
block|}
name|console
operator|.
name|printInfo
argument_list|(
name|output
argument_list|)
expr_stmt|;
name|lastReport
operator|=
name|report
expr_stmt|;
name|reportTime
operator|=
name|System
operator|.
name|currentTimeMillis
argument_list|()
expr_stmt|;
block|}
block|}
return|return
name|rj
return|;
block|}
comment|/**    * Estimate the number of reducers needed for this job, based on job input,    * and configuration parameters.    * @return the number of reducers.    */
specifier|public
name|int
name|estimateNumberOfReducers
parameter_list|(
name|HiveConf
name|hive
parameter_list|,
name|JobConf
name|job
parameter_list|,
name|mapredWork
name|work
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|hive
operator|==
literal|null
condition|)
block|{
name|hive
operator|=
operator|new
name|HiveConf
argument_list|()
expr_stmt|;
block|}
name|long
name|bytesPerReducer
init|=
name|hive
operator|.
name|getLongVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|BYTESPERREDUCER
argument_list|)
decl_stmt|;
name|int
name|maxReducers
init|=
name|hive
operator|.
name|getIntVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|MAXREDUCERS
argument_list|)
decl_stmt|;
name|long
name|totalInputFileSize
init|=
name|getTotalInputFileSize
argument_list|(
name|job
argument_list|,
name|work
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"BytesPerReducer="
operator|+
name|bytesPerReducer
operator|+
literal|" maxReducers="
operator|+
name|maxReducers
operator|+
literal|" totalInputFileSize="
operator|+
name|totalInputFileSize
argument_list|)
expr_stmt|;
name|int
name|reducers
init|=
call|(
name|int
call|)
argument_list|(
operator|(
name|totalInputFileSize
operator|+
name|bytesPerReducer
operator|-
literal|1
operator|)
operator|/
name|bytesPerReducer
argument_list|)
decl_stmt|;
name|reducers
operator|=
name|Math
operator|.
name|max
argument_list|(
literal|1
argument_list|,
name|reducers
argument_list|)
expr_stmt|;
name|reducers
operator|=
name|Math
operator|.
name|min
argument_list|(
name|maxReducers
argument_list|,
name|reducers
argument_list|)
expr_stmt|;
return|return
name|reducers
return|;
block|}
comment|/**    * Set the number of reducers for the mapred work.    */
specifier|protected
name|void
name|setNumberOfReducers
parameter_list|()
throws|throws
name|IOException
block|{
comment|// this is a temporary hack to fix things that are not fixed in the compiler
name|Integer
name|numReducersFromWork
init|=
name|work
operator|.
name|getNumReduceTasks
argument_list|()
decl_stmt|;
if|if
condition|(
name|work
operator|.
name|getReducer
argument_list|()
operator|==
literal|null
condition|)
block|{
name|console
operator|.
name|printInfo
argument_list|(
literal|"Number of reduce tasks is set to 0 since there's no reduce operator"
argument_list|)
expr_stmt|;
name|work
operator|.
name|setNumReduceTasks
argument_list|(
name|Integer
operator|.
name|valueOf
argument_list|(
literal|0
argument_list|)
argument_list|)
expr_stmt|;
block|}
else|else
block|{
if|if
condition|(
name|numReducersFromWork
operator|>=
literal|0
condition|)
block|{
name|console
operator|.
name|printInfo
argument_list|(
literal|"Number of reduce tasks determined at compile time: "
operator|+
name|work
operator|.
name|getNumReduceTasks
argument_list|()
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|job
operator|.
name|getNumReduceTasks
argument_list|()
operator|>
literal|0
condition|)
block|{
name|int
name|reducers
init|=
name|job
operator|.
name|getNumReduceTasks
argument_list|()
decl_stmt|;
name|work
operator|.
name|setNumReduceTasks
argument_list|(
name|reducers
argument_list|)
expr_stmt|;
name|console
operator|.
name|printInfo
argument_list|(
literal|"Number of reduce tasks not specified. Defaulting to jobconf value of: "
operator|+
name|reducers
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|int
name|reducers
init|=
name|estimateNumberOfReducers
argument_list|(
name|conf
argument_list|,
name|job
argument_list|,
name|work
argument_list|)
decl_stmt|;
name|work
operator|.
name|setNumReduceTasks
argument_list|(
name|reducers
argument_list|)
expr_stmt|;
name|console
operator|.
name|printInfo
argument_list|(
literal|"Number of reduce tasks not specified. Estimated from input data size: "
operator|+
name|reducers
argument_list|)
expr_stmt|;
block|}
name|console
operator|.
name|printInfo
argument_list|(
literal|"In order to change the average load for a reducer (in bytes):"
argument_list|)
expr_stmt|;
name|console
operator|.
name|printInfo
argument_list|(
literal|"  set "
operator|+
name|HiveConf
operator|.
name|ConfVars
operator|.
name|BYTESPERREDUCER
operator|.
name|varname
operator|+
literal|"=<number>"
argument_list|)
expr_stmt|;
name|console
operator|.
name|printInfo
argument_list|(
literal|"In order to limit the maximum number of reducers:"
argument_list|)
expr_stmt|;
name|console
operator|.
name|printInfo
argument_list|(
literal|"  set "
operator|+
name|HiveConf
operator|.
name|ConfVars
operator|.
name|MAXREDUCERS
operator|.
name|varname
operator|+
literal|"=<number>"
argument_list|)
expr_stmt|;
name|console
operator|.
name|printInfo
argument_list|(
literal|"In order to set a constant number of reducers:"
argument_list|)
expr_stmt|;
name|console
operator|.
name|printInfo
argument_list|(
literal|"  set "
operator|+
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HADOOPNUMREDUCERS
operator|+
literal|"=<number>"
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * Calculate the total size of input files.    * @param job the hadoop job conf.    * @return the total size in bytes.    * @throws IOException     */
specifier|public
name|long
name|getTotalInputFileSize
parameter_list|(
name|JobConf
name|job
parameter_list|,
name|mapredWork
name|work
parameter_list|)
throws|throws
name|IOException
block|{
name|long
name|r
init|=
literal|0
decl_stmt|;
comment|// For each input path, calculate the total size.
for|for
control|(
name|String
name|path
range|:
name|work
operator|.
name|getPathToAliases
argument_list|()
operator|.
name|keySet
argument_list|()
control|)
block|{
try|try
block|{
name|Path
name|p
init|=
operator|new
name|Path
argument_list|(
name|path
argument_list|)
decl_stmt|;
name|FileSystem
name|fs
init|=
name|p
operator|.
name|getFileSystem
argument_list|(
name|job
argument_list|)
decl_stmt|;
name|ContentSummary
name|cs
init|=
name|fs
operator|.
name|getContentSummary
argument_list|(
name|p
argument_list|)
decl_stmt|;
name|r
operator|+=
name|cs
operator|.
name|getLength
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Cannot get size of "
operator|+
name|path
operator|+
literal|". Safely ignored."
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|r
return|;
block|}
comment|/**    * Execute a query plan using Hadoop    */
specifier|public
name|int
name|execute
parameter_list|()
block|{
try|try
block|{
name|setNumberOfReducers
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|String
name|statusMesg
init|=
literal|"IOException while accessing HDFS to estimate the number of reducers: "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
decl_stmt|;
name|console
operator|.
name|printError
argument_list|(
name|statusMesg
argument_list|,
literal|"\n"
operator|+
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
return|return
literal|1
return|;
block|}
name|String
name|invalidReason
init|=
name|work
operator|.
name|isInvalid
argument_list|()
decl_stmt|;
if|if
condition|(
name|invalidReason
operator|!=
literal|null
condition|)
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
literal|"Plan invalid, Reason: "
operator|+
name|invalidReason
argument_list|)
throw|;
block|}
name|String
name|hiveScratchDir
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|SCRATCHDIR
argument_list|)
decl_stmt|;
name|Path
name|jobScratchDir
init|=
operator|new
name|Path
argument_list|(
name|hiveScratchDir
operator|+
name|Utilities
operator|.
name|randGen
operator|.
name|nextInt
argument_list|()
argument_list|)
decl_stmt|;
name|FileOutputFormat
operator|.
name|setOutputPath
argument_list|(
name|job
argument_list|,
name|jobScratchDir
argument_list|)
expr_stmt|;
name|job
operator|.
name|setMapperClass
argument_list|(
name|ExecMapper
operator|.
name|class
argument_list|)
expr_stmt|;
name|job
operator|.
name|setMapOutputKeyClass
argument_list|(
name|HiveKey
operator|.
name|class
argument_list|)
expr_stmt|;
comment|// LazySimpleSerDe writes to Text
comment|// Revert to DynamicSerDe: job.setMapOutputValueClass(BytesWritable.class);
name|job
operator|.
name|setMapOutputValueClass
argument_list|(
name|Text
operator|.
name|class
argument_list|)
expr_stmt|;
name|job
operator|.
name|setNumReduceTasks
argument_list|(
name|work
operator|.
name|getNumReduceTasks
argument_list|()
operator|.
name|intValue
argument_list|()
argument_list|)
expr_stmt|;
name|job
operator|.
name|setReducerClass
argument_list|(
name|ExecReducer
operator|.
name|class
argument_list|)
expr_stmt|;
name|job
operator|.
name|setInputFormat
argument_list|(
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|HiveInputFormat
operator|.
name|class
argument_list|)
expr_stmt|;
comment|// No-Op - we don't really write anything here ..
name|job
operator|.
name|setOutputKeyClass
argument_list|(
name|Text
operator|.
name|class
argument_list|)
expr_stmt|;
name|job
operator|.
name|setOutputValueClass
argument_list|(
name|Text
operator|.
name|class
argument_list|)
expr_stmt|;
comment|// Transfer HIVEAUXJARS and HIVEADDEDJARS to "tmpjars" so hadoop understands it
name|String
name|auxJars
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEAUXJARS
argument_list|)
decl_stmt|;
name|String
name|addedJars
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEADDEDJARS
argument_list|)
decl_stmt|;
if|if
condition|(
name|StringUtils
operator|.
name|isNotBlank
argument_list|(
name|auxJars
argument_list|)
operator|||
name|StringUtils
operator|.
name|isNotBlank
argument_list|(
name|addedJars
argument_list|)
condition|)
block|{
name|String
name|allJars
init|=
name|StringUtils
operator|.
name|isNotBlank
argument_list|(
name|auxJars
argument_list|)
condition|?
operator|(
name|StringUtils
operator|.
name|isNotBlank
argument_list|(
name|addedJars
argument_list|)
condition|?
name|addedJars
operator|+
literal|","
operator|+
name|auxJars
else|:
name|auxJars
operator|)
else|:
name|addedJars
decl_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"adding libjars: "
operator|+
name|allJars
argument_list|)
expr_stmt|;
name|initializeFiles
argument_list|(
literal|"tmpjars"
argument_list|,
name|allJars
argument_list|)
expr_stmt|;
block|}
comment|// Transfer HIVEADDEDFILES to "tmpfiles" so hadoop understands it
name|String
name|addedFiles
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEADDEDFILES
argument_list|)
decl_stmt|;
if|if
condition|(
name|StringUtils
operator|.
name|isNotBlank
argument_list|(
name|addedFiles
argument_list|)
condition|)
block|{
name|initializeFiles
argument_list|(
literal|"tmpfiles"
argument_list|,
name|addedFiles
argument_list|)
expr_stmt|;
block|}
name|int
name|returnVal
init|=
literal|0
decl_stmt|;
name|RunningJob
name|rj
init|=
literal|null
decl_stmt|,
name|orig_rj
init|=
literal|null
decl_stmt|;
name|boolean
name|success
init|=
literal|false
decl_stmt|;
try|try
block|{
name|addInputPaths
argument_list|(
name|job
argument_list|,
name|work
argument_list|,
name|hiveScratchDir
argument_list|)
expr_stmt|;
name|Utilities
operator|.
name|setMapRedWork
argument_list|(
name|job
argument_list|,
name|work
argument_list|)
expr_stmt|;
comment|// remove the pwd from conf file so that job tracker doesn't show this logs
name|String
name|pwd
init|=
name|job
operator|.
name|get
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|METASTOREPWD
operator|.
name|varname
argument_list|)
decl_stmt|;
if|if
condition|(
name|pwd
operator|!=
literal|null
condition|)
name|job
operator|.
name|set
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|METASTOREPWD
operator|.
name|varname
argument_list|,
literal|"HIVE"
argument_list|)
expr_stmt|;
name|JobClient
name|jc
init|=
operator|new
name|JobClient
argument_list|(
name|job
argument_list|)
decl_stmt|;
comment|// make this client wait if job trcker is not behaving well.
name|Throttle
operator|.
name|checkJobTracker
argument_list|(
name|job
argument_list|,
name|LOG
argument_list|)
expr_stmt|;
name|orig_rj
operator|=
name|rj
operator|=
name|jc
operator|.
name|submitJob
argument_list|(
name|job
argument_list|)
expr_stmt|;
comment|// replace it back
if|if
condition|(
name|pwd
operator|!=
literal|null
condition|)
name|job
operator|.
name|set
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|METASTOREPWD
operator|.
name|varname
argument_list|,
name|pwd
argument_list|)
expr_stmt|;
comment|// add to list of running jobs so in case of abnormal shutdown can kill
comment|// it.
name|runningJobKillURIs
operator|.
name|put
argument_list|(
name|rj
operator|.
name|getJobID
argument_list|()
argument_list|,
name|rj
operator|.
name|getTrackingURL
argument_list|()
operator|+
literal|"&action=kill"
argument_list|)
expr_stmt|;
name|jobInfo
argument_list|(
name|rj
argument_list|)
expr_stmt|;
name|rj
operator|=
name|jobProgress
argument_list|(
name|jc
argument_list|,
name|rj
argument_list|)
expr_stmt|;
if|if
condition|(
name|rj
operator|==
literal|null
condition|)
block|{
comment|// in the corner case where the running job has disappeared from JT memory
comment|// remember that we did actually submit the job.
name|rj
operator|=
name|orig_rj
expr_stmt|;
name|success
operator|=
literal|false
expr_stmt|;
block|}
else|else
block|{
name|success
operator|=
name|rj
operator|.
name|isSuccessful
argument_list|()
expr_stmt|;
block|}
name|String
name|statusMesg
init|=
literal|"Ended Job = "
operator|+
name|rj
operator|.
name|getJobID
argument_list|()
decl_stmt|;
if|if
condition|(
operator|!
name|success
condition|)
block|{
name|statusMesg
operator|+=
literal|" with errors"
expr_stmt|;
name|returnVal
operator|=
literal|2
expr_stmt|;
name|console
operator|.
name|printError
argument_list|(
name|statusMesg
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|console
operator|.
name|printInfo
argument_list|(
name|statusMesg
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|String
name|mesg
init|=
literal|" with exception '"
operator|+
name|Utilities
operator|.
name|getNameMessage
argument_list|(
name|e
argument_list|)
operator|+
literal|"'"
decl_stmt|;
if|if
condition|(
name|rj
operator|!=
literal|null
condition|)
block|{
name|mesg
operator|=
literal|"Ended Job = "
operator|+
name|rj
operator|.
name|getJobID
argument_list|()
operator|+
name|mesg
expr_stmt|;
block|}
else|else
block|{
name|mesg
operator|=
literal|"Job Submission failed"
operator|+
name|mesg
expr_stmt|;
block|}
comment|// Has to use full name to make sure it does not conflict with
comment|// org.apache.commons.lang.StringUtils
name|console
operator|.
name|printError
argument_list|(
name|mesg
argument_list|,
literal|"\n"
operator|+
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
name|success
operator|=
literal|false
expr_stmt|;
name|returnVal
operator|=
literal|1
expr_stmt|;
block|}
finally|finally
block|{
name|Utilities
operator|.
name|clearMapRedWork
argument_list|(
name|job
argument_list|)
expr_stmt|;
try|try
block|{
name|FileSystem
name|fs
init|=
name|jobScratchDir
operator|.
name|getFileSystem
argument_list|(
name|job
argument_list|)
decl_stmt|;
name|fs
operator|.
name|delete
argument_list|(
name|jobScratchDir
argument_list|,
literal|true
argument_list|)
expr_stmt|;
if|if
condition|(
name|returnVal
operator|!=
literal|0
operator|&&
name|rj
operator|!=
literal|null
condition|)
block|{
name|rj
operator|.
name|killJob
argument_list|()
expr_stmt|;
block|}
name|runningJobKillURIs
operator|.
name|remove
argument_list|(
name|rj
operator|.
name|getJobID
argument_list|()
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{       }
block|}
try|try
block|{
if|if
condition|(
name|rj
operator|!=
literal|null
condition|)
block|{
if|if
condition|(
name|work
operator|.
name|getAliasToWork
argument_list|()
operator|!=
literal|null
condition|)
block|{
for|for
control|(
name|Operator
argument_list|<
name|?
extends|extends
name|Serializable
argument_list|>
name|op
range|:
name|work
operator|.
name|getAliasToWork
argument_list|()
operator|.
name|values
argument_list|()
control|)
block|{
name|op
operator|.
name|jobClose
argument_list|(
name|job
argument_list|,
name|success
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
name|work
operator|.
name|getReducer
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|work
operator|.
name|getReducer
argument_list|()
operator|.
name|jobClose
argument_list|(
name|job
argument_list|,
name|success
argument_list|)
expr_stmt|;
block|}
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
comment|// jobClose needs to execute successfully otherwise fail task
if|if
condition|(
name|success
condition|)
block|{
name|success
operator|=
literal|false
expr_stmt|;
name|returnVal
operator|=
literal|3
expr_stmt|;
name|String
name|mesg
init|=
literal|"Job Commit failed with exception '"
operator|+
name|Utilities
operator|.
name|getNameMessage
argument_list|(
name|e
argument_list|)
operator|+
literal|"'"
decl_stmt|;
name|console
operator|.
name|printError
argument_list|(
name|mesg
argument_list|,
literal|"\n"
operator|+
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
return|return
operator|(
name|returnVal
operator|)
return|;
block|}
specifier|private
specifier|static
name|void
name|printUsage
parameter_list|()
block|{
name|System
operator|.
name|out
operator|.
name|println
argument_list|(
literal|"ExecDriver -plan<plan-file> [-jobconf k1=v1 [-jobconf k2=v2] ...] "
operator|+
literal|"[-files<file1>[,<file2>] ...]"
argument_list|)
expr_stmt|;
name|System
operator|.
name|exit
argument_list|(
literal|1
argument_list|)
expr_stmt|;
block|}
specifier|public
specifier|static
name|void
name|main
parameter_list|(
name|String
index|[]
name|args
parameter_list|)
throws|throws
name|IOException
throws|,
name|HiveException
block|{
name|String
name|planFileName
init|=
literal|null
decl_stmt|;
name|ArrayList
argument_list|<
name|String
argument_list|>
name|jobConfArgs
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
name|boolean
name|isSilent
init|=
literal|false
decl_stmt|;
name|String
name|files
init|=
literal|null
decl_stmt|;
try|try
block|{
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|args
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
if|if
condition|(
name|args
index|[
name|i
index|]
operator|.
name|equals
argument_list|(
literal|"-plan"
argument_list|)
condition|)
block|{
name|planFileName
operator|=
name|args
index|[
operator|++
name|i
index|]
expr_stmt|;
name|System
operator|.
name|out
operator|.
name|println
argument_list|(
literal|"plan = "
operator|+
name|planFileName
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|args
index|[
name|i
index|]
operator|.
name|equals
argument_list|(
literal|"-jobconf"
argument_list|)
condition|)
block|{
name|jobConfArgs
operator|.
name|add
argument_list|(
name|args
index|[
operator|++
name|i
index|]
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|args
index|[
name|i
index|]
operator|.
name|equals
argument_list|(
literal|"-silent"
argument_list|)
condition|)
block|{
name|isSilent
operator|=
literal|true
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|args
index|[
name|i
index|]
operator|.
name|equals
argument_list|(
literal|"-files"
argument_list|)
condition|)
block|{
name|files
operator|=
name|args
index|[
operator|++
name|i
index|]
expr_stmt|;
block|}
block|}
block|}
catch|catch
parameter_list|(
name|IndexOutOfBoundsException
name|e
parameter_list|)
block|{
name|System
operator|.
name|err
operator|.
name|println
argument_list|(
literal|"Missing argument to option"
argument_list|)
expr_stmt|;
name|printUsage
argument_list|()
expr_stmt|;
block|}
comment|// If started from main(), and isSilent is on, we should not output
comment|// any logs.
comment|// To turn the error log on, please set -Dtest.silent=false
if|if
condition|(
name|isSilent
condition|)
block|{
name|BasicConfigurator
operator|.
name|resetConfiguration
argument_list|()
expr_stmt|;
name|BasicConfigurator
operator|.
name|configure
argument_list|(
operator|new
name|NullAppender
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|planFileName
operator|==
literal|null
condition|)
block|{
name|System
operator|.
name|err
operator|.
name|println
argument_list|(
literal|"Must specify Plan File Name"
argument_list|)
expr_stmt|;
name|printUsage
argument_list|()
expr_stmt|;
block|}
name|JobConf
name|conf
init|=
operator|new
name|JobConf
argument_list|(
name|ExecDriver
operator|.
name|class
argument_list|)
decl_stmt|;
for|for
control|(
name|String
name|one
range|:
name|jobConfArgs
control|)
block|{
name|int
name|eqIndex
init|=
name|one
operator|.
name|indexOf
argument_list|(
literal|'='
argument_list|)
decl_stmt|;
if|if
condition|(
name|eqIndex
operator|!=
operator|-
literal|1
condition|)
block|{
try|try
block|{
name|conf
operator|.
name|set
argument_list|(
name|one
operator|.
name|substring
argument_list|(
literal|0
argument_list|,
name|eqIndex
argument_list|)
argument_list|,
name|URLDecoder
operator|.
name|decode
argument_list|(
name|one
operator|.
name|substring
argument_list|(
name|eqIndex
operator|+
literal|1
argument_list|)
argument_list|,
literal|"UTF-8"
argument_list|)
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|UnsupportedEncodingException
name|e
parameter_list|)
block|{
name|System
operator|.
name|err
operator|.
name|println
argument_list|(
literal|"Unexpected error "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
operator|+
literal|" while encoding "
operator|+
name|one
operator|.
name|substring
argument_list|(
name|eqIndex
operator|+
literal|1
argument_list|)
argument_list|)
expr_stmt|;
name|System
operator|.
name|exit
argument_list|(
literal|3
argument_list|)
expr_stmt|;
block|}
block|}
block|}
if|if
condition|(
name|files
operator|!=
literal|null
condition|)
block|{
name|conf
operator|.
name|set
argument_list|(
literal|"tmpfiles"
argument_list|,
name|files
argument_list|)
expr_stmt|;
block|}
name|URI
name|pathURI
init|=
operator|(
operator|new
name|Path
argument_list|(
name|planFileName
argument_list|)
operator|)
operator|.
name|toUri
argument_list|()
decl_stmt|;
name|InputStream
name|pathData
decl_stmt|;
if|if
condition|(
name|StringUtils
operator|.
name|isEmpty
argument_list|(
name|pathURI
operator|.
name|getScheme
argument_list|()
argument_list|)
condition|)
block|{
comment|// default to local file system
name|pathData
operator|=
operator|new
name|FileInputStream
argument_list|(
name|planFileName
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// otherwise may be in hadoop ..
name|FileSystem
name|fs
init|=
name|FileSystem
operator|.
name|get
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|pathData
operator|=
name|fs
operator|.
name|open
argument_list|(
operator|new
name|Path
argument_list|(
name|planFileName
argument_list|)
argument_list|)
expr_stmt|;
block|}
comment|// workaround for hadoop-17 - libjars are not added to classpath. this
comment|// affects local
comment|// mode execution
name|boolean
name|localMode
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|conf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HADOOPJT
argument_list|)
operator|.
name|equals
argument_list|(
literal|"local"
argument_list|)
decl_stmt|;
if|if
condition|(
name|localMode
condition|)
block|{
name|String
name|auxJars
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|conf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEAUXJARS
argument_list|)
decl_stmt|;
name|String
name|addedJars
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|conf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEADDEDJARS
argument_list|)
decl_stmt|;
try|try
block|{
name|ClassLoader
name|loader
init|=
name|conf
operator|.
name|getClassLoader
argument_list|()
decl_stmt|;
if|if
condition|(
name|StringUtils
operator|.
name|isNotBlank
argument_list|(
name|auxJars
argument_list|)
condition|)
block|{
name|loader
operator|=
name|Utilities
operator|.
name|addToClassPath
argument_list|(
name|loader
argument_list|,
name|StringUtils
operator|.
name|split
argument_list|(
name|auxJars
argument_list|,
literal|","
argument_list|)
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|StringUtils
operator|.
name|isNotBlank
argument_list|(
name|addedJars
argument_list|)
condition|)
block|{
name|loader
operator|=
name|Utilities
operator|.
name|addToClassPath
argument_list|(
name|loader
argument_list|,
name|StringUtils
operator|.
name|split
argument_list|(
name|addedJars
argument_list|,
literal|","
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|conf
operator|.
name|setClassLoader
argument_list|(
name|loader
argument_list|)
expr_stmt|;
comment|// Also set this to the Thread ContextClassLoader, so new threads will inherit
comment|// this class loader, and propagate into newly created Configurations by those
comment|// new threads.
name|Thread
operator|.
name|currentThread
argument_list|()
operator|.
name|setContextClassLoader
argument_list|(
name|loader
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
name|mapredWork
name|plan
init|=
name|Utilities
operator|.
name|deserializeMapRedWork
argument_list|(
name|pathData
argument_list|,
name|conf
argument_list|)
decl_stmt|;
name|ExecDriver
name|ed
init|=
operator|new
name|ExecDriver
argument_list|(
name|plan
argument_list|,
name|conf
argument_list|,
name|isSilent
argument_list|)
decl_stmt|;
name|int
name|ret
init|=
name|ed
operator|.
name|execute
argument_list|()
decl_stmt|;
if|if
condition|(
name|ret
operator|!=
literal|0
condition|)
block|{
name|System
operator|.
name|out
operator|.
name|println
argument_list|(
literal|"Job Failed"
argument_list|)
expr_stmt|;
name|System
operator|.
name|exit
argument_list|(
literal|2
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * Given a Hive Configuration object - generate a command line fragment for    * passing such configuration information to ExecDriver    */
specifier|public
specifier|static
name|String
name|generateCmdLine
parameter_list|(
name|HiveConf
name|hconf
parameter_list|)
block|{
try|try
block|{
name|StringBuilder
name|sb
init|=
operator|new
name|StringBuilder
argument_list|()
decl_stmt|;
name|Properties
name|deltaP
init|=
name|hconf
operator|.
name|getChangedProperties
argument_list|()
decl_stmt|;
name|boolean
name|localMode
init|=
name|hconf
operator|.
name|getVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HADOOPJT
argument_list|)
operator|.
name|equals
argument_list|(
literal|"local"
argument_list|)
decl_stmt|;
name|String
name|hadoopSysDir
init|=
literal|"mapred.system.dir"
decl_stmt|;
name|String
name|hadoopWorkDir
init|=
literal|"mapred.local.dir"
decl_stmt|;
for|for
control|(
name|Object
name|one
range|:
name|deltaP
operator|.
name|keySet
argument_list|()
control|)
block|{
name|String
name|oneProp
init|=
operator|(
name|String
operator|)
name|one
decl_stmt|;
if|if
condition|(
name|localMode
operator|&&
operator|(
name|oneProp
operator|.
name|equals
argument_list|(
name|hadoopSysDir
argument_list|)
operator|||
name|oneProp
operator|.
name|equals
argument_list|(
name|hadoopWorkDir
argument_list|)
operator|)
condition|)
continue|continue;
name|String
name|oneValue
init|=
name|deltaP
operator|.
name|getProperty
argument_list|(
name|oneProp
argument_list|)
decl_stmt|;
name|sb
operator|.
name|append
argument_list|(
literal|"-jobconf "
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
name|oneProp
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
literal|"="
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
name|URLEncoder
operator|.
name|encode
argument_list|(
name|oneValue
argument_list|,
literal|"UTF-8"
argument_list|)
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
literal|" "
argument_list|)
expr_stmt|;
block|}
comment|// Multiple concurrent local mode job submissions can cause collisions in
comment|// working dirs
comment|// Workaround is to rename map red working dir to a temp dir in such a
comment|// case
if|if
condition|(
name|localMode
condition|)
block|{
name|sb
operator|.
name|append
argument_list|(
literal|"-jobconf "
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
name|hadoopSysDir
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
literal|"="
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
name|URLEncoder
operator|.
name|encode
argument_list|(
name|hconf
operator|.
name|get
argument_list|(
name|hadoopSysDir
argument_list|)
operator|+
literal|"/"
operator|+
name|Utilities
operator|.
name|randGen
operator|.
name|nextInt
argument_list|()
argument_list|,
literal|"UTF-8"
argument_list|)
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
literal|" "
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
literal|"-jobconf "
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
name|hadoopWorkDir
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
literal|"="
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
name|URLEncoder
operator|.
name|encode
argument_list|(
name|hconf
operator|.
name|get
argument_list|(
name|hadoopWorkDir
argument_list|)
operator|+
literal|"/"
operator|+
name|Utilities
operator|.
name|randGen
operator|.
name|nextInt
argument_list|()
argument_list|,
literal|"UTF-8"
argument_list|)
argument_list|)
expr_stmt|;
block|}
return|return
name|sb
operator|.
name|toString
argument_list|()
return|;
block|}
catch|catch
parameter_list|(
name|UnsupportedEncodingException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|isMapRedTask
parameter_list|()
block|{
return|return
literal|true
return|;
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|hasReduce
parameter_list|()
block|{
name|mapredWork
name|w
init|=
name|getWork
argument_list|()
decl_stmt|;
return|return
name|w
operator|.
name|getReducer
argument_list|()
operator|!=
literal|null
return|;
block|}
specifier|private
name|void
name|addInputPaths
parameter_list|(
name|JobConf
name|job
parameter_list|,
name|mapredWork
name|work
parameter_list|,
name|String
name|hiveScratchDir
parameter_list|)
throws|throws
name|Exception
block|{
name|int
name|numEmptyPaths
init|=
literal|0
decl_stmt|;
comment|// If the query references non-existent partitions
if|if
condition|(
name|work
operator|.
name|getPathToAliases
argument_list|()
operator|.
name|isEmpty
argument_list|()
operator|&&
operator|!
name|work
operator|.
name|getAliasToWork
argument_list|()
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|String
name|oneAlias
init|=
operator|(
name|String
operator|)
name|work
operator|.
name|getAliasToWork
argument_list|()
operator|.
name|keySet
argument_list|()
operator|.
name|toArray
argument_list|()
index|[
literal|0
index|]
decl_stmt|;
name|Class
argument_list|<
name|?
extends|extends
name|HiveOutputFormat
argument_list|>
name|outFileFormat
init|=
operator|(
name|Class
argument_list|<
name|?
extends|extends
name|HiveOutputFormat
argument_list|>
operator|)
name|job
operator|.
name|getClassByName
argument_list|(
literal|"org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat"
argument_list|)
decl_stmt|;
name|String
name|newFile
init|=
name|hiveScratchDir
operator|+
name|File
operator|.
name|separator
operator|+
operator|(
operator|++
name|numEmptyPaths
operator|)
decl_stmt|;
name|Path
name|newPath
init|=
operator|new
name|Path
argument_list|(
name|newFile
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Changed input file to "
operator|+
name|newPath
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
comment|// add a dummy work
name|Map
argument_list|<
name|String
argument_list|,
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|>
name|pathToAliases
init|=
name|work
operator|.
name|getPathToAliases
argument_list|()
decl_stmt|;
name|ArrayList
argument_list|<
name|String
argument_list|>
name|newList
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
name|newList
operator|.
name|add
argument_list|(
name|oneAlias
argument_list|)
expr_stmt|;
name|pathToAliases
operator|.
name|put
argument_list|(
name|newPath
operator|.
name|toString
argument_list|()
argument_list|,
name|newList
argument_list|)
expr_stmt|;
name|Map
argument_list|<
name|String
argument_list|,
name|partitionDesc
argument_list|>
name|pathToPartitionInfo
init|=
name|work
operator|.
name|getPathToPartitionInfo
argument_list|()
decl_stmt|;
name|partitionDesc
name|pDesc
init|=
name|work
operator|.
name|getAliasToPartnInfo
argument_list|()
operator|.
name|get
argument_list|(
name|oneAlias
argument_list|)
decl_stmt|;
name|pathToPartitionInfo
operator|.
name|put
argument_list|(
name|newPath
operator|.
name|toString
argument_list|()
argument_list|,
name|pDesc
argument_list|)
expr_stmt|;
name|RecordWriter
name|recWriter
init|=
name|outFileFormat
operator|.
name|newInstance
argument_list|()
operator|.
name|getHiveRecordWriter
argument_list|(
name|job
argument_list|,
name|newPath
argument_list|,
name|Text
operator|.
name|class
argument_list|,
literal|false
argument_list|,
operator|new
name|Properties
argument_list|()
argument_list|,
literal|null
argument_list|)
decl_stmt|;
name|recWriter
operator|.
name|close
argument_list|(
literal|false
argument_list|)
expr_stmt|;
name|FileInputFormat
operator|.
name|addInputPaths
argument_list|(
name|job
argument_list|,
name|newPath
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|List
argument_list|<
name|String
argument_list|>
name|emptyPaths
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|String
name|onefile
range|:
name|work
operator|.
name|getPathToAliases
argument_list|()
operator|.
name|keySet
argument_list|()
control|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Adding input file "
operator|+
name|onefile
argument_list|)
expr_stmt|;
comment|// If the input file does not exist, replace it by a empty file
name|Path
name|dirPath
init|=
operator|new
name|Path
argument_list|(
name|onefile
argument_list|)
decl_stmt|;
name|FileSystem
name|inpFs
init|=
name|dirPath
operator|.
name|getFileSystem
argument_list|(
name|job
argument_list|)
decl_stmt|;
name|boolean
name|emptyInput
init|=
literal|true
decl_stmt|;
if|if
condition|(
name|inpFs
operator|.
name|exists
argument_list|(
name|dirPath
argument_list|)
condition|)
block|{
name|FileStatus
index|[]
name|fStats
init|=
name|inpFs
operator|.
name|listStatus
argument_list|(
name|dirPath
argument_list|)
decl_stmt|;
if|if
condition|(
name|fStats
operator|.
name|length
operator|>
literal|0
condition|)
name|emptyInput
operator|=
literal|false
expr_stmt|;
block|}
if|if
condition|(
name|emptyInput
condition|)
name|emptyPaths
operator|.
name|add
argument_list|(
name|onefile
argument_list|)
expr_stmt|;
else|else
name|FileInputFormat
operator|.
name|addInputPaths
argument_list|(
name|job
argument_list|,
name|onefile
argument_list|)
expr_stmt|;
block|}
for|for
control|(
name|String
name|emptyFile
range|:
name|emptyPaths
control|)
block|{
name|Class
argument_list|<
name|?
extends|extends
name|HiveOutputFormat
argument_list|>
name|outFileFormat
init|=
name|work
operator|.
name|getPathToPartitionInfo
argument_list|()
operator|.
name|get
argument_list|(
name|emptyFile
argument_list|)
operator|.
name|getTableDesc
argument_list|()
operator|.
name|getOutputFileFormatClass
argument_list|()
decl_stmt|;
name|String
name|newFile
init|=
name|hiveScratchDir
operator|+
name|File
operator|.
name|separator
operator|+
operator|(
operator|++
name|numEmptyPaths
operator|)
decl_stmt|;
name|Path
name|newPath
init|=
operator|new
name|Path
argument_list|(
name|newFile
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Changed input file to "
operator|+
name|newPath
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
comment|// toggle the work
name|LinkedHashMap
argument_list|<
name|String
argument_list|,
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|>
name|pathToAliases
init|=
name|work
operator|.
name|getPathToAliases
argument_list|()
decl_stmt|;
name|pathToAliases
operator|.
name|put
argument_list|(
name|newPath
operator|.
name|toUri
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|,
name|pathToAliases
operator|.
name|get
argument_list|(
name|emptyFile
argument_list|)
argument_list|)
expr_stmt|;
name|pathToAliases
operator|.
name|remove
argument_list|(
name|emptyFile
argument_list|)
expr_stmt|;
name|work
operator|.
name|setPathToAliases
argument_list|(
name|pathToAliases
argument_list|)
expr_stmt|;
name|LinkedHashMap
argument_list|<
name|String
argument_list|,
name|partitionDesc
argument_list|>
name|pathToPartitionInfo
init|=
name|work
operator|.
name|getPathToPartitionInfo
argument_list|()
decl_stmt|;
name|pathToPartitionInfo
operator|.
name|put
argument_list|(
name|newPath
operator|.
name|toUri
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|,
name|pathToPartitionInfo
operator|.
name|get
argument_list|(
name|emptyFile
argument_list|)
argument_list|)
expr_stmt|;
name|pathToPartitionInfo
operator|.
name|remove
argument_list|(
name|emptyFile
argument_list|)
expr_stmt|;
name|work
operator|.
name|setPathToPartitionInfo
argument_list|(
name|pathToPartitionInfo
argument_list|)
expr_stmt|;
name|String
name|onefile
init|=
name|newPath
operator|.
name|toString
argument_list|()
decl_stmt|;
name|RecordWriter
name|recWriter
init|=
name|outFileFormat
operator|.
name|newInstance
argument_list|()
operator|.
name|getHiveRecordWriter
argument_list|(
name|job
argument_list|,
name|newPath
argument_list|,
name|Text
operator|.
name|class
argument_list|,
literal|false
argument_list|,
operator|new
name|Properties
argument_list|()
argument_list|,
literal|null
argument_list|)
decl_stmt|;
name|recWriter
operator|.
name|close
argument_list|(
literal|false
argument_list|)
expr_stmt|;
name|FileInputFormat
operator|.
name|addInputPaths
argument_list|(
name|job
argument_list|,
name|onefile
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
end_class

end_unit

