begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/**  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_comment
comment|/**  * JDBM LICENSE v1.00  *  * Redistribution and use of this software and associated documentation  * ("Software"), with or without modification, are permitted provided  * that the following conditions are met:  *  * 1. Redistributions of source code must retain copyright  *    statements and notices.  Redistributions must also contain a  *    copy of this document.  *  * 2. Redistributions in binary form must reproduce the  *    above copyright notice, this list of conditions and the  *    following disclaimer in the documentation and/or other  *    materials provided with the distribution.  *  * 3. The name "JDBM" must not be used to endorse or promote  *    products derived from this Software without prior written  *    permission of Cees de Groot.  For written permission,  *    please contact cg@cdegroot.com.  *  * 4. Products derived from this Software may not be called "JDBM"  *    nor may "JDBM" appear in their names without prior written  *    permission of Cees de Groot.  *  * 5. Due credit should be given to the JDBM Project  *    (http://jdbm.sourceforge.net/).  *  * THIS SOFTWARE IS PROVIDED BY THE JDBM PROJECT AND CONTRIBUTORS  * ``AS IS'' AND ANY EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT  * NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND  * FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL  * CEES DE GROOT OR ANY CONTRIBUTORS BE LIABLE FOR ANY DIRECT,  * INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES  * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)  * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,  * STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)  * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED  * OF THE POSSIBILITY OF SUCH DAMAGE.  *  * Copyright 2000 (C) Cees de Groot. All Rights Reserved.  * Contributions are Copyright (C) 2000 by their associated contributors.  *  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|util
operator|.
name|jdbm
operator|.
name|htree
package|;
end_package

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|util
operator|.
name|jdbm
operator|.
name|RecordManager
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|util
operator|.
name|jdbm
operator|.
name|helper
operator|.
name|FastIterator
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|util
operator|.
name|jdbm
operator|.
name|helper
operator|.
name|IterationException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|Externalizable
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|ObjectInput
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|ObjectOutput
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Iterator
import|;
end_import

begin_comment
comment|/**  *  Hashtable directory page.  *  *  @author<a href="mailto:boisvert@exoffice.com">Alex Boisvert</a>  *  @version $Id: HashDirectory.java,v 1.5 2005/06/25 23:12:32 doomdark Exp $  */
end_comment

begin_class
specifier|final
class|class
name|HashDirectory
extends|extends
name|HashNode
implements|implements
name|Externalizable
block|{
specifier|static
specifier|final
name|long
name|serialVersionUID
init|=
literal|1L
decl_stmt|;
comment|/**      * Maximum number of children in a directory.      *      * (Must be a power of 2 -- if you update this value, you must also      *  update BIT_SIZE and MAX_DEPTH.)      */
specifier|static
specifier|final
name|int
name|MAX_CHILDREN
init|=
literal|256
decl_stmt|;
comment|/**      * Number of significant bits per directory level.      */
specifier|static
specifier|final
name|int
name|BIT_SIZE
init|=
literal|8
decl_stmt|;
comment|// log2(256) = 8
comment|/**      * Maximum number of levels (zero-based)      *      * (4 * 8 bits = 32 bits, which is the size of an "int", and as      *  you know, hashcodes in Java are "ints")      */
specifier|static
specifier|final
name|int
name|MAX_DEPTH
init|=
literal|3
decl_stmt|;
comment|// 4 levels
comment|/**      * Record ids of children pages.      */
specifier|private
name|long
index|[]
name|_children
decl_stmt|;
comment|/**      * Depth of this directory page, zero-based      */
specifier|private
name|byte
name|_depth
decl_stmt|;
comment|/**      * PageManager used to persist changes in directory and buckets      */
specifier|private
specifier|transient
name|RecordManager
name|_recman
decl_stmt|;
comment|/**      * This directory's record ID in the PageManager.  (transient)      */
specifier|private
specifier|transient
name|long
name|_recid
decl_stmt|;
comment|/**      * Public constructor used by serialization      */
specifier|public
name|HashDirectory
parameter_list|()
block|{
comment|// empty
block|}
comment|/**      * Construct a HashDirectory      *      * @param depth Depth of this directory page.      */
name|HashDirectory
parameter_list|(
name|byte
name|depth
parameter_list|)
block|{
name|_depth
operator|=
name|depth
expr_stmt|;
name|_children
operator|=
operator|new
name|long
index|[
name|MAX_CHILDREN
index|]
expr_stmt|;
block|}
comment|/**      * Sets persistence context.  This method must be called before any      * persistence-related operation.      *      * @param recman RecordManager which stores this directory      * @param recid Record id of this directory.      */
name|void
name|setPersistenceContext
parameter_list|(
name|RecordManager
name|recman
parameter_list|,
name|long
name|recid
parameter_list|)
block|{
name|this
operator|.
name|_recman
operator|=
name|recman
expr_stmt|;
name|this
operator|.
name|_recid
operator|=
name|recid
expr_stmt|;
block|}
comment|/**      * Get the record identifier used to load this hashtable.      */
name|long
name|getRecid
parameter_list|()
block|{
return|return
name|_recid
return|;
block|}
comment|/**      * Returns whether or not this directory is empty.  A directory      * is empty when it no longer contains buckets or sub-directories.      */
name|boolean
name|isEmpty
parameter_list|()
block|{
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|_children
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
if|if
condition|(
name|_children
index|[
name|i
index|]
operator|!=
literal|0
condition|)
block|{
return|return
literal|false
return|;
block|}
block|}
return|return
literal|true
return|;
block|}
comment|/**      * Returns the value which is associated with the given key. Returns      *<code>null</code> if there is not association for this key.      *      * @param key key whose associated value is to be returned      */
name|Object
name|get
parameter_list|(
name|Object
name|key
parameter_list|)
throws|throws
name|IOException
block|{
name|int
name|hash
init|=
name|hashCode
argument_list|(
name|key
argument_list|)
decl_stmt|;
name|long
name|child_recid
init|=
name|_children
index|[
name|hash
index|]
decl_stmt|;
if|if
condition|(
name|child_recid
operator|==
literal|0
condition|)
block|{
comment|// not bucket/page --> not found
return|return
literal|null
return|;
block|}
else|else
block|{
name|HashNode
name|node
init|=
operator|(
name|HashNode
operator|)
name|_recman
operator|.
name|fetch
argument_list|(
name|child_recid
argument_list|)
decl_stmt|;
comment|// System.out.println("HashDirectory.get() child is : "+node);
if|if
condition|(
name|node
operator|instanceof
name|HashDirectory
condition|)
block|{
comment|// recurse into next directory level
name|HashDirectory
name|dir
init|=
operator|(
name|HashDirectory
operator|)
name|node
decl_stmt|;
name|dir
operator|.
name|setPersistenceContext
argument_list|(
name|_recman
argument_list|,
name|child_recid
argument_list|)
expr_stmt|;
return|return
name|dir
operator|.
name|get
argument_list|(
name|key
argument_list|)
return|;
block|}
else|else
block|{
comment|// node is a bucket
name|HashBucket
name|bucket
init|=
operator|(
name|HashBucket
operator|)
name|node
decl_stmt|;
return|return
name|bucket
operator|.
name|getValue
argument_list|(
name|key
argument_list|)
return|;
block|}
block|}
block|}
comment|/**      * Associates the specified value with the specified key.      *      * @param key key with which the specified value is to be assocated.      * @param value value to be associated with the specified key.      * @return object which was previously associated with the given key,      *          or<code>null</code> if no association existed.      */
name|Object
name|put
parameter_list|(
name|Object
name|key
parameter_list|,
name|Object
name|value
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|value
operator|==
literal|null
condition|)
block|{
return|return
name|remove
argument_list|(
name|key
argument_list|)
return|;
block|}
name|int
name|hash
init|=
name|hashCode
argument_list|(
name|key
argument_list|)
decl_stmt|;
name|long
name|child_recid
init|=
name|_children
index|[
name|hash
index|]
decl_stmt|;
if|if
condition|(
name|child_recid
operator|==
literal|0
condition|)
block|{
comment|// no bucket/page here yet, let's create a bucket
name|HashBucket
name|bucket
init|=
operator|new
name|HashBucket
argument_list|(
name|_depth
operator|+
literal|1
argument_list|)
decl_stmt|;
comment|// insert (key,value) pair in bucket
name|Object
name|existing
init|=
name|bucket
operator|.
name|addElement
argument_list|(
name|key
argument_list|,
name|value
argument_list|)
decl_stmt|;
name|long
name|b_recid
init|=
name|_recman
operator|.
name|insert
argument_list|(
name|bucket
argument_list|)
decl_stmt|;
name|_children
index|[
name|hash
index|]
operator|=
name|b_recid
expr_stmt|;
name|_recman
operator|.
name|update
argument_list|(
name|_recid
argument_list|,
name|this
argument_list|)
expr_stmt|;
comment|// System.out.println("Added: "+bucket);
return|return
name|existing
return|;
block|}
else|else
block|{
name|HashNode
name|node
init|=
operator|(
name|HashNode
operator|)
name|_recman
operator|.
name|fetch
argument_list|(
name|child_recid
argument_list|)
decl_stmt|;
if|if
condition|(
name|node
operator|instanceof
name|HashDirectory
condition|)
block|{
comment|// recursive insert in next directory level
name|HashDirectory
name|dir
init|=
operator|(
name|HashDirectory
operator|)
name|node
decl_stmt|;
name|dir
operator|.
name|setPersistenceContext
argument_list|(
name|_recman
argument_list|,
name|child_recid
argument_list|)
expr_stmt|;
return|return
name|dir
operator|.
name|put
argument_list|(
name|key
argument_list|,
name|value
argument_list|)
return|;
block|}
else|else
block|{
comment|// node is a bucket
name|HashBucket
name|bucket
init|=
operator|(
name|HashBucket
operator|)
name|node
decl_stmt|;
if|if
condition|(
name|bucket
operator|.
name|hasRoom
argument_list|()
condition|)
block|{
name|Object
name|existing
init|=
name|bucket
operator|.
name|addElement
argument_list|(
name|key
argument_list|,
name|value
argument_list|)
decl_stmt|;
name|_recman
operator|.
name|update
argument_list|(
name|child_recid
argument_list|,
name|bucket
argument_list|)
expr_stmt|;
comment|// System.out.println("Added: "+bucket);
return|return
name|existing
return|;
block|}
else|else
block|{
comment|// overflow, so create a new directory
if|if
condition|(
name|_depth
operator|==
name|MAX_DEPTH
condition|)
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
literal|"Cannot create deeper directory. "
operator|+
literal|"Depth="
operator|+
name|_depth
argument_list|)
throw|;
block|}
name|HashDirectory
name|dir
init|=
operator|new
name|HashDirectory
argument_list|(
call|(
name|byte
call|)
argument_list|(
name|_depth
operator|+
literal|1
argument_list|)
argument_list|)
decl_stmt|;
name|long
name|dir_recid
init|=
name|_recman
operator|.
name|insert
argument_list|(
name|dir
argument_list|)
decl_stmt|;
name|dir
operator|.
name|setPersistenceContext
argument_list|(
name|_recman
argument_list|,
name|dir_recid
argument_list|)
expr_stmt|;
name|_children
index|[
name|hash
index|]
operator|=
name|dir_recid
expr_stmt|;
name|_recman
operator|.
name|update
argument_list|(
name|_recid
argument_list|,
name|this
argument_list|)
expr_stmt|;
comment|// discard overflown bucket
name|_recman
operator|.
name|delete
argument_list|(
name|child_recid
argument_list|)
expr_stmt|;
comment|// migrate existing bucket elements
name|ArrayList
name|keys
init|=
name|bucket
operator|.
name|getKeys
argument_list|()
decl_stmt|;
name|ArrayList
name|values
init|=
name|bucket
operator|.
name|getValues
argument_list|()
decl_stmt|;
name|int
name|entries
init|=
name|keys
operator|.
name|size
argument_list|()
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|entries
condition|;
name|i
operator|++
control|)
block|{
name|dir
operator|.
name|put
argument_list|(
name|keys
operator|.
name|get
argument_list|(
name|i
argument_list|)
argument_list|,
name|values
operator|.
name|get
argument_list|(
name|i
argument_list|)
argument_list|)
expr_stmt|;
block|}
comment|// (finally!) insert new element
return|return
name|dir
operator|.
name|put
argument_list|(
name|key
argument_list|,
name|value
argument_list|)
return|;
block|}
block|}
block|}
block|}
comment|/**      * Remove the value which is associated with the given key.  If the      * key does not exist, this method simply ignores the operation.      *      * @param key key whose associated value is to be removed      * @return object which was associated with the given key, or      *<code>null</code> if no association existed with given key.      */
name|Object
name|remove
parameter_list|(
name|Object
name|key
parameter_list|)
throws|throws
name|IOException
block|{
name|int
name|hash
init|=
name|hashCode
argument_list|(
name|key
argument_list|)
decl_stmt|;
name|long
name|child_recid
init|=
name|_children
index|[
name|hash
index|]
decl_stmt|;
if|if
condition|(
name|child_recid
operator|==
literal|0
condition|)
block|{
comment|// not bucket/page --> not found
return|return
literal|null
return|;
block|}
else|else
block|{
name|HashNode
name|node
init|=
operator|(
name|HashNode
operator|)
name|_recman
operator|.
name|fetch
argument_list|(
name|child_recid
argument_list|)
decl_stmt|;
comment|// System.out.println("HashDirectory.remove() child is : "+node);
if|if
condition|(
name|node
operator|instanceof
name|HashDirectory
condition|)
block|{
comment|// recurse into next directory level
name|HashDirectory
name|dir
init|=
operator|(
name|HashDirectory
operator|)
name|node
decl_stmt|;
name|dir
operator|.
name|setPersistenceContext
argument_list|(
name|_recman
argument_list|,
name|child_recid
argument_list|)
expr_stmt|;
name|Object
name|existing
init|=
name|dir
operator|.
name|remove
argument_list|(
name|key
argument_list|)
decl_stmt|;
if|if
condition|(
name|existing
operator|!=
literal|null
condition|)
block|{
if|if
condition|(
name|dir
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
comment|// delete empty directory
name|_recman
operator|.
name|delete
argument_list|(
name|child_recid
argument_list|)
expr_stmt|;
name|_children
index|[
name|hash
index|]
operator|=
literal|0
expr_stmt|;
name|_recman
operator|.
name|update
argument_list|(
name|_recid
argument_list|,
name|this
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|existing
return|;
block|}
else|else
block|{
comment|// node is a bucket
name|HashBucket
name|bucket
init|=
operator|(
name|HashBucket
operator|)
name|node
decl_stmt|;
name|Object
name|existing
init|=
name|bucket
operator|.
name|removeElement
argument_list|(
name|key
argument_list|)
decl_stmt|;
if|if
condition|(
name|existing
operator|!=
literal|null
condition|)
block|{
if|if
condition|(
name|bucket
operator|.
name|getElementCount
argument_list|()
operator|>=
literal|1
condition|)
block|{
name|_recman
operator|.
name|update
argument_list|(
name|child_recid
argument_list|,
name|bucket
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// delete bucket, it's empty
name|_recman
operator|.
name|delete
argument_list|(
name|child_recid
argument_list|)
expr_stmt|;
name|_children
index|[
name|hash
index|]
operator|=
literal|0
expr_stmt|;
name|_recman
operator|.
name|update
argument_list|(
name|_recid
argument_list|,
name|this
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|existing
return|;
block|}
block|}
block|}
comment|/**      * Calculates the hashcode of a key, based on the current directory      * depth.      */
specifier|private
name|int
name|hashCode
parameter_list|(
name|Object
name|key
parameter_list|)
block|{
name|int
name|hashMask
init|=
name|hashMask
argument_list|()
decl_stmt|;
name|int
name|hash
init|=
name|key
operator|.
name|hashCode
argument_list|()
decl_stmt|;
name|hash
operator|=
name|hash
operator|&
name|hashMask
expr_stmt|;
name|hash
operator|=
name|hash
operator|>>>
operator|(
operator|(
name|MAX_DEPTH
operator|-
name|_depth
operator|)
operator|*
name|BIT_SIZE
operator|)
expr_stmt|;
name|hash
operator|=
name|hash
operator|%
name|MAX_CHILDREN
expr_stmt|;
comment|/*         System.out.println("HashDirectory.hashCode() is: 0x"                            +Integer.toHexString(hash)                            +" for object hashCode() 0x"                            +Integer.toHexString(key.hashCode()));         */
return|return
name|hash
return|;
block|}
comment|/**      * Calculates the hashmask of this directory.  The hashmask is the      * bit mask applied to a hashcode to retain only bits that are      * relevant to this directory level.      */
name|int
name|hashMask
parameter_list|()
block|{
name|int
name|bits
init|=
name|MAX_CHILDREN
operator|-
literal|1
decl_stmt|;
name|int
name|hashMask
init|=
name|bits
operator|<<
operator|(
operator|(
name|MAX_DEPTH
operator|-
name|_depth
operator|)
operator|*
name|BIT_SIZE
operator|)
decl_stmt|;
comment|/*         System.out.println("HashDirectory.hashMask() is: 0x"                            +Integer.toHexString(hashMask));         */
return|return
name|hashMask
return|;
block|}
comment|/**      * Returns an enumeration of the keys contained in this      */
name|FastIterator
name|keys
parameter_list|()
throws|throws
name|IOException
block|{
return|return
operator|new
name|HDIterator
argument_list|(
literal|true
argument_list|)
return|;
block|}
comment|/**      * Returns an enumeration of the values contained in this      */
name|FastIterator
name|values
parameter_list|()
throws|throws
name|IOException
block|{
return|return
operator|new
name|HDIterator
argument_list|(
literal|false
argument_list|)
return|;
block|}
comment|/**      * Implement Externalizable interface      */
specifier|public
name|void
name|writeExternal
parameter_list|(
name|ObjectOutput
name|out
parameter_list|)
throws|throws
name|IOException
block|{
name|out
operator|.
name|writeByte
argument_list|(
name|_depth
argument_list|)
expr_stmt|;
name|out
operator|.
name|writeObject
argument_list|(
name|_children
argument_list|)
expr_stmt|;
block|}
comment|/**      * Implement Externalizable interface      */
specifier|public
specifier|synchronized
name|void
name|readExternal
parameter_list|(
name|ObjectInput
name|in
parameter_list|)
throws|throws
name|IOException
throws|,
name|ClassNotFoundException
block|{
name|_depth
operator|=
name|in
operator|.
name|readByte
argument_list|()
expr_stmt|;
name|_children
operator|=
operator|(
name|long
index|[]
operator|)
name|in
operator|.
name|readObject
argument_list|()
expr_stmt|;
block|}
comment|////////////////////////////////////////////////////////////////////////
comment|// INNER CLASS
comment|////////////////////////////////////////////////////////////////////////
comment|/**      * Utility class to enumerate keys/values in a HTree      */
specifier|public
class|class
name|HDIterator
extends|extends
name|FastIterator
block|{
comment|/**          * True if we're iterating on keys, False if enumerating on values.          */
specifier|private
name|boolean
name|_iterateKeys
decl_stmt|;
comment|/**          * Stacks of directories& last enumerated child position          */
specifier|private
name|ArrayList
name|_dirStack
decl_stmt|;
specifier|private
name|ArrayList
name|_childStack
decl_stmt|;
comment|/**          * Current HashDirectory in the hierarchy          */
specifier|private
name|HashDirectory
name|_dir
decl_stmt|;
comment|/**          * Current child position          */
specifier|private
name|int
name|_child
decl_stmt|;
comment|/**          * Current bucket iterator          */
specifier|private
name|Iterator
name|_iter
decl_stmt|;
comment|/**          * Construct an iterator on this directory.          *          * @param iterateKeys True if iteration supplies keys, False          *                  if iterateKeys supplies values.          */
name|HDIterator
parameter_list|(
name|boolean
name|iterateKeys
parameter_list|)
throws|throws
name|IOException
block|{
name|_dirStack
operator|=
operator|new
name|ArrayList
argument_list|()
expr_stmt|;
name|_childStack
operator|=
operator|new
name|ArrayList
argument_list|()
expr_stmt|;
name|_dir
operator|=
name|HashDirectory
operator|.
name|this
expr_stmt|;
name|_child
operator|=
operator|-
literal|1
expr_stmt|;
name|_iterateKeys
operator|=
name|iterateKeys
expr_stmt|;
name|prepareNext
argument_list|()
expr_stmt|;
block|}
comment|/**          * Returns the next object.          */
specifier|public
name|Object
name|next
parameter_list|()
block|{
name|Object
name|next
init|=
literal|null
decl_stmt|;
if|if
condition|(
name|_iter
operator|!=
literal|null
operator|&&
name|_iter
operator|.
name|hasNext
argument_list|()
condition|)
block|{
name|next
operator|=
name|_iter
operator|.
name|next
argument_list|()
expr_stmt|;
block|}
else|else
block|{
try|try
block|{
name|prepareNext
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|except
parameter_list|)
block|{
throw|throw
operator|new
name|IterationException
argument_list|(
name|except
argument_list|)
throw|;
block|}
if|if
condition|(
name|_iter
operator|!=
literal|null
operator|&&
name|_iter
operator|.
name|hasNext
argument_list|()
condition|)
block|{
return|return
name|next
argument_list|()
return|;
block|}
block|}
return|return
name|next
return|;
block|}
comment|/**          * Prepare internal state so we can answer<code>hasMoreElements</code>          *          * Actually, this code prepares an Enumeration on the next          * Bucket to enumerate.   If no following bucket is found,          * the next Enumeration is set to<code>null</code>.          */
specifier|private
name|void
name|prepareNext
parameter_list|()
throws|throws
name|IOException
block|{
name|long
name|child_recid
init|=
literal|0
decl_stmt|;
comment|// find next bucket/directory to enumerate
do|do
block|{
name|_child
operator|++
expr_stmt|;
if|if
condition|(
name|_child
operator|>=
name|MAX_CHILDREN
condition|)
block|{
if|if
condition|(
name|_dirStack
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
comment|// no more directory in the stack, we're finished
return|return;
block|}
comment|// try next page
name|_dir
operator|=
operator|(
name|HashDirectory
operator|)
name|_dirStack
operator|.
name|remove
argument_list|(
name|_dirStack
operator|.
name|size
argument_list|()
operator|-
literal|1
argument_list|)
expr_stmt|;
name|_child
operator|=
operator|(
operator|(
name|Integer
operator|)
name|_childStack
operator|.
name|remove
argument_list|(
name|_childStack
operator|.
name|size
argument_list|()
operator|-
literal|1
argument_list|)
operator|)
operator|.
name|intValue
argument_list|()
expr_stmt|;
continue|continue;
block|}
name|child_recid
operator|=
name|_dir
operator|.
name|_children
index|[
name|_child
index|]
expr_stmt|;
block|}
do|while
condition|(
name|child_recid
operator|==
literal|0
condition|)
do|;
if|if
condition|(
name|child_recid
operator|==
literal|0
condition|)
block|{
throw|throw
operator|new
name|Error
argument_list|(
literal|"child_recid cannot be 0"
argument_list|)
throw|;
block|}
name|HashNode
name|node
init|=
operator|(
name|HashNode
operator|)
name|_recman
operator|.
name|fetch
argument_list|(
name|child_recid
argument_list|)
decl_stmt|;
comment|// System.out.println("HDEnumeration.get() child is : "+node);
if|if
condition|(
name|node
operator|instanceof
name|HashDirectory
condition|)
block|{
comment|// save current position
name|_dirStack
operator|.
name|add
argument_list|(
name|_dir
argument_list|)
expr_stmt|;
name|_childStack
operator|.
name|add
argument_list|(
operator|new
name|Integer
argument_list|(
name|_child
argument_list|)
argument_list|)
expr_stmt|;
name|_dir
operator|=
operator|(
name|HashDirectory
operator|)
name|node
expr_stmt|;
name|_child
operator|=
operator|-
literal|1
expr_stmt|;
comment|// recurse into
name|_dir
operator|.
name|setPersistenceContext
argument_list|(
name|_recman
argument_list|,
name|child_recid
argument_list|)
expr_stmt|;
name|prepareNext
argument_list|()
expr_stmt|;
block|}
else|else
block|{
comment|// node is a bucket
name|HashBucket
name|bucket
init|=
operator|(
name|HashBucket
operator|)
name|node
decl_stmt|;
if|if
condition|(
name|_iterateKeys
condition|)
block|{
name|_iter
operator|=
name|bucket
operator|.
name|getKeys
argument_list|()
operator|.
name|iterator
argument_list|()
expr_stmt|;
block|}
else|else
block|{
name|_iter
operator|=
name|bucket
operator|.
name|getValues
argument_list|()
operator|.
name|iterator
argument_list|()
expr_stmt|;
block|}
block|}
block|}
block|}
block|}
end_class

end_unit

