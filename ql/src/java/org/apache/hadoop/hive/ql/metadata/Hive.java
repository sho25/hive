begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/**  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
package|;
end_package

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|hive_metastoreConstants
operator|.
name|META_TABLE_STORAGE
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde
operator|.
name|serdeConstants
operator|.
name|COLLECTION_DELIM
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde
operator|.
name|serdeConstants
operator|.
name|ESCAPE_CHAR
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde
operator|.
name|serdeConstants
operator|.
name|FIELD_DELIM
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde
operator|.
name|serdeConstants
operator|.
name|LINE_DELIM
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde
operator|.
name|serdeConstants
operator|.
name|MAPKEY_DELIM
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde
operator|.
name|serdeConstants
operator|.
name|SERIALIZATION_FORMAT
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde
operator|.
name|serdeConstants
operator|.
name|STRING_TYPE_NAME
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|FileNotFoundException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|PrintStream
import|;
end_import

begin_import
import|import
name|java
operator|.
name|nio
operator|.
name|ByteBuffer
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Collection
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Collections
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashSet
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Iterator
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|LinkedHashMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|LinkedHashSet
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|LinkedList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
operator|.
name|Entry
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Set
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|Callable
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|ConcurrentHashMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|ExecutionException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|ExecutorService
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|Executors
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|Future
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|atomic
operator|.
name|AtomicInteger
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|base
operator|.
name|Preconditions
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|collect
operator|.
name|ImmutableMap
import|;
end_import

begin_import
import|import
name|javax
operator|.
name|jdo
operator|.
name|JDODataStoreException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|calcite
operator|.
name|plan
operator|.
name|RelOptMaterialization
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|io
operator|.
name|FilenameUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configuration
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileChecksum
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileStatus
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|PathFilter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|FileUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|HiveStatsUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|JavaUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|ObjectPair
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|StatsSetupConst
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|classification
operator|.
name|InterfaceAudience
operator|.
name|LimitedPrivate
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|classification
operator|.
name|InterfaceStability
operator|.
name|Unstable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
operator|.
name|ConfVars
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|io
operator|.
name|HdfsUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|HiveMetaException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|HiveMetaHook
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|HiveMetaHookLoader
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|HiveMetaStoreClient
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|IMetaStoreClient
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|MetaStoreUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|PartitionDropOptions
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|RetryingMetaStoreClient
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|SynchronizedMetaStoreClient
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|TableType
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|Warehouse
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|AggrStats
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|AlreadyExistsException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|ColumnStatisticsObj
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|CompactionResponse
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|CompactionType
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Database
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|EnvironmentContext
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|FieldSchema
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|FireEventRequest
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|FireEventRequestData
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|ForeignKeysRequest
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Function
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|GetOpenTxnsInfoResponse
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|GetRoleGrantsForPrincipalRequest
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|GetRoleGrantsForPrincipalResponse
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|HiveObjectPrivilege
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|HiveObjectRef
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|HiveObjectType
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Index
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|InsertEventRequestData
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|InvalidOperationException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|MetaException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|MetadataPpdResult
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|NoSuchObjectException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|NotNullConstraintsRequest
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Order
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|PrimaryKeysRequest
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|PrincipalPrivilegeSet
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|PrincipalType
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|PrivilegeBag
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Role
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|RolePrincipalGrant
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|SQLForeignKey
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|SQLNotNullConstraint
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|SQLPrimaryKey
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|SQLUniqueConstraint
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|SerDeInfo
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|SetPartitionsStatsRequest
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|ShowCompactResponse
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|SkewedInfo
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|StorageDescriptor
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|UniqueConstraintsRequest
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|hive_metastoreConstants
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|ErrorMsg
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|FunctionRegistry
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|FunctionTask
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|FunctionUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|SerializationUtilities
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|Utilities
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|index
operator|.
name|HiveIndexHandler
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|AcidUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|log
operator|.
name|InPlaceUpdate
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|log
operator|.
name|PerfLogger
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|optimizer
operator|.
name|listbucketingpruner
operator|.
name|ListBucketingPrunerUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|AddPartitionDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|DropTableDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|ExprNodeGenericFuncDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|session
operator|.
name|CreateTableAutomaticGrant
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|session
operator|.
name|SessionState
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|Deserializer
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|SerDeException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|lazy
operator|.
name|LazySimpleSerDe
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|shims
operator|.
name|HadoopShims
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|shims
operator|.
name|ShimLoader
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|InputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|security
operator|.
name|UserGroupInformation
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|StringUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|thrift
operator|.
name|TException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|Logger
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|LoggerFactory
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|collect
operator|.
name|Lists
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|collect
operator|.
name|Maps
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|collect
operator|.
name|Sets
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|util
operator|.
name|concurrent
operator|.
name|ThreadFactoryBuilder
import|;
end_import

begin_comment
comment|/**  * This class has functions that implement meta data/DDL operations using calls  * to the metastore.  * It has a metastore client instance it uses to communicate with the metastore.  *  * It is a thread local variable, and the instances is accessed using static  * get methods in this class.  */
end_comment

begin_class
annotation|@
name|SuppressWarnings
argument_list|(
block|{
literal|"deprecation"
block|,
literal|"rawtypes"
block|}
argument_list|)
specifier|public
class|class
name|Hive
block|{
specifier|static
specifier|final
specifier|private
name|Logger
name|LOG
init|=
name|LoggerFactory
operator|.
name|getLogger
argument_list|(
literal|"hive.ql.metadata.Hive"
argument_list|)
decl_stmt|;
specifier|private
name|HiveConf
name|conf
init|=
literal|null
decl_stmt|;
specifier|private
name|IMetaStoreClient
name|metaStoreClient
decl_stmt|;
specifier|private
name|SynchronizedMetaStoreClient
name|syncMetaStoreClient
decl_stmt|;
specifier|private
name|UserGroupInformation
name|owner
decl_stmt|;
comment|// metastore calls timing information
specifier|private
specifier|final
name|ConcurrentHashMap
argument_list|<
name|String
argument_list|,
name|Long
argument_list|>
name|metaCallTimeMap
init|=
operator|new
name|ConcurrentHashMap
argument_list|<>
argument_list|()
decl_stmt|;
specifier|private
specifier|static
name|ThreadLocal
argument_list|<
name|Hive
argument_list|>
name|hiveDB
init|=
operator|new
name|ThreadLocal
argument_list|<
name|Hive
argument_list|>
argument_list|()
block|{
annotation|@
name|Override
specifier|protected
name|Hive
name|initialValue
parameter_list|()
block|{
return|return
literal|null
return|;
block|}
annotation|@
name|Override
specifier|public
specifier|synchronized
name|void
name|remove
parameter_list|()
block|{
if|if
condition|(
name|this
operator|.
name|get
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|this
operator|.
name|get
argument_list|()
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
name|super
operator|.
name|remove
argument_list|()
expr_stmt|;
block|}
block|}
decl_stmt|;
comment|// Note that while this is an improvement over static initialization, it is still not,
comment|// technically, valid, cause nothing prevents us from connecting to several metastores in
comment|// the same process. This will still only get the functions from the first metastore.
specifier|private
specifier|final
specifier|static
name|AtomicInteger
name|didRegisterAllFuncs
init|=
operator|new
name|AtomicInteger
argument_list|(
literal|0
argument_list|)
decl_stmt|;
specifier|private
specifier|final
specifier|static
name|int
name|REG_FUNCS_NO
init|=
literal|0
decl_stmt|,
name|REG_FUNCS_DONE
init|=
literal|2
decl_stmt|,
name|REG_FUNCS_PENDING
init|=
literal|1
decl_stmt|;
comment|// register all permanent functions. need improvement
specifier|private
name|void
name|registerAllFunctionsOnce
parameter_list|()
throws|throws
name|HiveException
block|{
name|boolean
name|breakLoop
init|=
literal|false
decl_stmt|;
while|while
condition|(
operator|!
name|breakLoop
condition|)
block|{
name|int
name|val
init|=
name|didRegisterAllFuncs
operator|.
name|get
argument_list|()
decl_stmt|;
switch|switch
condition|(
name|val
condition|)
block|{
case|case
name|REG_FUNCS_NO
case|:
block|{
if|if
condition|(
name|didRegisterAllFuncs
operator|.
name|compareAndSet
argument_list|(
name|val
argument_list|,
name|REG_FUNCS_PENDING
argument_list|)
condition|)
block|{
name|breakLoop
operator|=
literal|true
expr_stmt|;
break|break;
block|}
continue|continue;
block|}
case|case
name|REG_FUNCS_PENDING
case|:
block|{
synchronized|synchronized
init|(
name|didRegisterAllFuncs
init|)
block|{
try|try
block|{
name|didRegisterAllFuncs
operator|.
name|wait
argument_list|(
literal|100
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|InterruptedException
name|e
parameter_list|)
block|{
name|Thread
operator|.
name|currentThread
argument_list|()
operator|.
name|interrupt
argument_list|()
expr_stmt|;
return|return;
block|}
block|}
continue|continue;
block|}
case|case
name|REG_FUNCS_DONE
case|:
return|return;
default|default:
throw|throw
operator|new
name|AssertionError
argument_list|(
name|val
argument_list|)
throw|;
block|}
block|}
try|try
block|{
name|reloadFunctions
argument_list|()
expr_stmt|;
name|didRegisterAllFuncs
operator|.
name|compareAndSet
argument_list|(
name|REG_FUNCS_PENDING
argument_list|,
name|REG_FUNCS_DONE
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Failed to register all functions."
argument_list|,
name|e
argument_list|)
expr_stmt|;
name|didRegisterAllFuncs
operator|.
name|compareAndSet
argument_list|(
name|REG_FUNCS_PENDING
argument_list|,
name|REG_FUNCS_NO
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
finally|finally
block|{
synchronized|synchronized
init|(
name|didRegisterAllFuncs
init|)
block|{
name|didRegisterAllFuncs
operator|.
name|notifyAll
argument_list|()
expr_stmt|;
block|}
block|}
block|}
specifier|public
name|void
name|reloadFunctions
parameter_list|()
throws|throws
name|HiveException
block|{
name|HashSet
argument_list|<
name|String
argument_list|>
name|registryFunctions
init|=
operator|new
name|HashSet
argument_list|<
name|String
argument_list|>
argument_list|(
name|FunctionRegistry
operator|.
name|getFunctionNames
argument_list|(
literal|".+\\..+"
argument_list|)
argument_list|)
decl_stmt|;
for|for
control|(
name|Function
name|function
range|:
name|getAllFunctions
argument_list|()
control|)
block|{
name|String
name|functionName
init|=
name|function
operator|.
name|getFunctionName
argument_list|()
decl_stmt|;
try|try
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Registering function "
operator|+
name|functionName
operator|+
literal|" "
operator|+
name|function
operator|.
name|getClassName
argument_list|()
argument_list|)
expr_stmt|;
name|String
name|qualFunc
init|=
name|FunctionUtils
operator|.
name|qualifyFunctionName
argument_list|(
name|functionName
argument_list|,
name|function
operator|.
name|getDbName
argument_list|()
argument_list|)
decl_stmt|;
name|FunctionRegistry
operator|.
name|registerPermanentFunction
argument_list|(
name|qualFunc
argument_list|,
name|function
operator|.
name|getClassName
argument_list|()
argument_list|,
literal|false
argument_list|,
name|FunctionTask
operator|.
name|toFunctionResource
argument_list|(
name|function
operator|.
name|getResourceUris
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
name|registryFunctions
operator|.
name|remove
argument_list|(
name|qualFunc
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Failed to register persistent function "
operator|+
name|functionName
operator|+
literal|":"
operator|+
name|function
operator|.
name|getClassName
argument_list|()
operator|+
literal|". Ignore and continue."
argument_list|)
expr_stmt|;
block|}
block|}
comment|// unregister functions from local system registry that are not in getAllFunctions()
for|for
control|(
name|String
name|functionName
range|:
name|registryFunctions
control|)
block|{
try|try
block|{
name|FunctionRegistry
operator|.
name|unregisterPermanentFunction
argument_list|(
name|functionName
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Failed to unregister persistent function "
operator|+
name|functionName
operator|+
literal|"on reload. Ignore and continue."
argument_list|)
expr_stmt|;
block|}
block|}
block|}
specifier|public
specifier|static
name|Hive
name|get
parameter_list|(
name|Configuration
name|c
parameter_list|,
name|Class
argument_list|<
name|?
argument_list|>
name|clazz
parameter_list|)
throws|throws
name|HiveException
block|{
return|return
name|get
argument_list|(
name|c
operator|instanceof
name|HiveConf
condition|?
operator|(
name|HiveConf
operator|)
name|c
else|:
operator|new
name|HiveConf
argument_list|(
name|c
argument_list|,
name|clazz
argument_list|)
argument_list|)
return|;
block|}
comment|/**    * Gets hive object for the current thread. If one is not initialized then a    * new one is created If the new configuration is different in metadata conf    * vars, or the owner will be different then a new one is created.    *    * @param c    *          new Hive Configuration    * @return Hive object for current thread    * @throws HiveException    *    */
specifier|public
specifier|static
name|Hive
name|get
parameter_list|(
name|HiveConf
name|c
parameter_list|)
throws|throws
name|HiveException
block|{
return|return
name|getInternal
argument_list|(
name|c
argument_list|,
literal|false
argument_list|,
literal|false
argument_list|,
literal|true
argument_list|)
return|;
block|}
comment|/**    * Same as {@link #get(HiveConf)}, except that it checks only the object identity of existing    * MS client, assuming the relevant settings would be unchanged within the same conf object.    */
specifier|public
specifier|static
name|Hive
name|getWithFastCheck
parameter_list|(
name|HiveConf
name|c
parameter_list|)
throws|throws
name|HiveException
block|{
return|return
name|getWithFastCheck
argument_list|(
name|c
argument_list|,
literal|true
argument_list|)
return|;
block|}
comment|/**    * Same as {@link #get(HiveConf)}, except that it checks only the object identity of existing    * MS client, assuming the relevant settings would be unchanged within the same conf object.    */
specifier|public
specifier|static
name|Hive
name|getWithFastCheck
parameter_list|(
name|HiveConf
name|c
parameter_list|,
name|boolean
name|doRegisterAllFns
parameter_list|)
throws|throws
name|HiveException
block|{
return|return
name|getInternal
argument_list|(
name|c
argument_list|,
literal|false
argument_list|,
literal|true
argument_list|,
name|doRegisterAllFns
argument_list|)
return|;
block|}
specifier|private
specifier|static
name|Hive
name|getInternal
parameter_list|(
name|HiveConf
name|c
parameter_list|,
name|boolean
name|needsRefresh
parameter_list|,
name|boolean
name|isFastCheck
parameter_list|,
name|boolean
name|doRegisterAllFns
parameter_list|)
throws|throws
name|HiveException
block|{
name|Hive
name|db
init|=
name|hiveDB
operator|.
name|get
argument_list|()
decl_stmt|;
if|if
condition|(
name|db
operator|==
literal|null
operator|||
operator|!
name|db
operator|.
name|isCurrentUserOwner
argument_list|()
operator|||
name|needsRefresh
operator|||
operator|(
name|c
operator|!=
literal|null
operator|&&
name|db
operator|.
name|metaStoreClient
operator|!=
literal|null
operator|&&
operator|!
name|isCompatible
argument_list|(
name|db
argument_list|,
name|c
argument_list|,
name|isFastCheck
argument_list|)
operator|)
condition|)
block|{
name|db
operator|=
name|create
argument_list|(
name|c
argument_list|,
literal|false
argument_list|,
name|db
argument_list|,
name|doRegisterAllFns
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|c
operator|!=
literal|null
condition|)
block|{
name|db
operator|.
name|conf
operator|=
name|c
expr_stmt|;
block|}
return|return
name|db
return|;
block|}
specifier|private
specifier|static
name|Hive
name|create
parameter_list|(
name|HiveConf
name|c
parameter_list|,
name|boolean
name|needsRefresh
parameter_list|,
name|Hive
name|db
parameter_list|,
name|boolean
name|doRegisterAllFns
parameter_list|)
throws|throws
name|HiveException
block|{
if|if
condition|(
name|db
operator|!=
literal|null
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Creating new db. db = "
operator|+
name|db
operator|+
literal|", needsRefresh = "
operator|+
name|needsRefresh
operator|+
literal|", db.isCurrentUserOwner = "
operator|+
name|db
operator|.
name|isCurrentUserOwner
argument_list|()
argument_list|)
expr_stmt|;
name|db
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
name|closeCurrent
argument_list|()
expr_stmt|;
if|if
condition|(
name|c
operator|==
literal|null
condition|)
block|{
name|c
operator|=
name|createHiveConf
argument_list|()
expr_stmt|;
block|}
name|c
operator|.
name|set
argument_list|(
literal|"fs.scheme.class"
argument_list|,
literal|"dfs"
argument_list|)
expr_stmt|;
name|Hive
name|newdb
init|=
operator|new
name|Hive
argument_list|(
name|c
argument_list|,
name|doRegisterAllFns
argument_list|)
decl_stmt|;
name|hiveDB
operator|.
name|set
argument_list|(
name|newdb
argument_list|)
expr_stmt|;
return|return
name|newdb
return|;
block|}
specifier|private
specifier|static
name|HiveConf
name|createHiveConf
parameter_list|()
block|{
name|SessionState
name|session
init|=
name|SessionState
operator|.
name|get
argument_list|()
decl_stmt|;
return|return
operator|(
name|session
operator|==
literal|null
operator|)
condition|?
operator|new
name|HiveConf
argument_list|(
name|Hive
operator|.
name|class
argument_list|)
else|:
name|session
operator|.
name|getConf
argument_list|()
return|;
block|}
specifier|private
specifier|static
name|boolean
name|isCompatible
parameter_list|(
name|Hive
name|db
parameter_list|,
name|HiveConf
name|c
parameter_list|,
name|boolean
name|isFastCheck
parameter_list|)
block|{
return|return
name|isFastCheck
condition|?
name|db
operator|.
name|metaStoreClient
operator|.
name|isSameConfObj
argument_list|(
name|c
argument_list|)
else|:
name|db
operator|.
name|metaStoreClient
operator|.
name|isCompatibleWith
argument_list|(
name|c
argument_list|)
return|;
block|}
specifier|public
specifier|static
name|Hive
name|get
parameter_list|()
throws|throws
name|HiveException
block|{
return|return
name|get
argument_list|(
literal|true
argument_list|)
return|;
block|}
specifier|public
specifier|static
name|Hive
name|get
parameter_list|(
name|boolean
name|doRegisterAllFns
parameter_list|)
throws|throws
name|HiveException
block|{
return|return
name|getInternal
argument_list|(
literal|null
argument_list|,
literal|false
argument_list|,
literal|false
argument_list|,
name|doRegisterAllFns
argument_list|)
return|;
block|}
comment|/**    * get a connection to metastore. see get(HiveConf) function for comments    *    * @param c    *          new conf    * @param needsRefresh    *          if true then creates a new one    * @return The connection to the metastore    * @throws HiveException    */
specifier|public
specifier|static
name|Hive
name|get
parameter_list|(
name|HiveConf
name|c
parameter_list|,
name|boolean
name|needsRefresh
parameter_list|)
throws|throws
name|HiveException
block|{
return|return
name|getInternal
argument_list|(
name|c
argument_list|,
name|needsRefresh
argument_list|,
literal|false
argument_list|,
literal|true
argument_list|)
return|;
block|}
specifier|public
specifier|static
name|void
name|set
parameter_list|(
name|Hive
name|hive
parameter_list|)
block|{
name|hiveDB
operator|.
name|set
argument_list|(
name|hive
argument_list|)
expr_stmt|;
block|}
specifier|public
specifier|static
name|void
name|closeCurrent
parameter_list|()
block|{
name|hiveDB
operator|.
name|remove
argument_list|()
expr_stmt|;
block|}
comment|/**    * Hive    *    * @param c    *    */
specifier|private
name|Hive
parameter_list|(
name|HiveConf
name|c
parameter_list|,
name|boolean
name|doRegisterAllFns
parameter_list|)
throws|throws
name|HiveException
block|{
name|conf
operator|=
name|c
expr_stmt|;
if|if
condition|(
name|doRegisterAllFns
condition|)
block|{
name|registerAllFunctionsOnce
argument_list|()
expr_stmt|;
block|}
block|}
specifier|private
name|boolean
name|isCurrentUserOwner
parameter_list|()
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|owner
operator|==
literal|null
operator|||
name|owner
operator|.
name|equals
argument_list|(
name|UserGroupInformation
operator|.
name|getCurrentUser
argument_list|()
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Error getting current user: "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    * closes the connection to metastore for the calling thread    */
specifier|private
name|void
name|close
parameter_list|()
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Closing current thread's connection to Hive Metastore."
argument_list|)
expr_stmt|;
if|if
condition|(
name|metaStoreClient
operator|!=
literal|null
condition|)
block|{
name|metaStoreClient
operator|.
name|close
argument_list|()
expr_stmt|;
name|metaStoreClient
operator|=
literal|null
expr_stmt|;
block|}
if|if
condition|(
name|owner
operator|!=
literal|null
condition|)
block|{
name|owner
operator|=
literal|null
expr_stmt|;
block|}
block|}
comment|/**    * Create a database    * @param db    * @param ifNotExist if true, will ignore AlreadyExistsException exception    * @throws AlreadyExistsException    * @throws HiveException    */
specifier|public
name|void
name|createDatabase
parameter_list|(
name|Database
name|db
parameter_list|,
name|boolean
name|ifNotExist
parameter_list|)
throws|throws
name|AlreadyExistsException
throws|,
name|HiveException
block|{
try|try
block|{
name|getMSC
argument_list|()
operator|.
name|createDatabase
argument_list|(
name|db
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|AlreadyExistsException
name|e
parameter_list|)
block|{
if|if
condition|(
operator|!
name|ifNotExist
condition|)
block|{
throw|throw
name|e
throw|;
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    * Create a Database. Raise an error if a database with the same name already exists.    * @param db    * @throws AlreadyExistsException    * @throws HiveException    */
specifier|public
name|void
name|createDatabase
parameter_list|(
name|Database
name|db
parameter_list|)
throws|throws
name|AlreadyExistsException
throws|,
name|HiveException
block|{
name|createDatabase
argument_list|(
name|db
argument_list|,
literal|false
argument_list|)
expr_stmt|;
block|}
comment|/**    * Drop a database.    * @param name    * @throws NoSuchObjectException    * @throws HiveException    * @see org.apache.hadoop.hive.metastore.HiveMetaStoreClient#dropDatabase(java.lang.String)    */
specifier|public
name|void
name|dropDatabase
parameter_list|(
name|String
name|name
parameter_list|)
throws|throws
name|HiveException
throws|,
name|NoSuchObjectException
block|{
name|dropDatabase
argument_list|(
name|name
argument_list|,
literal|true
argument_list|,
literal|false
argument_list|,
literal|false
argument_list|)
expr_stmt|;
block|}
comment|/**    * Drop a database    * @param name    * @param deleteData    * @param ignoreUnknownDb if true, will ignore NoSuchObjectException    * @throws HiveException    * @throws NoSuchObjectException    */
specifier|public
name|void
name|dropDatabase
parameter_list|(
name|String
name|name
parameter_list|,
name|boolean
name|deleteData
parameter_list|,
name|boolean
name|ignoreUnknownDb
parameter_list|)
throws|throws
name|HiveException
throws|,
name|NoSuchObjectException
block|{
name|dropDatabase
argument_list|(
name|name
argument_list|,
name|deleteData
argument_list|,
name|ignoreUnknownDb
argument_list|,
literal|false
argument_list|)
expr_stmt|;
block|}
comment|/**    * Drop a database    * @param name    * @param deleteData    * @param ignoreUnknownDb if true, will ignore NoSuchObjectException    * @param cascade         if true, delete all tables on the DB if exists. Otherwise, the query    *                        will fail if table still exists.    * @throws HiveException    * @throws NoSuchObjectException    */
specifier|public
name|void
name|dropDatabase
parameter_list|(
name|String
name|name
parameter_list|,
name|boolean
name|deleteData
parameter_list|,
name|boolean
name|ignoreUnknownDb
parameter_list|,
name|boolean
name|cascade
parameter_list|)
throws|throws
name|HiveException
throws|,
name|NoSuchObjectException
block|{
try|try
block|{
name|getMSC
argument_list|()
operator|.
name|dropDatabase
argument_list|(
name|name
argument_list|,
name|deleteData
argument_list|,
name|ignoreUnknownDb
argument_list|,
name|cascade
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|NoSuchObjectException
name|e
parameter_list|)
block|{
throw|throw
name|e
throw|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    * Creates a table metadata and the directory for the table data    *    * @param tableName    *          name of the table    * @param columns    *          list of fields of the table    * @param partCols    *          partition keys of the table    * @param fileInputFormat    *          Class of the input format of the table data file    * @param fileOutputFormat    *          Class of the output format of the table data file    * @throws HiveException    *           thrown if the args are invalid or if the metadata or the data    *           directory couldn't be created    */
specifier|public
name|void
name|createTable
parameter_list|(
name|String
name|tableName
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|columns
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|partCols
parameter_list|,
name|Class
argument_list|<
name|?
extends|extends
name|InputFormat
argument_list|>
name|fileInputFormat
parameter_list|,
name|Class
argument_list|<
name|?
argument_list|>
name|fileOutputFormat
parameter_list|)
throws|throws
name|HiveException
block|{
name|this
operator|.
name|createTable
argument_list|(
name|tableName
argument_list|,
name|columns
argument_list|,
name|partCols
argument_list|,
name|fileInputFormat
argument_list|,
name|fileOutputFormat
argument_list|,
operator|-
literal|1
argument_list|,
literal|null
argument_list|)
expr_stmt|;
block|}
comment|/**    * Creates a table metadata and the directory for the table data    *    * @param tableName    *          name of the table    * @param columns    *          list of fields of the table    * @param partCols    *          partition keys of the table    * @param fileInputFormat    *          Class of the input format of the table data file    * @param fileOutputFormat    *          Class of the output format of the table data file    * @param bucketCount    *          number of buckets that each partition (or the table itself) should    *          be divided into    * @throws HiveException    *           thrown if the args are invalid or if the metadata or the data    *           directory couldn't be created    */
specifier|public
name|void
name|createTable
parameter_list|(
name|String
name|tableName
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|columns
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|partCols
parameter_list|,
name|Class
argument_list|<
name|?
extends|extends
name|InputFormat
argument_list|>
name|fileInputFormat
parameter_list|,
name|Class
argument_list|<
name|?
argument_list|>
name|fileOutputFormat
parameter_list|,
name|int
name|bucketCount
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|bucketCols
parameter_list|)
throws|throws
name|HiveException
block|{
name|createTable
argument_list|(
name|tableName
argument_list|,
name|columns
argument_list|,
name|partCols
argument_list|,
name|fileInputFormat
argument_list|,
name|fileOutputFormat
argument_list|,
name|bucketCount
argument_list|,
name|bucketCols
argument_list|,
literal|null
argument_list|)
expr_stmt|;
block|}
comment|/**    * Create a table metadata and the directory for the table data    * @param tableName table name    * @param columns list of fields of the table    * @param partCols partition keys of the table    * @param fileInputFormat Class of the input format of the table data file    * @param fileOutputFormat Class of the output format of the table data file    * @param bucketCount number of buckets that each partition (or the table itself) should be    *                    divided into    * @param bucketCols Bucket columns    * @param parameters Parameters for the table    * @throws HiveException    */
specifier|public
name|void
name|createTable
parameter_list|(
name|String
name|tableName
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|columns
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|partCols
parameter_list|,
name|Class
argument_list|<
name|?
extends|extends
name|InputFormat
argument_list|>
name|fileInputFormat
parameter_list|,
name|Class
argument_list|<
name|?
argument_list|>
name|fileOutputFormat
parameter_list|,
name|int
name|bucketCount
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|bucketCols
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|parameters
parameter_list|)
throws|throws
name|HiveException
block|{
if|if
condition|(
name|columns
operator|==
literal|null
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"columns not specified for table "
operator|+
name|tableName
argument_list|)
throw|;
block|}
name|Table
name|tbl
init|=
name|newTable
argument_list|(
name|tableName
argument_list|)
decl_stmt|;
name|tbl
operator|.
name|setInputFormatClass
argument_list|(
name|fileInputFormat
operator|.
name|getName
argument_list|()
argument_list|)
expr_stmt|;
name|tbl
operator|.
name|setOutputFormatClass
argument_list|(
name|fileOutputFormat
operator|.
name|getName
argument_list|()
argument_list|)
expr_stmt|;
for|for
control|(
name|String
name|col
range|:
name|columns
control|)
block|{
name|FieldSchema
name|field
init|=
operator|new
name|FieldSchema
argument_list|(
name|col
argument_list|,
name|STRING_TYPE_NAME
argument_list|,
literal|"default"
argument_list|)
decl_stmt|;
name|tbl
operator|.
name|getCols
argument_list|()
operator|.
name|add
argument_list|(
name|field
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|partCols
operator|!=
literal|null
condition|)
block|{
for|for
control|(
name|String
name|partCol
range|:
name|partCols
control|)
block|{
name|FieldSchema
name|part
init|=
operator|new
name|FieldSchema
argument_list|()
decl_stmt|;
name|part
operator|.
name|setName
argument_list|(
name|partCol
argument_list|)
expr_stmt|;
name|part
operator|.
name|setType
argument_list|(
name|STRING_TYPE_NAME
argument_list|)
expr_stmt|;
comment|// default partition key
name|tbl
operator|.
name|getPartCols
argument_list|()
operator|.
name|add
argument_list|(
name|part
argument_list|)
expr_stmt|;
block|}
block|}
name|tbl
operator|.
name|setSerializationLib
argument_list|(
name|LazySimpleSerDe
operator|.
name|class
operator|.
name|getName
argument_list|()
argument_list|)
expr_stmt|;
name|tbl
operator|.
name|setNumBuckets
argument_list|(
name|bucketCount
argument_list|)
expr_stmt|;
name|tbl
operator|.
name|setBucketCols
argument_list|(
name|bucketCols
argument_list|)
expr_stmt|;
if|if
condition|(
name|parameters
operator|!=
literal|null
condition|)
block|{
name|tbl
operator|.
name|setParameters
argument_list|(
name|parameters
argument_list|)
expr_stmt|;
block|}
name|createTable
argument_list|(
name|tbl
argument_list|)
expr_stmt|;
block|}
comment|/**    * Updates the existing table metadata with the new metadata.    *    * @param tblName    *          name of the existing table    * @param newTbl    *          new name of the table. could be the old name    * @throws InvalidOperationException    *           if the changes in metadata is not acceptable    * @throws TException    */
specifier|public
name|void
name|alterTable
parameter_list|(
name|String
name|tblName
parameter_list|,
name|Table
name|newTbl
parameter_list|,
name|EnvironmentContext
name|environmentContext
parameter_list|)
throws|throws
name|InvalidOperationException
throws|,
name|HiveException
block|{
name|alterTable
argument_list|(
name|tblName
argument_list|,
name|newTbl
argument_list|,
literal|false
argument_list|,
name|environmentContext
argument_list|)
expr_stmt|;
block|}
specifier|public
name|void
name|alterTable
parameter_list|(
name|String
name|tblName
parameter_list|,
name|Table
name|newTbl
parameter_list|,
name|boolean
name|cascade
parameter_list|,
name|EnvironmentContext
name|environmentContext
parameter_list|)
throws|throws
name|InvalidOperationException
throws|,
name|HiveException
block|{
name|String
index|[]
name|names
init|=
name|Utilities
operator|.
name|getDbTableName
argument_list|(
name|tblName
argument_list|)
decl_stmt|;
try|try
block|{
comment|// Remove the DDL_TIME so it gets refreshed
if|if
condition|(
name|newTbl
operator|.
name|getParameters
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|newTbl
operator|.
name|getParameters
argument_list|()
operator|.
name|remove
argument_list|(
name|hive_metastoreConstants
operator|.
name|DDL_TIME
argument_list|)
expr_stmt|;
block|}
name|newTbl
operator|.
name|checkValidity
argument_list|(
name|conf
argument_list|)
expr_stmt|;
if|if
condition|(
name|environmentContext
operator|==
literal|null
condition|)
block|{
name|environmentContext
operator|=
operator|new
name|EnvironmentContext
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
name|cascade
condition|)
block|{
name|environmentContext
operator|.
name|putToProperties
argument_list|(
name|StatsSetupConst
operator|.
name|CASCADE
argument_list|,
name|StatsSetupConst
operator|.
name|TRUE
argument_list|)
expr_stmt|;
block|}
name|getMSC
argument_list|()
operator|.
name|alter_table_with_environmentContext
argument_list|(
name|names
index|[
literal|0
index|]
argument_list|,
name|names
index|[
literal|1
index|]
argument_list|,
name|newTbl
operator|.
name|getTTable
argument_list|()
argument_list|,
name|environmentContext
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|MetaException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Unable to alter table. "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
catch|catch
parameter_list|(
name|TException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Unable to alter table. "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|void
name|alterIndex
parameter_list|(
name|String
name|baseTableName
parameter_list|,
name|String
name|indexName
parameter_list|,
name|Index
name|newIdx
parameter_list|)
throws|throws
name|InvalidOperationException
throws|,
name|HiveException
block|{
name|String
index|[]
name|names
init|=
name|Utilities
operator|.
name|getDbTableName
argument_list|(
name|baseTableName
argument_list|)
decl_stmt|;
name|alterIndex
argument_list|(
name|names
index|[
literal|0
index|]
argument_list|,
name|names
index|[
literal|1
index|]
argument_list|,
name|indexName
argument_list|,
name|newIdx
argument_list|)
expr_stmt|;
block|}
comment|/**    * Updates the existing index metadata with the new metadata.    *    * @param idxName    *          name of the existing index    * @param newIdx    *          new name of the index. could be the old name    * @throws InvalidOperationException    *           if the changes in metadata is not acceptable    * @throws TException    */
specifier|public
name|void
name|alterIndex
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|baseTblName
parameter_list|,
name|String
name|idxName
parameter_list|,
name|Index
name|newIdx
parameter_list|)
throws|throws
name|InvalidOperationException
throws|,
name|HiveException
block|{
try|try
block|{
name|getMSC
argument_list|()
operator|.
name|alter_index
argument_list|(
name|dbName
argument_list|,
name|baseTblName
argument_list|,
name|idxName
argument_list|,
name|newIdx
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|MetaException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Unable to alter index. "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
catch|catch
parameter_list|(
name|TException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Unable to alter index. "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    * Updates the existing partition metadata with the new metadata.    *    * @param tblName    *          name of the existing table    * @param newPart    *          new partition    * @throws InvalidOperationException    *           if the changes in metadata is not acceptable    * @throws TException    */
specifier|public
name|void
name|alterPartition
parameter_list|(
name|String
name|tblName
parameter_list|,
name|Partition
name|newPart
parameter_list|,
name|EnvironmentContext
name|environmentContext
parameter_list|)
throws|throws
name|InvalidOperationException
throws|,
name|HiveException
block|{
name|String
index|[]
name|names
init|=
name|Utilities
operator|.
name|getDbTableName
argument_list|(
name|tblName
argument_list|)
decl_stmt|;
name|alterPartition
argument_list|(
name|names
index|[
literal|0
index|]
argument_list|,
name|names
index|[
literal|1
index|]
argument_list|,
name|newPart
argument_list|,
name|environmentContext
argument_list|)
expr_stmt|;
block|}
comment|/**    * Updates the existing partition metadata with the new metadata.    *    * @param dbName    *          name of the exiting table's database    * @param tblName    *          name of the existing table    * @param newPart    *          new partition    * @throws InvalidOperationException    *           if the changes in metadata is not acceptable    * @throws TException    */
specifier|public
name|void
name|alterPartition
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|tblName
parameter_list|,
name|Partition
name|newPart
parameter_list|,
name|EnvironmentContext
name|environmentContext
parameter_list|)
throws|throws
name|InvalidOperationException
throws|,
name|HiveException
block|{
try|try
block|{
name|validatePartition
argument_list|(
name|newPart
argument_list|)
expr_stmt|;
name|String
name|location
init|=
name|newPart
operator|.
name|getLocation
argument_list|()
decl_stmt|;
if|if
condition|(
name|location
operator|!=
literal|null
operator|&&
operator|!
name|Utilities
operator|.
name|isDefaultNameNode
argument_list|(
name|conf
argument_list|)
condition|)
block|{
name|location
operator|=
name|Utilities
operator|.
name|getQualifiedPath
argument_list|(
name|conf
argument_list|,
operator|new
name|Path
argument_list|(
name|location
argument_list|)
argument_list|)
expr_stmt|;
name|newPart
operator|.
name|setLocation
argument_list|(
name|location
argument_list|)
expr_stmt|;
block|}
name|getMSC
argument_list|()
operator|.
name|alter_partition
argument_list|(
name|dbName
argument_list|,
name|tblName
argument_list|,
name|newPart
operator|.
name|getTPartition
argument_list|()
argument_list|,
name|environmentContext
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|MetaException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Unable to alter partition. "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
catch|catch
parameter_list|(
name|TException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Unable to alter partition. "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
specifier|private
name|void
name|validatePartition
parameter_list|(
name|Partition
name|newPart
parameter_list|)
throws|throws
name|HiveException
block|{
comment|// Remove the DDL time so that it gets refreshed
if|if
condition|(
name|newPart
operator|.
name|getParameters
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|newPart
operator|.
name|getParameters
argument_list|()
operator|.
name|remove
argument_list|(
name|hive_metastoreConstants
operator|.
name|DDL_TIME
argument_list|)
expr_stmt|;
block|}
name|newPart
operator|.
name|checkValidity
argument_list|()
expr_stmt|;
block|}
comment|/**    * Updates the existing table metadata with the new metadata.    *    * @param tblName    *          name of the existing table    * @param newParts    *          new partitions    * @throws InvalidOperationException    *           if the changes in metadata is not acceptable    * @throws TException    */
specifier|public
name|void
name|alterPartitions
parameter_list|(
name|String
name|tblName
parameter_list|,
name|List
argument_list|<
name|Partition
argument_list|>
name|newParts
parameter_list|,
name|EnvironmentContext
name|environmentContext
parameter_list|)
throws|throws
name|InvalidOperationException
throws|,
name|HiveException
block|{
name|String
index|[]
name|names
init|=
name|Utilities
operator|.
name|getDbTableName
argument_list|(
name|tblName
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
argument_list|>
name|newTParts
init|=
operator|new
name|ArrayList
argument_list|<
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
argument_list|>
argument_list|()
decl_stmt|;
try|try
block|{
comment|// Remove the DDL time so that it gets refreshed
for|for
control|(
name|Partition
name|tmpPart
range|:
name|newParts
control|)
block|{
if|if
condition|(
name|tmpPart
operator|.
name|getParameters
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|tmpPart
operator|.
name|getParameters
argument_list|()
operator|.
name|remove
argument_list|(
name|hive_metastoreConstants
operator|.
name|DDL_TIME
argument_list|)
expr_stmt|;
block|}
name|String
name|location
init|=
name|tmpPart
operator|.
name|getLocation
argument_list|()
decl_stmt|;
if|if
condition|(
name|location
operator|!=
literal|null
operator|&&
operator|!
name|Utilities
operator|.
name|isDefaultNameNode
argument_list|(
name|conf
argument_list|)
condition|)
block|{
name|location
operator|=
name|Utilities
operator|.
name|getQualifiedPath
argument_list|(
name|conf
argument_list|,
operator|new
name|Path
argument_list|(
name|location
argument_list|)
argument_list|)
expr_stmt|;
name|tmpPart
operator|.
name|setLocation
argument_list|(
name|location
argument_list|)
expr_stmt|;
block|}
name|newTParts
operator|.
name|add
argument_list|(
name|tmpPart
operator|.
name|getTPartition
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|getMSC
argument_list|()
operator|.
name|alter_partitions
argument_list|(
name|names
index|[
literal|0
index|]
argument_list|,
name|names
index|[
literal|1
index|]
argument_list|,
name|newTParts
argument_list|,
name|environmentContext
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|MetaException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Unable to alter partition. "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
catch|catch
parameter_list|(
name|TException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Unable to alter partition. "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    * Rename a old partition to new partition    *    * @param tbl    *          existing table    * @param oldPartSpec    *          spec of old partition    * @param newPart    *          new partition    * @throws InvalidOperationException    *           if the changes in metadata is not acceptable    * @throws TException    */
specifier|public
name|void
name|renamePartition
parameter_list|(
name|Table
name|tbl
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|oldPartSpec
parameter_list|,
name|Partition
name|newPart
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|newPartSpec
init|=
name|newPart
operator|.
name|getSpec
argument_list|()
decl_stmt|;
if|if
condition|(
name|oldPartSpec
operator|.
name|keySet
argument_list|()
operator|.
name|size
argument_list|()
operator|!=
name|tbl
operator|.
name|getPartCols
argument_list|()
operator|.
name|size
argument_list|()
operator|||
name|newPartSpec
operator|.
name|keySet
argument_list|()
operator|.
name|size
argument_list|()
operator|!=
name|tbl
operator|.
name|getPartCols
argument_list|()
operator|.
name|size
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Unable to rename partition to the same name: number of partition cols don't match. "
argument_list|)
throw|;
block|}
if|if
condition|(
operator|!
name|oldPartSpec
operator|.
name|keySet
argument_list|()
operator|.
name|equals
argument_list|(
name|newPartSpec
operator|.
name|keySet
argument_list|()
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Unable to rename partition to the same name: old and new partition cols don't match. "
argument_list|)
throw|;
block|}
name|List
argument_list|<
name|String
argument_list|>
name|pvals
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|FieldSchema
name|field
range|:
name|tbl
operator|.
name|getPartCols
argument_list|()
control|)
block|{
name|String
name|val
init|=
name|oldPartSpec
operator|.
name|get
argument_list|(
name|field
operator|.
name|getName
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|val
operator|==
literal|null
operator|||
name|val
operator|.
name|length
argument_list|()
operator|==
literal|0
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"get partition: Value for key "
operator|+
name|field
operator|.
name|getName
argument_list|()
operator|+
literal|" is null or empty"
argument_list|)
throw|;
block|}
elseif|else
if|if
condition|(
name|val
operator|!=
literal|null
condition|)
block|{
name|pvals
operator|.
name|add
argument_list|(
name|val
argument_list|)
expr_stmt|;
block|}
block|}
name|getMSC
argument_list|()
operator|.
name|renamePartition
argument_list|(
name|tbl
operator|.
name|getDbName
argument_list|()
argument_list|,
name|tbl
operator|.
name|getTableName
argument_list|()
argument_list|,
name|pvals
argument_list|,
name|newPart
operator|.
name|getTPartition
argument_list|()
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|InvalidOperationException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Unable to rename partition. "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
catch|catch
parameter_list|(
name|MetaException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Unable to rename partition. "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
catch|catch
parameter_list|(
name|TException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Unable to rename partition. "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|void
name|alterDatabase
parameter_list|(
name|String
name|dbName
parameter_list|,
name|Database
name|db
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|getMSC
argument_list|()
operator|.
name|alterDatabase
argument_list|(
name|dbName
argument_list|,
name|db
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|MetaException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Unable to alter database "
operator|+
name|dbName
operator|+
literal|". "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
catch|catch
parameter_list|(
name|NoSuchObjectException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Database "
operator|+
name|dbName
operator|+
literal|" does not exists."
argument_list|,
name|e
argument_list|)
throw|;
block|}
catch|catch
parameter_list|(
name|TException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Unable to alter database "
operator|+
name|dbName
operator|+
literal|". "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    * Creates the table with the give objects    *    * @param tbl    *          a table object    * @throws HiveException    */
specifier|public
name|void
name|createTable
parameter_list|(
name|Table
name|tbl
parameter_list|)
throws|throws
name|HiveException
block|{
name|createTable
argument_list|(
name|tbl
argument_list|,
literal|false
argument_list|)
expr_stmt|;
block|}
comment|/**    * Creates the table with the given objects. It takes additional arguments for    * primary keys and foreign keys associated with the table.    *    * @param tbl    *          a table object    * @param ifNotExists    *          if true, ignore AlreadyExistsException    * @param primaryKeys    *          primary key columns associated with the table    * @param foreignKeys    *          foreign key columns associated with the table    * @throws HiveException    */
specifier|public
name|void
name|createTable
parameter_list|(
name|Table
name|tbl
parameter_list|,
name|boolean
name|ifNotExists
parameter_list|,
name|List
argument_list|<
name|SQLPrimaryKey
argument_list|>
name|primaryKeys
parameter_list|,
name|List
argument_list|<
name|SQLForeignKey
argument_list|>
name|foreignKeys
parameter_list|,
name|List
argument_list|<
name|SQLUniqueConstraint
argument_list|>
name|uniqueConstraints
parameter_list|,
name|List
argument_list|<
name|SQLNotNullConstraint
argument_list|>
name|notNullConstraints
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
if|if
condition|(
name|tbl
operator|.
name|getDbName
argument_list|()
operator|==
literal|null
operator|||
literal|""
operator|.
name|equals
argument_list|(
name|tbl
operator|.
name|getDbName
argument_list|()
operator|.
name|trim
argument_list|()
argument_list|)
condition|)
block|{
name|tbl
operator|.
name|setDbName
argument_list|(
name|SessionState
operator|.
name|get
argument_list|()
operator|.
name|getCurrentDatabase
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|tbl
operator|.
name|getCols
argument_list|()
operator|.
name|size
argument_list|()
operator|==
literal|0
operator|||
name|tbl
operator|.
name|getSd
argument_list|()
operator|.
name|getColsSize
argument_list|()
operator|==
literal|0
condition|)
block|{
name|tbl
operator|.
name|setFields
argument_list|(
name|MetaStoreUtils
operator|.
name|getFieldsFromDeserializer
argument_list|(
name|tbl
operator|.
name|getTableName
argument_list|()
argument_list|,
name|tbl
operator|.
name|getDeserializer
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|tbl
operator|.
name|checkValidity
argument_list|(
name|conf
argument_list|)
expr_stmt|;
if|if
condition|(
name|tbl
operator|.
name|getParameters
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|tbl
operator|.
name|getParameters
argument_list|()
operator|.
name|remove
argument_list|(
name|hive_metastoreConstants
operator|.
name|DDL_TIME
argument_list|)
expr_stmt|;
block|}
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Table
name|tTbl
init|=
name|tbl
operator|.
name|getTTable
argument_list|()
decl_stmt|;
name|PrincipalPrivilegeSet
name|principalPrivs
init|=
operator|new
name|PrincipalPrivilegeSet
argument_list|()
decl_stmt|;
name|SessionState
name|ss
init|=
name|SessionState
operator|.
name|get
argument_list|()
decl_stmt|;
if|if
condition|(
name|ss
operator|!=
literal|null
condition|)
block|{
name|CreateTableAutomaticGrant
name|grants
init|=
name|ss
operator|.
name|getCreateTableGrants
argument_list|()
decl_stmt|;
if|if
condition|(
name|grants
operator|!=
literal|null
condition|)
block|{
name|principalPrivs
operator|.
name|setUserPrivileges
argument_list|(
name|grants
operator|.
name|getUserGrants
argument_list|()
argument_list|)
expr_stmt|;
name|principalPrivs
operator|.
name|setGroupPrivileges
argument_list|(
name|grants
operator|.
name|getGroupGrants
argument_list|()
argument_list|)
expr_stmt|;
name|principalPrivs
operator|.
name|setRolePrivileges
argument_list|(
name|grants
operator|.
name|getRoleGrants
argument_list|()
argument_list|)
expr_stmt|;
name|tTbl
operator|.
name|setPrivileges
argument_list|(
name|principalPrivs
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
name|primaryKeys
operator|==
literal|null
operator|&&
name|foreignKeys
operator|==
literal|null
operator|&&
name|uniqueConstraints
operator|==
literal|null
operator|&&
name|notNullConstraints
operator|==
literal|null
condition|)
block|{
name|getMSC
argument_list|()
operator|.
name|createTable
argument_list|(
name|tTbl
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|getMSC
argument_list|()
operator|.
name|createTableWithConstraints
argument_list|(
name|tTbl
argument_list|,
name|primaryKeys
argument_list|,
name|foreignKeys
argument_list|,
name|uniqueConstraints
argument_list|,
name|notNullConstraints
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|AlreadyExistsException
name|e
parameter_list|)
block|{
if|if
condition|(
operator|!
name|ifNotExists
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|void
name|createTable
parameter_list|(
name|Table
name|tbl
parameter_list|,
name|boolean
name|ifNotExists
parameter_list|)
throws|throws
name|HiveException
block|{
name|createTable
argument_list|(
name|tbl
argument_list|,
name|ifNotExists
argument_list|,
literal|null
argument_list|,
literal|null
argument_list|,
literal|null
argument_list|,
literal|null
argument_list|)
expr_stmt|;
block|}
specifier|public
specifier|static
name|List
argument_list|<
name|FieldSchema
argument_list|>
name|getFieldsFromDeserializerForMsStorage
parameter_list|(
name|Table
name|tbl
parameter_list|,
name|Deserializer
name|deserializer
parameter_list|)
throws|throws
name|SerDeException
throws|,
name|MetaException
block|{
name|List
argument_list|<
name|FieldSchema
argument_list|>
name|schema
init|=
name|MetaStoreUtils
operator|.
name|getFieldsFromDeserializer
argument_list|(
name|tbl
operator|.
name|getTableName
argument_list|()
argument_list|,
name|deserializer
argument_list|)
decl_stmt|;
for|for
control|(
name|FieldSchema
name|field
range|:
name|schema
control|)
block|{
name|field
operator|.
name|setType
argument_list|(
name|MetaStoreUtils
operator|.
name|TYPE_FROM_DESERIALIZER
argument_list|)
expr_stmt|;
block|}
return|return
name|schema
return|;
block|}
comment|/**    *    * @param tableName    *          table name    * @param indexName    *          index name    * @param indexHandlerClass    *          index handler class    * @param indexedCols    *          index columns    * @param indexTblName    *          index table's name    * @param deferredRebuild    *          referred build index table's data    * @param inputFormat    *          input format    * @param outputFormat    *          output format    * @param serde    * @param storageHandler    *          index table's storage handler    * @param location    *          location    * @param idxProps    *          idx    * @param serdeProps    *          serde properties    * @param collItemDelim    * @param fieldDelim    * @param fieldEscape    * @param lineDelim    * @param mapKeyDelim    * @throws HiveException    */
specifier|public
name|void
name|createIndex
parameter_list|(
name|String
name|tableName
parameter_list|,
name|String
name|indexName
parameter_list|,
name|String
name|indexHandlerClass
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|indexedCols
parameter_list|,
name|String
name|indexTblName
parameter_list|,
name|boolean
name|deferredRebuild
parameter_list|,
name|String
name|inputFormat
parameter_list|,
name|String
name|outputFormat
parameter_list|,
name|String
name|serde
parameter_list|,
name|String
name|storageHandler
parameter_list|,
name|String
name|location
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|idxProps
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|tblProps
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|serdeProps
parameter_list|,
name|String
name|collItemDelim
parameter_list|,
name|String
name|fieldDelim
parameter_list|,
name|String
name|fieldEscape
parameter_list|,
name|String
name|lineDelim
parameter_list|,
name|String
name|mapKeyDelim
parameter_list|,
name|String
name|indexComment
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|String
name|tdname
init|=
name|Utilities
operator|.
name|getDatabaseName
argument_list|(
name|tableName
argument_list|)
decl_stmt|;
name|String
name|idname
init|=
name|Utilities
operator|.
name|getDatabaseName
argument_list|(
name|indexTblName
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|idname
operator|.
name|equals
argument_list|(
name|tdname
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Index on different database ("
operator|+
name|idname
operator|+
literal|") from base table ("
operator|+
name|tdname
operator|+
literal|") is not supported."
argument_list|)
throw|;
block|}
name|Index
name|old_index
init|=
literal|null
decl_stmt|;
try|try
block|{
name|old_index
operator|=
name|getIndex
argument_list|(
name|tableName
argument_list|,
name|indexName
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{       }
if|if
condition|(
name|old_index
operator|!=
literal|null
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Index "
operator|+
name|indexName
operator|+
literal|" already exists on table "
operator|+
name|tableName
argument_list|)
throw|;
block|}
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Table
name|baseTbl
init|=
name|getTable
argument_list|(
name|tableName
argument_list|)
operator|.
name|getTTable
argument_list|()
decl_stmt|;
if|if
condition|(
name|TableType
operator|.
name|VIRTUAL_VIEW
operator|.
name|toString
argument_list|()
operator|.
name|equals
argument_list|(
name|baseTbl
operator|.
name|getTableType
argument_list|()
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"tableName="
operator|+
name|tableName
operator|+
literal|" is a VIRTUAL VIEW. Index on VIRTUAL VIEW is not supported."
argument_list|)
throw|;
block|}
if|if
condition|(
name|baseTbl
operator|.
name|isTemporary
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"tableName="
operator|+
name|tableName
operator|+
literal|" is a TEMPORARY TABLE. Index on TEMPORARY TABLE is not supported."
argument_list|)
throw|;
block|}
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Table
name|temp
init|=
literal|null
decl_stmt|;
try|try
block|{
name|temp
operator|=
name|getTable
argument_list|(
name|indexTblName
argument_list|)
operator|.
name|getTTable
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{       }
if|if
condition|(
name|temp
operator|!=
literal|null
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Table name "
operator|+
name|indexTblName
operator|+
literal|" already exists. Choose another name."
argument_list|)
throw|;
block|}
name|SerDeInfo
name|serdeInfo
init|=
operator|new
name|SerDeInfo
argument_list|()
decl_stmt|;
name|serdeInfo
operator|.
name|setName
argument_list|(
name|indexTblName
argument_list|)
expr_stmt|;
if|if
condition|(
name|serde
operator|!=
literal|null
condition|)
block|{
name|serdeInfo
operator|.
name|setSerializationLib
argument_list|(
name|serde
argument_list|)
expr_stmt|;
block|}
else|else
block|{
if|if
condition|(
name|storageHandler
operator|==
literal|null
condition|)
block|{
name|serdeInfo
operator|.
name|setSerializationLib
argument_list|(
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|lazy
operator|.
name|LazySimpleSerDe
operator|.
name|class
operator|.
name|getName
argument_list|()
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|HiveStorageHandler
name|sh
init|=
name|HiveUtils
operator|.
name|getStorageHandler
argument_list|(
name|getConf
argument_list|()
argument_list|,
name|storageHandler
argument_list|)
decl_stmt|;
name|String
name|serDeClassName
init|=
name|sh
operator|.
name|getSerDeClass
argument_list|()
operator|.
name|getName
argument_list|()
decl_stmt|;
name|serdeInfo
operator|.
name|setSerializationLib
argument_list|(
name|serDeClassName
argument_list|)
expr_stmt|;
block|}
block|}
name|serdeInfo
operator|.
name|setParameters
argument_list|(
operator|new
name|HashMap
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|fieldDelim
operator|!=
literal|null
condition|)
block|{
name|serdeInfo
operator|.
name|getParameters
argument_list|()
operator|.
name|put
argument_list|(
name|FIELD_DELIM
argument_list|,
name|fieldDelim
argument_list|)
expr_stmt|;
name|serdeInfo
operator|.
name|getParameters
argument_list|()
operator|.
name|put
argument_list|(
name|SERIALIZATION_FORMAT
argument_list|,
name|fieldDelim
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|fieldEscape
operator|!=
literal|null
condition|)
block|{
name|serdeInfo
operator|.
name|getParameters
argument_list|()
operator|.
name|put
argument_list|(
name|ESCAPE_CHAR
argument_list|,
name|fieldEscape
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|collItemDelim
operator|!=
literal|null
condition|)
block|{
name|serdeInfo
operator|.
name|getParameters
argument_list|()
operator|.
name|put
argument_list|(
name|COLLECTION_DELIM
argument_list|,
name|collItemDelim
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|mapKeyDelim
operator|!=
literal|null
condition|)
block|{
name|serdeInfo
operator|.
name|getParameters
argument_list|()
operator|.
name|put
argument_list|(
name|MAPKEY_DELIM
argument_list|,
name|mapKeyDelim
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|lineDelim
operator|!=
literal|null
condition|)
block|{
name|serdeInfo
operator|.
name|getParameters
argument_list|()
operator|.
name|put
argument_list|(
name|LINE_DELIM
argument_list|,
name|lineDelim
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|serdeProps
operator|!=
literal|null
condition|)
block|{
name|Iterator
argument_list|<
name|Entry
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
argument_list|>
name|iter
init|=
name|serdeProps
operator|.
name|entrySet
argument_list|()
operator|.
name|iterator
argument_list|()
decl_stmt|;
while|while
condition|(
name|iter
operator|.
name|hasNext
argument_list|()
condition|)
block|{
name|Entry
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|m
init|=
name|iter
operator|.
name|next
argument_list|()
decl_stmt|;
name|serdeInfo
operator|.
name|getParameters
argument_list|()
operator|.
name|put
argument_list|(
name|m
operator|.
name|getKey
argument_list|()
argument_list|,
name|m
operator|.
name|getValue
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
name|List
argument_list|<
name|FieldSchema
argument_list|>
name|indexTblCols
init|=
operator|new
name|ArrayList
argument_list|<
name|FieldSchema
argument_list|>
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|Order
argument_list|>
name|sortCols
init|=
operator|new
name|ArrayList
argument_list|<
name|Order
argument_list|>
argument_list|()
decl_stmt|;
name|int
name|k
init|=
literal|0
decl_stmt|;
name|Table
name|metaBaseTbl
init|=
operator|new
name|Table
argument_list|(
name|baseTbl
argument_list|)
decl_stmt|;
comment|// Even though we are storing these in metastore, get regular columns. Indexes on lengthy
comment|// types from e.g. Avro schema will just fail to create the index table (by design).
name|List
argument_list|<
name|FieldSchema
argument_list|>
name|cols
init|=
name|metaBaseTbl
operator|.
name|getCols
argument_list|()
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|cols
operator|.
name|size
argument_list|()
condition|;
name|i
operator|++
control|)
block|{
name|FieldSchema
name|col
init|=
name|cols
operator|.
name|get
argument_list|(
name|i
argument_list|)
decl_stmt|;
if|if
condition|(
name|indexedCols
operator|.
name|contains
argument_list|(
name|col
operator|.
name|getName
argument_list|()
argument_list|)
condition|)
block|{
name|indexTblCols
operator|.
name|add
argument_list|(
name|col
argument_list|)
expr_stmt|;
name|sortCols
operator|.
name|add
argument_list|(
operator|new
name|Order
argument_list|(
name|col
operator|.
name|getName
argument_list|()
argument_list|,
literal|1
argument_list|)
argument_list|)
expr_stmt|;
name|k
operator|++
expr_stmt|;
block|}
block|}
if|if
condition|(
name|k
operator|!=
name|indexedCols
operator|.
name|size
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
literal|"Check the index columns, they should appear in the table being indexed."
argument_list|)
throw|;
block|}
name|int
name|time
init|=
call|(
name|int
call|)
argument_list|(
name|System
operator|.
name|currentTimeMillis
argument_list|()
operator|/
literal|1000
argument_list|)
decl_stmt|;
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Table
name|tt
init|=
literal|null
decl_stmt|;
name|HiveIndexHandler
name|indexHandler
init|=
name|HiveUtils
operator|.
name|getIndexHandler
argument_list|(
name|this
operator|.
name|getConf
argument_list|()
argument_list|,
name|indexHandlerClass
argument_list|)
decl_stmt|;
name|String
name|itname
init|=
name|Utilities
operator|.
name|getTableName
argument_list|(
name|indexTblName
argument_list|)
decl_stmt|;
if|if
condition|(
name|indexHandler
operator|.
name|usesIndexTable
argument_list|()
condition|)
block|{
name|tt
operator|=
operator|new
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|Table
argument_list|(
name|idname
argument_list|,
name|itname
argument_list|)
operator|.
name|getTTable
argument_list|()
expr_stmt|;
name|List
argument_list|<
name|FieldSchema
argument_list|>
name|partKeys
init|=
name|baseTbl
operator|.
name|getPartitionKeys
argument_list|()
decl_stmt|;
name|tt
operator|.
name|setPartitionKeys
argument_list|(
name|partKeys
argument_list|)
expr_stmt|;
name|tt
operator|.
name|setTableType
argument_list|(
name|TableType
operator|.
name|INDEX_TABLE
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|tblProps
operator|!=
literal|null
condition|)
block|{
for|for
control|(
name|Entry
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|prop
range|:
name|tblProps
operator|.
name|entrySet
argument_list|()
control|)
block|{
name|tt
operator|.
name|putToParameters
argument_list|(
name|prop
operator|.
name|getKey
argument_list|()
argument_list|,
name|prop
operator|.
name|getValue
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
name|SessionState
name|ss
init|=
name|SessionState
operator|.
name|get
argument_list|()
decl_stmt|;
name|CreateTableAutomaticGrant
name|grants
decl_stmt|;
if|if
condition|(
name|ss
operator|!=
literal|null
operator|&&
operator|(
operator|(
name|grants
operator|=
name|ss
operator|.
name|getCreateTableGrants
argument_list|()
operator|)
operator|!=
literal|null
operator|)
condition|)
block|{
name|PrincipalPrivilegeSet
name|principalPrivs
init|=
operator|new
name|PrincipalPrivilegeSet
argument_list|()
decl_stmt|;
name|principalPrivs
operator|.
name|setUserPrivileges
argument_list|(
name|grants
operator|.
name|getUserGrants
argument_list|()
argument_list|)
expr_stmt|;
name|principalPrivs
operator|.
name|setGroupPrivileges
argument_list|(
name|grants
operator|.
name|getGroupGrants
argument_list|()
argument_list|)
expr_stmt|;
name|principalPrivs
operator|.
name|setRolePrivileges
argument_list|(
name|grants
operator|.
name|getRoleGrants
argument_list|()
argument_list|)
expr_stmt|;
name|tt
operator|.
name|setPrivileges
argument_list|(
name|principalPrivs
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
operator|!
name|deferredRebuild
condition|)
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
literal|"Please specify deferred rebuild using \" WITH DEFERRED REBUILD \"."
argument_list|)
throw|;
block|}
name|StorageDescriptor
name|indexSd
init|=
operator|new
name|StorageDescriptor
argument_list|(
name|indexTblCols
argument_list|,
name|location
argument_list|,
name|inputFormat
argument_list|,
name|outputFormat
argument_list|,
literal|false
comment|/*compressed - not used*/
argument_list|,
operator|-
literal|1
comment|/*numBuckets - default is -1 when the table has no buckets*/
argument_list|,
name|serdeInfo
argument_list|,
literal|null
comment|/*bucketCols*/
argument_list|,
name|sortCols
argument_list|,
literal|null
comment|/*parameters*/
argument_list|)
decl_stmt|;
name|String
name|ttname
init|=
name|Utilities
operator|.
name|getTableName
argument_list|(
name|tableName
argument_list|)
decl_stmt|;
name|Index
name|indexDesc
init|=
operator|new
name|Index
argument_list|(
name|indexName
argument_list|,
name|indexHandlerClass
argument_list|,
name|tdname
argument_list|,
name|ttname
argument_list|,
name|time
argument_list|,
name|time
argument_list|,
name|itname
argument_list|,
name|indexSd
argument_list|,
operator|new
name|HashMap
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
argument_list|()
argument_list|,
name|deferredRebuild
argument_list|)
decl_stmt|;
if|if
condition|(
name|indexComment
operator|!=
literal|null
condition|)
block|{
name|indexDesc
operator|.
name|getParameters
argument_list|()
operator|.
name|put
argument_list|(
literal|"comment"
argument_list|,
name|indexComment
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|idxProps
operator|!=
literal|null
condition|)
block|{
name|indexDesc
operator|.
name|getParameters
argument_list|()
operator|.
name|putAll
argument_list|(
name|idxProps
argument_list|)
expr_stmt|;
block|}
name|indexHandler
operator|.
name|analyzeIndexDefinition
argument_list|(
name|baseTbl
argument_list|,
name|indexDesc
argument_list|,
name|tt
argument_list|)
expr_stmt|;
name|this
operator|.
name|getMSC
argument_list|()
operator|.
name|createIndex
argument_list|(
name|indexDesc
argument_list|,
name|tt
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|Index
name|getIndex
parameter_list|(
name|String
name|baseTableName
parameter_list|,
name|String
name|indexName
parameter_list|)
throws|throws
name|HiveException
block|{
name|String
index|[]
name|names
init|=
name|Utilities
operator|.
name|getDbTableName
argument_list|(
name|baseTableName
argument_list|)
decl_stmt|;
return|return
name|this
operator|.
name|getIndex
argument_list|(
name|names
index|[
literal|0
index|]
argument_list|,
name|names
index|[
literal|1
index|]
argument_list|,
name|indexName
argument_list|)
return|;
block|}
specifier|public
name|Index
name|getIndex
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|baseTableName
parameter_list|,
name|String
name|indexName
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|this
operator|.
name|getMSC
argument_list|()
operator|.
name|getIndex
argument_list|(
name|dbName
argument_list|,
name|baseTableName
argument_list|,
name|indexName
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|boolean
name|dropIndex
parameter_list|(
name|String
name|baseTableName
parameter_list|,
name|String
name|index_name
parameter_list|,
name|boolean
name|throwException
parameter_list|,
name|boolean
name|deleteData
parameter_list|)
throws|throws
name|HiveException
block|{
name|String
index|[]
name|names
init|=
name|Utilities
operator|.
name|getDbTableName
argument_list|(
name|baseTableName
argument_list|)
decl_stmt|;
return|return
name|dropIndex
argument_list|(
name|names
index|[
literal|0
index|]
argument_list|,
name|names
index|[
literal|1
index|]
argument_list|,
name|index_name
argument_list|,
name|throwException
argument_list|,
name|deleteData
argument_list|)
return|;
block|}
specifier|public
name|boolean
name|dropIndex
parameter_list|(
name|String
name|db_name
parameter_list|,
name|String
name|tbl_name
parameter_list|,
name|String
name|index_name
parameter_list|,
name|boolean
name|throwException
parameter_list|,
name|boolean
name|deleteData
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|dropIndex
argument_list|(
name|db_name
argument_list|,
name|tbl_name
argument_list|,
name|index_name
argument_list|,
name|deleteData
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|NoSuchObjectException
name|e
parameter_list|)
block|{
if|if
condition|(
name|throwException
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Index "
operator|+
name|index_name
operator|+
literal|" doesn't exist. "
argument_list|,
name|e
argument_list|)
throw|;
block|}
return|return
literal|false
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    * Drops table along with the data in it. If the table doesn't exist then it    * is a no-op. If ifPurge option is specified it is passed to the    * hdfs command that removes table data from warehouse to make it skip trash.    *    * @param tableName    *          table to drop    * @param ifPurge    *          completely purge the table (skipping trash) while removing data from warehouse    * @throws HiveException    *           thrown if the drop fails    */
specifier|public
name|void
name|dropTable
parameter_list|(
name|String
name|tableName
parameter_list|,
name|boolean
name|ifPurge
parameter_list|)
throws|throws
name|HiveException
block|{
name|String
index|[]
name|names
init|=
name|Utilities
operator|.
name|getDbTableName
argument_list|(
name|tableName
argument_list|)
decl_stmt|;
name|dropTable
argument_list|(
name|names
index|[
literal|0
index|]
argument_list|,
name|names
index|[
literal|1
index|]
argument_list|,
literal|true
argument_list|,
literal|true
argument_list|,
name|ifPurge
argument_list|)
expr_stmt|;
block|}
comment|/**    * Drops table along with the data in it. If the table doesn't exist then it    * is a no-op    *    * @param tableName    *          table to drop    * @throws HiveException    *           thrown if the drop fails    */
specifier|public
name|void
name|dropTable
parameter_list|(
name|String
name|tableName
parameter_list|)
throws|throws
name|HiveException
block|{
name|dropTable
argument_list|(
name|tableName
argument_list|,
literal|false
argument_list|)
expr_stmt|;
block|}
comment|/**    * Drops table along with the data in it. If the table doesn't exist then it    * is a no-op    *    * @param dbName    *          database where the table lives    * @param tableName    *          table to drop    * @throws HiveException    *           thrown if the drop fails    */
specifier|public
name|void
name|dropTable
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|tableName
parameter_list|)
throws|throws
name|HiveException
block|{
name|dropTable
argument_list|(
name|dbName
argument_list|,
name|tableName
argument_list|,
literal|true
argument_list|,
literal|true
argument_list|,
literal|false
argument_list|)
expr_stmt|;
block|}
comment|/**    * Drops the table.    *    * @param dbName    * @param tableName    * @param deleteData    *          deletes the underlying data along with metadata    * @param ignoreUnknownTab    *          an exception is thrown if this is false and the table doesn't exist    * @throws HiveException    */
specifier|public
name|void
name|dropTable
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|tableName
parameter_list|,
name|boolean
name|deleteData
parameter_list|,
name|boolean
name|ignoreUnknownTab
parameter_list|)
throws|throws
name|HiveException
block|{
name|dropTable
argument_list|(
name|dbName
argument_list|,
name|tableName
argument_list|,
name|deleteData
argument_list|,
name|ignoreUnknownTab
argument_list|,
literal|false
argument_list|)
expr_stmt|;
block|}
comment|/**    * Drops the table.    *    * @param dbName    * @param tableName    * @param deleteData    *          deletes the underlying data along with metadata    * @param ignoreUnknownTab    *          an exception is thrown if this is false and the table doesn't exist    * @param ifPurge    *          completely purge the table skipping trash while removing data from warehouse    * @throws HiveException    */
specifier|public
name|void
name|dropTable
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|tableName
parameter_list|,
name|boolean
name|deleteData
parameter_list|,
name|boolean
name|ignoreUnknownTab
parameter_list|,
name|boolean
name|ifPurge
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|getMSC
argument_list|()
operator|.
name|dropTable
argument_list|(
name|dbName
argument_list|,
name|tableName
argument_list|,
name|deleteData
argument_list|,
name|ignoreUnknownTab
argument_list|,
name|ifPurge
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|NoSuchObjectException
name|e
parameter_list|)
block|{
if|if
condition|(
operator|!
name|ignoreUnknownTab
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    * Truncates the table/partition as per specifications. Just trash the data files    *    * @param dbDotTableName    *          name of the table    * @throws HiveException    */
specifier|public
name|void
name|truncateTable
parameter_list|(
name|String
name|dbDotTableName
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|partSpec
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|Table
name|table
init|=
name|getTable
argument_list|(
name|dbDotTableName
argument_list|,
literal|true
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|partNames
init|=
operator|(
operator|(
literal|null
operator|==
name|partSpec
operator|)
condition|?
literal|null
else|:
name|getPartitionNames
argument_list|(
name|table
operator|.
name|getDbName
argument_list|()
argument_list|,
name|table
operator|.
name|getTableName
argument_list|()
argument_list|,
name|partSpec
argument_list|,
operator|(
name|short
operator|)
operator|-
literal|1
argument_list|)
operator|)
decl_stmt|;
name|getMSC
argument_list|()
operator|.
name|truncateTable
argument_list|(
name|table
operator|.
name|getDbName
argument_list|()
argument_list|,
name|table
operator|.
name|getTableName
argument_list|()
argument_list|,
name|partNames
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|HiveConf
name|getConf
parameter_list|()
block|{
return|return
operator|(
name|conf
operator|)
return|;
block|}
comment|/**    * Returns metadata for the table named tableName    * @param tableName the name of the table    * @return the table metadata    * @throws HiveException if there's an internal error or if the    * table doesn't exist    */
specifier|public
name|Table
name|getTable
parameter_list|(
specifier|final
name|String
name|tableName
parameter_list|)
throws|throws
name|HiveException
block|{
return|return
name|this
operator|.
name|getTable
argument_list|(
name|tableName
argument_list|,
literal|true
argument_list|)
return|;
block|}
comment|/**    * Returns metadata for the table named tableName    * @param tableName the name of the table    * @param throwException controls whether an exception is thrown or a returns a null    * @return the table metadata    * @throws HiveException if there's an internal error or if the    * table doesn't exist    */
specifier|public
name|Table
name|getTable
parameter_list|(
specifier|final
name|String
name|tableName
parameter_list|,
name|boolean
name|throwException
parameter_list|)
throws|throws
name|HiveException
block|{
name|String
index|[]
name|names
init|=
name|Utilities
operator|.
name|getDbTableName
argument_list|(
name|tableName
argument_list|)
decl_stmt|;
return|return
name|this
operator|.
name|getTable
argument_list|(
name|names
index|[
literal|0
index|]
argument_list|,
name|names
index|[
literal|1
index|]
argument_list|,
name|throwException
argument_list|)
return|;
block|}
comment|/**    * Returns metadata of the table    *    * @param dbName    *          the name of the database    * @param tableName    *          the name of the table    * @return the table    * @exception HiveException    *              if there's an internal error or if the table doesn't exist    */
specifier|public
name|Table
name|getTable
parameter_list|(
specifier|final
name|String
name|dbName
parameter_list|,
specifier|final
name|String
name|tableName
parameter_list|)
throws|throws
name|HiveException
block|{
if|if
condition|(
name|tableName
operator|.
name|contains
argument_list|(
literal|"."
argument_list|)
condition|)
block|{
name|String
index|[]
name|names
init|=
name|Utilities
operator|.
name|getDbTableName
argument_list|(
name|tableName
argument_list|)
decl_stmt|;
return|return
name|this
operator|.
name|getTable
argument_list|(
name|names
index|[
literal|0
index|]
argument_list|,
name|names
index|[
literal|1
index|]
argument_list|,
literal|true
argument_list|)
return|;
block|}
else|else
block|{
return|return
name|this
operator|.
name|getTable
argument_list|(
name|dbName
argument_list|,
name|tableName
argument_list|,
literal|true
argument_list|)
return|;
block|}
block|}
comment|/**    * Returns metadata of the table    *    * @param dbName    *          the name of the database    * @param tableName    *          the name of the table    * @param throwException    *          controls whether an exception is thrown or a returns a null    * @return the table or if throwException is false a null value.    * @throws HiveException    */
specifier|public
name|Table
name|getTable
parameter_list|(
specifier|final
name|String
name|dbName
parameter_list|,
specifier|final
name|String
name|tableName
parameter_list|,
name|boolean
name|throwException
parameter_list|)
throws|throws
name|HiveException
block|{
if|if
condition|(
name|tableName
operator|==
literal|null
operator|||
name|tableName
operator|.
name|equals
argument_list|(
literal|""
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"empty table creation??"
argument_list|)
throw|;
block|}
comment|// Get the table from metastore
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Table
name|tTable
init|=
literal|null
decl_stmt|;
try|try
block|{
name|tTable
operator|=
name|getMSC
argument_list|()
operator|.
name|getTable
argument_list|(
name|dbName
argument_list|,
name|tableName
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|NoSuchObjectException
name|e
parameter_list|)
block|{
if|if
condition|(
name|throwException
condition|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Table "
operator|+
name|tableName
operator|+
literal|" not found: "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|InvalidTableException
argument_list|(
name|tableName
argument_list|)
throw|;
block|}
return|return
literal|null
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Unable to fetch table "
operator|+
name|tableName
operator|+
literal|". "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
comment|// For non-views, we need to do some extra fixes
if|if
condition|(
operator|!
name|TableType
operator|.
name|VIRTUAL_VIEW
operator|.
name|toString
argument_list|()
operator|.
name|equals
argument_list|(
name|tTable
operator|.
name|getTableType
argument_list|()
argument_list|)
condition|)
block|{
comment|// Fix the non-printable chars
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|parameters
init|=
name|tTable
operator|.
name|getSd
argument_list|()
operator|.
name|getParameters
argument_list|()
decl_stmt|;
name|String
name|sf
init|=
name|parameters
operator|!=
literal|null
condition|?
name|parameters
operator|.
name|get
argument_list|(
name|SERIALIZATION_FORMAT
argument_list|)
else|:
literal|null
decl_stmt|;
if|if
condition|(
name|sf
operator|!=
literal|null
condition|)
block|{
name|char
index|[]
name|b
init|=
name|sf
operator|.
name|toCharArray
argument_list|()
decl_stmt|;
if|if
condition|(
operator|(
name|b
operator|.
name|length
operator|==
literal|1
operator|)
operator|&&
operator|(
name|b
index|[
literal|0
index|]
operator|<
literal|10
operator|)
condition|)
block|{
comment|// ^A, ^B, ^C, ^D, \t
name|parameters
operator|.
name|put
argument_list|(
name|SERIALIZATION_FORMAT
argument_list|,
name|Integer
operator|.
name|toString
argument_list|(
name|b
index|[
literal|0
index|]
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
comment|// Use LazySimpleSerDe for MetadataTypedColumnsetSerDe.
comment|// NOTE: LazySimpleSerDe does not support tables with a single column of
comment|// col
comment|// of type "array<string>". This happens when the table is created using
comment|// an
comment|// earlier version of Hive.
if|if
condition|(
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|MetadataTypedColumnsetSerDe
operator|.
name|class
operator|.
name|getName
argument_list|()
operator|.
name|equals
argument_list|(
name|tTable
operator|.
name|getSd
argument_list|()
operator|.
name|getSerdeInfo
argument_list|()
operator|.
name|getSerializationLib
argument_list|()
argument_list|)
operator|&&
name|tTable
operator|.
name|getSd
argument_list|()
operator|.
name|getColsSize
argument_list|()
operator|>
literal|0
operator|&&
name|tTable
operator|.
name|getSd
argument_list|()
operator|.
name|getCols
argument_list|()
operator|.
name|get
argument_list|(
literal|0
argument_list|)
operator|.
name|getType
argument_list|()
operator|.
name|indexOf
argument_list|(
literal|'<'
argument_list|)
operator|==
operator|-
literal|1
condition|)
block|{
name|tTable
operator|.
name|getSd
argument_list|()
operator|.
name|getSerdeInfo
argument_list|()
operator|.
name|setSerializationLib
argument_list|(
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|lazy
operator|.
name|LazySimpleSerDe
operator|.
name|class
operator|.
name|getName
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
return|return
operator|new
name|Table
argument_list|(
name|tTable
argument_list|)
return|;
block|}
comment|/**    * Get all table names for the current database.    * @return List of table names    * @throws HiveException    */
specifier|public
name|List
argument_list|<
name|String
argument_list|>
name|getAllTables
parameter_list|()
throws|throws
name|HiveException
block|{
return|return
name|getTablesByType
argument_list|(
name|SessionState
operator|.
name|get
argument_list|()
operator|.
name|getCurrentDatabase
argument_list|()
argument_list|,
literal|null
argument_list|,
literal|null
argument_list|)
return|;
block|}
comment|/**    * Get all table names for the specified database.    * @param dbName    * @return List of table names    * @throws HiveException    */
specifier|public
name|List
argument_list|<
name|String
argument_list|>
name|getAllTables
parameter_list|(
name|String
name|dbName
parameter_list|)
throws|throws
name|HiveException
block|{
return|return
name|getTablesByType
argument_list|(
name|dbName
argument_list|,
literal|".*"
argument_list|,
literal|null
argument_list|)
return|;
block|}
comment|/**    * Get all tables for the specified database.    * @param dbName    * @return List of table names    * @throws HiveException    */
specifier|public
name|List
argument_list|<
name|Table
argument_list|>
name|getAllTableObjects
parameter_list|(
name|String
name|dbName
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|Lists
operator|.
name|transform
argument_list|(
name|getMSC
argument_list|()
operator|.
name|getTableObjectsByName
argument_list|(
name|dbName
argument_list|,
name|getMSC
argument_list|()
operator|.
name|getAllTables
argument_list|(
name|dbName
argument_list|)
argument_list|)
argument_list|,
operator|new
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|base
operator|.
name|Function
argument_list|<
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Table
argument_list|,
name|Table
argument_list|>
argument_list|()
block|{
annotation|@
name|Override
specifier|public
name|Table
name|apply
parameter_list|(
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Table
name|table
parameter_list|)
block|{
return|return
operator|new
name|Table
argument_list|(
name|table
argument_list|)
return|;
block|}
block|}
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    * Returns all existing tables from default database which match the given    * pattern. The matching occurs as per Java regular expressions    *    * @param tablePattern    *          java re pattern    * @return list of table names    * @throws HiveException    */
specifier|public
name|List
argument_list|<
name|String
argument_list|>
name|getTablesByPattern
parameter_list|(
name|String
name|tablePattern
parameter_list|)
throws|throws
name|HiveException
block|{
return|return
name|getTablesByType
argument_list|(
name|SessionState
operator|.
name|get
argument_list|()
operator|.
name|getCurrentDatabase
argument_list|()
argument_list|,
name|tablePattern
argument_list|,
literal|null
argument_list|)
return|;
block|}
comment|/**    * Returns all existing tables from the specified database which match the given    * pattern. The matching occurs as per Java regular expressions.    * @param dbName    * @param tablePattern    * @return list of table names    * @throws HiveException    */
specifier|public
name|List
argument_list|<
name|String
argument_list|>
name|getTablesByPattern
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|tablePattern
parameter_list|)
throws|throws
name|HiveException
block|{
return|return
name|getTablesByType
argument_list|(
name|dbName
argument_list|,
name|tablePattern
argument_list|,
literal|null
argument_list|)
return|;
block|}
comment|/**    * Returns all existing tables from the given database which match the given    * pattern. The matching occurs as per Java regular expressions    *    * @param database    *          the database name    * @param tablePattern    *          java re pattern    * @return list of table names    * @throws HiveException    */
specifier|public
name|List
argument_list|<
name|String
argument_list|>
name|getTablesForDb
parameter_list|(
name|String
name|database
parameter_list|,
name|String
name|tablePattern
parameter_list|)
throws|throws
name|HiveException
block|{
return|return
name|getTablesByType
argument_list|(
name|database
argument_list|,
name|tablePattern
argument_list|,
literal|null
argument_list|)
return|;
block|}
comment|/**    * Returns all existing tables of a type (VIRTUAL_VIEW|EXTERNAL_TABLE|MANAGED_TABLE) from the specified    * database which match the given pattern. The matching occurs as per Java regular expressions.    * @param dbName Database name to find the tables in. if null, uses the current database in this session.    * @param pattern A pattern to match for the table names.If null, returns all names from this DB.    * @param type The type of tables to return. VIRTUAL_VIEWS for views. If null, returns all tables and views.    * @return list of table names that match the pattern.    * @throws HiveException    */
specifier|public
name|List
argument_list|<
name|String
argument_list|>
name|getTablesByType
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|pattern
parameter_list|,
name|TableType
name|type
parameter_list|)
throws|throws
name|HiveException
block|{
if|if
condition|(
name|dbName
operator|==
literal|null
condition|)
name|dbName
operator|=
name|SessionState
operator|.
name|get
argument_list|()
operator|.
name|getCurrentDatabase
argument_list|()
expr_stmt|;
try|try
block|{
if|if
condition|(
name|type
operator|!=
literal|null
condition|)
block|{
if|if
condition|(
name|pattern
operator|!=
literal|null
condition|)
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|getTables
argument_list|(
name|dbName
argument_list|,
name|pattern
argument_list|,
name|type
argument_list|)
return|;
block|}
else|else
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|getTables
argument_list|(
name|dbName
argument_list|,
literal|".*"
argument_list|,
name|type
argument_list|)
return|;
block|}
block|}
else|else
block|{
if|if
condition|(
name|pattern
operator|!=
literal|null
condition|)
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|getTables
argument_list|(
name|dbName
argument_list|,
name|pattern
argument_list|)
return|;
block|}
else|else
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|getTables
argument_list|(
name|dbName
argument_list|,
literal|".*"
argument_list|)
return|;
block|}
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    * Get the materialized views that have been enabled for rewriting from the    * metastore. If the materialized view is in the cache, we do not need to    * parse it to generate a logical plan for the rewriting. Instead, we    * return the version present in the cache.    *    * @return the list of materialized views available for rewriting    * @throws HiveException    */
specifier|public
name|List
argument_list|<
name|RelOptMaterialization
argument_list|>
name|getRewritingMaterializedViews
parameter_list|()
throws|throws
name|HiveException
block|{
try|try
block|{
comment|// Final result
name|List
argument_list|<
name|RelOptMaterialization
argument_list|>
name|result
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|()
decl_stmt|;
for|for
control|(
name|String
name|dbName
range|:
name|getMSC
argument_list|()
operator|.
name|getAllDatabases
argument_list|()
control|)
block|{
comment|// From metastore (for security)
name|List
argument_list|<
name|String
argument_list|>
name|tables
init|=
name|getMSC
argument_list|()
operator|.
name|getAllTables
argument_list|(
name|dbName
argument_list|)
decl_stmt|;
comment|// Cached views (includes all)
name|Collection
argument_list|<
name|RelOptMaterialization
argument_list|>
name|cachedViews
init|=
name|HiveMaterializedViewsRegistry
operator|.
name|get
argument_list|()
operator|.
name|getRewritingMaterializedViews
argument_list|(
name|dbName
argument_list|)
decl_stmt|;
if|if
condition|(
name|cachedViews
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
comment|// Bail out: empty list
continue|continue;
block|}
name|Map
argument_list|<
name|String
argument_list|,
name|RelOptMaterialization
argument_list|>
name|qualifiedNameToView
init|=
operator|new
name|HashMap
argument_list|<
name|String
argument_list|,
name|RelOptMaterialization
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|RelOptMaterialization
name|materialization
range|:
name|cachedViews
control|)
block|{
name|qualifiedNameToView
operator|.
name|put
argument_list|(
name|materialization
operator|.
name|table
operator|.
name|getQualifiedName
argument_list|()
operator|.
name|get
argument_list|(
literal|0
argument_list|)
argument_list|,
name|materialization
argument_list|)
expr_stmt|;
block|}
for|for
control|(
name|String
name|table
range|:
name|tables
control|)
block|{
comment|// Compose qualified name
name|String
name|fullyQualifiedName
init|=
name|dbName
decl_stmt|;
if|if
condition|(
name|fullyQualifiedName
operator|!=
literal|null
operator|&&
operator|!
name|fullyQualifiedName
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|fullyQualifiedName
operator|=
name|fullyQualifiedName
operator|+
literal|"."
operator|+
name|table
expr_stmt|;
block|}
else|else
block|{
name|fullyQualifiedName
operator|=
name|table
expr_stmt|;
block|}
name|RelOptMaterialization
name|materialization
init|=
name|qualifiedNameToView
operator|.
name|get
argument_list|(
name|fullyQualifiedName
argument_list|)
decl_stmt|;
if|if
condition|(
name|materialization
operator|!=
literal|null
condition|)
block|{
comment|// Add to final result set
name|result
operator|.
name|add
argument_list|(
name|materialization
argument_list|)
expr_stmt|;
block|}
block|}
block|}
return|return
name|result
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    * Get all existing database names.    *    * @return List of database names.    * @throws HiveException    */
specifier|public
name|List
argument_list|<
name|String
argument_list|>
name|getAllDatabases
parameter_list|()
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|getAllDatabases
argument_list|()
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    * Get all existing databases that match the given    * pattern. The matching occurs as per Java regular expressions    *    * @param databasePattern    *          java re pattern    * @return list of database names    * @throws HiveException    */
specifier|public
name|List
argument_list|<
name|String
argument_list|>
name|getDatabasesByPattern
parameter_list|(
name|String
name|databasePattern
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|getDatabases
argument_list|(
name|databasePattern
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|boolean
name|grantPrivileges
parameter_list|(
name|PrivilegeBag
name|privileges
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|grant_privileges
argument_list|(
name|privileges
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    * @param privileges    *          a bag of privileges    * @return true on success    * @throws HiveException    */
specifier|public
name|boolean
name|revokePrivileges
parameter_list|(
name|PrivilegeBag
name|privileges
parameter_list|,
name|boolean
name|grantOption
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|revoke_privileges
argument_list|(
name|privileges
argument_list|,
name|grantOption
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    * Query metadata to see if a database with the given name already exists.    *    * @param dbName    * @return true if a database with the given name already exists, false if    *         does not exist.    * @throws HiveException    */
specifier|public
name|boolean
name|databaseExists
parameter_list|(
name|String
name|dbName
parameter_list|)
throws|throws
name|HiveException
block|{
return|return
name|getDatabase
argument_list|(
name|dbName
argument_list|)
operator|!=
literal|null
return|;
block|}
comment|/**    * Get the database by name.    * @param dbName the name of the database.    * @return a Database object if this database exists, null otherwise.    * @throws HiveException    */
specifier|public
name|Database
name|getDatabase
parameter_list|(
name|String
name|dbName
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|getDatabase
argument_list|(
name|dbName
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|NoSuchObjectException
name|e
parameter_list|)
block|{
return|return
literal|null
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    * Get the Database object for current database    * @return a Database object if this database exists, null otherwise.    * @throws HiveException    */
specifier|public
name|Database
name|getDatabaseCurrent
parameter_list|()
throws|throws
name|HiveException
block|{
name|String
name|currentDb
init|=
name|SessionState
operator|.
name|get
argument_list|()
operator|.
name|getCurrentDatabase
argument_list|()
decl_stmt|;
return|return
name|getDatabase
argument_list|(
name|currentDb
argument_list|)
return|;
block|}
comment|/**    * @param loadPath    * @param tableName    * @param partSpec    * @param replace    * @param inheritTableSpecs    * @param isSkewedStoreAsSubdir    * @param isSrcLocal    * @param isAcid    * @param hasFollowingStatsTask    * @return    * @throws HiveException    */
specifier|public
name|void
name|loadPartition
parameter_list|(
name|Path
name|loadPath
parameter_list|,
name|String
name|tableName
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|partSpec
parameter_list|,
name|boolean
name|replace
parameter_list|,
name|boolean
name|inheritTableSpecs
parameter_list|,
name|boolean
name|isSkewedStoreAsSubdir
parameter_list|,
name|boolean
name|isSrcLocal
parameter_list|,
name|boolean
name|isAcid
parameter_list|,
name|boolean
name|hasFollowingStatsTask
parameter_list|,
name|Long
name|txnId
parameter_list|,
name|int
name|stmtId
parameter_list|)
throws|throws
name|HiveException
block|{
name|Table
name|tbl
init|=
name|getTable
argument_list|(
name|tableName
argument_list|)
decl_stmt|;
name|loadPartition
argument_list|(
name|loadPath
argument_list|,
name|tbl
argument_list|,
name|partSpec
argument_list|,
name|replace
argument_list|,
name|inheritTableSpecs
argument_list|,
name|isSkewedStoreAsSubdir
argument_list|,
name|isSrcLocal
argument_list|,
name|isAcid
argument_list|,
name|hasFollowingStatsTask
argument_list|,
name|txnId
argument_list|,
name|stmtId
argument_list|)
expr_stmt|;
block|}
comment|/**    * Load a directory into a Hive Table Partition - Alters existing content of    * the partition with the contents of loadPath. - If the partition does not    * exist - one is created - files in loadPath are moved into Hive. But the    * directory itself is not removed.    *    * @param loadPath    *          Directory containing files to load into Table    * @param  tbl    *          name of table to be loaded.    * @param partSpec    *          defines which partition needs to be loaded    * @param replace    *          if true - replace files in the partition, otherwise add files to    *          the partition    * @param inheritTableSpecs if true, on [re]creating the partition, take the    *          location/inputformat/outputformat/serde details from table spec    * @param isSrcLocal    *          If the source directory is LOCAL    * @param isAcid    *          true if this is an ACID operation    * @param hasFollowingStatsTask    *          true if there is a following task which updates the stats, so, this method need not update.    * @return Partition object being loaded with data    */
specifier|public
name|Partition
name|loadPartition
parameter_list|(
name|Path
name|loadPath
parameter_list|,
name|Table
name|tbl
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|partSpec
parameter_list|,
name|boolean
name|replace
parameter_list|,
name|boolean
name|inheritTableSpecs
parameter_list|,
name|boolean
name|isSkewedStoreAsSubdir
parameter_list|,
name|boolean
name|isSrcLocal
parameter_list|,
name|boolean
name|isAcid
parameter_list|,
name|boolean
name|hasFollowingStatsTask
parameter_list|,
name|Long
name|txnId
parameter_list|,
name|int
name|stmtId
parameter_list|)
throws|throws
name|HiveException
block|{
name|Path
name|tblDataLocationPath
init|=
name|tbl
operator|.
name|getDataLocation
argument_list|()
decl_stmt|;
name|boolean
name|isMmTableWrite
init|=
name|MetaStoreUtils
operator|.
name|isInsertOnlyTable
argument_list|(
name|tbl
operator|.
name|getParameters
argument_list|()
argument_list|)
decl_stmt|;
try|try
block|{
comment|// Get the partition object if it already exists
name|Partition
name|oldPart
init|=
name|getPartition
argument_list|(
name|tbl
argument_list|,
name|partSpec
argument_list|,
literal|false
argument_list|)
decl_stmt|;
comment|/**        * Move files before creating the partition since down stream processes        * check for existence of partition in metadata before accessing the data.        * If partition is created before data is moved, downstream waiting        * processes might move forward with partial data        */
name|Path
name|oldPartPath
init|=
operator|(
name|oldPart
operator|!=
literal|null
operator|)
condition|?
name|oldPart
operator|.
name|getDataLocation
argument_list|()
else|:
literal|null
decl_stmt|;
name|Path
name|newPartPath
init|=
literal|null
decl_stmt|;
if|if
condition|(
name|inheritTableSpecs
condition|)
block|{
name|Path
name|partPath
init|=
operator|new
name|Path
argument_list|(
name|tbl
operator|.
name|getDataLocation
argument_list|()
argument_list|,
name|Warehouse
operator|.
name|makePartPath
argument_list|(
name|partSpec
argument_list|)
argument_list|)
decl_stmt|;
name|newPartPath
operator|=
operator|new
name|Path
argument_list|(
name|tblDataLocationPath
operator|.
name|toUri
argument_list|()
operator|.
name|getScheme
argument_list|()
argument_list|,
name|tblDataLocationPath
operator|.
name|toUri
argument_list|()
operator|.
name|getAuthority
argument_list|()
argument_list|,
name|partPath
operator|.
name|toUri
argument_list|()
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|oldPart
operator|!=
literal|null
condition|)
block|{
comment|/*            * If we are moving the partition across filesystem boundaries            * inherit from the table properties. Otherwise (same filesystem) use the            * original partition location.            *            * See: HIVE-1707 and HIVE-2117 for background            */
name|FileSystem
name|oldPartPathFS
init|=
name|oldPartPath
operator|.
name|getFileSystem
argument_list|(
name|getConf
argument_list|()
argument_list|)
decl_stmt|;
name|FileSystem
name|loadPathFS
init|=
name|loadPath
operator|.
name|getFileSystem
argument_list|(
name|getConf
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|FileUtils
operator|.
name|equalsFileSystem
argument_list|(
name|oldPartPathFS
argument_list|,
name|loadPathFS
argument_list|)
condition|)
block|{
name|newPartPath
operator|=
name|oldPartPath
expr_stmt|;
block|}
block|}
block|}
else|else
block|{
name|newPartPath
operator|=
name|oldPartPath
expr_stmt|;
block|}
name|List
argument_list|<
name|Path
argument_list|>
name|newFiles
init|=
literal|null
decl_stmt|;
name|PerfLogger
name|perfLogger
init|=
name|SessionState
operator|.
name|getPerfLogger
argument_list|()
decl_stmt|;
name|perfLogger
operator|.
name|PerfLogBegin
argument_list|(
literal|"MoveTask"
argument_list|,
literal|"FileMoves"
argument_list|)
expr_stmt|;
comment|// If config is set, table is not temporary and partition being inserted exists, capture
comment|// the list of files added. For not yet existing partitions (insert overwrite to new partition
comment|// or dynamic partition inserts), the add partition event will capture the list of files added.
if|if
condition|(
name|conf
operator|.
name|getBoolVar
argument_list|(
name|ConfVars
operator|.
name|FIRE_EVENTS_FOR_DML
argument_list|)
operator|&&
operator|!
name|tbl
operator|.
name|isTemporary
argument_list|()
operator|&&
operator|(
literal|null
operator|!=
name|oldPart
operator|)
condition|)
block|{
name|newFiles
operator|=
name|Collections
operator|.
name|synchronizedList
argument_list|(
operator|new
name|ArrayList
argument_list|<
name|Path
argument_list|>
argument_list|()
argument_list|)
expr_stmt|;
block|}
comment|// TODO: this assumes both paths are qualified; which they are, currently.
if|if
condition|(
name|isMmTableWrite
operator|&&
name|loadPath
operator|.
name|equals
argument_list|(
name|newPartPath
argument_list|)
condition|)
block|{
comment|// MM insert query, move itself is a no-op.
name|Utilities
operator|.
name|LOG14535
operator|.
name|info
argument_list|(
literal|"not moving "
operator|+
name|loadPath
operator|+
literal|" to "
operator|+
name|newPartPath
operator|+
literal|" (MM)"
argument_list|)
expr_stmt|;
assert|assert
operator|!
name|isAcid
assert|;
if|if
condition|(
name|areEventsForDmlNeeded
argument_list|(
name|tbl
argument_list|,
name|oldPart
argument_list|)
condition|)
block|{
name|newFiles
operator|=
name|listFilesCreatedByQuery
argument_list|(
name|loadPath
argument_list|,
name|txnId
argument_list|,
name|stmtId
argument_list|)
expr_stmt|;
block|}
name|Utilities
operator|.
name|LOG14535
operator|.
name|info
argument_list|(
literal|"maybe deleting stuff from "
operator|+
name|oldPartPath
operator|+
literal|" (new "
operator|+
name|newPartPath
operator|+
literal|") for replace"
argument_list|)
expr_stmt|;
if|if
condition|(
name|replace
operator|&&
name|oldPartPath
operator|!=
literal|null
condition|)
block|{
name|boolean
name|isAutoPurge
init|=
literal|"true"
operator|.
name|equalsIgnoreCase
argument_list|(
name|tbl
operator|.
name|getProperty
argument_list|(
literal|"auto.purge"
argument_list|)
argument_list|)
decl_stmt|;
name|deleteOldPathForReplace
argument_list|(
name|newPartPath
argument_list|,
name|oldPartPath
argument_list|,
name|getConf
argument_list|()
argument_list|,
name|isAutoPurge
argument_list|,
operator|new
name|JavaUtils
operator|.
name|IdPathFilter
argument_list|(
name|txnId
argument_list|,
name|stmtId
argument_list|,
literal|false
argument_list|,
literal|true
argument_list|)
argument_list|,
literal|true
argument_list|,
name|tbl
operator|.
name|isStoredAsSubDirectories
argument_list|()
condition|?
name|tbl
operator|.
name|getSkewedColNames
argument_list|()
operator|.
name|size
argument_list|()
else|:
literal|0
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
comment|// Either a non-MM query, or a load into MM table from an external source.
name|PathFilter
name|filter
init|=
name|FileUtils
operator|.
name|HIDDEN_FILES_PATH_FILTER
decl_stmt|;
name|Path
name|destPath
init|=
name|newPartPath
decl_stmt|;
if|if
condition|(
name|isMmTableWrite
condition|)
block|{
comment|// We will load into MM directory, and delete from the parent if needed.
name|destPath
operator|=
operator|new
name|Path
argument_list|(
name|destPath
argument_list|,
name|AcidUtils
operator|.
name|deltaSubdir
argument_list|(
name|txnId
argument_list|,
name|txnId
argument_list|,
name|stmtId
argument_list|)
argument_list|)
expr_stmt|;
name|filter
operator|=
name|replace
condition|?
operator|new
name|JavaUtils
operator|.
name|IdPathFilter
argument_list|(
name|txnId
argument_list|,
name|stmtId
argument_list|,
literal|false
argument_list|,
literal|true
argument_list|)
else|:
name|filter
expr_stmt|;
block|}
name|Utilities
operator|.
name|LOG14535
operator|.
name|info
argument_list|(
literal|"moving "
operator|+
name|loadPath
operator|+
literal|" to "
operator|+
name|destPath
argument_list|)
expr_stmt|;
if|if
condition|(
name|replace
operator|||
operator|(
name|oldPart
operator|==
literal|null
operator|&&
operator|!
name|isAcid
operator|)
condition|)
block|{
name|boolean
name|isAutoPurge
init|=
literal|"true"
operator|.
name|equalsIgnoreCase
argument_list|(
name|tbl
operator|.
name|getProperty
argument_list|(
literal|"auto.purge"
argument_list|)
argument_list|)
decl_stmt|;
name|replaceFiles
argument_list|(
name|tbl
operator|.
name|getPath
argument_list|()
argument_list|,
name|loadPath
argument_list|,
name|destPath
argument_list|,
name|oldPartPath
argument_list|,
name|getConf
argument_list|()
argument_list|,
name|isSrcLocal
argument_list|,
name|isAutoPurge
argument_list|,
name|newFiles
argument_list|,
name|filter
argument_list|,
name|isMmTableWrite
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|FileSystem
name|fs
init|=
name|tbl
operator|.
name|getDataLocation
argument_list|()
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|Hive
operator|.
name|copyFiles
argument_list|(
name|conf
argument_list|,
name|loadPath
argument_list|,
name|destPath
argument_list|,
name|fs
argument_list|,
name|isSrcLocal
argument_list|,
name|isAcid
argument_list|,
name|newFiles
argument_list|)
expr_stmt|;
block|}
block|}
name|perfLogger
operator|.
name|PerfLogEnd
argument_list|(
literal|"MoveTask"
argument_list|,
literal|"FileMoves"
argument_list|)
expr_stmt|;
name|Partition
name|newTPart
init|=
name|oldPart
operator|!=
literal|null
condition|?
name|oldPart
else|:
operator|new
name|Partition
argument_list|(
name|tbl
argument_list|,
name|partSpec
argument_list|,
name|newPartPath
argument_list|)
decl_stmt|;
name|alterPartitionSpecInMemory
argument_list|(
name|tbl
argument_list|,
name|partSpec
argument_list|,
name|newTPart
operator|.
name|getTPartition
argument_list|()
argument_list|,
name|inheritTableSpecs
argument_list|,
name|newPartPath
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
name|validatePartition
argument_list|(
name|newTPart
argument_list|)
expr_stmt|;
comment|// Generate an insert event only if inserting into an existing partition
comment|// When inserting into a new partition, the add partition event takes care of insert event
if|if
condition|(
operator|(
literal|null
operator|!=
name|oldPart
operator|)
operator|&&
operator|(
literal|null
operator|!=
name|newFiles
operator|)
condition|)
block|{
name|fireInsertEvent
argument_list|(
name|tbl
argument_list|,
name|partSpec
argument_list|,
name|replace
argument_list|,
name|newFiles
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"No new files were created, and is not a replace, or we're inserting into a "
operator|+
literal|"partition that does not exist yet. Skipping generating INSERT event."
argument_list|)
expr_stmt|;
block|}
comment|// column stats will be inaccurate
name|StatsSetupConst
operator|.
name|clearColumnStatsState
argument_list|(
name|newTPart
operator|.
name|getParameters
argument_list|()
argument_list|)
expr_stmt|;
comment|// recreate the partition if it existed before
if|if
condition|(
name|isSkewedStoreAsSubdir
condition|)
block|{
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
name|newCreatedTpart
init|=
name|newTPart
operator|.
name|getTPartition
argument_list|()
decl_stmt|;
name|SkewedInfo
name|skewedInfo
init|=
name|newCreatedTpart
operator|.
name|getSd
argument_list|()
operator|.
name|getSkewedInfo
argument_list|()
decl_stmt|;
comment|/* Construct list bucketing location mappings from sub-directory name. */
name|Map
argument_list|<
name|List
argument_list|<
name|String
argument_list|>
argument_list|,
name|String
argument_list|>
name|skewedColValueLocationMaps
init|=
name|constructListBucketingLocationMap
argument_list|(
name|newPartPath
argument_list|,
name|skewedInfo
argument_list|)
decl_stmt|;
comment|/* Add list bucketing location mappings. */
name|skewedInfo
operator|.
name|setSkewedColValueLocationMaps
argument_list|(
name|skewedColValueLocationMaps
argument_list|)
expr_stmt|;
name|newCreatedTpart
operator|.
name|getSd
argument_list|()
operator|.
name|setSkewedInfo
argument_list|(
name|skewedInfo
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
operator|!
name|this
operator|.
name|getConf
argument_list|()
operator|.
name|getBoolVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVESTATSAUTOGATHER
argument_list|)
condition|)
block|{
name|StatsSetupConst
operator|.
name|setBasicStatsState
argument_list|(
name|newTPart
operator|.
name|getParameters
argument_list|()
argument_list|,
name|StatsSetupConst
operator|.
name|FALSE
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|oldPart
operator|==
literal|null
condition|)
block|{
name|newTPart
operator|.
name|getTPartition
argument_list|()
operator|.
name|setParameters
argument_list|(
operator|new
name|HashMap
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|this
operator|.
name|getConf
argument_list|()
operator|.
name|getBoolVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVESTATSAUTOGATHER
argument_list|)
condition|)
block|{
name|StatsSetupConst
operator|.
name|setBasicStatsStateForCreateTable
argument_list|(
name|newTPart
operator|.
name|getParameters
argument_list|()
argument_list|,
name|StatsSetupConst
operator|.
name|TRUE
argument_list|)
expr_stmt|;
block|}
name|MetaStoreUtils
operator|.
name|populateQuickStats
argument_list|(
name|HiveStatsUtils
operator|.
name|getFileStatusRecurse
argument_list|(
name|newPartPath
argument_list|,
operator|-
literal|1
argument_list|,
name|newPartPath
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
argument_list|)
argument_list|,
name|newTPart
operator|.
name|getParameters
argument_list|()
argument_list|)
expr_stmt|;
try|try
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Adding new partition "
operator|+
name|newTPart
operator|.
name|getSpec
argument_list|()
argument_list|)
expr_stmt|;
name|getSychronizedMSC
argument_list|()
operator|.
name|add_partition
argument_list|(
name|newTPart
operator|.
name|getTPartition
argument_list|()
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|AlreadyExistsException
name|aee
parameter_list|)
block|{
comment|// With multiple users concurrently issuing insert statements on the same partition has
comment|// a side effect that some queries may not see a partition at the time when they're issued,
comment|// but will realize the partition is actually there when it is trying to add such partition
comment|// to the metastore and thus get AlreadyExistsException, because some earlier query just created it (race condition).
comment|// For example, imagine such a table is created:
comment|//  create table T (name char(50)) partitioned by (ds string);
comment|// and the following two queries are launched at the same time, from different sessions:
comment|//  insert into table T partition (ds) values ('Bob', 'today'); -- creates the partition 'today'
comment|//  insert into table T partition (ds) values ('Joe', 'today'); -- will fail with AlreadyExistsException
comment|// In that case, we want to retry with alterPartition.
name|LOG
operator|.
name|debug
argument_list|(
literal|"Caught AlreadyExistsException, trying to alter partition instead"
argument_list|)
expr_stmt|;
name|setStatsPropAndAlterPartition
argument_list|(
name|hasFollowingStatsTask
argument_list|,
name|tbl
argument_list|,
name|newTPart
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
name|setStatsPropAndAlterPartition
argument_list|(
name|hasFollowingStatsTask
argument_list|,
name|tbl
argument_list|,
name|newTPart
argument_list|)
expr_stmt|;
block|}
return|return
name|newTPart
return|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
catch|catch
parameter_list|(
name|MetaException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
catch|catch
parameter_list|(
name|InvalidOperationException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
catch|catch
parameter_list|(
name|TException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|private
name|boolean
name|areEventsForDmlNeeded
parameter_list|(
name|Table
name|tbl
parameter_list|,
name|Partition
name|oldPart
parameter_list|)
block|{
return|return
name|conf
operator|.
name|getBoolVar
argument_list|(
name|ConfVars
operator|.
name|FIRE_EVENTS_FOR_DML
argument_list|)
operator|&&
operator|!
name|tbl
operator|.
name|isTemporary
argument_list|()
operator|&&
name|oldPart
operator|!=
literal|null
return|;
block|}
specifier|private
name|List
argument_list|<
name|Path
argument_list|>
name|listFilesCreatedByQuery
parameter_list|(
name|Path
name|loadPath
parameter_list|,
name|long
name|txnId
parameter_list|,
name|int
name|stmtId
parameter_list|)
throws|throws
name|HiveException
block|{
name|List
argument_list|<
name|Path
argument_list|>
name|newFiles
init|=
operator|new
name|ArrayList
argument_list|<
name|Path
argument_list|>
argument_list|()
decl_stmt|;
specifier|final
name|String
name|filePrefix
init|=
name|AcidUtils
operator|.
name|deltaSubdir
argument_list|(
name|txnId
argument_list|,
name|txnId
argument_list|,
name|stmtId
argument_list|)
decl_stmt|;
name|FileStatus
index|[]
name|srcs
decl_stmt|;
name|FileSystem
name|srcFs
decl_stmt|;
try|try
block|{
name|srcFs
operator|=
name|loadPath
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
expr_stmt|;
name|srcs
operator|=
name|srcFs
operator|.
name|listStatus
argument_list|(
name|loadPath
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Error listing files"
argument_list|,
name|e
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
if|if
condition|(
name|srcs
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"No sources specified: "
operator|+
name|loadPath
argument_list|)
expr_stmt|;
return|return
name|newFiles
return|;
block|}
name|PathFilter
name|subdirFilter
init|=
literal|null
decl_stmt|;
comment|// TODO: just like the move path, we only do one level of recursion.
for|for
control|(
name|FileStatus
name|src
range|:
name|srcs
control|)
block|{
if|if
condition|(
name|src
operator|.
name|isDirectory
argument_list|()
condition|)
block|{
if|if
condition|(
name|subdirFilter
operator|==
literal|null
condition|)
block|{
name|subdirFilter
operator|=
operator|new
name|PathFilter
argument_list|()
block|{
annotation|@
name|Override
specifier|public
name|boolean
name|accept
parameter_list|(
name|Path
name|path
parameter_list|)
block|{
return|return
name|path
operator|.
name|getName
argument_list|()
operator|.
name|startsWith
argument_list|(
name|filePrefix
argument_list|)
return|;
block|}
block|}
expr_stmt|;
block|}
try|try
block|{
for|for
control|(
name|FileStatus
name|srcFile
range|:
name|srcFs
operator|.
name|listStatus
argument_list|(
name|src
operator|.
name|getPath
argument_list|()
argument_list|,
name|subdirFilter
argument_list|)
control|)
block|{
name|newFiles
operator|.
name|add
argument_list|(
name|srcFile
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
elseif|else
if|if
condition|(
name|src
operator|.
name|getPath
argument_list|()
operator|.
name|getName
argument_list|()
operator|.
name|startsWith
argument_list|(
name|filePrefix
argument_list|)
condition|)
block|{
name|newFiles
operator|.
name|add
argument_list|(
name|src
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|newFiles
return|;
block|}
specifier|private
name|void
name|setStatsPropAndAlterPartition
parameter_list|(
name|boolean
name|hasFollowingStatsTask
parameter_list|,
name|Table
name|tbl
parameter_list|,
name|Partition
name|newTPart
parameter_list|)
throws|throws
name|MetaException
throws|,
name|TException
block|{
name|EnvironmentContext
name|environmentContext
init|=
literal|null
decl_stmt|;
if|if
condition|(
name|hasFollowingStatsTask
condition|)
block|{
name|environmentContext
operator|=
operator|new
name|EnvironmentContext
argument_list|()
expr_stmt|;
name|environmentContext
operator|.
name|putToProperties
argument_list|(
name|StatsSetupConst
operator|.
name|DO_NOT_UPDATE_STATS
argument_list|,
name|StatsSetupConst
operator|.
name|TRUE
argument_list|)
expr_stmt|;
block|}
name|LOG
operator|.
name|debug
argument_list|(
literal|"Altering existing partition "
operator|+
name|newTPart
operator|.
name|getSpec
argument_list|()
argument_list|)
expr_stmt|;
name|getSychronizedMSC
argument_list|()
operator|.
name|alter_partition
argument_list|(
name|tbl
operator|.
name|getDbName
argument_list|()
argument_list|,
name|tbl
operator|.
name|getTableName
argument_list|()
argument_list|,
name|newTPart
operator|.
name|getTPartition
argument_list|()
argument_list|,
name|environmentContext
argument_list|)
expr_stmt|;
block|}
comment|/**  * Walk through sub-directory tree to construct list bucketing location map.  *  * @param fSta  * @param fSys  * @param skewedColValueLocationMaps  * @param newPartPath  * @param skewedInfo  * @throws IOException  */
specifier|private
name|void
name|walkDirTree
parameter_list|(
name|FileStatus
name|fSta
parameter_list|,
name|FileSystem
name|fSys
parameter_list|,
name|Map
argument_list|<
name|List
argument_list|<
name|String
argument_list|>
argument_list|,
name|String
argument_list|>
name|skewedColValueLocationMaps
parameter_list|,
name|Path
name|newPartPath
parameter_list|,
name|SkewedInfo
name|skewedInfo
parameter_list|)
throws|throws
name|IOException
block|{
comment|// TODO# HERE broken
comment|/* Base Case. It's leaf. */
if|if
condition|(
operator|!
name|fSta
operator|.
name|isDir
argument_list|()
condition|)
block|{
name|Utilities
operator|.
name|LOG14535
operator|.
name|info
argument_list|(
literal|"Processing LB leaf "
operator|+
name|fSta
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
comment|/* construct one location map if not exists. */
name|constructOneLBLocationMap
argument_list|(
name|fSta
argument_list|,
name|skewedColValueLocationMaps
argument_list|,
name|newPartPath
argument_list|,
name|skewedInfo
argument_list|)
expr_stmt|;
return|return;
block|}
comment|/* dfs. */
name|FileStatus
index|[]
name|children
init|=
name|fSys
operator|.
name|listStatus
argument_list|(
name|fSta
operator|.
name|getPath
argument_list|()
argument_list|,
name|FileUtils
operator|.
name|HIDDEN_FILES_PATH_FILTER
argument_list|)
decl_stmt|;
if|if
condition|(
name|children
operator|!=
literal|null
condition|)
block|{
name|Utilities
operator|.
name|LOG14535
operator|.
name|info
argument_list|(
literal|"Processing LB dir "
operator|+
name|fSta
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
for|for
control|(
name|FileStatus
name|child
range|:
name|children
control|)
block|{
name|walkDirTree
argument_list|(
name|child
argument_list|,
name|fSys
argument_list|,
name|skewedColValueLocationMaps
argument_list|,
name|newPartPath
argument_list|,
name|skewedInfo
argument_list|)
expr_stmt|;
block|}
block|}
block|}
comment|/**  * Construct a list bucketing location map  * @param fSta  * @param skewedColValueLocationMaps  * @param newPartPath  * @param skewedInfo  */
specifier|private
name|void
name|constructOneLBLocationMap
parameter_list|(
name|FileStatus
name|fSta
parameter_list|,
name|Map
argument_list|<
name|List
argument_list|<
name|String
argument_list|>
argument_list|,
name|String
argument_list|>
name|skewedColValueLocationMaps
parameter_list|,
name|Path
name|newPartPath
parameter_list|,
name|SkewedInfo
name|skewedInfo
parameter_list|)
block|{
name|Path
name|lbdPath
init|=
name|fSta
operator|.
name|getPath
argument_list|()
operator|.
name|getParent
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|skewedValue
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
name|String
name|lbDirName
init|=
name|FileUtils
operator|.
name|unescapePathName
argument_list|(
name|lbdPath
operator|.
name|toString
argument_list|()
argument_list|)
decl_stmt|;
name|String
name|partDirName
init|=
name|FileUtils
operator|.
name|unescapePathName
argument_list|(
name|newPartPath
operator|.
name|toString
argument_list|()
argument_list|)
decl_stmt|;
name|String
name|lbDirSuffix
init|=
name|lbDirName
operator|.
name|replace
argument_list|(
name|partDirName
argument_list|,
literal|""
argument_list|)
decl_stmt|;
comment|// TODO: wtf?
if|if
condition|(
name|lbDirSuffix
operator|.
name|startsWith
argument_list|(
name|Path
operator|.
name|SEPARATOR
argument_list|)
condition|)
block|{
name|lbDirSuffix
operator|=
name|lbDirSuffix
operator|.
name|substring
argument_list|(
literal|1
argument_list|)
expr_stmt|;
block|}
name|String
index|[]
name|dirNames
init|=
name|lbDirSuffix
operator|.
name|split
argument_list|(
name|Path
operator|.
name|SEPARATOR
argument_list|)
decl_stmt|;
name|int
name|keysFound
init|=
literal|0
decl_stmt|,
name|dirsToTake
init|=
literal|0
decl_stmt|;
name|int
name|colCount
init|=
name|skewedInfo
operator|.
name|getSkewedColNames
argument_list|()
operator|.
name|size
argument_list|()
decl_stmt|;
while|while
condition|(
name|dirsToTake
operator|<
name|dirNames
operator|.
name|length
operator|&&
name|keysFound
operator|<
name|colCount
condition|)
block|{
name|String
name|dirName
init|=
name|dirNames
index|[
name|dirsToTake
operator|++
index|]
decl_stmt|;
comment|// Construct skewed-value to location map except default directory.
comment|// why? query logic knows default-dir structure and don't need to get from map
if|if
condition|(
name|dirName
operator|.
name|equalsIgnoreCase
argument_list|(
name|ListBucketingPrunerUtils
operator|.
name|HIVE_LIST_BUCKETING_DEFAULT_DIR_NAME
argument_list|)
condition|)
block|{
operator|++
name|keysFound
expr_stmt|;
block|}
else|else
block|{
name|String
index|[]
name|kv
init|=
name|dirName
operator|.
name|split
argument_list|(
literal|"="
argument_list|)
decl_stmt|;
if|if
condition|(
name|kv
operator|.
name|length
operator|==
literal|2
condition|)
block|{
name|skewedValue
operator|.
name|add
argument_list|(
name|kv
index|[
literal|1
index|]
argument_list|)
expr_stmt|;
operator|++
name|keysFound
expr_stmt|;
block|}
else|else
block|{
comment|// TODO: we should really probably throw. Keep the existing logic for now.
name|LOG
operator|.
name|warn
argument_list|(
literal|"Skipping unknown directory "
operator|+
name|dirName
operator|+
literal|" when expecting LB keys or default directory (from "
operator|+
name|lbDirName
operator|+
literal|")"
argument_list|)
expr_stmt|;
block|}
block|}
block|}
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
operator|(
name|dirNames
operator|.
name|length
operator|-
name|dirsToTake
operator|)
condition|;
operator|++
name|i
control|)
block|{
name|lbdPath
operator|=
name|lbdPath
operator|.
name|getParent
argument_list|()
expr_stmt|;
block|}
name|Utilities
operator|.
name|LOG14535
operator|.
name|info
argument_list|(
literal|"Saving LB location "
operator|+
name|lbdPath
operator|+
literal|" based on "
operator|+
name|colCount
operator|+
literal|" keys and "
operator|+
name|fSta
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
operator|(
name|skewedValue
operator|.
name|size
argument_list|()
operator|>
literal|0
operator|)
operator|&&
operator|(
name|skewedValue
operator|.
name|size
argument_list|()
operator|==
name|colCount
operator|)
operator|&&
operator|!
name|skewedColValueLocationMaps
operator|.
name|containsKey
argument_list|(
name|skewedValue
argument_list|)
condition|)
block|{
name|skewedColValueLocationMaps
operator|.
name|put
argument_list|(
name|skewedValue
argument_list|,
name|lbdPath
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * Construct location map from path    *    * @param newPartPath    * @param skewedInfo    * @return    * @throws IOException    * @throws FileNotFoundException    */
specifier|private
name|Map
argument_list|<
name|List
argument_list|<
name|String
argument_list|>
argument_list|,
name|String
argument_list|>
name|constructListBucketingLocationMap
parameter_list|(
name|Path
name|newPartPath
parameter_list|,
name|SkewedInfo
name|skewedInfo
parameter_list|)
throws|throws
name|IOException
throws|,
name|FileNotFoundException
block|{
name|Utilities
operator|.
name|LOG14535
operator|.
name|info
argument_list|(
literal|"Constructing list bucketing map for "
operator|+
name|newPartPath
argument_list|)
expr_stmt|;
name|Map
argument_list|<
name|List
argument_list|<
name|String
argument_list|>
argument_list|,
name|String
argument_list|>
name|skewedColValueLocationMaps
init|=
operator|new
name|HashMap
argument_list|<
name|List
argument_list|<
name|String
argument_list|>
argument_list|,
name|String
argument_list|>
argument_list|()
decl_stmt|;
name|FileSystem
name|fSys
init|=
name|newPartPath
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|walkDirTree
argument_list|(
name|fSys
operator|.
name|getFileStatus
argument_list|(
name|newPartPath
argument_list|)
argument_list|,
name|fSys
argument_list|,
name|skewedColValueLocationMaps
argument_list|,
name|newPartPath
argument_list|,
name|skewedInfo
argument_list|)
expr_stmt|;
return|return
name|skewedColValueLocationMaps
return|;
block|}
comment|/**    * Get the valid partitions from the path    * @param numDP number of dynamic partitions    * @param loadPath    * @return Set of valid partitions    * @throws HiveException    */
specifier|private
name|Set
argument_list|<
name|Path
argument_list|>
name|getValidPartitionsInPath
parameter_list|(
name|int
name|numDP
parameter_list|,
name|int
name|numLB
parameter_list|,
name|Path
name|loadPath
parameter_list|,
name|Long
name|txnId
parameter_list|,
name|int
name|stmtId
parameter_list|,
name|boolean
name|isMmTable
parameter_list|)
throws|throws
name|HiveException
block|{
name|Set
argument_list|<
name|Path
argument_list|>
name|validPartitions
init|=
operator|new
name|HashSet
argument_list|<
name|Path
argument_list|>
argument_list|()
decl_stmt|;
try|try
block|{
name|FileSystem
name|fs
init|=
name|loadPath
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|isMmTable
condition|)
block|{
name|FileStatus
index|[]
name|leafStatus
init|=
name|HiveStatsUtils
operator|.
name|getFileStatusRecurse
argument_list|(
name|loadPath
argument_list|,
name|numDP
argument_list|,
name|fs
argument_list|)
decl_stmt|;
comment|// Check for empty partitions
for|for
control|(
name|FileStatus
name|s
range|:
name|leafStatus
control|)
block|{
if|if
condition|(
operator|!
name|s
operator|.
name|isDirectory
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"partition "
operator|+
name|s
operator|.
name|getPath
argument_list|()
operator|+
literal|" is not a directory!"
argument_list|)
throw|;
block|}
name|Path
name|dpPath
init|=
name|s
operator|.
name|getPath
argument_list|()
decl_stmt|;
name|Utilities
operator|.
name|LOG14535
operator|.
name|info
argument_list|(
literal|"Found DP "
operator|+
name|dpPath
argument_list|)
expr_stmt|;
name|validPartitions
operator|.
name|add
argument_list|(
name|dpPath
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
comment|// The non-MM path only finds new partitions, as it is looking at the temp path.
comment|// To produce the same effect, we will find all the partitions affected by this write ID.
name|Path
index|[]
name|leafStatus
init|=
name|Utilities
operator|.
name|getMmDirectoryCandidates
argument_list|(
name|fs
argument_list|,
name|loadPath
argument_list|,
name|numDP
argument_list|,
name|numLB
argument_list|,
literal|null
argument_list|,
name|txnId
argument_list|,
name|stmtId
argument_list|,
name|conf
argument_list|)
decl_stmt|;
for|for
control|(
name|Path
name|p
range|:
name|leafStatus
control|)
block|{
name|Path
name|dpPath
init|=
name|p
operator|.
name|getParent
argument_list|()
decl_stmt|;
comment|// Skip the MM directory that we have found.
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|numLB
condition|;
operator|++
name|i
control|)
block|{
name|dpPath
operator|=
name|dpPath
operator|.
name|getParent
argument_list|()
expr_stmt|;
comment|// Now skip the LB directories, if any...
block|}
name|Utilities
operator|.
name|LOG14535
operator|.
name|info
argument_list|(
literal|"Found DP "
operator|+
name|dpPath
argument_list|)
expr_stmt|;
name|validPartitions
operator|.
name|add
argument_list|(
name|dpPath
argument_list|)
expr_stmt|;
block|}
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
name|int
name|partsToLoad
init|=
name|validPartitions
operator|.
name|size
argument_list|()
decl_stmt|;
if|if
condition|(
name|partsToLoad
operator|==
literal|0
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"No partition is generated by dynamic partitioning"
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|partsToLoad
operator|>
name|conf
operator|.
name|getIntVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|DYNAMICPARTITIONMAXPARTS
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Number of dynamic partitions created is "
operator|+
name|partsToLoad
operator|+
literal|", which is more than "
operator|+
name|conf
operator|.
name|getIntVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|DYNAMICPARTITIONMAXPARTS
argument_list|)
operator|+
literal|". To solve this try to set "
operator|+
name|HiveConf
operator|.
name|ConfVars
operator|.
name|DYNAMICPARTITIONMAXPARTS
operator|.
name|varname
operator|+
literal|" to at least "
operator|+
name|partsToLoad
operator|+
literal|'.'
argument_list|)
throw|;
block|}
return|return
name|validPartitions
return|;
block|}
comment|/**    * Given a source directory name of the load path, load all dynamically generated partitions    * into the specified table and return a list of strings that represent the dynamic partition    * paths.    * @param loadPath    * @param tableName    * @param partSpec    * @param replace    * @param numDP number of dynamic partitions    * @param listBucketingEnabled    * @param isAcid true if this is an ACID operation    * @param txnId txnId, can be 0 unless isAcid == true    * @return partition map details (PartitionSpec and Partition)    * @throws HiveException    */
specifier|public
name|Map
argument_list|<
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
argument_list|,
name|Partition
argument_list|>
name|loadDynamicPartitions
parameter_list|(
specifier|final
name|Path
name|loadPath
parameter_list|,
specifier|final
name|String
name|tableName
parameter_list|,
specifier|final
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|partSpec
parameter_list|,
specifier|final
name|boolean
name|replace
parameter_list|,
specifier|final
name|int
name|numDP
parameter_list|,
specifier|final
name|int
name|numLB
parameter_list|,
specifier|final
name|boolean
name|isAcid
parameter_list|,
specifier|final
name|long
name|txnId
parameter_list|,
specifier|final
name|int
name|stmtId
parameter_list|,
specifier|final
name|boolean
name|hasFollowingStatsTask
parameter_list|,
specifier|final
name|AcidUtils
operator|.
name|Operation
name|operation
parameter_list|)
throws|throws
name|HiveException
block|{
specifier|final
name|Map
argument_list|<
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
argument_list|,
name|Partition
argument_list|>
name|partitionsMap
init|=
name|Collections
operator|.
name|synchronizedMap
argument_list|(
operator|new
name|LinkedHashMap
argument_list|<
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
argument_list|,
name|Partition
argument_list|>
argument_list|()
argument_list|)
decl_stmt|;
name|int
name|poolSize
init|=
name|conf
operator|.
name|getInt
argument_list|(
name|ConfVars
operator|.
name|HIVE_LOAD_DYNAMIC_PARTITIONS_THREAD_COUNT
operator|.
name|varname
argument_list|,
literal|1
argument_list|)
decl_stmt|;
specifier|final
name|ExecutorService
name|pool
init|=
name|Executors
operator|.
name|newFixedThreadPool
argument_list|(
name|poolSize
argument_list|,
operator|new
name|ThreadFactoryBuilder
argument_list|()
operator|.
name|setDaemon
argument_list|(
literal|true
argument_list|)
operator|.
name|setNameFormat
argument_list|(
literal|"load-dynamic-partitions-%d"
argument_list|)
operator|.
name|build
argument_list|()
argument_list|)
decl_stmt|;
comment|// Get all valid partition paths and existing partitions for them (if any)
specifier|final
name|Table
name|tbl
init|=
name|getTable
argument_list|(
name|tableName
argument_list|)
decl_stmt|;
specifier|final
name|Set
argument_list|<
name|Path
argument_list|>
name|validPartitions
init|=
name|getValidPartitionsInPath
argument_list|(
name|numDP
argument_list|,
name|numLB
argument_list|,
name|loadPath
argument_list|,
name|txnId
argument_list|,
name|stmtId
argument_list|,
name|MetaStoreUtils
operator|.
name|isInsertOnlyTable
argument_list|(
name|tbl
operator|.
name|getParameters
argument_list|()
argument_list|)
argument_list|)
decl_stmt|;
specifier|final
name|int
name|partsToLoad
init|=
name|validPartitions
operator|.
name|size
argument_list|()
decl_stmt|;
specifier|final
name|AtomicInteger
name|partitionsLoaded
init|=
operator|new
name|AtomicInteger
argument_list|(
literal|0
argument_list|)
decl_stmt|;
specifier|final
name|boolean
name|inPlaceEligible
init|=
name|conf
operator|.
name|getLong
argument_list|(
literal|"fs.trash.interval"
argument_list|,
literal|0
argument_list|)
operator|<=
literal|0
operator|&&
name|InPlaceUpdate
operator|.
name|canRenderInPlace
argument_list|(
name|conf
argument_list|)
operator|&&
operator|!
name|SessionState
operator|.
name|getConsole
argument_list|()
operator|.
name|getIsSilent
argument_list|()
decl_stmt|;
specifier|final
name|PrintStream
name|ps
init|=
operator|(
name|inPlaceEligible
operator|)
condition|?
name|SessionState
operator|.
name|getConsole
argument_list|()
operator|.
name|getInfoStream
argument_list|()
else|:
literal|null
decl_stmt|;
specifier|final
name|SessionState
name|parentSession
init|=
name|SessionState
operator|.
name|get
argument_list|()
decl_stmt|;
specifier|final
name|List
argument_list|<
name|Future
argument_list|<
name|Void
argument_list|>
argument_list|>
name|futures
init|=
name|Lists
operator|.
name|newLinkedList
argument_list|()
decl_stmt|;
try|try
block|{
comment|// for each dynamically created DP directory, construct a full partition spec
comment|// and load the partition based on that
for|for
control|(
specifier|final
name|Path
name|partPath
range|:
name|validPartitions
control|)
block|{
comment|// generate a full partition specification
specifier|final
name|LinkedHashMap
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|fullPartSpec
init|=
name|Maps
operator|.
name|newLinkedHashMap
argument_list|(
name|partSpec
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|Warehouse
operator|.
name|makeSpecFromName
argument_list|(
name|fullPartSpec
argument_list|,
name|partPath
argument_list|,
operator|new
name|HashSet
argument_list|<
name|String
argument_list|>
argument_list|(
name|partSpec
operator|.
name|keySet
argument_list|()
argument_list|)
argument_list|)
condition|)
block|{
name|Utilities
operator|.
name|LOG14535
operator|.
name|warn
argument_list|(
literal|"Ignoring invalid DP directory "
operator|+
name|partPath
argument_list|)
expr_stmt|;
continue|continue;
block|}
name|futures
operator|.
name|add
argument_list|(
name|pool
operator|.
name|submit
argument_list|(
operator|new
name|Callable
argument_list|<
name|Void
argument_list|>
argument_list|()
block|{
annotation|@
name|Override
specifier|public
name|Void
name|call
parameter_list|()
throws|throws
name|Exception
block|{
try|try
block|{
comment|// move file would require session details (needCopy() invokes SessionState.get)
name|SessionState
operator|.
name|setCurrentSessionState
argument_list|(
name|parentSession
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"New loading path = "
operator|+
name|partPath
operator|+
literal|" with partSpec "
operator|+
name|fullPartSpec
argument_list|)
expr_stmt|;
comment|// load the partition
name|Utilities
operator|.
name|LOG14535
operator|.
name|info
argument_list|(
literal|"loadPartition called for DPP from "
operator|+
name|partPath
operator|+
literal|" to "
operator|+
name|tbl
operator|.
name|getTableName
argument_list|()
argument_list|)
expr_stmt|;
name|Partition
name|newPartition
init|=
name|loadPartition
argument_list|(
name|partPath
argument_list|,
name|tbl
argument_list|,
name|fullPartSpec
argument_list|,
name|replace
argument_list|,
literal|true
argument_list|,
name|numLB
operator|>
literal|0
argument_list|,
literal|false
argument_list|,
name|isAcid
argument_list|,
name|hasFollowingStatsTask
argument_list|,
name|txnId
argument_list|,
name|stmtId
argument_list|)
decl_stmt|;
name|partitionsMap
operator|.
name|put
argument_list|(
name|fullPartSpec
argument_list|,
name|newPartition
argument_list|)
expr_stmt|;
if|if
condition|(
name|inPlaceEligible
condition|)
block|{
synchronized|synchronized
init|(
name|ps
init|)
block|{
name|InPlaceUpdate
operator|.
name|rePositionCursor
argument_list|(
name|ps
argument_list|)
expr_stmt|;
name|partitionsLoaded
operator|.
name|incrementAndGet
argument_list|()
expr_stmt|;
name|InPlaceUpdate
operator|.
name|reprintLine
argument_list|(
name|ps
argument_list|,
literal|"Loaded : "
operator|+
name|partitionsLoaded
operator|.
name|get
argument_list|()
operator|+
literal|"/"
operator|+
name|partsToLoad
operator|+
literal|" partitions."
argument_list|)
expr_stmt|;
block|}
block|}
return|return
literal|null
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|t
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Exception when loading partition with parameters "
operator|+
literal|" partPath="
operator|+
name|partPath
operator|+
literal|", "
operator|+
literal|" table="
operator|+
name|tbl
operator|.
name|getTableName
argument_list|()
operator|+
literal|", "
operator|+
literal|" partSpec="
operator|+
name|fullPartSpec
operator|+
literal|", "
operator|+
literal|" replace="
operator|+
name|replace
operator|+
literal|", "
operator|+
literal|" listBucketingLevel="
operator|+
name|numLB
operator|+
literal|", "
operator|+
literal|" isAcid="
operator|+
name|isAcid
operator|+
literal|", "
operator|+
literal|" hasFollowingStatsTask="
operator|+
name|hasFollowingStatsTask
argument_list|,
name|t
argument_list|)
expr_stmt|;
throw|throw
name|t
throw|;
block|}
block|}
block|}
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|pool
operator|.
name|shutdown
argument_list|()
expr_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Number of partitions to be added is "
operator|+
name|futures
operator|.
name|size
argument_list|()
argument_list|)
expr_stmt|;
for|for
control|(
name|Future
name|future
range|:
name|futures
control|)
block|{
name|future
operator|.
name|get
argument_list|()
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|InterruptedException
decl||
name|ExecutionException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Cancelling "
operator|+
name|futures
operator|.
name|size
argument_list|()
operator|+
literal|" dynamic loading tasks"
argument_list|)
expr_stmt|;
comment|//cancel other futures
for|for
control|(
name|Future
name|future
range|:
name|futures
control|)
block|{
name|future
operator|.
name|cancel
argument_list|(
literal|true
argument_list|)
expr_stmt|;
block|}
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Exception when loading "
operator|+
name|partsToLoad
operator|+
literal|" in table "
operator|+
name|tbl
operator|.
name|getTableName
argument_list|()
operator|+
literal|" with loadPath="
operator|+
name|loadPath
argument_list|,
name|e
argument_list|)
throw|;
block|}
try|try
block|{
if|if
condition|(
name|isAcid
condition|)
block|{
name|List
argument_list|<
name|String
argument_list|>
name|partNames
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|(
name|partitionsMap
operator|.
name|size
argument_list|()
argument_list|)
decl_stmt|;
for|for
control|(
name|Partition
name|p
range|:
name|partitionsMap
operator|.
name|values
argument_list|()
control|)
block|{
name|partNames
operator|.
name|add
argument_list|(
name|p
operator|.
name|getName
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|getMSC
argument_list|()
operator|.
name|addDynamicPartitions
argument_list|(
name|txnId
argument_list|,
name|tbl
operator|.
name|getDbName
argument_list|()
argument_list|,
name|tbl
operator|.
name|getTableName
argument_list|()
argument_list|,
name|partNames
argument_list|,
name|AcidUtils
operator|.
name|toDataOperationType
argument_list|(
name|operation
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|LOG
operator|.
name|info
argument_list|(
literal|"Loaded "
operator|+
name|partitionsMap
operator|.
name|size
argument_list|()
operator|+
literal|" partitions"
argument_list|)
expr_stmt|;
return|return
name|partitionsMap
return|;
block|}
catch|catch
parameter_list|(
name|TException
name|te
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Exception updating metastore for acid table "
operator|+
name|tableName
operator|+
literal|" with partitions "
operator|+
name|partitionsMap
operator|.
name|values
argument_list|()
argument_list|,
name|te
argument_list|)
throw|;
block|}
block|}
comment|/**    * Load a directory into a Hive Table. - Alters existing content of table with    * the contents of loadPath. - If table does not exist - an exception is    * thrown - files in loadPath are moved into Hive. But the directory itself is    * not removed.    *    * @param loadPath    *          Directory containing files to load into Table    * @param tableName    *          name of table to be loaded.    * @param replace    *          if true - replace files in the table, otherwise add files to table    * @param isSrcLocal    *          If the source directory is LOCAL    * @param isSkewedStoreAsSubdir    *          if list bucketing enabled    * @param hasFollowingStatsTask    *          if there is any following stats task    * @param isAcid true if this is an ACID based write    */
specifier|public
name|void
name|loadTable
parameter_list|(
name|Path
name|loadPath
parameter_list|,
name|String
name|tableName
parameter_list|,
name|boolean
name|replace
parameter_list|,
name|boolean
name|isSrcLocal
parameter_list|,
name|boolean
name|isSkewedStoreAsSubdir
parameter_list|,
name|boolean
name|isAcid
parameter_list|,
name|boolean
name|hasFollowingStatsTask
parameter_list|,
name|Long
name|txnId
parameter_list|,
name|int
name|stmtId
parameter_list|,
name|boolean
name|isMmTable
parameter_list|)
throws|throws
name|HiveException
block|{
name|List
argument_list|<
name|Path
argument_list|>
name|newFiles
init|=
literal|null
decl_stmt|;
name|Table
name|tbl
init|=
name|getTable
argument_list|(
name|tableName
argument_list|)
decl_stmt|;
name|HiveConf
name|sessionConf
init|=
name|SessionState
operator|.
name|getSessionConf
argument_list|()
decl_stmt|;
if|if
condition|(
name|conf
operator|.
name|getBoolVar
argument_list|(
name|ConfVars
operator|.
name|FIRE_EVENTS_FOR_DML
argument_list|)
operator|&&
operator|!
name|tbl
operator|.
name|isTemporary
argument_list|()
condition|)
block|{
name|newFiles
operator|=
name|Collections
operator|.
name|synchronizedList
argument_list|(
operator|new
name|ArrayList
argument_list|<
name|Path
argument_list|>
argument_list|()
argument_list|)
expr_stmt|;
block|}
comment|// TODO: this assumes both paths are qualified; which they are, currently.
if|if
condition|(
name|isMmTable
operator|&&
name|loadPath
operator|.
name|equals
argument_list|(
name|tbl
operator|.
name|getPath
argument_list|()
argument_list|)
condition|)
block|{
name|Utilities
operator|.
name|LOG14535
operator|.
name|info
argument_list|(
literal|"not moving "
operator|+
name|loadPath
operator|+
literal|" to "
operator|+
name|tbl
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|replace
condition|)
block|{
name|Path
name|tableDest
init|=
name|tbl
operator|.
name|getPath
argument_list|()
decl_stmt|;
name|boolean
name|isAutopurge
init|=
literal|"true"
operator|.
name|equalsIgnoreCase
argument_list|(
name|tbl
operator|.
name|getProperty
argument_list|(
literal|"auto.purge"
argument_list|)
argument_list|)
decl_stmt|;
name|deleteOldPathForReplace
argument_list|(
name|tableDest
argument_list|,
name|tableDest
argument_list|,
name|sessionConf
argument_list|,
name|isAutopurge
argument_list|,
operator|new
name|JavaUtils
operator|.
name|IdPathFilter
argument_list|(
name|txnId
argument_list|,
name|stmtId
argument_list|,
literal|false
argument_list|,
literal|true
argument_list|)
argument_list|,
literal|true
argument_list|,
name|tbl
operator|.
name|isStoredAsSubDirectories
argument_list|()
condition|?
name|tbl
operator|.
name|getSkewedColNames
argument_list|()
operator|.
name|size
argument_list|()
else|:
literal|0
argument_list|)
expr_stmt|;
block|}
name|newFiles
operator|=
name|listFilesCreatedByQuery
argument_list|(
name|loadPath
argument_list|,
name|txnId
argument_list|,
name|stmtId
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// Either a non-MM query, or a load into MM table from an external source.
name|Path
name|tblPath
init|=
name|tbl
operator|.
name|getPath
argument_list|()
decl_stmt|,
name|destPath
init|=
name|tblPath
decl_stmt|;
name|PathFilter
name|filter
init|=
name|FileUtils
operator|.
name|HIDDEN_FILES_PATH_FILTER
decl_stmt|;
if|if
condition|(
name|isMmTable
condition|)
block|{
comment|// We will load into MM directory, and delete from the parent if needed.
name|destPath
operator|=
operator|new
name|Path
argument_list|(
name|destPath
argument_list|,
name|AcidUtils
operator|.
name|deltaSubdir
argument_list|(
name|txnId
argument_list|,
name|txnId
argument_list|,
name|stmtId
argument_list|)
argument_list|)
expr_stmt|;
name|filter
operator|=
name|replace
condition|?
operator|new
name|JavaUtils
operator|.
name|IdPathFilter
argument_list|(
name|txnId
argument_list|,
name|stmtId
argument_list|,
literal|false
argument_list|,
literal|true
argument_list|)
else|:
name|filter
expr_stmt|;
block|}
name|Utilities
operator|.
name|LOG14535
operator|.
name|info
argument_list|(
literal|"moving "
operator|+
name|loadPath
operator|+
literal|" to "
operator|+
name|tblPath
operator|+
literal|" (replace = "
operator|+
name|replace
operator|+
literal|")"
argument_list|)
expr_stmt|;
if|if
condition|(
name|replace
condition|)
block|{
name|boolean
name|isAutopurge
init|=
literal|"true"
operator|.
name|equalsIgnoreCase
argument_list|(
name|tbl
operator|.
name|getProperty
argument_list|(
literal|"auto.purge"
argument_list|)
argument_list|)
decl_stmt|;
name|replaceFiles
argument_list|(
name|tblPath
argument_list|,
name|loadPath
argument_list|,
name|destPath
argument_list|,
name|tblPath
argument_list|,
name|sessionConf
argument_list|,
name|isSrcLocal
argument_list|,
name|isAutopurge
argument_list|,
name|newFiles
argument_list|,
name|filter
argument_list|,
name|isMmTable
argument_list|)
expr_stmt|;
block|}
else|else
block|{
try|try
block|{
name|FileSystem
name|fs
init|=
name|tbl
operator|.
name|getDataLocation
argument_list|()
operator|.
name|getFileSystem
argument_list|(
name|sessionConf
argument_list|)
decl_stmt|;
name|copyFiles
argument_list|(
name|sessionConf
argument_list|,
name|loadPath
argument_list|,
name|destPath
argument_list|,
name|fs
argument_list|,
name|isSrcLocal
argument_list|,
name|isAcid
argument_list|,
name|newFiles
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"addFiles: filesystem error in check phase"
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
block|}
if|if
condition|(
operator|!
name|this
operator|.
name|getConf
argument_list|()
operator|.
name|getBoolVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVESTATSAUTOGATHER
argument_list|)
condition|)
block|{
name|StatsSetupConst
operator|.
name|setBasicStatsState
argument_list|(
name|tbl
operator|.
name|getParameters
argument_list|()
argument_list|,
name|StatsSetupConst
operator|.
name|FALSE
argument_list|)
expr_stmt|;
block|}
comment|//column stats will be inaccurate
name|StatsSetupConst
operator|.
name|clearColumnStatsState
argument_list|(
name|tbl
operator|.
name|getParameters
argument_list|()
argument_list|)
expr_stmt|;
try|try
block|{
if|if
condition|(
name|isSkewedStoreAsSubdir
condition|)
block|{
name|SkewedInfo
name|skewedInfo
init|=
name|tbl
operator|.
name|getSkewedInfo
argument_list|()
decl_stmt|;
comment|// Construct list bucketing location mappings from sub-directory name.
name|Map
argument_list|<
name|List
argument_list|<
name|String
argument_list|>
argument_list|,
name|String
argument_list|>
name|skewedColValueLocationMaps
init|=
name|constructListBucketingLocationMap
argument_list|(
name|tbl
operator|.
name|getPath
argument_list|()
argument_list|,
name|skewedInfo
argument_list|)
decl_stmt|;
comment|// Add list bucketing location mappings.
name|skewedInfo
operator|.
name|setSkewedColValueLocationMaps
argument_list|(
name|skewedColValueLocationMaps
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
name|EnvironmentContext
name|environmentContext
init|=
literal|null
decl_stmt|;
if|if
condition|(
name|hasFollowingStatsTask
condition|)
block|{
name|environmentContext
operator|=
operator|new
name|EnvironmentContext
argument_list|()
expr_stmt|;
name|environmentContext
operator|.
name|putToProperties
argument_list|(
name|StatsSetupConst
operator|.
name|DO_NOT_UPDATE_STATS
argument_list|,
name|StatsSetupConst
operator|.
name|TRUE
argument_list|)
expr_stmt|;
block|}
try|try
block|{
name|alterTable
argument_list|(
name|tableName
argument_list|,
name|tbl
argument_list|,
name|environmentContext
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|InvalidOperationException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
name|fireInsertEvent
argument_list|(
name|tbl
argument_list|,
literal|null
argument_list|,
name|replace
argument_list|,
name|newFiles
argument_list|)
expr_stmt|;
block|}
comment|/**    * Creates a partition.    *    * @param tbl    *          table for which partition needs to be created    * @param partSpec    *          partition keys and their values    * @return created partition object    * @throws HiveException    *           if table doesn't exist or partition already exists    */
specifier|public
name|Partition
name|createPartition
parameter_list|(
name|Table
name|tbl
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|partSpec
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
operator|new
name|Partition
argument_list|(
name|tbl
argument_list|,
name|getMSC
argument_list|()
operator|.
name|add_partition
argument_list|(
name|Partition
operator|.
name|createMetaPartitionObject
argument_list|(
name|tbl
argument_list|,
name|partSpec
argument_list|,
literal|null
argument_list|)
argument_list|)
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|List
argument_list|<
name|Partition
argument_list|>
name|createPartitions
parameter_list|(
name|AddPartitionDesc
name|addPartitionDesc
parameter_list|)
throws|throws
name|HiveException
block|{
name|Table
name|tbl
init|=
name|getTable
argument_list|(
name|addPartitionDesc
operator|.
name|getDbName
argument_list|()
argument_list|,
name|addPartitionDesc
operator|.
name|getTableName
argument_list|()
argument_list|)
decl_stmt|;
name|int
name|size
init|=
name|addPartitionDesc
operator|.
name|getPartitionCount
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
argument_list|>
name|in
init|=
operator|new
name|ArrayList
argument_list|<
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
argument_list|>
argument_list|(
name|size
argument_list|)
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|size
condition|;
operator|++
name|i
control|)
block|{
name|in
operator|.
name|add
argument_list|(
name|convertAddSpecToMetaPartition
argument_list|(
name|tbl
argument_list|,
name|addPartitionDesc
operator|.
name|getPartition
argument_list|(
name|i
argument_list|)
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|List
argument_list|<
name|Partition
argument_list|>
name|out
init|=
operator|new
name|ArrayList
argument_list|<
name|Partition
argument_list|>
argument_list|()
decl_stmt|;
try|try
block|{
if|if
condition|(
operator|!
name|addPartitionDesc
operator|.
name|getReplicationSpec
argument_list|()
operator|.
name|isInReplicationScope
argument_list|()
condition|)
block|{
comment|// TODO: normally, the result is not necessary; might make sense to pass false
for|for
control|(
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
name|outPart
range|:
name|getMSC
argument_list|()
operator|.
name|add_partitions
argument_list|(
name|in
argument_list|,
name|addPartitionDesc
operator|.
name|isIfNotExists
argument_list|()
argument_list|,
literal|true
argument_list|)
control|)
block|{
name|out
operator|.
name|add
argument_list|(
operator|new
name|Partition
argument_list|(
name|tbl
argument_list|,
name|outPart
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
comment|// For replication add-ptns, we need to follow a insert-if-not-exist, alter-if-exists scenario.
comment|// TODO : ideally, we should push this mechanism to the metastore, because, otherwise, we have
comment|// no choice but to iterate over the partitions here.
name|List
argument_list|<
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
argument_list|>
name|partsToAdd
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
argument_list|>
name|partsToAlter
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|part_names
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|()
decl_stmt|;
for|for
control|(
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
name|p
range|:
name|in
control|)
block|{
name|part_names
operator|.
name|add
argument_list|(
name|Warehouse
operator|.
name|makePartName
argument_list|(
name|tbl
operator|.
name|getPartitionKeys
argument_list|()
argument_list|,
name|p
operator|.
name|getValues
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
try|try
block|{
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
name|ptn
init|=
name|getMSC
argument_list|()
operator|.
name|getPartition
argument_list|(
name|addPartitionDesc
operator|.
name|getDbName
argument_list|()
argument_list|,
name|addPartitionDesc
operator|.
name|getTableName
argument_list|()
argument_list|,
name|p
operator|.
name|getValues
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|addPartitionDesc
operator|.
name|getReplicationSpec
argument_list|()
operator|.
name|allowReplacementInto
argument_list|(
name|ptn
argument_list|)
condition|)
block|{
name|partsToAlter
operator|.
name|add
argument_list|(
name|p
argument_list|)
expr_stmt|;
block|}
comment|// else ptn already exists, but we do nothing with it.
block|}
catch|catch
parameter_list|(
name|NoSuchObjectException
name|nsoe
parameter_list|)
block|{
comment|// if the object does not exist, we want to add it.
name|partsToAdd
operator|.
name|add
argument_list|(
name|p
argument_list|)
expr_stmt|;
block|}
block|}
for|for
control|(
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
name|outPart
range|:
name|getMSC
argument_list|()
operator|.
name|add_partitions
argument_list|(
name|partsToAdd
argument_list|,
name|addPartitionDesc
operator|.
name|isIfNotExists
argument_list|()
argument_list|,
literal|true
argument_list|)
control|)
block|{
name|out
operator|.
name|add
argument_list|(
operator|new
name|Partition
argument_list|(
name|tbl
argument_list|,
name|outPart
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|getMSC
argument_list|()
operator|.
name|alter_partitions
argument_list|(
name|addPartitionDesc
operator|.
name|getDbName
argument_list|()
argument_list|,
name|addPartitionDesc
operator|.
name|getTableName
argument_list|()
argument_list|,
name|partsToAlter
argument_list|,
literal|null
argument_list|)
expr_stmt|;
for|for
control|(
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
name|outPart
range|:
name|getMSC
argument_list|()
operator|.
name|getPartitionsByNames
argument_list|(
name|addPartitionDesc
operator|.
name|getDbName
argument_list|()
argument_list|,
name|addPartitionDesc
operator|.
name|getTableName
argument_list|()
argument_list|,
name|part_names
argument_list|)
control|)
block|{
name|out
operator|.
name|add
argument_list|(
operator|new
name|Partition
argument_list|(
name|tbl
argument_list|,
name|outPart
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
return|return
name|out
return|;
block|}
specifier|private
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
name|convertAddSpecToMetaPartition
parameter_list|(
name|Table
name|tbl
parameter_list|,
name|AddPartitionDesc
operator|.
name|OnePartitionDesc
name|addSpec
parameter_list|)
throws|throws
name|HiveException
block|{
name|Path
name|location
init|=
name|addSpec
operator|.
name|getLocation
argument_list|()
operator|!=
literal|null
condition|?
operator|new
name|Path
argument_list|(
name|tbl
operator|.
name|getPath
argument_list|()
argument_list|,
name|addSpec
operator|.
name|getLocation
argument_list|()
argument_list|)
else|:
literal|null
decl_stmt|;
if|if
condition|(
name|location
operator|!=
literal|null
operator|&&
operator|!
name|Utilities
operator|.
name|isDefaultNameNode
argument_list|(
name|conf
argument_list|)
condition|)
block|{
comment|// Ensure that it is a full qualified path (in most cases it will be since tbl.getPath() is full qualified)
name|location
operator|=
operator|new
name|Path
argument_list|(
name|Utilities
operator|.
name|getQualifiedPath
argument_list|(
name|conf
argument_list|,
name|location
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
name|part
init|=
name|Partition
operator|.
name|createMetaPartitionObject
argument_list|(
name|tbl
argument_list|,
name|addSpec
operator|.
name|getPartSpec
argument_list|()
argument_list|,
name|location
argument_list|)
decl_stmt|;
if|if
condition|(
name|addSpec
operator|.
name|getPartParams
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|part
operator|.
name|setParameters
argument_list|(
name|addSpec
operator|.
name|getPartParams
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|addSpec
operator|.
name|getInputFormat
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|part
operator|.
name|getSd
argument_list|()
operator|.
name|setInputFormat
argument_list|(
name|addSpec
operator|.
name|getInputFormat
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|addSpec
operator|.
name|getOutputFormat
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|part
operator|.
name|getSd
argument_list|()
operator|.
name|setOutputFormat
argument_list|(
name|addSpec
operator|.
name|getOutputFormat
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|addSpec
operator|.
name|getNumBuckets
argument_list|()
operator|!=
operator|-
literal|1
condition|)
block|{
name|part
operator|.
name|getSd
argument_list|()
operator|.
name|setNumBuckets
argument_list|(
name|addSpec
operator|.
name|getNumBuckets
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|addSpec
operator|.
name|getCols
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|part
operator|.
name|getSd
argument_list|()
operator|.
name|setCols
argument_list|(
name|addSpec
operator|.
name|getCols
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|addSpec
operator|.
name|getSerializationLib
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|part
operator|.
name|getSd
argument_list|()
operator|.
name|getSerdeInfo
argument_list|()
operator|.
name|setSerializationLib
argument_list|(
name|addSpec
operator|.
name|getSerializationLib
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|addSpec
operator|.
name|getSerdeParams
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|part
operator|.
name|getSd
argument_list|()
operator|.
name|getSerdeInfo
argument_list|()
operator|.
name|setParameters
argument_list|(
name|addSpec
operator|.
name|getSerdeParams
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|addSpec
operator|.
name|getBucketCols
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|part
operator|.
name|getSd
argument_list|()
operator|.
name|setBucketCols
argument_list|(
name|addSpec
operator|.
name|getBucketCols
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|addSpec
operator|.
name|getSortCols
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|part
operator|.
name|getSd
argument_list|()
operator|.
name|setSortCols
argument_list|(
name|addSpec
operator|.
name|getSortCols
argument_list|()
argument_list|)
expr_stmt|;
block|}
return|return
name|part
return|;
block|}
specifier|public
name|Partition
name|getPartition
parameter_list|(
name|Table
name|tbl
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|partSpec
parameter_list|,
name|boolean
name|forceCreate
parameter_list|)
throws|throws
name|HiveException
block|{
return|return
name|getPartition
argument_list|(
name|tbl
argument_list|,
name|partSpec
argument_list|,
name|forceCreate
argument_list|,
literal|null
argument_list|,
literal|true
argument_list|,
literal|null
argument_list|)
return|;
block|}
comment|/**    * Returns partition metadata    *    * @param tbl    *          the partition's table    * @param partSpec    *          partition keys and values    * @param forceCreate    *          if this is true and partition doesn't exist then a partition is    *          created    * @param partPath the path where the partition data is located    * @param inheritTableSpecs whether to copy over the table specs for if/of/serde    * @return result partition object or null if there is no partition    * @throws HiveException    */
specifier|public
name|Partition
name|getPartition
parameter_list|(
name|Table
name|tbl
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|partSpec
parameter_list|,
name|boolean
name|forceCreate
parameter_list|,
name|String
name|partPath
parameter_list|,
name|boolean
name|inheritTableSpecs
parameter_list|)
throws|throws
name|HiveException
block|{
return|return
name|getPartition
argument_list|(
name|tbl
argument_list|,
name|partSpec
argument_list|,
name|forceCreate
argument_list|,
name|partPath
argument_list|,
name|inheritTableSpecs
argument_list|,
literal|null
argument_list|)
return|;
block|}
comment|/**    * Returns partition metadata    *    * @param tbl    *          the partition's table    * @param partSpec    *          partition keys and values    * @param forceCreate    *          if this is true and partition doesn't exist then a partition is    *          created    * @param partPath the path where the partition data is located    * @param inheritTableSpecs whether to copy over the table specs for if/of/serde    * @param newFiles An optional list of new files that were moved into this partition.  If    *                 non-null these will be included in the DML event sent to the metastore.    * @return result partition object or null if there is no partition    * @throws HiveException    */
specifier|public
name|Partition
name|getPartition
parameter_list|(
name|Table
name|tbl
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|partSpec
parameter_list|,
name|boolean
name|forceCreate
parameter_list|,
name|String
name|partPath
parameter_list|,
name|boolean
name|inheritTableSpecs
parameter_list|,
name|List
argument_list|<
name|Path
argument_list|>
name|newFiles
parameter_list|)
throws|throws
name|HiveException
block|{
name|tbl
operator|.
name|validatePartColumnNames
argument_list|(
name|partSpec
argument_list|,
literal|true
argument_list|)
expr_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|pvals
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|FieldSchema
name|field
range|:
name|tbl
operator|.
name|getPartCols
argument_list|()
control|)
block|{
name|String
name|val
init|=
name|partSpec
operator|.
name|get
argument_list|(
name|field
operator|.
name|getName
argument_list|()
argument_list|)
decl_stmt|;
comment|// enable dynamic partitioning
if|if
condition|(
operator|(
name|val
operator|==
literal|null
operator|&&
operator|!
name|HiveConf
operator|.
name|getBoolVar
argument_list|(
name|conf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|DYNAMICPARTITIONING
argument_list|)
operator|)
operator|||
operator|(
name|val
operator|!=
literal|null
operator|&&
name|val
operator|.
name|length
argument_list|()
operator|==
literal|0
operator|)
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"get partition: Value for key "
operator|+
name|field
operator|.
name|getName
argument_list|()
operator|+
literal|" is null or empty"
argument_list|)
throw|;
block|}
elseif|else
if|if
condition|(
name|val
operator|!=
literal|null
condition|)
block|{
name|pvals
operator|.
name|add
argument_list|(
name|val
argument_list|)
expr_stmt|;
block|}
block|}
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
name|tpart
init|=
literal|null
decl_stmt|;
try|try
block|{
name|tpart
operator|=
name|getMSC
argument_list|()
operator|.
name|getPartitionWithAuthInfo
argument_list|(
name|tbl
operator|.
name|getDbName
argument_list|()
argument_list|,
name|tbl
operator|.
name|getTableName
argument_list|()
argument_list|,
name|pvals
argument_list|,
name|getUserName
argument_list|()
argument_list|,
name|getGroupNames
argument_list|()
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|NoSuchObjectException
name|nsoe
parameter_list|)
block|{
comment|// this means no partition exists for the given partition
comment|// key value pairs - thrift cannot handle null return values, hence
comment|// getPartition() throws NoSuchObjectException to indicate null partition
name|tpart
operator|=
literal|null
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
try|try
block|{
if|if
condition|(
name|forceCreate
condition|)
block|{
if|if
condition|(
name|tpart
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"creating partition for table "
operator|+
name|tbl
operator|.
name|getTableName
argument_list|()
operator|+
literal|" with partition spec : "
operator|+
name|partSpec
argument_list|)
expr_stmt|;
try|try
block|{
name|tpart
operator|=
name|getMSC
argument_list|()
operator|.
name|appendPartition
argument_list|(
name|tbl
operator|.
name|getDbName
argument_list|()
argument_list|,
name|tbl
operator|.
name|getTableName
argument_list|()
argument_list|,
name|pvals
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|AlreadyExistsException
name|aee
parameter_list|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Caught already exists exception, trying to alter partition instead"
argument_list|)
expr_stmt|;
name|tpart
operator|=
name|getMSC
argument_list|()
operator|.
name|getPartitionWithAuthInfo
argument_list|(
name|tbl
operator|.
name|getDbName
argument_list|()
argument_list|,
name|tbl
operator|.
name|getTableName
argument_list|()
argument_list|,
name|pvals
argument_list|,
name|getUserName
argument_list|()
argument_list|,
name|getGroupNames
argument_list|()
argument_list|)
expr_stmt|;
name|alterPartitionSpec
argument_list|(
name|tbl
argument_list|,
name|partSpec
argument_list|,
name|tpart
argument_list|,
name|inheritTableSpecs
argument_list|,
name|partPath
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
if|if
condition|(
name|CheckJDOException
operator|.
name|isJDODataStoreException
argument_list|(
name|e
argument_list|)
condition|)
block|{
comment|// Using utility method above, so that JDODataStoreException doesn't
comment|// have to be used here. This helps avoid adding jdo dependency for
comment|// hcatalog client uses
name|LOG
operator|.
name|debug
argument_list|(
literal|"Caught JDO exception, trying to alter partition instead"
argument_list|)
expr_stmt|;
name|tpart
operator|=
name|getMSC
argument_list|()
operator|.
name|getPartitionWithAuthInfo
argument_list|(
name|tbl
operator|.
name|getDbName
argument_list|()
argument_list|,
name|tbl
operator|.
name|getTableName
argument_list|()
argument_list|,
name|pvals
argument_list|,
name|getUserName
argument_list|()
argument_list|,
name|getGroupNames
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|tpart
operator|==
literal|null
condition|)
block|{
comment|// This means the exception was caused by something other than a race condition
comment|// in creating the partition, since the partition still doesn't exist.
throw|throw
name|e
throw|;
block|}
name|alterPartitionSpec
argument_list|(
name|tbl
argument_list|,
name|partSpec
argument_list|,
name|tpart
argument_list|,
name|inheritTableSpecs
argument_list|,
name|partPath
argument_list|)
expr_stmt|;
block|}
else|else
block|{
throw|throw
name|e
throw|;
block|}
block|}
block|}
else|else
block|{
name|alterPartitionSpec
argument_list|(
name|tbl
argument_list|,
name|partSpec
argument_list|,
name|tpart
argument_list|,
name|inheritTableSpecs
argument_list|,
name|partPath
argument_list|)
expr_stmt|;
name|fireInsertEvent
argument_list|(
name|tbl
argument_list|,
name|partSpec
argument_list|,
literal|true
argument_list|,
name|newFiles
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
name|tpart
operator|==
literal|null
condition|)
block|{
return|return
literal|null
return|;
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
return|return
operator|new
name|Partition
argument_list|(
name|tbl
argument_list|,
name|tpart
argument_list|)
return|;
block|}
specifier|private
name|void
name|alterPartitionSpec
parameter_list|(
name|Table
name|tbl
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|partSpec
parameter_list|,
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
name|tpart
parameter_list|,
name|boolean
name|inheritTableSpecs
parameter_list|,
name|String
name|partPath
parameter_list|)
throws|throws
name|HiveException
throws|,
name|InvalidOperationException
block|{
name|alterPartitionSpecInMemory
argument_list|(
name|tbl
argument_list|,
name|partSpec
argument_list|,
name|tpart
argument_list|,
name|inheritTableSpecs
argument_list|,
name|partPath
argument_list|)
expr_stmt|;
name|String
name|fullName
init|=
name|tbl
operator|.
name|getTableName
argument_list|()
decl_stmt|;
if|if
condition|(
operator|!
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|lang
operator|.
name|StringUtils
operator|.
name|isEmpty
argument_list|(
name|tbl
operator|.
name|getDbName
argument_list|()
argument_list|)
condition|)
block|{
name|fullName
operator|=
name|tbl
operator|.
name|getDbName
argument_list|()
operator|+
literal|"."
operator|+
name|tbl
operator|.
name|getTableName
argument_list|()
expr_stmt|;
block|}
name|alterPartition
argument_list|(
name|fullName
argument_list|,
operator|new
name|Partition
argument_list|(
name|tbl
argument_list|,
name|tpart
argument_list|)
argument_list|,
literal|null
argument_list|)
expr_stmt|;
block|}
specifier|private
name|void
name|alterPartitionSpecInMemory
parameter_list|(
name|Table
name|tbl
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|partSpec
parameter_list|,
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
name|tpart
parameter_list|,
name|boolean
name|inheritTableSpecs
parameter_list|,
name|String
name|partPath
parameter_list|)
throws|throws
name|HiveException
throws|,
name|InvalidOperationException
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"altering partition for table "
operator|+
name|tbl
operator|.
name|getTableName
argument_list|()
operator|+
literal|" with partition spec : "
operator|+
name|partSpec
argument_list|)
expr_stmt|;
if|if
condition|(
name|inheritTableSpecs
condition|)
block|{
name|tpart
operator|.
name|getSd
argument_list|()
operator|.
name|setOutputFormat
argument_list|(
name|tbl
operator|.
name|getTTable
argument_list|()
operator|.
name|getSd
argument_list|()
operator|.
name|getOutputFormat
argument_list|()
argument_list|)
expr_stmt|;
name|tpart
operator|.
name|getSd
argument_list|()
operator|.
name|setInputFormat
argument_list|(
name|tbl
operator|.
name|getTTable
argument_list|()
operator|.
name|getSd
argument_list|()
operator|.
name|getInputFormat
argument_list|()
argument_list|)
expr_stmt|;
name|tpart
operator|.
name|getSd
argument_list|()
operator|.
name|getSerdeInfo
argument_list|()
operator|.
name|setSerializationLib
argument_list|(
name|tbl
operator|.
name|getSerializationLib
argument_list|()
argument_list|)
expr_stmt|;
name|tpart
operator|.
name|getSd
argument_list|()
operator|.
name|getSerdeInfo
argument_list|()
operator|.
name|setParameters
argument_list|(
name|tbl
operator|.
name|getTTable
argument_list|()
operator|.
name|getSd
argument_list|()
operator|.
name|getSerdeInfo
argument_list|()
operator|.
name|getParameters
argument_list|()
argument_list|)
expr_stmt|;
name|tpart
operator|.
name|getSd
argument_list|()
operator|.
name|setBucketCols
argument_list|(
name|tbl
operator|.
name|getBucketCols
argument_list|()
argument_list|)
expr_stmt|;
name|tpart
operator|.
name|getSd
argument_list|()
operator|.
name|setNumBuckets
argument_list|(
name|tbl
operator|.
name|getNumBuckets
argument_list|()
argument_list|)
expr_stmt|;
name|tpart
operator|.
name|getSd
argument_list|()
operator|.
name|setSortCols
argument_list|(
name|tbl
operator|.
name|getSortCols
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|partPath
operator|==
literal|null
operator|||
name|partPath
operator|.
name|trim
argument_list|()
operator|.
name|equals
argument_list|(
literal|""
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"new partition path should not be null or empty."
argument_list|)
throw|;
block|}
name|tpart
operator|.
name|getSd
argument_list|()
operator|.
name|setLocation
argument_list|(
name|partPath
argument_list|)
expr_stmt|;
block|}
specifier|private
name|void
name|fireInsertEvent
parameter_list|(
name|Table
name|tbl
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|partitionSpec
parameter_list|,
name|boolean
name|replace
parameter_list|,
name|List
argument_list|<
name|Path
argument_list|>
name|newFiles
parameter_list|)
throws|throws
name|HiveException
block|{
if|if
condition|(
name|conf
operator|.
name|getBoolVar
argument_list|(
name|ConfVars
operator|.
name|FIRE_EVENTS_FOR_DML
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Firing dml insert event"
argument_list|)
expr_stmt|;
if|if
condition|(
name|tbl
operator|.
name|isTemporary
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Not firing dml insert event as "
operator|+
name|tbl
operator|.
name|getTableName
argument_list|()
operator|+
literal|" is temporary"
argument_list|)
expr_stmt|;
return|return;
block|}
try|try
block|{
name|FileSystem
name|fileSystem
init|=
name|tbl
operator|.
name|getDataLocation
argument_list|()
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|FireEventRequestData
name|data
init|=
operator|new
name|FireEventRequestData
argument_list|()
decl_stmt|;
name|InsertEventRequestData
name|insertData
init|=
operator|new
name|InsertEventRequestData
argument_list|()
decl_stmt|;
name|insertData
operator|.
name|setReplace
argument_list|(
name|replace
argument_list|)
expr_stmt|;
name|data
operator|.
name|setInsertData
argument_list|(
name|insertData
argument_list|)
expr_stmt|;
if|if
condition|(
name|newFiles
operator|!=
literal|null
operator|&&
name|newFiles
operator|.
name|size
argument_list|()
operator|>
literal|0
condition|)
block|{
for|for
control|(
name|Path
name|p
range|:
name|newFiles
control|)
block|{
name|insertData
operator|.
name|addToFilesAdded
argument_list|(
name|p
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
name|FileChecksum
name|cksum
init|=
name|fileSystem
operator|.
name|getFileChecksum
argument_list|(
name|p
argument_list|)
decl_stmt|;
comment|// File checksum is not implemented for local filesystem (RawLocalFileSystem)
if|if
condition|(
name|cksum
operator|!=
literal|null
condition|)
block|{
name|String
name|checksumString
init|=
name|StringUtils
operator|.
name|byteToHexString
argument_list|(
name|cksum
operator|.
name|getBytes
argument_list|()
argument_list|,
literal|0
argument_list|,
name|cksum
operator|.
name|getLength
argument_list|()
argument_list|)
decl_stmt|;
name|insertData
operator|.
name|addToFilesAddedChecksum
argument_list|(
name|checksumString
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// Add an empty checksum string for filesystems that don't generate one
name|insertData
operator|.
name|addToFilesAddedChecksum
argument_list|(
literal|""
argument_list|)
expr_stmt|;
block|}
block|}
block|}
else|else
block|{
name|insertData
operator|.
name|setFilesAdded
argument_list|(
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|FireEventRequest
name|rqst
init|=
operator|new
name|FireEventRequest
argument_list|(
literal|true
argument_list|,
name|data
argument_list|)
decl_stmt|;
name|rqst
operator|.
name|setDbName
argument_list|(
name|tbl
operator|.
name|getDbName
argument_list|()
argument_list|)
expr_stmt|;
name|rqst
operator|.
name|setTableName
argument_list|(
name|tbl
operator|.
name|getTableName
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|partitionSpec
operator|!=
literal|null
operator|&&
name|partitionSpec
operator|.
name|size
argument_list|()
operator|>
literal|0
condition|)
block|{
name|List
argument_list|<
name|String
argument_list|>
name|partVals
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|(
name|partitionSpec
operator|.
name|size
argument_list|()
argument_list|)
decl_stmt|;
for|for
control|(
name|FieldSchema
name|fs
range|:
name|tbl
operator|.
name|getPartitionKeys
argument_list|()
control|)
block|{
name|partVals
operator|.
name|add
argument_list|(
name|partitionSpec
operator|.
name|get
argument_list|(
name|fs
operator|.
name|getName
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|rqst
operator|.
name|setPartitionVals
argument_list|(
name|partVals
argument_list|)
expr_stmt|;
block|}
name|getMSC
argument_list|()
operator|.
name|fireListenerEvent
argument_list|(
name|rqst
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
decl||
name|TException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
block|}
specifier|public
name|boolean
name|dropPartition
parameter_list|(
name|String
name|tblName
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|part_vals
parameter_list|,
name|boolean
name|deleteData
parameter_list|)
throws|throws
name|HiveException
block|{
name|String
index|[]
name|names
init|=
name|Utilities
operator|.
name|getDbTableName
argument_list|(
name|tblName
argument_list|)
decl_stmt|;
return|return
name|dropPartition
argument_list|(
name|names
index|[
literal|0
index|]
argument_list|,
name|names
index|[
literal|1
index|]
argument_list|,
name|part_vals
argument_list|,
name|deleteData
argument_list|)
return|;
block|}
specifier|public
name|boolean
name|dropPartition
parameter_list|(
name|String
name|db_name
parameter_list|,
name|String
name|tbl_name
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|part_vals
parameter_list|,
name|boolean
name|deleteData
parameter_list|)
throws|throws
name|HiveException
block|{
return|return
name|dropPartition
argument_list|(
name|db_name
argument_list|,
name|tbl_name
argument_list|,
name|part_vals
argument_list|,
name|PartitionDropOptions
operator|.
name|instance
argument_list|()
operator|.
name|deleteData
argument_list|(
name|deleteData
argument_list|)
argument_list|)
return|;
block|}
specifier|public
name|boolean
name|dropPartition
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|tableName
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|partVals
parameter_list|,
name|PartitionDropOptions
name|options
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|dropPartition
argument_list|(
name|dbName
argument_list|,
name|tableName
argument_list|,
name|partVals
argument_list|,
name|options
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|NoSuchObjectException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Partition or table doesn't exist."
argument_list|,
name|e
argument_list|)
throw|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|List
argument_list|<
name|Partition
argument_list|>
name|dropPartitions
parameter_list|(
name|String
name|tblName
parameter_list|,
name|List
argument_list|<
name|DropTableDesc
operator|.
name|PartSpec
argument_list|>
name|partSpecs
parameter_list|,
name|boolean
name|deleteData
parameter_list|,
name|boolean
name|ifExists
parameter_list|)
throws|throws
name|HiveException
block|{
name|String
index|[]
name|names
init|=
name|Utilities
operator|.
name|getDbTableName
argument_list|(
name|tblName
argument_list|)
decl_stmt|;
return|return
name|dropPartitions
argument_list|(
name|names
index|[
literal|0
index|]
argument_list|,
name|names
index|[
literal|1
index|]
argument_list|,
name|partSpecs
argument_list|,
name|deleteData
argument_list|,
name|ifExists
argument_list|)
return|;
block|}
specifier|public
name|List
argument_list|<
name|Partition
argument_list|>
name|dropPartitions
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|tblName
parameter_list|,
name|List
argument_list|<
name|DropTableDesc
operator|.
name|PartSpec
argument_list|>
name|partSpecs
parameter_list|,
name|boolean
name|deleteData
parameter_list|,
name|boolean
name|ifExists
parameter_list|)
throws|throws
name|HiveException
block|{
return|return
name|dropPartitions
argument_list|(
name|dbName
argument_list|,
name|tblName
argument_list|,
name|partSpecs
argument_list|,
name|PartitionDropOptions
operator|.
name|instance
argument_list|()
operator|.
name|deleteData
argument_list|(
name|deleteData
argument_list|)
operator|.
name|ifExists
argument_list|(
name|ifExists
argument_list|)
argument_list|)
return|;
block|}
specifier|public
name|List
argument_list|<
name|Partition
argument_list|>
name|dropPartitions
parameter_list|(
name|String
name|tblName
parameter_list|,
name|List
argument_list|<
name|DropTableDesc
operator|.
name|PartSpec
argument_list|>
name|partSpecs
parameter_list|,
name|PartitionDropOptions
name|dropOptions
parameter_list|)
throws|throws
name|HiveException
block|{
name|String
index|[]
name|names
init|=
name|Utilities
operator|.
name|getDbTableName
argument_list|(
name|tblName
argument_list|)
decl_stmt|;
return|return
name|dropPartitions
argument_list|(
name|names
index|[
literal|0
index|]
argument_list|,
name|names
index|[
literal|1
index|]
argument_list|,
name|partSpecs
argument_list|,
name|dropOptions
argument_list|)
return|;
block|}
specifier|public
name|List
argument_list|<
name|Partition
argument_list|>
name|dropPartitions
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|tblName
parameter_list|,
name|List
argument_list|<
name|DropTableDesc
operator|.
name|PartSpec
argument_list|>
name|partSpecs
parameter_list|,
name|PartitionDropOptions
name|dropOptions
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|Table
name|tbl
init|=
name|getTable
argument_list|(
name|dbName
argument_list|,
name|tblName
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|ObjectPair
argument_list|<
name|Integer
argument_list|,
name|byte
index|[]
argument_list|>
argument_list|>
name|partExprs
init|=
operator|new
name|ArrayList
argument_list|<
name|ObjectPair
argument_list|<
name|Integer
argument_list|,
name|byte
index|[]
argument_list|>
argument_list|>
argument_list|(
name|partSpecs
operator|.
name|size
argument_list|()
argument_list|)
decl_stmt|;
for|for
control|(
name|DropTableDesc
operator|.
name|PartSpec
name|partSpec
range|:
name|partSpecs
control|)
block|{
name|partExprs
operator|.
name|add
argument_list|(
operator|new
name|ObjectPair
argument_list|<
name|Integer
argument_list|,
name|byte
index|[]
argument_list|>
argument_list|(
name|partSpec
operator|.
name|getPrefixLength
argument_list|()
argument_list|,
name|SerializationUtilities
operator|.
name|serializeExpressionToKryo
argument_list|(
name|partSpec
operator|.
name|getPartSpec
argument_list|()
argument_list|)
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|List
argument_list|<
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
argument_list|>
name|tParts
init|=
name|getMSC
argument_list|()
operator|.
name|dropPartitions
argument_list|(
name|dbName
argument_list|,
name|tblName
argument_list|,
name|partExprs
argument_list|,
name|dropOptions
argument_list|)
decl_stmt|;
return|return
name|convertFromMetastore
argument_list|(
name|tbl
argument_list|,
name|tParts
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|NoSuchObjectException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Partition or table doesn't exist."
argument_list|,
name|e
argument_list|)
throw|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|List
argument_list|<
name|String
argument_list|>
name|getPartitionNames
parameter_list|(
name|String
name|tblName
parameter_list|,
name|short
name|max
parameter_list|)
throws|throws
name|HiveException
block|{
name|String
index|[]
name|names
init|=
name|Utilities
operator|.
name|getDbTableName
argument_list|(
name|tblName
argument_list|)
decl_stmt|;
return|return
name|getPartitionNames
argument_list|(
name|names
index|[
literal|0
index|]
argument_list|,
name|names
index|[
literal|1
index|]
argument_list|,
name|max
argument_list|)
return|;
block|}
specifier|public
name|List
argument_list|<
name|String
argument_list|>
name|getPartitionNames
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|tblName
parameter_list|,
name|short
name|max
parameter_list|)
throws|throws
name|HiveException
block|{
name|List
argument_list|<
name|String
argument_list|>
name|names
init|=
literal|null
decl_stmt|;
try|try
block|{
name|names
operator|=
name|getMSC
argument_list|()
operator|.
name|listPartitionNames
argument_list|(
name|dbName
argument_list|,
name|tblName
argument_list|,
name|max
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
return|return
name|names
return|;
block|}
specifier|public
name|List
argument_list|<
name|String
argument_list|>
name|getPartitionNames
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|tblName
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|partSpec
parameter_list|,
name|short
name|max
parameter_list|)
throws|throws
name|HiveException
block|{
name|List
argument_list|<
name|String
argument_list|>
name|names
init|=
literal|null
decl_stmt|;
name|Table
name|t
init|=
name|getTable
argument_list|(
name|dbName
argument_list|,
name|tblName
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|pvals
init|=
name|MetaStoreUtils
operator|.
name|getPvals
argument_list|(
name|t
operator|.
name|getPartCols
argument_list|()
argument_list|,
name|partSpec
argument_list|)
decl_stmt|;
try|try
block|{
name|names
operator|=
name|getMSC
argument_list|()
operator|.
name|listPartitionNames
argument_list|(
name|dbName
argument_list|,
name|tblName
argument_list|,
name|pvals
argument_list|,
name|max
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
return|return
name|names
return|;
block|}
comment|/**    * get all the partitions that the table has    *    * @param tbl    *          object for which partition is needed    * @return list of partition objects    */
specifier|public
name|List
argument_list|<
name|Partition
argument_list|>
name|getPartitions
parameter_list|(
name|Table
name|tbl
parameter_list|)
throws|throws
name|HiveException
block|{
if|if
condition|(
name|tbl
operator|.
name|isPartitioned
argument_list|()
condition|)
block|{
name|List
argument_list|<
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
argument_list|>
name|tParts
decl_stmt|;
try|try
block|{
name|tParts
operator|=
name|getMSC
argument_list|()
operator|.
name|listPartitionsWithAuthInfo
argument_list|(
name|tbl
operator|.
name|getDbName
argument_list|()
argument_list|,
name|tbl
operator|.
name|getTableName
argument_list|()
argument_list|,
operator|(
name|short
operator|)
operator|-
literal|1
argument_list|,
name|getUserName
argument_list|()
argument_list|,
name|getGroupNames
argument_list|()
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
name|List
argument_list|<
name|Partition
argument_list|>
name|parts
init|=
operator|new
name|ArrayList
argument_list|<
name|Partition
argument_list|>
argument_list|(
name|tParts
operator|.
name|size
argument_list|()
argument_list|)
decl_stmt|;
for|for
control|(
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
name|tpart
range|:
name|tParts
control|)
block|{
name|parts
operator|.
name|add
argument_list|(
operator|new
name|Partition
argument_list|(
name|tbl
argument_list|,
name|tpart
argument_list|)
argument_list|)
expr_stmt|;
block|}
return|return
name|parts
return|;
block|}
else|else
block|{
name|Partition
name|part
init|=
operator|new
name|Partition
argument_list|(
name|tbl
argument_list|)
decl_stmt|;
name|ArrayList
argument_list|<
name|Partition
argument_list|>
name|parts
init|=
operator|new
name|ArrayList
argument_list|<
name|Partition
argument_list|>
argument_list|(
literal|1
argument_list|)
decl_stmt|;
name|parts
operator|.
name|add
argument_list|(
name|part
argument_list|)
expr_stmt|;
return|return
name|parts
return|;
block|}
block|}
comment|/**    * Get all the partitions; unlike {@link #getPartitions(Table)}, does not include auth.    * @param tbl table for which partitions are needed    * @return list of partition objects    */
specifier|public
name|Set
argument_list|<
name|Partition
argument_list|>
name|getAllPartitionsOf
parameter_list|(
name|Table
name|tbl
parameter_list|)
throws|throws
name|HiveException
block|{
if|if
condition|(
operator|!
name|tbl
operator|.
name|isPartitioned
argument_list|()
condition|)
block|{
return|return
name|Sets
operator|.
name|newHashSet
argument_list|(
operator|new
name|Partition
argument_list|(
name|tbl
argument_list|)
argument_list|)
return|;
block|}
name|List
argument_list|<
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
argument_list|>
name|tParts
decl_stmt|;
try|try
block|{
name|tParts
operator|=
name|getMSC
argument_list|()
operator|.
name|listPartitions
argument_list|(
name|tbl
operator|.
name|getDbName
argument_list|()
argument_list|,
name|tbl
operator|.
name|getTableName
argument_list|()
argument_list|,
operator|(
name|short
operator|)
operator|-
literal|1
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
name|Set
argument_list|<
name|Partition
argument_list|>
name|parts
init|=
operator|new
name|LinkedHashSet
argument_list|<
name|Partition
argument_list|>
argument_list|(
name|tParts
operator|.
name|size
argument_list|()
argument_list|)
decl_stmt|;
for|for
control|(
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
name|tpart
range|:
name|tParts
control|)
block|{
name|parts
operator|.
name|add
argument_list|(
operator|new
name|Partition
argument_list|(
name|tbl
argument_list|,
name|tpart
argument_list|)
argument_list|)
expr_stmt|;
block|}
return|return
name|parts
return|;
block|}
comment|/**    * get all the partitions of the table that matches the given partial    * specification. partition columns whose value is can be anything should be    * an empty string.    *    * @param tbl    *          object for which partition is needed. Must be partitioned.    * @param limit number of partitions to return    * @return list of partition objects    * @throws HiveException    */
specifier|public
name|List
argument_list|<
name|Partition
argument_list|>
name|getPartitions
parameter_list|(
name|Table
name|tbl
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|partialPartSpec
parameter_list|,
name|short
name|limit
parameter_list|)
throws|throws
name|HiveException
block|{
if|if
condition|(
operator|!
name|tbl
operator|.
name|isPartitioned
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|ErrorMsg
operator|.
name|TABLE_NOT_PARTITIONED
argument_list|,
name|tbl
operator|.
name|getTableName
argument_list|()
argument_list|)
throw|;
block|}
name|List
argument_list|<
name|String
argument_list|>
name|partialPvals
init|=
name|MetaStoreUtils
operator|.
name|getPvals
argument_list|(
name|tbl
operator|.
name|getPartCols
argument_list|()
argument_list|,
name|partialPartSpec
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
argument_list|>
name|partitions
init|=
literal|null
decl_stmt|;
try|try
block|{
name|partitions
operator|=
name|getMSC
argument_list|()
operator|.
name|listPartitionsWithAuthInfo
argument_list|(
name|tbl
operator|.
name|getDbName
argument_list|()
argument_list|,
name|tbl
operator|.
name|getTableName
argument_list|()
argument_list|,
name|partialPvals
argument_list|,
name|limit
argument_list|,
name|getUserName
argument_list|()
argument_list|,
name|getGroupNames
argument_list|()
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
name|List
argument_list|<
name|Partition
argument_list|>
name|qlPartitions
init|=
operator|new
name|ArrayList
argument_list|<
name|Partition
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
name|p
range|:
name|partitions
control|)
block|{
name|qlPartitions
operator|.
name|add
argument_list|(
operator|new
name|Partition
argument_list|(
name|tbl
argument_list|,
name|p
argument_list|)
argument_list|)
expr_stmt|;
block|}
return|return
name|qlPartitions
return|;
block|}
comment|/**    * get all the partitions of the table that matches the given partial    * specification. partition columns whose value is can be anything should be    * an empty string.    *    * @param tbl    *          object for which partition is needed. Must be partitioned.    * @return list of partition objects    * @throws HiveException    */
specifier|public
name|List
argument_list|<
name|Partition
argument_list|>
name|getPartitions
parameter_list|(
name|Table
name|tbl
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|partialPartSpec
parameter_list|)
throws|throws
name|HiveException
block|{
return|return
name|getPartitions
argument_list|(
name|tbl
argument_list|,
name|partialPartSpec
argument_list|,
operator|(
name|short
operator|)
operator|-
literal|1
argument_list|)
return|;
block|}
comment|/**    * get all the partitions of the table that matches the given partial    * specification. partition columns whose value is can be anything should be    * an empty string.    *    * @param tbl    *          object for which partition is needed. Must be partitioned.    * @param partialPartSpec    *          partial partition specification (some subpartitions can be empty).    * @return list of partition objects    * @throws HiveException    */
specifier|public
name|List
argument_list|<
name|Partition
argument_list|>
name|getPartitionsByNames
parameter_list|(
name|Table
name|tbl
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|partialPartSpec
parameter_list|)
throws|throws
name|HiveException
block|{
if|if
condition|(
operator|!
name|tbl
operator|.
name|isPartitioned
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|ErrorMsg
operator|.
name|TABLE_NOT_PARTITIONED
argument_list|,
name|tbl
operator|.
name|getTableName
argument_list|()
argument_list|)
throw|;
block|}
name|List
argument_list|<
name|String
argument_list|>
name|names
init|=
name|getPartitionNames
argument_list|(
name|tbl
operator|.
name|getDbName
argument_list|()
argument_list|,
name|tbl
operator|.
name|getTableName
argument_list|()
argument_list|,
name|partialPartSpec
argument_list|,
operator|(
name|short
operator|)
operator|-
literal|1
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|Partition
argument_list|>
name|partitions
init|=
name|getPartitionsByNames
argument_list|(
name|tbl
argument_list|,
name|names
argument_list|)
decl_stmt|;
return|return
name|partitions
return|;
block|}
comment|/**    * Get all partitions of the table that matches the list of given partition names.    *    * @param tbl    *          object for which partition is needed. Must be partitioned.    * @param partNames    *          list of partition names    * @return list of partition objects    * @throws HiveException    */
specifier|public
name|List
argument_list|<
name|Partition
argument_list|>
name|getPartitionsByNames
parameter_list|(
name|Table
name|tbl
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|partNames
parameter_list|)
throws|throws
name|HiveException
block|{
if|if
condition|(
operator|!
name|tbl
operator|.
name|isPartitioned
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|ErrorMsg
operator|.
name|TABLE_NOT_PARTITIONED
argument_list|,
name|tbl
operator|.
name|getTableName
argument_list|()
argument_list|)
throw|;
block|}
name|List
argument_list|<
name|Partition
argument_list|>
name|partitions
init|=
operator|new
name|ArrayList
argument_list|<
name|Partition
argument_list|>
argument_list|(
name|partNames
operator|.
name|size
argument_list|()
argument_list|)
decl_stmt|;
name|int
name|batchSize
init|=
name|HiveConf
operator|.
name|getIntVar
argument_list|(
name|conf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|METASTORE_BATCH_RETRIEVE_MAX
argument_list|)
decl_stmt|;
comment|// TODO: might want to increase the default batch size. 1024 is viable; MS gets OOM if too high.
name|int
name|nParts
init|=
name|partNames
operator|.
name|size
argument_list|()
decl_stmt|;
name|int
name|nBatches
init|=
name|nParts
operator|/
name|batchSize
decl_stmt|;
try|try
block|{
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|nBatches
condition|;
operator|++
name|i
control|)
block|{
name|List
argument_list|<
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
argument_list|>
name|tParts
init|=
name|getMSC
argument_list|()
operator|.
name|getPartitionsByNames
argument_list|(
name|tbl
operator|.
name|getDbName
argument_list|()
argument_list|,
name|tbl
operator|.
name|getTableName
argument_list|()
argument_list|,
name|partNames
operator|.
name|subList
argument_list|(
name|i
operator|*
name|batchSize
argument_list|,
operator|(
name|i
operator|+
literal|1
operator|)
operator|*
name|batchSize
argument_list|)
argument_list|)
decl_stmt|;
if|if
condition|(
name|tParts
operator|!=
literal|null
condition|)
block|{
for|for
control|(
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
name|tpart
range|:
name|tParts
control|)
block|{
name|partitions
operator|.
name|add
argument_list|(
operator|new
name|Partition
argument_list|(
name|tbl
argument_list|,
name|tpart
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
block|}
if|if
condition|(
name|nParts
operator|>
name|nBatches
operator|*
name|batchSize
condition|)
block|{
name|List
argument_list|<
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
argument_list|>
name|tParts
init|=
name|getMSC
argument_list|()
operator|.
name|getPartitionsByNames
argument_list|(
name|tbl
operator|.
name|getDbName
argument_list|()
argument_list|,
name|tbl
operator|.
name|getTableName
argument_list|()
argument_list|,
name|partNames
operator|.
name|subList
argument_list|(
name|nBatches
operator|*
name|batchSize
argument_list|,
name|nParts
argument_list|)
argument_list|)
decl_stmt|;
if|if
condition|(
name|tParts
operator|!=
literal|null
condition|)
block|{
for|for
control|(
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
name|tpart
range|:
name|tParts
control|)
block|{
name|partitions
operator|.
name|add
argument_list|(
operator|new
name|Partition
argument_list|(
name|tbl
argument_list|,
name|tpart
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
return|return
name|partitions
return|;
block|}
comment|/**    * Get a list of Partitions by filter.    * @param tbl The table containing the partitions.    * @param filter A string represent partition predicates.    * @return a list of partitions satisfying the partition predicates.    * @throws HiveException    * @throws MetaException    * @throws NoSuchObjectException    * @throws TException    */
specifier|public
name|List
argument_list|<
name|Partition
argument_list|>
name|getPartitionsByFilter
parameter_list|(
name|Table
name|tbl
parameter_list|,
name|String
name|filter
parameter_list|)
throws|throws
name|HiveException
throws|,
name|MetaException
throws|,
name|NoSuchObjectException
throws|,
name|TException
block|{
if|if
condition|(
operator|!
name|tbl
operator|.
name|isPartitioned
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|ErrorMsg
operator|.
name|TABLE_NOT_PARTITIONED
argument_list|,
name|tbl
operator|.
name|getTableName
argument_list|()
argument_list|)
throw|;
block|}
name|List
argument_list|<
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
argument_list|>
name|tParts
init|=
name|getMSC
argument_list|()
operator|.
name|listPartitionsByFilter
argument_list|(
name|tbl
operator|.
name|getDbName
argument_list|()
argument_list|,
name|tbl
operator|.
name|getTableName
argument_list|()
argument_list|,
name|filter
argument_list|,
operator|(
name|short
operator|)
operator|-
literal|1
argument_list|)
decl_stmt|;
return|return
name|convertFromMetastore
argument_list|(
name|tbl
argument_list|,
name|tParts
argument_list|)
return|;
block|}
specifier|private
specifier|static
name|List
argument_list|<
name|Partition
argument_list|>
name|convertFromMetastore
parameter_list|(
name|Table
name|tbl
parameter_list|,
name|List
argument_list|<
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
argument_list|>
name|partitions
parameter_list|)
throws|throws
name|HiveException
block|{
if|if
condition|(
name|partitions
operator|==
literal|null
condition|)
block|{
return|return
operator|new
name|ArrayList
argument_list|<
name|Partition
argument_list|>
argument_list|()
return|;
block|}
name|List
argument_list|<
name|Partition
argument_list|>
name|results
init|=
operator|new
name|ArrayList
argument_list|<
name|Partition
argument_list|>
argument_list|(
name|partitions
operator|.
name|size
argument_list|()
argument_list|)
decl_stmt|;
for|for
control|(
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
name|tPart
range|:
name|partitions
control|)
block|{
name|results
operator|.
name|add
argument_list|(
operator|new
name|Partition
argument_list|(
name|tbl
argument_list|,
name|tPart
argument_list|)
argument_list|)
expr_stmt|;
block|}
return|return
name|results
return|;
block|}
comment|/**    * Get a list of Partitions by expr.    * @param tbl The table containing the partitions.    * @param expr A serialized expression for partition predicates.    * @param conf Hive config.    * @param result the resulting list of partitions    * @return whether the resulting list contains partitions which may or may not match the expr    */
specifier|public
name|boolean
name|getPartitionsByExpr
parameter_list|(
name|Table
name|tbl
parameter_list|,
name|ExprNodeGenericFuncDesc
name|expr
parameter_list|,
name|HiveConf
name|conf
parameter_list|,
name|List
argument_list|<
name|Partition
argument_list|>
name|result
parameter_list|)
throws|throws
name|HiveException
throws|,
name|TException
block|{
assert|assert
name|result
operator|!=
literal|null
assert|;
name|byte
index|[]
name|exprBytes
init|=
name|SerializationUtilities
operator|.
name|serializeExpressionToKryo
argument_list|(
name|expr
argument_list|)
decl_stmt|;
name|String
name|defaultPartitionName
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|conf
argument_list|,
name|ConfVars
operator|.
name|DEFAULTPARTITIONNAME
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
argument_list|>
name|msParts
init|=
operator|new
name|ArrayList
argument_list|<
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
argument_list|>
argument_list|()
decl_stmt|;
name|boolean
name|hasUnknownParts
init|=
name|getMSC
argument_list|()
operator|.
name|listPartitionsByExpr
argument_list|(
name|tbl
operator|.
name|getDbName
argument_list|()
argument_list|,
name|tbl
operator|.
name|getTableName
argument_list|()
argument_list|,
name|exprBytes
argument_list|,
name|defaultPartitionName
argument_list|,
operator|(
name|short
operator|)
operator|-
literal|1
argument_list|,
name|msParts
argument_list|)
decl_stmt|;
name|result
operator|.
name|addAll
argument_list|(
name|convertFromMetastore
argument_list|(
name|tbl
argument_list|,
name|msParts
argument_list|)
argument_list|)
expr_stmt|;
return|return
name|hasUnknownParts
return|;
block|}
comment|/**    * Get a number of Partitions by filter.    * @param tbl The table containing the partitions.    * @param filter A string represent partition predicates.    * @return the number of partitions satisfying the partition predicates.    * @throws HiveException    * @throws MetaException    * @throws NoSuchObjectException    * @throws TException    */
specifier|public
name|int
name|getNumPartitionsByFilter
parameter_list|(
name|Table
name|tbl
parameter_list|,
name|String
name|filter
parameter_list|)
throws|throws
name|HiveException
throws|,
name|MetaException
throws|,
name|NoSuchObjectException
throws|,
name|TException
block|{
if|if
condition|(
operator|!
name|tbl
operator|.
name|isPartitioned
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Partition spec should only be supplied for a "
operator|+
literal|"partitioned table"
argument_list|)
throw|;
block|}
name|int
name|numParts
init|=
name|getMSC
argument_list|()
operator|.
name|getNumPartitionsByFilter
argument_list|(
name|tbl
operator|.
name|getDbName
argument_list|()
argument_list|,
name|tbl
operator|.
name|getTableName
argument_list|()
argument_list|,
name|filter
argument_list|)
decl_stmt|;
return|return
name|numParts
return|;
block|}
specifier|public
name|void
name|validatePartitionNameCharacters
parameter_list|(
name|List
argument_list|<
name|String
argument_list|>
name|partVals
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|getMSC
argument_list|()
operator|.
name|validatePartitionNameCharacters
argument_list|(
name|partVals
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|void
name|createRole
parameter_list|(
name|String
name|roleName
parameter_list|,
name|String
name|ownerName
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|getMSC
argument_list|()
operator|.
name|create_role
argument_list|(
operator|new
name|Role
argument_list|(
name|roleName
argument_list|,
operator|-
literal|1
argument_list|,
name|ownerName
argument_list|)
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|void
name|dropRole
parameter_list|(
name|String
name|roleName
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|getMSC
argument_list|()
operator|.
name|drop_role
argument_list|(
name|roleName
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    * Get all existing role names.    *    * @return List of role names.    * @throws HiveException    */
specifier|public
name|List
argument_list|<
name|String
argument_list|>
name|getAllRoleNames
parameter_list|()
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|listRoleNames
argument_list|()
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|List
argument_list|<
name|RolePrincipalGrant
argument_list|>
name|getRoleGrantInfoForPrincipal
parameter_list|(
name|String
name|principalName
parameter_list|,
name|PrincipalType
name|principalType
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|GetRoleGrantsForPrincipalRequest
name|req
init|=
operator|new
name|GetRoleGrantsForPrincipalRequest
argument_list|(
name|principalName
argument_list|,
name|principalType
argument_list|)
decl_stmt|;
name|GetRoleGrantsForPrincipalResponse
name|resp
init|=
name|getMSC
argument_list|()
operator|.
name|get_role_grants_for_principal
argument_list|(
name|req
argument_list|)
decl_stmt|;
return|return
name|resp
operator|.
name|getPrincipalGrants
argument_list|()
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|boolean
name|grantRole
parameter_list|(
name|String
name|roleName
parameter_list|,
name|String
name|userName
parameter_list|,
name|PrincipalType
name|principalType
parameter_list|,
name|String
name|grantor
parameter_list|,
name|PrincipalType
name|grantorType
parameter_list|,
name|boolean
name|grantOption
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|grant_role
argument_list|(
name|roleName
argument_list|,
name|userName
argument_list|,
name|principalType
argument_list|,
name|grantor
argument_list|,
name|grantorType
argument_list|,
name|grantOption
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|boolean
name|revokeRole
parameter_list|(
name|String
name|roleName
parameter_list|,
name|String
name|userName
parameter_list|,
name|PrincipalType
name|principalType
parameter_list|,
name|boolean
name|grantOption
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|revoke_role
argument_list|(
name|roleName
argument_list|,
name|userName
argument_list|,
name|principalType
argument_list|,
name|grantOption
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|List
argument_list|<
name|Role
argument_list|>
name|listRoles
parameter_list|(
name|String
name|userName
parameter_list|,
name|PrincipalType
name|principalType
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|list_roles
argument_list|(
name|userName
argument_list|,
name|principalType
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    * @param objectType    *          hive object type    * @param db_name    *          database name    * @param table_name    *          table name    * @param part_values    *          partition values    * @param column_name    *          column name    * @param user_name    *          user name    * @param group_names    *          group names    * @return the privilege set    * @throws HiveException    */
specifier|public
name|PrincipalPrivilegeSet
name|get_privilege_set
parameter_list|(
name|HiveObjectType
name|objectType
parameter_list|,
name|String
name|db_name
parameter_list|,
name|String
name|table_name
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|part_values
parameter_list|,
name|String
name|column_name
parameter_list|,
name|String
name|user_name
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|group_names
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|HiveObjectRef
name|hiveObj
init|=
operator|new
name|HiveObjectRef
argument_list|(
name|objectType
argument_list|,
name|db_name
argument_list|,
name|table_name
argument_list|,
name|part_values
argument_list|,
name|column_name
argument_list|)
decl_stmt|;
return|return
name|getMSC
argument_list|()
operator|.
name|get_privilege_set
argument_list|(
name|hiveObj
argument_list|,
name|user_name
argument_list|,
name|group_names
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    * @param objectType    *          hive object type    * @param principalName    * @param principalType    * @param dbName    * @param tableName    * @param partValues    * @param columnName    * @return list of privileges    * @throws HiveException    */
specifier|public
name|List
argument_list|<
name|HiveObjectPrivilege
argument_list|>
name|showPrivilegeGrant
parameter_list|(
name|HiveObjectType
name|objectType
parameter_list|,
name|String
name|principalName
parameter_list|,
name|PrincipalType
name|principalType
parameter_list|,
name|String
name|dbName
parameter_list|,
name|String
name|tableName
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|partValues
parameter_list|,
name|String
name|columnName
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|HiveObjectRef
name|hiveObj
init|=
operator|new
name|HiveObjectRef
argument_list|(
name|objectType
argument_list|,
name|dbName
argument_list|,
name|tableName
argument_list|,
name|partValues
argument_list|,
name|columnName
argument_list|)
decl_stmt|;
return|return
name|getMSC
argument_list|()
operator|.
name|list_privileges
argument_list|(
name|principalName
argument_list|,
name|principalType
argument_list|,
name|hiveObj
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|private
specifier|static
name|void
name|copyFiles
parameter_list|(
specifier|final
name|HiveConf
name|conf
parameter_list|,
specifier|final
name|FileSystem
name|destFs
parameter_list|,
name|FileStatus
index|[]
name|srcs
parameter_list|,
specifier|final
name|FileSystem
name|srcFs
parameter_list|,
specifier|final
name|Path
name|destf
parameter_list|,
specifier|final
name|boolean
name|isSrcLocal
parameter_list|,
specifier|final
name|List
argument_list|<
name|Path
argument_list|>
name|newFiles
parameter_list|)
throws|throws
name|HiveException
block|{
specifier|final
name|HdfsUtils
operator|.
name|HadoopFileStatus
name|fullDestStatus
decl_stmt|;
try|try
block|{
name|fullDestStatus
operator|=
operator|new
name|HdfsUtils
operator|.
name|HadoopFileStatus
argument_list|(
name|conf
argument_list|,
name|destFs
argument_list|,
name|destf
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e1
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e1
argument_list|)
throw|;
block|}
if|if
condition|(
operator|!
name|fullDestStatus
operator|.
name|getFileStatus
argument_list|()
operator|.
name|isDirectory
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|destf
operator|+
literal|" is not a directory."
argument_list|)
throw|;
block|}
specifier|final
name|List
argument_list|<
name|Future
argument_list|<
name|ObjectPair
argument_list|<
name|Path
argument_list|,
name|Path
argument_list|>
argument_list|>
argument_list|>
name|futures
init|=
operator|new
name|LinkedList
argument_list|<>
argument_list|()
decl_stmt|;
specifier|final
name|ExecutorService
name|pool
init|=
name|conf
operator|.
name|getInt
argument_list|(
name|ConfVars
operator|.
name|HIVE_MOVE_FILES_THREAD_COUNT
operator|.
name|varname
argument_list|,
literal|25
argument_list|)
operator|>
literal|0
condition|?
name|Executors
operator|.
name|newFixedThreadPool
argument_list|(
name|conf
operator|.
name|getInt
argument_list|(
name|ConfVars
operator|.
name|HIVE_MOVE_FILES_THREAD_COUNT
operator|.
name|varname
argument_list|,
literal|25
argument_list|)
argument_list|,
operator|new
name|ThreadFactoryBuilder
argument_list|()
operator|.
name|setDaemon
argument_list|(
literal|true
argument_list|)
operator|.
name|setNameFormat
argument_list|(
literal|"Move-Thread-%d"
argument_list|)
operator|.
name|build
argument_list|()
argument_list|)
else|:
literal|null
decl_stmt|;
for|for
control|(
name|FileStatus
name|src
range|:
name|srcs
control|)
block|{
name|FileStatus
index|[]
name|files
decl_stmt|;
if|if
condition|(
name|src
operator|.
name|isDirectory
argument_list|()
condition|)
block|{
try|try
block|{
name|files
operator|=
name|srcFs
operator|.
name|listStatus
argument_list|(
name|src
operator|.
name|getPath
argument_list|()
argument_list|,
name|FileUtils
operator|.
name|HIDDEN_FILES_PATH_FILTER
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|pool
operator|.
name|shutdownNow
argument_list|()
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
else|else
block|{
name|files
operator|=
operator|new
name|FileStatus
index|[]
block|{
name|src
block|}
expr_stmt|;
block|}
specifier|final
name|SessionState
name|parentSession
init|=
name|SessionState
operator|.
name|get
argument_list|()
decl_stmt|;
for|for
control|(
specifier|final
name|FileStatus
name|srcFile
range|:
name|files
control|)
block|{
specifier|final
name|Path
name|srcP
init|=
name|srcFile
operator|.
name|getPath
argument_list|()
decl_stmt|;
specifier|final
name|boolean
name|needToCopy
init|=
name|needToCopy
argument_list|(
name|srcP
argument_list|,
name|destf
argument_list|,
name|srcFs
argument_list|,
name|destFs
argument_list|)
decl_stmt|;
specifier|final
name|boolean
name|isRenameAllowed
init|=
operator|!
name|needToCopy
operator|&&
operator|!
name|isSrcLocal
decl_stmt|;
comment|// If we do a rename for a non-local file, we will be transfering the original
comment|// file permissions from source to the destination. Else, in case of mvFile() where we
comment|// copy from source to destination, we will inherit the destination's parent group ownership.
if|if
condition|(
literal|null
operator|==
name|pool
condition|)
block|{
try|try
block|{
name|Path
name|destPath
init|=
name|mvFile
argument_list|(
name|conf
argument_list|,
name|srcFs
argument_list|,
name|srcP
argument_list|,
name|destFs
argument_list|,
name|destf
argument_list|,
name|isSrcLocal
argument_list|,
name|isRenameAllowed
argument_list|)
decl_stmt|;
if|if
condition|(
literal|null
operator|!=
name|newFiles
condition|)
block|{
name|newFiles
operator|.
name|add
argument_list|(
name|destPath
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|ioe
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Failed to move: {}"
argument_list|,
name|ioe
operator|.
name|getMessage
argument_list|()
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|ioe
operator|.
name|getCause
argument_list|()
argument_list|)
throw|;
block|}
block|}
else|else
block|{
name|futures
operator|.
name|add
argument_list|(
name|pool
operator|.
name|submit
argument_list|(
operator|new
name|Callable
argument_list|<
name|ObjectPair
argument_list|<
name|Path
argument_list|,
name|Path
argument_list|>
argument_list|>
argument_list|()
block|{
annotation|@
name|Override
specifier|public
name|ObjectPair
argument_list|<
name|Path
argument_list|,
name|Path
argument_list|>
name|call
parameter_list|()
throws|throws
name|Exception
block|{
name|SessionState
operator|.
name|setCurrentSessionState
argument_list|(
name|parentSession
argument_list|)
expr_stmt|;
name|Path
name|destPath
init|=
name|mvFile
argument_list|(
name|conf
argument_list|,
name|srcFs
argument_list|,
name|srcP
argument_list|,
name|destFs
argument_list|,
name|destf
argument_list|,
name|isSrcLocal
argument_list|,
name|isRenameAllowed
argument_list|)
decl_stmt|;
if|if
condition|(
literal|null
operator|!=
name|newFiles
condition|)
block|{
name|newFiles
operator|.
name|add
argument_list|(
name|destPath
argument_list|)
expr_stmt|;
block|}
return|return
name|ObjectPair
operator|.
name|create
argument_list|(
name|srcP
argument_list|,
name|destPath
argument_list|)
return|;
block|}
block|}
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
block|}
if|if
condition|(
literal|null
operator|!=
name|pool
condition|)
block|{
name|pool
operator|.
name|shutdown
argument_list|()
expr_stmt|;
for|for
control|(
name|Future
argument_list|<
name|ObjectPair
argument_list|<
name|Path
argument_list|,
name|Path
argument_list|>
argument_list|>
name|future
range|:
name|futures
control|)
block|{
try|try
block|{
name|ObjectPair
argument_list|<
name|Path
argument_list|,
name|Path
argument_list|>
name|pair
init|=
name|future
operator|.
name|get
argument_list|()
decl_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Moved src: {}"
argument_list|,
name|pair
operator|.
name|getFirst
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|,
literal|", to dest: {}"
argument_list|,
name|pair
operator|.
name|getSecond
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Failed to move: {}"
argument_list|,
name|e
operator|.
name|getMessage
argument_list|()
argument_list|)
expr_stmt|;
name|pool
operator|.
name|shutdownNow
argument_list|()
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
operator|.
name|getCause
argument_list|()
argument_list|)
throw|;
block|}
block|}
block|}
block|}
specifier|private
specifier|static
name|boolean
name|isSubDir
parameter_list|(
name|Path
name|srcf
parameter_list|,
name|Path
name|destf
parameter_list|,
name|FileSystem
name|srcFs
parameter_list|,
name|FileSystem
name|destFs
parameter_list|,
name|boolean
name|isSrcLocal
parameter_list|)
block|{
if|if
condition|(
name|srcf
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"The source path is null for isSubDir method."
argument_list|)
expr_stmt|;
return|return
literal|false
return|;
block|}
name|String
name|fullF1
init|=
name|getQualifiedPathWithoutSchemeAndAuthority
argument_list|(
name|srcf
argument_list|,
name|srcFs
argument_list|)
operator|.
name|toString
argument_list|()
operator|+
name|Path
operator|.
name|SEPARATOR
decl_stmt|;
name|String
name|fullF2
init|=
name|getQualifiedPathWithoutSchemeAndAuthority
argument_list|(
name|destf
argument_list|,
name|destFs
argument_list|)
operator|.
name|toString
argument_list|()
operator|+
name|Path
operator|.
name|SEPARATOR
decl_stmt|;
name|boolean
name|isInTest
init|=
name|HiveConf
operator|.
name|getBoolVar
argument_list|(
name|srcFs
operator|.
name|getConf
argument_list|()
argument_list|,
name|ConfVars
operator|.
name|HIVE_IN_TEST
argument_list|)
decl_stmt|;
comment|// In the automation, the data warehouse is the local file system based.
name|LOG
operator|.
name|debug
argument_list|(
literal|"The source path is "
operator|+
name|fullF1
operator|+
literal|" and the destination path is "
operator|+
name|fullF2
argument_list|)
expr_stmt|;
if|if
condition|(
name|isInTest
condition|)
block|{
return|return
name|fullF1
operator|.
name|startsWith
argument_list|(
name|fullF2
argument_list|)
return|;
block|}
comment|// schema is diff, return false
name|String
name|schemaSrcf
init|=
name|srcf
operator|.
name|toUri
argument_list|()
operator|.
name|getScheme
argument_list|()
decl_stmt|;
name|String
name|schemaDestf
init|=
name|destf
operator|.
name|toUri
argument_list|()
operator|.
name|getScheme
argument_list|()
decl_stmt|;
comment|// if the schemaDestf is null, it means the destination is not in the local file system
if|if
condition|(
name|schemaDestf
operator|==
literal|null
operator|&&
name|isSrcLocal
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"The source file is in the local while the dest not."
argument_list|)
expr_stmt|;
return|return
literal|false
return|;
block|}
comment|// If both schema information are provided, they should be the same.
if|if
condition|(
name|schemaSrcf
operator|!=
literal|null
operator|&&
name|schemaDestf
operator|!=
literal|null
operator|&&
operator|!
name|schemaSrcf
operator|.
name|equals
argument_list|(
name|schemaDestf
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"The source path's schema is "
operator|+
name|schemaSrcf
operator|+
literal|" and the destination path's schema is "
operator|+
name|schemaDestf
operator|+
literal|"."
argument_list|)
expr_stmt|;
return|return
literal|false
return|;
block|}
name|LOG
operator|.
name|debug
argument_list|(
literal|"The source path is "
operator|+
name|fullF1
operator|+
literal|" and the destination path is "
operator|+
name|fullF2
argument_list|)
expr_stmt|;
return|return
name|fullF1
operator|.
name|startsWith
argument_list|(
name|fullF2
argument_list|)
return|;
block|}
specifier|private
specifier|static
name|Path
name|getQualifiedPathWithoutSchemeAndAuthority
parameter_list|(
name|Path
name|srcf
parameter_list|,
name|FileSystem
name|fs
parameter_list|)
block|{
name|Path
name|currentWorkingDir
init|=
name|fs
operator|.
name|getWorkingDirectory
argument_list|()
decl_stmt|;
name|Path
name|path
init|=
name|srcf
operator|.
name|makeQualified
argument_list|(
name|srcf
operator|.
name|toUri
argument_list|()
argument_list|,
name|currentWorkingDir
argument_list|)
decl_stmt|;
return|return
name|ShimLoader
operator|.
name|getHadoopShims
argument_list|()
operator|.
name|getPathWithoutSchemeAndAuthority
argument_list|(
name|path
argument_list|)
return|;
block|}
comment|/**    *<p>    *   Moves a file from one {@link Path} to another. If {@code isRenameAllowed} is true then the    *   {@link FileSystem#rename(Path, Path)} method is used to move the file. If its false then the data is copied, if    *   {@code isSrcLocal} is true then the {@link FileSystem#copyFromLocalFile(Path, Path)} method is used, else    *   {@link FileUtils#copy(FileSystem, Path, FileSystem, Path, boolean, boolean, HiveConf)} is used.    *</p>    *    *<p>    *   If the destination file already exists, then {@code _copy_[counter]} is appended to the file name, where counter    *   is an integer starting from 1.    *</p>    *    * @param conf the {@link HiveConf} to use if copying data    * @param sourceFs the {@link FileSystem} where the source file exists    * @param sourcePath the {@link Path} to move    * @param destFs the {@link FileSystem} to move the file to    * @param destDirPath the {@link Path} to move the file to    * @param isSrcLocal if the source file is on the local filesystem    * @param isRenameAllowed true if the data should be renamed and not copied, false otherwise    *    * @return the {@link Path} the source file was moved to    *    * @throws IOException if there was an issue moving the file    */
specifier|private
specifier|static
name|Path
name|mvFile
parameter_list|(
name|HiveConf
name|conf
parameter_list|,
name|FileSystem
name|sourceFs
parameter_list|,
name|Path
name|sourcePath
parameter_list|,
name|FileSystem
name|destFs
parameter_list|,
name|Path
name|destDirPath
parameter_list|,
name|boolean
name|isSrcLocal
parameter_list|,
name|boolean
name|isRenameAllowed
parameter_list|)
throws|throws
name|IOException
block|{
comment|// Strip off the file type, if any so we don't make:
comment|// 000000_0.gz -> 000000_0.gz_copy_1
specifier|final
name|String
name|fullname
init|=
name|sourcePath
operator|.
name|getName
argument_list|()
decl_stmt|;
specifier|final
name|String
name|name
init|=
name|FilenameUtils
operator|.
name|getBaseName
argument_list|(
name|sourcePath
operator|.
name|getName
argument_list|()
argument_list|)
decl_stmt|;
specifier|final
name|String
name|type
init|=
name|FilenameUtils
operator|.
name|getExtension
argument_list|(
name|sourcePath
operator|.
name|getName
argument_list|()
argument_list|)
decl_stmt|;
name|Path
name|destFilePath
init|=
operator|new
name|Path
argument_list|(
name|destDirPath
argument_list|,
name|fullname
argument_list|)
decl_stmt|;
comment|/*     * The below loop may perform bad when the destination file already exists and it has too many _copy_     * files as well. A desired approach was to call listFiles() and get a complete list of files from     * the destination, and check whether the file exists or not on that list. However, millions of files     * could live on the destination directory, and on concurrent situations, this can cause OOM problems.     *     * I'll leave the below loop for now until a better approach is found.     */
for|for
control|(
name|int
name|counter
init|=
literal|1
init|;
name|destFs
operator|.
name|exists
argument_list|(
name|destFilePath
argument_list|)
condition|;
name|counter
operator|++
control|)
block|{
name|destFilePath
operator|=
operator|new
name|Path
argument_list|(
name|destDirPath
argument_list|,
name|name
operator|+
operator|(
literal|"_copy_"
operator|+
name|counter
operator|)
operator|+
operator|(
operator|!
name|type
operator|.
name|isEmpty
argument_list|()
condition|?
literal|"."
operator|+
name|type
else|:
literal|""
operator|)
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|isRenameAllowed
condition|)
block|{
name|destFs
operator|.
name|rename
argument_list|(
name|sourcePath
argument_list|,
name|destFilePath
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|isSrcLocal
condition|)
block|{
name|destFs
operator|.
name|copyFromLocalFile
argument_list|(
name|sourcePath
argument_list|,
name|destFilePath
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|FileUtils
operator|.
name|copy
argument_list|(
name|sourceFs
argument_list|,
name|sourcePath
argument_list|,
name|destFs
argument_list|,
name|destFilePath
argument_list|,
literal|true
argument_list|,
comment|// delete source
literal|false
argument_list|,
comment|// overwrite destination
name|conf
argument_list|)
expr_stmt|;
block|}
return|return
name|destFilePath
return|;
block|}
comment|// Clears the dest dir when src is sub-dir of dest.
specifier|public
specifier|static
name|void
name|clearDestForSubDirSrc
parameter_list|(
specifier|final
name|HiveConf
name|conf
parameter_list|,
name|Path
name|dest
parameter_list|,
name|Path
name|src
parameter_list|,
name|boolean
name|isSrcLocal
parameter_list|)
throws|throws
name|IOException
block|{
name|FileSystem
name|destFS
init|=
name|dest
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|FileSystem
name|srcFS
init|=
name|src
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
if|if
condition|(
name|isSubDir
argument_list|(
name|src
argument_list|,
name|dest
argument_list|,
name|srcFS
argument_list|,
name|destFS
argument_list|,
name|isSrcLocal
argument_list|)
condition|)
block|{
specifier|final
name|Path
name|fullSrcPath
init|=
name|getQualifiedPathWithoutSchemeAndAuthority
argument_list|(
name|src
argument_list|,
name|srcFS
argument_list|)
decl_stmt|;
specifier|final
name|Path
name|fullDestPath
init|=
name|getQualifiedPathWithoutSchemeAndAuthority
argument_list|(
name|dest
argument_list|,
name|destFS
argument_list|)
decl_stmt|;
if|if
condition|(
name|fullSrcPath
operator|.
name|equals
argument_list|(
name|fullDestPath
argument_list|)
condition|)
block|{
return|return;
block|}
name|Path
name|parent
init|=
name|fullSrcPath
decl_stmt|;
while|while
condition|(
operator|!
name|parent
operator|.
name|getParent
argument_list|()
operator|.
name|equals
argument_list|(
name|fullDestPath
argument_list|)
condition|)
block|{
name|parent
operator|=
name|parent
operator|.
name|getParent
argument_list|()
expr_stmt|;
block|}
name|FileStatus
index|[]
name|existingFiles
init|=
name|destFS
operator|.
name|listStatus
argument_list|(
name|dest
argument_list|,
name|FileUtils
operator|.
name|HIDDEN_FILES_PATH_FILTER
argument_list|)
decl_stmt|;
for|for
control|(
name|FileStatus
name|fileStatus
range|:
name|existingFiles
control|)
block|{
if|if
condition|(
operator|!
name|fileStatus
operator|.
name|getPath
argument_list|()
operator|.
name|getName
argument_list|()
operator|.
name|equals
argument_list|(
name|parent
operator|.
name|getName
argument_list|()
argument_list|)
condition|)
block|{
name|destFS
operator|.
name|delete
argument_list|(
name|fileStatus
operator|.
name|getPath
argument_list|()
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
comment|// List the new files in destination path which gets copied from source.
specifier|public
specifier|static
name|void
name|listNewFilesRecursively
parameter_list|(
specifier|final
name|FileSystem
name|destFs
parameter_list|,
name|Path
name|dest
parameter_list|,
name|List
argument_list|<
name|Path
argument_list|>
name|newFiles
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
for|for
control|(
name|FileStatus
name|fileStatus
range|:
name|destFs
operator|.
name|listStatus
argument_list|(
name|dest
argument_list|,
name|FileUtils
operator|.
name|HIDDEN_FILES_PATH_FILTER
argument_list|)
control|)
block|{
if|if
condition|(
name|fileStatus
operator|.
name|isDirectory
argument_list|()
condition|)
block|{
comment|// If it is a sub-directory, then recursively list the files.
name|listNewFilesRecursively
argument_list|(
name|destFs
argument_list|,
name|fileStatus
operator|.
name|getPath
argument_list|()
argument_list|,
name|newFiles
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|newFiles
operator|.
name|add
argument_list|(
name|fileStatus
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Failed to get source file statuses"
argument_list|,
name|e
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
comment|//it is assumed that parent directory of the destf should already exist when this
comment|//method is called. when the replace value is true, this method works a little different
comment|//from mv command if the destf is a directory, it replaces the destf instead of moving under
comment|//the destf. in this case, the replaced destf still preserves the original destf's permission
specifier|public
specifier|static
name|boolean
name|moveFile
parameter_list|(
specifier|final
name|HiveConf
name|conf
parameter_list|,
name|Path
name|srcf
parameter_list|,
specifier|final
name|Path
name|destf
parameter_list|,
name|boolean
name|replace
parameter_list|,
name|boolean
name|isSrcLocal
parameter_list|)
throws|throws
name|HiveException
block|{
specifier|final
name|FileSystem
name|srcFs
decl_stmt|,
name|destFs
decl_stmt|;
try|try
block|{
name|destFs
operator|=
name|destf
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Failed to get dest fs"
argument_list|,
name|e
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
try|try
block|{
name|srcFs
operator|=
name|srcf
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Failed to get src fs"
argument_list|,
name|e
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
name|HdfsUtils
operator|.
name|HadoopFileStatus
name|destStatus
init|=
literal|null
decl_stmt|;
comment|// If source path is a subdirectory of the destination path (or the other way around):
comment|//   ex: INSERT OVERWRITE DIRECTORY 'target/warehouse/dest4.out' SELECT src.value WHERE src.key>= 300;
comment|//   where the staging directory is a subdirectory of the destination directory
comment|// (1) Do not delete the dest dir before doing the move operation.
comment|// (2) It is assumed that subdir and dir are in same encryption zone.
comment|// (3) Move individual files from scr dir to dest dir.
name|boolean
name|srcIsSubDirOfDest
init|=
name|isSubDir
argument_list|(
name|srcf
argument_list|,
name|destf
argument_list|,
name|srcFs
argument_list|,
name|destFs
argument_list|,
name|isSrcLocal
argument_list|)
decl_stmt|,
name|destIsSubDirOfSrc
init|=
name|isSubDir
argument_list|(
name|destf
argument_list|,
name|srcf
argument_list|,
name|destFs
argument_list|,
name|srcFs
argument_list|,
literal|false
argument_list|)
decl_stmt|;
try|try
block|{
if|if
condition|(
name|replace
condition|)
block|{
try|try
block|{
name|destStatus
operator|=
operator|new
name|HdfsUtils
operator|.
name|HadoopFileStatus
argument_list|(
name|conf
argument_list|,
name|destFs
argument_list|,
name|destf
argument_list|)
expr_stmt|;
comment|//if destf is an existing directory:
comment|//if replace is true, delete followed by rename(mv) is equivalent to replace
comment|//if replace is false, rename (mv) actually move the src under dest dir
comment|//if destf is an existing file, rename is actually a replace, and do not need
comment|// to delete the file first
if|if
condition|(
name|replace
operator|&&
operator|!
name|srcIsSubDirOfDest
condition|)
block|{
name|destFs
operator|.
name|delete
argument_list|(
name|destf
argument_list|,
literal|true
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"The path "
operator|+
name|destf
operator|.
name|toString
argument_list|()
operator|+
literal|" is deleted"
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|FileNotFoundException
name|ignore
parameter_list|)
block|{         }
block|}
specifier|final
name|HdfsUtils
operator|.
name|HadoopFileStatus
name|desiredStatus
init|=
name|destStatus
decl_stmt|;
specifier|final
name|SessionState
name|parentSession
init|=
name|SessionState
operator|.
name|get
argument_list|()
decl_stmt|;
if|if
condition|(
name|isSrcLocal
condition|)
block|{
comment|// For local src file, copy to hdfs
name|destFs
operator|.
name|copyFromLocalFile
argument_list|(
name|srcf
argument_list|,
name|destf
argument_list|)
expr_stmt|;
return|return
literal|true
return|;
block|}
else|else
block|{
if|if
condition|(
name|needToCopy
argument_list|(
name|srcf
argument_list|,
name|destf
argument_list|,
name|srcFs
argument_list|,
name|destFs
argument_list|)
condition|)
block|{
comment|//copy if across file system or encryption zones.
name|LOG
operator|.
name|debug
argument_list|(
literal|"Copying source "
operator|+
name|srcf
operator|+
literal|" to "
operator|+
name|destf
operator|+
literal|" because HDFS encryption zones are different."
argument_list|)
expr_stmt|;
return|return
name|FileUtils
operator|.
name|copy
argument_list|(
name|srcf
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
argument_list|,
name|srcf
argument_list|,
name|destf
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
argument_list|,
name|destf
argument_list|,
literal|true
argument_list|,
comment|// delete source
name|replace
argument_list|,
comment|// overwrite destination
name|conf
argument_list|)
return|;
block|}
else|else
block|{
if|if
condition|(
name|srcIsSubDirOfDest
operator|||
name|destIsSubDirOfSrc
condition|)
block|{
name|FileStatus
index|[]
name|srcs
init|=
name|destFs
operator|.
name|listStatus
argument_list|(
name|srcf
argument_list|,
name|FileUtils
operator|.
name|HIDDEN_FILES_PATH_FILTER
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|Future
argument_list|<
name|Void
argument_list|>
argument_list|>
name|futures
init|=
operator|new
name|LinkedList
argument_list|<>
argument_list|()
decl_stmt|;
specifier|final
name|ExecutorService
name|pool
init|=
name|conf
operator|.
name|getInt
argument_list|(
name|ConfVars
operator|.
name|HIVE_MOVE_FILES_THREAD_COUNT
operator|.
name|varname
argument_list|,
literal|25
argument_list|)
operator|>
literal|0
condition|?
name|Executors
operator|.
name|newFixedThreadPool
argument_list|(
name|conf
operator|.
name|getInt
argument_list|(
name|ConfVars
operator|.
name|HIVE_MOVE_FILES_THREAD_COUNT
operator|.
name|varname
argument_list|,
literal|25
argument_list|)
argument_list|,
operator|new
name|ThreadFactoryBuilder
argument_list|()
operator|.
name|setDaemon
argument_list|(
literal|true
argument_list|)
operator|.
name|setNameFormat
argument_list|(
literal|"Move-Thread-%d"
argument_list|)
operator|.
name|build
argument_list|()
argument_list|)
else|:
literal|null
decl_stmt|;
if|if
condition|(
name|destIsSubDirOfSrc
operator|&&
operator|!
name|destFs
operator|.
name|exists
argument_list|(
name|destf
argument_list|)
condition|)
block|{
name|Utilities
operator|.
name|LOG14535
operator|.
name|info
argument_list|(
literal|"Creating "
operator|+
name|destf
argument_list|)
expr_stmt|;
name|destFs
operator|.
name|mkdirs
argument_list|(
name|destf
argument_list|)
expr_stmt|;
block|}
comment|/* Move files one by one because source is a subdirectory of destination */
for|for
control|(
specifier|final
name|FileStatus
name|srcStatus
range|:
name|srcs
control|)
block|{
specifier|final
name|Path
name|destFile
init|=
operator|new
name|Path
argument_list|(
name|destf
argument_list|,
name|srcStatus
operator|.
name|getPath
argument_list|()
operator|.
name|getName
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
literal|null
operator|==
name|pool
condition|)
block|{
if|if
condition|(
operator|!
name|destFs
operator|.
name|rename
argument_list|(
name|srcStatus
operator|.
name|getPath
argument_list|()
argument_list|,
name|destFile
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"rename for src path: "
operator|+
name|srcStatus
operator|.
name|getPath
argument_list|()
operator|+
literal|" to dest:"
operator|+
name|destf
operator|+
literal|" returned false"
argument_list|)
throw|;
block|}
block|}
else|else
block|{
name|futures
operator|.
name|add
argument_list|(
name|pool
operator|.
name|submit
argument_list|(
operator|new
name|Callable
argument_list|<
name|Void
argument_list|>
argument_list|()
block|{
annotation|@
name|Override
specifier|public
name|Void
name|call
parameter_list|()
throws|throws
name|Exception
block|{
name|SessionState
operator|.
name|setCurrentSessionState
argument_list|(
name|parentSession
argument_list|)
expr_stmt|;
specifier|final
name|String
name|group
init|=
name|srcStatus
operator|.
name|getGroup
argument_list|()
decl_stmt|;
if|if
condition|(
operator|!
name|destFs
operator|.
name|rename
argument_list|(
name|srcStatus
operator|.
name|getPath
argument_list|()
argument_list|,
name|destFile
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"rename for src path: "
operator|+
name|srcStatus
operator|.
name|getPath
argument_list|()
operator|+
literal|" to dest path:"
operator|+
name|destFile
operator|+
literal|" returned false"
argument_list|)
throw|;
block|}
return|return
literal|null
return|;
block|}
block|}
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
literal|null
operator|!=
name|pool
condition|)
block|{
name|pool
operator|.
name|shutdown
argument_list|()
expr_stmt|;
for|for
control|(
name|Future
argument_list|<
name|Void
argument_list|>
name|future
range|:
name|futures
control|)
block|{
try|try
block|{
name|future
operator|.
name|get
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
name|e
operator|.
name|getMessage
argument_list|()
argument_list|)
expr_stmt|;
name|pool
operator|.
name|shutdownNow
argument_list|()
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
operator|.
name|getCause
argument_list|()
argument_list|)
throw|;
block|}
block|}
block|}
return|return
literal|true
return|;
block|}
else|else
block|{
if|if
condition|(
name|destFs
operator|.
name|rename
argument_list|(
name|srcf
argument_list|,
name|destf
argument_list|)
condition|)
block|{
return|return
literal|true
return|;
block|}
return|return
literal|false
return|;
block|}
block|}
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|ioe
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Unable to move source "
operator|+
name|srcf
operator|+
literal|" to destination "
operator|+
name|destf
argument_list|,
name|ioe
argument_list|)
throw|;
block|}
block|}
comment|/**    * If moving across different FileSystems or differnent encryption zone, need to do a File copy instead of rename.    * TODO- consider if need to do this for different file authority.    * @throws HiveException    */
specifier|static
specifier|protected
name|boolean
name|needToCopy
parameter_list|(
name|Path
name|srcf
parameter_list|,
name|Path
name|destf
parameter_list|,
name|FileSystem
name|srcFs
parameter_list|,
name|FileSystem
name|destFs
parameter_list|)
throws|throws
name|HiveException
block|{
comment|//Check if different FileSystems
if|if
condition|(
operator|!
name|FileUtils
operator|.
name|equalsFileSystem
argument_list|(
name|srcFs
argument_list|,
name|destFs
argument_list|)
condition|)
block|{
return|return
literal|true
return|;
block|}
comment|//Check if different encryption zones
name|HadoopShims
operator|.
name|HdfsEncryptionShim
name|srcHdfsEncryptionShim
init|=
name|SessionState
operator|.
name|get
argument_list|()
operator|.
name|getHdfsEncryptionShim
argument_list|(
name|srcFs
argument_list|)
decl_stmt|;
name|HadoopShims
operator|.
name|HdfsEncryptionShim
name|destHdfsEncryptionShim
init|=
name|SessionState
operator|.
name|get
argument_list|()
operator|.
name|getHdfsEncryptionShim
argument_list|(
name|destFs
argument_list|)
decl_stmt|;
try|try
block|{
return|return
name|srcHdfsEncryptionShim
operator|!=
literal|null
operator|&&
name|destHdfsEncryptionShim
operator|!=
literal|null
operator|&&
operator|(
name|srcHdfsEncryptionShim
operator|.
name|isPathEncrypted
argument_list|(
name|srcf
argument_list|)
operator|||
name|destHdfsEncryptionShim
operator|.
name|isPathEncrypted
argument_list|(
name|destf
argument_list|)
operator|)
operator|&&
operator|!
name|srcHdfsEncryptionShim
operator|.
name|arePathsOnSameEncryptionZone
argument_list|(
name|srcf
argument_list|,
name|destf
argument_list|,
name|destHdfsEncryptionShim
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    * Copy files.  This handles building the mapping for buckets and such between the source and    * destination    * @param conf Configuration object    * @param srcf source directory, if bucketed should contain bucket files    * @param destf directory to move files into    * @param fs Filesystem    * @param isSrcLocal true if source is on local file system    * @param isAcid true if this is an ACID based write    * @param newFiles if this is non-null, a list of files that were created as a result of this    *                 move will be returned.    * @throws HiveException    */
specifier|static
specifier|protected
name|void
name|copyFiles
parameter_list|(
name|HiveConf
name|conf
parameter_list|,
name|Path
name|srcf
parameter_list|,
name|Path
name|destf
parameter_list|,
name|FileSystem
name|fs
parameter_list|,
name|boolean
name|isSrcLocal
parameter_list|,
name|boolean
name|isAcid
parameter_list|,
name|List
argument_list|<
name|Path
argument_list|>
name|newFiles
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
comment|// create the destination if it does not exist
if|if
condition|(
operator|!
name|fs
operator|.
name|exists
argument_list|(
name|destf
argument_list|)
condition|)
block|{
name|FileUtils
operator|.
name|mkdir
argument_list|(
name|fs
argument_list|,
name|destf
argument_list|,
name|conf
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"copyFiles: error while checking/creating destination directory!!!"
argument_list|,
name|e
argument_list|)
throw|;
block|}
name|FileStatus
index|[]
name|srcs
decl_stmt|;
name|FileSystem
name|srcFs
decl_stmt|;
try|try
block|{
name|srcFs
operator|=
name|srcf
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
expr_stmt|;
name|srcs
operator|=
name|srcFs
operator|.
name|globStatus
argument_list|(
name|srcf
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"addFiles: filesystem error in check phase. "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
if|if
condition|(
name|srcs
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"No sources specified to move: "
operator|+
name|srcf
argument_list|)
expr_stmt|;
return|return;
comment|// srcs = new FileStatus[0]; Why is this needed?
block|}
comment|// If we're moving files around for an ACID write then the rules and paths are all different.
comment|// You can blame this on Owen.
if|if
condition|(
name|isAcid
condition|)
block|{
name|moveAcidFiles
argument_list|(
name|srcFs
argument_list|,
name|srcs
argument_list|,
name|destf
argument_list|,
name|newFiles
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|copyFiles
argument_list|(
name|conf
argument_list|,
name|fs
argument_list|,
name|srcs
argument_list|,
name|srcFs
argument_list|,
name|destf
argument_list|,
name|isSrcLocal
argument_list|,
name|newFiles
argument_list|)
expr_stmt|;
block|}
block|}
specifier|private
specifier|static
name|void
name|moveAcidFiles
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|FileStatus
index|[]
name|stats
parameter_list|,
name|Path
name|dst
parameter_list|,
name|List
argument_list|<
name|Path
argument_list|>
name|newFiles
parameter_list|)
throws|throws
name|HiveException
block|{
comment|// The layout for ACID files is table|partname/base|delta|delete_delta/bucket
comment|// We will always only be writing delta files.  In the buckets created by FileSinkOperator
comment|// it will look like bucket/delta|delete_delta/bucket.  So we need to move that into
comment|// the above structure. For the first mover there will be no delta directory,
comment|// so we can move the whole directory.
comment|// For everyone else we will need to just move the buckets under the existing delta
comment|// directory.
name|Set
argument_list|<
name|Path
argument_list|>
name|createdDeltaDirs
init|=
operator|new
name|HashSet
argument_list|<
name|Path
argument_list|>
argument_list|()
decl_stmt|;
comment|// Open the original path we've been given and find the list of original buckets
for|for
control|(
name|FileStatus
name|stat
range|:
name|stats
control|)
block|{
name|Path
name|srcPath
init|=
name|stat
operator|.
name|getPath
argument_list|()
decl_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Acid move Looking for original buckets in "
operator|+
name|srcPath
argument_list|)
expr_stmt|;
name|FileStatus
index|[]
name|origBucketStats
init|=
literal|null
decl_stmt|;
try|try
block|{
name|origBucketStats
operator|=
name|fs
operator|.
name|listStatus
argument_list|(
name|srcPath
argument_list|,
name|AcidUtils
operator|.
name|originalBucketFilter
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|String
name|msg
init|=
literal|"Unable to look for bucket files in src path "
operator|+
name|srcPath
operator|.
name|toUri
argument_list|()
operator|.
name|toString
argument_list|()
decl_stmt|;
name|LOG
operator|.
name|error
argument_list|(
name|msg
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|msg
argument_list|,
name|e
argument_list|)
throw|;
block|}
name|LOG
operator|.
name|debug
argument_list|(
literal|"Acid move found "
operator|+
name|origBucketStats
operator|.
name|length
operator|+
literal|" original buckets"
argument_list|)
expr_stmt|;
for|for
control|(
name|FileStatus
name|origBucketStat
range|:
name|origBucketStats
control|)
block|{
name|Path
name|origBucketPath
init|=
name|origBucketStat
operator|.
name|getPath
argument_list|()
decl_stmt|;
name|moveAcidDeltaFiles
argument_list|(
name|AcidUtils
operator|.
name|DELTA_PREFIX
argument_list|,
name|AcidUtils
operator|.
name|deltaFileFilter
argument_list|,
name|fs
argument_list|,
name|dst
argument_list|,
name|origBucketPath
argument_list|,
name|createdDeltaDirs
argument_list|,
name|newFiles
argument_list|)
expr_stmt|;
name|moveAcidDeltaFiles
argument_list|(
name|AcidUtils
operator|.
name|DELETE_DELTA_PREFIX
argument_list|,
name|AcidUtils
operator|.
name|deleteEventDeltaDirFilter
argument_list|,
name|fs
argument_list|,
name|dst
argument_list|,
name|origBucketPath
argument_list|,
name|createdDeltaDirs
argument_list|,
name|newFiles
argument_list|)
expr_stmt|;
block|}
block|}
block|}
specifier|private
specifier|static
name|void
name|moveAcidDeltaFiles
parameter_list|(
name|String
name|deltaFileType
parameter_list|,
name|PathFilter
name|pathFilter
parameter_list|,
name|FileSystem
name|fs
parameter_list|,
name|Path
name|dst
parameter_list|,
name|Path
name|origBucketPath
parameter_list|,
name|Set
argument_list|<
name|Path
argument_list|>
name|createdDeltaDirs
parameter_list|,
name|List
argument_list|<
name|Path
argument_list|>
name|newFiles
parameter_list|)
throws|throws
name|HiveException
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Acid move looking for "
operator|+
name|deltaFileType
operator|+
literal|" files in bucket "
operator|+
name|origBucketPath
argument_list|)
expr_stmt|;
name|FileStatus
index|[]
name|deltaStats
init|=
literal|null
decl_stmt|;
try|try
block|{
name|deltaStats
operator|=
name|fs
operator|.
name|listStatus
argument_list|(
name|origBucketPath
argument_list|,
name|pathFilter
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Unable to look for "
operator|+
name|deltaFileType
operator|+
literal|" files in original bucket "
operator|+
name|origBucketPath
operator|.
name|toUri
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
name|LOG
operator|.
name|debug
argument_list|(
literal|"Acid move found "
operator|+
name|deltaStats
operator|.
name|length
operator|+
literal|" "
operator|+
name|deltaFileType
operator|+
literal|" files"
argument_list|)
expr_stmt|;
for|for
control|(
name|FileStatus
name|deltaStat
range|:
name|deltaStats
control|)
block|{
name|Path
name|deltaPath
init|=
name|deltaStat
operator|.
name|getPath
argument_list|()
decl_stmt|;
comment|// Create the delta directory.  Don't worry if it already exists,
comment|// as that likely means another task got to it first.  Then move each of the buckets.
comment|// it would be more efficient to try to move the delta with it's buckets but that is
comment|// harder to make race condition proof.
name|Path
name|deltaDest
init|=
operator|new
name|Path
argument_list|(
name|dst
argument_list|,
name|deltaPath
operator|.
name|getName
argument_list|()
argument_list|)
decl_stmt|;
try|try
block|{
if|if
condition|(
operator|!
name|createdDeltaDirs
operator|.
name|contains
argument_list|(
name|deltaDest
argument_list|)
condition|)
block|{
try|try
block|{
name|fs
operator|.
name|mkdirs
argument_list|(
name|deltaDest
argument_list|)
expr_stmt|;
name|createdDeltaDirs
operator|.
name|add
argument_list|(
name|deltaDest
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|swallowIt
parameter_list|)
block|{
comment|// Don't worry about this, as it likely just means it's already been created.
name|LOG
operator|.
name|info
argument_list|(
literal|"Unable to create "
operator|+
name|deltaFileType
operator|+
literal|" directory "
operator|+
name|deltaDest
operator|+
literal|", assuming it already exists: "
operator|+
name|swallowIt
operator|.
name|getMessage
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
name|FileStatus
index|[]
name|bucketStats
init|=
name|fs
operator|.
name|listStatus
argument_list|(
name|deltaPath
argument_list|,
name|AcidUtils
operator|.
name|bucketFileFilter
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Acid move found "
operator|+
name|bucketStats
operator|.
name|length
operator|+
literal|" bucket files"
argument_list|)
expr_stmt|;
for|for
control|(
name|FileStatus
name|bucketStat
range|:
name|bucketStats
control|)
block|{
name|Path
name|bucketSrc
init|=
name|bucketStat
operator|.
name|getPath
argument_list|()
decl_stmt|;
name|Path
name|bucketDest
init|=
operator|new
name|Path
argument_list|(
name|deltaDest
argument_list|,
name|bucketSrc
operator|.
name|getName
argument_list|()
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Moving bucket "
operator|+
name|bucketSrc
operator|.
name|toUri
argument_list|()
operator|.
name|toString
argument_list|()
operator|+
literal|" to "
operator|+
name|bucketDest
operator|.
name|toUri
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
name|fs
operator|.
name|rename
argument_list|(
name|bucketSrc
argument_list|,
name|bucketDest
argument_list|)
expr_stmt|;
if|if
condition|(
name|newFiles
operator|!=
literal|null
condition|)
name|newFiles
operator|.
name|add
argument_list|(
name|bucketDest
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Error moving acid files "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
block|}
comment|/**    * Replaces files in the partition with new data set specified by srcf. Works    * by renaming directory of srcf to the destination file.    * srcf, destf, and tmppath should resident in the same DFS, but the oldPath can be in a    * different DFS.    *    * @param tablePath path of the table.  Used to identify permission inheritance.    * @param srcf    *          Source directory to be renamed to tmppath. It should be a    *          leaf directory where the final data files reside. However it    *          could potentially contain subdirectories as well.    * @param destf    *          The directory where the final data needs to go    * @param oldPath    *          The directory where the old data location, need to be cleaned up.  Most of time, will be the same    *          as destf, unless its across FileSystem boundaries.    * @param purge    *          When set to true files which needs to be deleted are not moved to Trash    * @param isSrcLocal    *          If the source directory is LOCAL    * @param newFiles    *          Output the list of new files replaced in the destination path    */
specifier|protected
name|void
name|replaceFiles
parameter_list|(
name|Path
name|tablePath
parameter_list|,
name|Path
name|srcf
parameter_list|,
name|Path
name|destf
parameter_list|,
name|Path
name|oldPath
parameter_list|,
name|HiveConf
name|conf
parameter_list|,
name|boolean
name|isSrcLocal
parameter_list|,
name|boolean
name|purge
parameter_list|,
name|List
argument_list|<
name|Path
argument_list|>
name|newFiles
parameter_list|,
name|PathFilter
name|deletePathFilter
parameter_list|,
name|boolean
name|isMmTable
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|FileSystem
name|destFs
init|=
name|destf
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
comment|// check if srcf contains nested sub-directories
name|FileStatus
index|[]
name|srcs
decl_stmt|;
name|FileSystem
name|srcFs
decl_stmt|;
try|try
block|{
name|srcFs
operator|=
name|srcf
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
expr_stmt|;
name|srcs
operator|=
name|srcFs
operator|.
name|globStatus
argument_list|(
name|srcf
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Getting globStatus "
operator|+
name|srcf
operator|.
name|toString
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
if|if
condition|(
name|srcs
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"No sources specified to move: "
operator|+
name|srcf
argument_list|)
expr_stmt|;
return|return;
block|}
if|if
condition|(
name|oldPath
operator|!=
literal|null
condition|)
block|{
comment|// TODO: we assume lbLevels is 0 here. Same as old code for non-MM.
comment|//       For MM tables, this can only be a LOAD command. Does LOAD even support LB?
name|deleteOldPathForReplace
argument_list|(
name|destf
argument_list|,
name|oldPath
argument_list|,
name|conf
argument_list|,
name|purge
argument_list|,
name|deletePathFilter
argument_list|,
name|isMmTable
argument_list|,
literal|0
argument_list|)
expr_stmt|;
block|}
comment|// first call FileUtils.mkdir to make sure that destf directory exists, if not, it creates
comment|// destf
name|boolean
name|destfExist
init|=
name|FileUtils
operator|.
name|mkdir
argument_list|(
name|destFs
argument_list|,
name|destf
argument_list|,
name|conf
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|destfExist
condition|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Directory "
operator|+
name|destf
operator|.
name|toString
argument_list|()
operator|+
literal|" does not exist and could not be created."
argument_list|)
throw|;
block|}
comment|// Two cases:
comment|// 1. srcs has only a src directory, if rename src directory to destf, we also need to
comment|// Copy/move each file under the source directory to avoid to delete the destination
comment|// directory if it is the root of an HDFS encryption zone.
comment|// 2. srcs must be a list of files -- ensured by LoadSemanticAnalyzer
comment|// in both cases, we move the file under destf
if|if
condition|(
name|srcs
operator|.
name|length
operator|==
literal|1
operator|&&
name|srcs
index|[
literal|0
index|]
operator|.
name|isDirectory
argument_list|()
condition|)
block|{
if|if
condition|(
operator|!
name|moveFile
argument_list|(
name|conf
argument_list|,
name|srcs
index|[
literal|0
index|]
operator|.
name|getPath
argument_list|()
argument_list|,
name|destf
argument_list|,
literal|true
argument_list|,
name|isSrcLocal
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Error moving: "
operator|+
name|srcf
operator|+
literal|" into: "
operator|+
name|destf
argument_list|)
throw|;
block|}
comment|// Add file paths of the files that will be moved to the destination if the caller needs it
if|if
condition|(
literal|null
operator|!=
name|newFiles
condition|)
block|{
name|listNewFilesRecursively
argument_list|(
name|destFs
argument_list|,
name|destf
argument_list|,
name|newFiles
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
comment|// its either a file or glob
for|for
control|(
name|FileStatus
name|src
range|:
name|srcs
control|)
block|{
name|Path
name|destFile
init|=
operator|new
name|Path
argument_list|(
name|destf
argument_list|,
name|src
operator|.
name|getPath
argument_list|()
operator|.
name|getName
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|moveFile
argument_list|(
name|conf
argument_list|,
name|src
operator|.
name|getPath
argument_list|()
argument_list|,
name|destFile
argument_list|,
literal|true
argument_list|,
name|isSrcLocal
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Error moving: "
operator|+
name|srcf
operator|+
literal|" into: "
operator|+
name|destf
argument_list|)
throw|;
block|}
comment|// Add file paths of the files that will be moved to the destination if the caller needs it
if|if
condition|(
literal|null
operator|!=
name|newFiles
condition|)
block|{
name|newFiles
operator|.
name|add
argument_list|(
name|destFile
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
specifier|private
name|void
name|deleteOldPathForReplace
parameter_list|(
name|Path
name|destPath
parameter_list|,
name|Path
name|oldPath
parameter_list|,
name|HiveConf
name|conf
parameter_list|,
name|boolean
name|purge
parameter_list|,
name|PathFilter
name|pathFilter
parameter_list|,
name|boolean
name|isMmTable
parameter_list|,
name|int
name|lbLevels
parameter_list|)
throws|throws
name|HiveException
block|{
name|Utilities
operator|.
name|LOG14535
operator|.
name|info
argument_list|(
literal|"Deleting old paths for replace in "
operator|+
name|destPath
operator|+
literal|" and old path "
operator|+
name|oldPath
argument_list|)
expr_stmt|;
name|boolean
name|isOldPathUnderDestf
init|=
literal|false
decl_stmt|;
try|try
block|{
name|FileSystem
name|oldFs
init|=
name|oldPath
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|FileSystem
name|destFs
init|=
name|destPath
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
comment|// if oldPath is destf or its subdir, its should definitely be deleted, otherwise its
comment|// existing content might result in incorrect (extra) data.
comment|// But not sure why we changed not to delete the oldPath in HIVE-8750 if it is
comment|// not the destf or its subdir?
name|isOldPathUnderDestf
operator|=
name|isSubDir
argument_list|(
name|oldPath
argument_list|,
name|destPath
argument_list|,
name|oldFs
argument_list|,
name|destFs
argument_list|,
literal|false
argument_list|)
expr_stmt|;
if|if
condition|(
name|isOldPathUnderDestf
operator|||
name|isMmTable
condition|)
block|{
if|if
condition|(
name|lbLevels
operator|==
literal|0
operator|||
operator|!
name|isMmTable
condition|)
block|{
name|cleanUpOneDirectoryForReplace
argument_list|(
name|oldPath
argument_list|,
name|oldFs
argument_list|,
name|pathFilter
argument_list|,
name|conf
argument_list|,
name|purge
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// We need to clean up different MM IDs from each LB directory separately.
comment|// Avoid temporary directories in the immediate table/part dir.
comment|// TODO: we could just find directories with any MM directories inside?
comment|//       the rest doesn't have to be cleaned up.
name|String
name|mask
init|=
literal|"[^._]*"
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|lbLevels
operator|-
literal|1
condition|;
operator|++
name|i
control|)
block|{
name|mask
operator|+=
name|Path
operator|.
name|SEPARATOR
operator|+
literal|"*"
expr_stmt|;
block|}
name|Path
name|glob
init|=
operator|new
name|Path
argument_list|(
name|oldPath
argument_list|,
name|mask
argument_list|)
decl_stmt|;
name|FileStatus
index|[]
name|lbDirs
init|=
name|oldFs
operator|.
name|globStatus
argument_list|(
name|glob
argument_list|)
decl_stmt|;
for|for
control|(
name|FileStatus
name|lbDir
range|:
name|lbDirs
control|)
block|{
name|Path
name|lbPath
init|=
name|lbDir
operator|.
name|getPath
argument_list|()
decl_stmt|;
if|if
condition|(
operator|!
name|lbDir
operator|.
name|isDirectory
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Unexpected path during overwrite: "
operator|+
name|lbPath
argument_list|)
throw|;
block|}
name|Utilities
operator|.
name|LOG14535
operator|.
name|info
argument_list|(
literal|"Cleaning up LB directory "
operator|+
name|lbPath
argument_list|)
expr_stmt|;
name|cleanUpOneDirectoryForReplace
argument_list|(
name|lbPath
argument_list|,
name|oldFs
argument_list|,
name|pathFilter
argument_list|,
name|conf
argument_list|,
name|purge
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
if|if
condition|(
name|isOldPathUnderDestf
operator|||
name|isMmTable
condition|)
block|{
comment|// if oldPath is a subdir of destf but it could not be cleaned
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Directory "
operator|+
name|oldPath
operator|.
name|toString
argument_list|()
operator|+
literal|" could not be cleaned up."
argument_list|,
name|e
argument_list|)
throw|;
block|}
else|else
block|{
comment|//swallow the exception since it won't affect the final result
name|LOG
operator|.
name|warn
argument_list|(
literal|"Directory "
operator|+
name|oldPath
operator|.
name|toString
argument_list|()
operator|+
literal|" cannot be cleaned: "
operator|+
name|e
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
block|}
block|}
specifier|private
name|void
name|cleanUpOneDirectoryForReplace
parameter_list|(
name|Path
name|path
parameter_list|,
name|FileSystem
name|fs
parameter_list|,
name|PathFilter
name|pathFilter
parameter_list|,
name|HiveConf
name|conf
parameter_list|,
name|boolean
name|purge
parameter_list|)
throws|throws
name|IOException
throws|,
name|HiveException
block|{
name|FileStatus
index|[]
name|statuses
init|=
name|fs
operator|.
name|listStatus
argument_list|(
name|path
argument_list|,
name|pathFilter
argument_list|)
decl_stmt|;
if|if
condition|(
name|statuses
operator|==
literal|null
operator|||
name|statuses
operator|.
name|length
operator|==
literal|0
condition|)
return|return;
name|String
name|s
init|=
literal|"Deleting files under "
operator|+
name|path
operator|+
literal|" for replace: "
decl_stmt|;
for|for
control|(
name|FileStatus
name|file
range|:
name|statuses
control|)
block|{
name|s
operator|+=
name|file
operator|.
name|getPath
argument_list|()
operator|.
name|getName
argument_list|()
operator|+
literal|", "
expr_stmt|;
block|}
name|Utilities
operator|.
name|LOG14535
operator|.
name|info
argument_list|(
name|s
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|trashFiles
argument_list|(
name|fs
argument_list|,
name|statuses
argument_list|,
name|conf
argument_list|,
name|purge
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Old path "
operator|+
name|path
operator|+
literal|" has not been cleaned up."
argument_list|)
throw|;
block|}
block|}
comment|/**    * Trashes or deletes all files under a directory. Leaves the directory as is.    * @param fs FileSystem to use    * @param statuses fileStatuses of files to be deleted    * @param conf hive configuration    * @return true if deletion successful    * @throws IOException    */
specifier|public
specifier|static
name|boolean
name|trashFiles
parameter_list|(
specifier|final
name|FileSystem
name|fs
parameter_list|,
specifier|final
name|FileStatus
index|[]
name|statuses
parameter_list|,
specifier|final
name|Configuration
name|conf
parameter_list|,
specifier|final
name|boolean
name|purge
parameter_list|)
throws|throws
name|IOException
block|{
name|boolean
name|result
init|=
literal|true
decl_stmt|;
if|if
condition|(
name|statuses
operator|==
literal|null
operator|||
name|statuses
operator|.
name|length
operator|==
literal|0
condition|)
block|{
return|return
literal|false
return|;
block|}
specifier|final
name|List
argument_list|<
name|Future
argument_list|<
name|Boolean
argument_list|>
argument_list|>
name|futures
init|=
operator|new
name|LinkedList
argument_list|<>
argument_list|()
decl_stmt|;
specifier|final
name|ExecutorService
name|pool
init|=
name|conf
operator|.
name|getInt
argument_list|(
name|ConfVars
operator|.
name|HIVE_MOVE_FILES_THREAD_COUNT
operator|.
name|varname
argument_list|,
literal|25
argument_list|)
operator|>
literal|0
condition|?
name|Executors
operator|.
name|newFixedThreadPool
argument_list|(
name|conf
operator|.
name|getInt
argument_list|(
name|ConfVars
operator|.
name|HIVE_MOVE_FILES_THREAD_COUNT
operator|.
name|varname
argument_list|,
literal|25
argument_list|)
argument_list|,
operator|new
name|ThreadFactoryBuilder
argument_list|()
operator|.
name|setDaemon
argument_list|(
literal|true
argument_list|)
operator|.
name|setNameFormat
argument_list|(
literal|"Delete-Thread-%d"
argument_list|)
operator|.
name|build
argument_list|()
argument_list|)
else|:
literal|null
decl_stmt|;
specifier|final
name|SessionState
name|parentSession
init|=
name|SessionState
operator|.
name|get
argument_list|()
decl_stmt|;
for|for
control|(
specifier|final
name|FileStatus
name|status
range|:
name|statuses
control|)
block|{
if|if
condition|(
literal|null
operator|==
name|pool
condition|)
block|{
name|result
operator|&=
name|FileUtils
operator|.
name|moveToTrash
argument_list|(
name|fs
argument_list|,
name|status
operator|.
name|getPath
argument_list|()
argument_list|,
name|conf
argument_list|,
name|purge
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|futures
operator|.
name|add
argument_list|(
name|pool
operator|.
name|submit
argument_list|(
operator|new
name|Callable
argument_list|<
name|Boolean
argument_list|>
argument_list|()
block|{
annotation|@
name|Override
specifier|public
name|Boolean
name|call
parameter_list|()
throws|throws
name|Exception
block|{
name|SessionState
operator|.
name|setCurrentSessionState
argument_list|(
name|parentSession
argument_list|)
expr_stmt|;
return|return
name|FileUtils
operator|.
name|moveToTrash
argument_list|(
name|fs
argument_list|,
name|status
operator|.
name|getPath
argument_list|()
argument_list|,
name|conf
argument_list|,
name|purge
argument_list|)
return|;
block|}
block|}
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
literal|null
operator|!=
name|pool
condition|)
block|{
name|pool
operator|.
name|shutdown
argument_list|()
expr_stmt|;
for|for
control|(
name|Future
argument_list|<
name|Boolean
argument_list|>
name|future
range|:
name|futures
control|)
block|{
try|try
block|{
name|result
operator|&=
name|future
operator|.
name|get
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|InterruptedException
decl||
name|ExecutionException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Failed to delete: "
argument_list|,
name|e
argument_list|)
expr_stmt|;
name|pool
operator|.
name|shutdownNow
argument_list|()
expr_stmt|;
throw|throw
operator|new
name|IOException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
block|}
return|return
name|result
return|;
block|}
specifier|public
specifier|static
name|boolean
name|isHadoop1
parameter_list|()
block|{
return|return
name|ShimLoader
operator|.
name|getMajorVersion
argument_list|()
operator|.
name|startsWith
argument_list|(
literal|"0.20"
argument_list|)
return|;
block|}
specifier|public
name|List
argument_list|<
name|Partition
argument_list|>
name|exchangeTablePartitions
parameter_list|(
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|partitionSpecs
parameter_list|,
name|String
name|sourceDb
parameter_list|,
name|String
name|sourceTable
parameter_list|,
name|String
name|destDb
parameter_list|,
name|String
name|destinationTableName
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|List
argument_list|<
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
argument_list|>
name|partitions
init|=
name|getMSC
argument_list|()
operator|.
name|exchange_partitions
argument_list|(
name|partitionSpecs
argument_list|,
name|sourceDb
argument_list|,
name|sourceTable
argument_list|,
name|destDb
argument_list|,
name|destinationTableName
argument_list|)
decl_stmt|;
return|return
name|convertFromMetastore
argument_list|(
name|getTable
argument_list|(
name|destDb
argument_list|,
name|destinationTableName
argument_list|)
argument_list|,
name|partitions
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|ex
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|ex
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|ex
argument_list|)
throw|;
block|}
block|}
comment|/**    * Creates a metastore client. Currently it creates only JDBC based client as    * File based store support is removed    *    * @returns a Meta Store Client    * @throws HiveMetaException    *           if a working client can't be created    */
specifier|private
name|IMetaStoreClient
name|createMetaStoreClient
parameter_list|(
name|boolean
name|allowEmbedded
parameter_list|)
throws|throws
name|MetaException
block|{
name|HiveMetaHookLoader
name|hookLoader
init|=
operator|new
name|HiveMetaHookLoader
argument_list|()
block|{
annotation|@
name|Override
specifier|public
name|HiveMetaHook
name|getHook
parameter_list|(
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Table
name|tbl
parameter_list|)
throws|throws
name|MetaException
block|{
try|try
block|{
if|if
condition|(
name|tbl
operator|==
literal|null
condition|)
block|{
return|return
literal|null
return|;
block|}
name|HiveStorageHandler
name|storageHandler
init|=
name|HiveUtils
operator|.
name|getStorageHandler
argument_list|(
name|conf
argument_list|,
name|tbl
operator|.
name|getParameters
argument_list|()
operator|.
name|get
argument_list|(
name|META_TABLE_STORAGE
argument_list|)
argument_list|)
decl_stmt|;
if|if
condition|(
name|storageHandler
operator|==
literal|null
condition|)
block|{
return|return
literal|null
return|;
block|}
return|return
name|storageHandler
operator|.
name|getMetaHook
argument_list|()
return|;
block|}
catch|catch
parameter_list|(
name|HiveException
name|ex
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|ex
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|MetaException
argument_list|(
literal|"Failed to load storage handler:  "
operator|+
name|ex
operator|.
name|getMessage
argument_list|()
argument_list|)
throw|;
block|}
block|}
block|}
decl_stmt|;
if|if
condition|(
name|conf
operator|.
name|getBoolVar
argument_list|(
name|ConfVars
operator|.
name|METASTORE_FASTPATH
argument_list|)
condition|)
block|{
return|return
operator|new
name|SessionHiveMetaStoreClient
argument_list|(
name|conf
argument_list|,
name|hookLoader
argument_list|,
name|allowEmbedded
argument_list|)
return|;
block|}
else|else
block|{
return|return
name|RetryingMetaStoreClient
operator|.
name|getProxy
argument_list|(
name|conf
argument_list|,
name|hookLoader
argument_list|,
name|metaCallTimeMap
argument_list|,
name|SessionHiveMetaStoreClient
operator|.
name|class
operator|.
name|getName
argument_list|()
argument_list|,
name|allowEmbedded
argument_list|)
return|;
block|}
block|}
specifier|public
specifier|static
class|class
name|SchemaException
extends|extends
name|MetaException
block|{
specifier|private
specifier|static
specifier|final
name|long
name|serialVersionUID
init|=
literal|1L
decl_stmt|;
specifier|public
name|SchemaException
parameter_list|(
name|String
name|message
parameter_list|)
block|{
name|super
argument_list|(
name|message
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * @return synchronized metastore client    * @throws MetaException    */
annotation|@
name|LimitedPrivate
argument_list|(
name|value
operator|=
block|{
literal|"Hive"
block|}
argument_list|)
annotation|@
name|Unstable
specifier|public
specifier|synchronized
name|SynchronizedMetaStoreClient
name|getSychronizedMSC
parameter_list|()
throws|throws
name|MetaException
block|{
if|if
condition|(
name|syncMetaStoreClient
operator|==
literal|null
condition|)
block|{
name|syncMetaStoreClient
operator|=
operator|new
name|SynchronizedMetaStoreClient
argument_list|(
name|getMSC
argument_list|(
literal|true
argument_list|,
literal|false
argument_list|)
argument_list|)
expr_stmt|;
block|}
return|return
name|syncMetaStoreClient
return|;
block|}
comment|/**    * @return the metastore client for the current thread    * @throws MetaException    */
annotation|@
name|LimitedPrivate
argument_list|(
name|value
operator|=
block|{
literal|"Hive"
block|}
argument_list|)
annotation|@
name|Unstable
specifier|public
specifier|synchronized
name|IMetaStoreClient
name|getMSC
parameter_list|()
throws|throws
name|MetaException
block|{
return|return
name|getMSC
argument_list|(
literal|true
argument_list|,
literal|false
argument_list|)
return|;
block|}
comment|/**    * @return the metastore client for the current thread    * @throws MetaException    */
annotation|@
name|LimitedPrivate
argument_list|(
name|value
operator|=
block|{
literal|"Hive"
block|}
argument_list|)
annotation|@
name|Unstable
specifier|public
specifier|synchronized
name|IMetaStoreClient
name|getMSC
parameter_list|(
name|boolean
name|allowEmbedded
parameter_list|,
name|boolean
name|forceCreate
parameter_list|)
throws|throws
name|MetaException
block|{
if|if
condition|(
name|metaStoreClient
operator|==
literal|null
operator|||
name|forceCreate
condition|)
block|{
try|try
block|{
name|owner
operator|=
name|UserGroupInformation
operator|.
name|getCurrentUser
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|String
name|msg
init|=
literal|"Error getting current user: "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
decl_stmt|;
name|LOG
operator|.
name|error
argument_list|(
name|msg
argument_list|,
name|e
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|MetaException
argument_list|(
name|msg
operator|+
literal|"\n"
operator|+
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
throw|;
block|}
try|try
block|{
name|metaStoreClient
operator|=
name|createMetaStoreClient
argument_list|(
name|allowEmbedded
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|RuntimeException
name|ex
parameter_list|)
block|{
name|Throwable
name|t
init|=
name|ex
operator|.
name|getCause
argument_list|()
decl_stmt|;
while|while
condition|(
name|t
operator|!=
literal|null
condition|)
block|{
if|if
condition|(
name|t
operator|instanceof
name|JDODataStoreException
operator|&&
name|t
operator|.
name|getMessage
argument_list|()
operator|!=
literal|null
operator|&&
name|t
operator|.
name|getMessage
argument_list|()
operator|.
name|contains
argument_list|(
literal|"autoCreate"
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Cannot initialize metastore due to autoCreate error"
argument_list|,
name|t
argument_list|)
expr_stmt|;
comment|// DataNucleus wants us to auto-create, but we shall do no such thing.
throw|throw
operator|new
name|SchemaException
argument_list|(
literal|"Hive metastore database is not initialized. Please use "
operator|+
literal|"schematool (e.g. ./schematool -initSchema -dbType ...) to create the schema. If "
operator|+
literal|"needed, don't forget to include the option to auto-create the underlying database"
operator|+
literal|" in your JDBC connection string (e.g. ?createDatabaseIfNotExist=true for mysql)"
argument_list|)
throw|;
block|}
name|t
operator|=
name|t
operator|.
name|getCause
argument_list|()
expr_stmt|;
block|}
throw|throw
name|ex
throw|;
block|}
name|String
name|metaStoreUris
init|=
name|conf
operator|.
name|getVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|METASTOREURIS
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|lang3
operator|.
name|StringUtils
operator|.
name|isEmpty
argument_list|(
name|metaStoreUris
argument_list|)
condition|)
block|{
comment|// get a synchronized wrapper if the meta store is remote.
name|metaStoreClient
operator|=
name|HiveMetaStoreClient
operator|.
name|newSynchronizedClient
argument_list|(
name|metaStoreClient
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|metaStoreClient
return|;
block|}
specifier|private
name|String
name|getUserName
parameter_list|()
block|{
return|return
name|SessionState
operator|.
name|getUserFromAuthenticator
argument_list|()
return|;
block|}
specifier|private
name|List
argument_list|<
name|String
argument_list|>
name|getGroupNames
parameter_list|()
block|{
name|SessionState
name|ss
init|=
name|SessionState
operator|.
name|get
argument_list|()
decl_stmt|;
if|if
condition|(
name|ss
operator|!=
literal|null
operator|&&
name|ss
operator|.
name|getAuthenticator
argument_list|()
operator|!=
literal|null
condition|)
block|{
return|return
name|ss
operator|.
name|getAuthenticator
argument_list|()
operator|.
name|getGroupNames
argument_list|()
return|;
block|}
return|return
literal|null
return|;
block|}
specifier|public
specifier|static
name|List
argument_list|<
name|FieldSchema
argument_list|>
name|getFieldsFromDeserializer
parameter_list|(
name|String
name|name
parameter_list|,
name|Deserializer
name|serde
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|MetaStoreUtils
operator|.
name|getFieldsFromDeserializer
argument_list|(
name|name
argument_list|,
name|serde
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|SerDeException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Error in getting fields from serde. "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
catch|catch
parameter_list|(
name|MetaException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Error in getting fields from serde."
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|List
argument_list|<
name|Index
argument_list|>
name|getIndexes
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|tblName
parameter_list|,
name|short
name|max
parameter_list|)
throws|throws
name|HiveException
block|{
name|List
argument_list|<
name|Index
argument_list|>
name|indexes
init|=
literal|null
decl_stmt|;
try|try
block|{
name|indexes
operator|=
name|getMSC
argument_list|()
operator|.
name|listIndexes
argument_list|(
name|dbName
argument_list|,
name|tblName
argument_list|,
name|max
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
return|return
name|indexes
return|;
block|}
specifier|public
name|boolean
name|setPartitionColumnStatistics
parameter_list|(
name|SetPartitionsStatsRequest
name|request
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|setPartitionColumnStatistics
argument_list|(
name|request
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|List
argument_list|<
name|ColumnStatisticsObj
argument_list|>
name|getTableColumnStatistics
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|tableName
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|colNames
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|getTableColumnStatistics
argument_list|(
name|dbName
argument_list|,
name|tableName
argument_list|,
name|colNames
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|Map
argument_list|<
name|String
argument_list|,
name|List
argument_list|<
name|ColumnStatisticsObj
argument_list|>
argument_list|>
name|getPartitionColumnStatistics
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|tableName
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|partNames
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|colNames
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|getPartitionColumnStatistics
argument_list|(
name|dbName
argument_list|,
name|tableName
argument_list|,
name|partNames
argument_list|,
name|colNames
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|AggrStats
name|getAggrColStatsFor
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|tblName
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|colNames
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|partName
parameter_list|)
block|{
try|try
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|getAggrColStatsFor
argument_list|(
name|dbName
argument_list|,
name|tblName
argument_list|,
name|colNames
argument_list|,
name|partName
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
return|return
operator|new
name|AggrStats
argument_list|(
operator|new
name|ArrayList
argument_list|<
name|ColumnStatisticsObj
argument_list|>
argument_list|()
argument_list|,
literal|0
argument_list|)
return|;
block|}
block|}
specifier|public
name|boolean
name|deleteTableColumnStatistics
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|tableName
parameter_list|,
name|String
name|colName
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|deleteTableColumnStatistics
argument_list|(
name|dbName
argument_list|,
name|tableName
argument_list|,
name|colName
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|boolean
name|deletePartitionColumnStatistics
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|tableName
parameter_list|,
name|String
name|partName
parameter_list|,
name|String
name|colName
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|deletePartitionColumnStatistics
argument_list|(
name|dbName
argument_list|,
name|tableName
argument_list|,
name|partName
argument_list|,
name|colName
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|Table
name|newTable
parameter_list|(
name|String
name|tableName
parameter_list|)
throws|throws
name|HiveException
block|{
name|String
index|[]
name|names
init|=
name|Utilities
operator|.
name|getDbTableName
argument_list|(
name|tableName
argument_list|)
decl_stmt|;
return|return
operator|new
name|Table
argument_list|(
name|names
index|[
literal|0
index|]
argument_list|,
name|names
index|[
literal|1
index|]
argument_list|)
return|;
block|}
specifier|public
name|String
name|getDelegationToken
parameter_list|(
name|String
name|owner
parameter_list|,
name|String
name|renewer
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|getDelegationToken
argument_list|(
name|owner
argument_list|,
name|renewer
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|void
name|cancelDelegationToken
parameter_list|(
name|String
name|tokenStrForm
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|getMSC
argument_list|()
operator|.
name|cancelDelegationToken
argument_list|(
name|tokenStrForm
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    * @deprecated use {@link #compact2(String, String, String, String, Map)}    */
specifier|public
name|void
name|compact
parameter_list|(
name|String
name|dbname
parameter_list|,
name|String
name|tableName
parameter_list|,
name|String
name|partName
parameter_list|,
name|String
name|compactType
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|tblproperties
parameter_list|)
throws|throws
name|HiveException
block|{
name|compact2
argument_list|(
name|dbname
argument_list|,
name|tableName
argument_list|,
name|partName
argument_list|,
name|compactType
argument_list|,
name|tblproperties
argument_list|)
expr_stmt|;
block|}
comment|/**    * Enqueue a compaction request.  Only 1 compaction for a given resource (db/table/partSpec) can    * be scheduled/running at any given time.    * @param dbname name of the database, if null default will be used.    * @param tableName name of the table, cannot be null    * @param partName name of the partition, if null table will be compacted (valid only for    *                 non-partitioned tables).    * @param compactType major or minor    * @param tblproperties the list of tblproperties to overwrite for this compaction    * @return id of new request or id already existing request for specified resource    * @throws HiveException    */
specifier|public
name|CompactionResponse
name|compact2
parameter_list|(
name|String
name|dbname
parameter_list|,
name|String
name|tableName
parameter_list|,
name|String
name|partName
parameter_list|,
name|String
name|compactType
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|tblproperties
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|CompactionType
name|cr
init|=
literal|null
decl_stmt|;
if|if
condition|(
literal|"major"
operator|.
name|equals
argument_list|(
name|compactType
argument_list|)
condition|)
name|cr
operator|=
name|CompactionType
operator|.
name|MAJOR
expr_stmt|;
elseif|else
if|if
condition|(
literal|"minor"
operator|.
name|equals
argument_list|(
name|compactType
argument_list|)
condition|)
name|cr
operator|=
name|CompactionType
operator|.
name|MINOR
expr_stmt|;
else|else
throw|throw
operator|new
name|RuntimeException
argument_list|(
literal|"Unknown compaction type "
operator|+
name|compactType
argument_list|)
throw|;
return|return
name|getMSC
argument_list|()
operator|.
name|compact2
argument_list|(
name|dbname
argument_list|,
name|tableName
argument_list|,
name|partName
argument_list|,
name|cr
argument_list|,
name|tblproperties
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|ShowCompactResponse
name|showCompactions
parameter_list|()
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|showCompactions
argument_list|()
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|GetOpenTxnsInfoResponse
name|showTransactions
parameter_list|()
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|showTxns
argument_list|()
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|void
name|abortTransactions
parameter_list|(
name|List
argument_list|<
name|Long
argument_list|>
name|txnids
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|getMSC
argument_list|()
operator|.
name|abortTxns
argument_list|(
name|txnids
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|void
name|createFunction
parameter_list|(
name|Function
name|func
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|getMSC
argument_list|()
operator|.
name|createFunction
argument_list|(
name|func
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|TException
name|te
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|te
argument_list|)
throw|;
block|}
block|}
specifier|public
name|void
name|alterFunction
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|funcName
parameter_list|,
name|Function
name|newFunction
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|getMSC
argument_list|()
operator|.
name|alterFunction
argument_list|(
name|dbName
argument_list|,
name|funcName
argument_list|,
name|newFunction
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|TException
name|te
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|te
argument_list|)
throw|;
block|}
block|}
specifier|public
name|void
name|dropFunction
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|funcName
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|getMSC
argument_list|()
operator|.
name|dropFunction
argument_list|(
name|dbName
argument_list|,
name|funcName
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|TException
name|te
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|te
argument_list|)
throw|;
block|}
block|}
specifier|public
name|Function
name|getFunction
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|funcName
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|getFunction
argument_list|(
name|dbName
argument_list|,
name|funcName
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|TException
name|te
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|te
argument_list|)
throw|;
block|}
block|}
specifier|public
name|List
argument_list|<
name|Function
argument_list|>
name|getAllFunctions
parameter_list|()
throws|throws
name|HiveException
block|{
try|try
block|{
name|List
argument_list|<
name|Function
argument_list|>
name|functions
init|=
name|getMSC
argument_list|()
operator|.
name|getAllFunctions
argument_list|()
operator|.
name|getFunctions
argument_list|()
decl_stmt|;
return|return
name|functions
operator|==
literal|null
condition|?
operator|new
name|ArrayList
argument_list|<
name|Function
argument_list|>
argument_list|()
else|:
name|functions
return|;
block|}
catch|catch
parameter_list|(
name|TException
name|te
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|te
argument_list|)
throw|;
block|}
block|}
specifier|public
name|List
argument_list|<
name|String
argument_list|>
name|getFunctions
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|pattern
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|getFunctions
argument_list|(
name|dbName
argument_list|,
name|pattern
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|TException
name|te
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|te
argument_list|)
throw|;
block|}
block|}
specifier|public
name|void
name|setMetaConf
parameter_list|(
name|String
name|propName
parameter_list|,
name|String
name|propValue
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|getMSC
argument_list|()
operator|.
name|setMetaConf
argument_list|(
name|propName
argument_list|,
name|propValue
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|TException
name|te
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|te
argument_list|)
throw|;
block|}
block|}
specifier|public
name|String
name|getMetaConf
parameter_list|(
name|String
name|propName
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|getMetaConf
argument_list|(
name|propName
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|TException
name|te
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|te
argument_list|)
throw|;
block|}
block|}
specifier|public
name|void
name|clearMetaCallTiming
parameter_list|()
block|{
name|metaCallTimeMap
operator|.
name|clear
argument_list|()
expr_stmt|;
block|}
specifier|public
name|ImmutableMap
argument_list|<
name|String
argument_list|,
name|Long
argument_list|>
name|dumpAndClearMetaCallTiming
parameter_list|(
name|String
name|phase
parameter_list|)
block|{
name|boolean
name|phaseInfoLogged
init|=
literal|false
decl_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|phaseInfoLogged
operator|=
name|logDumpPhase
argument_list|(
name|phase
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Total time spent in each metastore function (ms): "
operator|+
name|metaCallTimeMap
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|LOG
operator|.
name|isInfoEnabled
argument_list|()
condition|)
block|{
comment|// print information about calls that took longer time at INFO level
for|for
control|(
name|Entry
argument_list|<
name|String
argument_list|,
name|Long
argument_list|>
name|callTime
range|:
name|metaCallTimeMap
operator|.
name|entrySet
argument_list|()
control|)
block|{
comment|// dump information if call took more than 1 sec (1000ms)
if|if
condition|(
name|callTime
operator|.
name|getValue
argument_list|()
operator|>
literal|1000
condition|)
block|{
if|if
condition|(
operator|!
name|phaseInfoLogged
condition|)
block|{
name|phaseInfoLogged
operator|=
name|logDumpPhase
argument_list|(
name|phase
argument_list|)
expr_stmt|;
block|}
name|LOG
operator|.
name|info
argument_list|(
literal|"Total time spent in this metastore function was greater than 1000ms : "
operator|+
name|callTime
argument_list|)
expr_stmt|;
block|}
block|}
block|}
name|ImmutableMap
argument_list|<
name|String
argument_list|,
name|Long
argument_list|>
name|result
init|=
name|ImmutableMap
operator|.
name|copyOf
argument_list|(
name|metaCallTimeMap
argument_list|)
decl_stmt|;
name|metaCallTimeMap
operator|.
name|clear
argument_list|()
expr_stmt|;
return|return
name|result
return|;
block|}
specifier|private
name|boolean
name|logDumpPhase
parameter_list|(
name|String
name|phase
parameter_list|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Dumping metastore api call timing information for : "
operator|+
name|phase
operator|+
literal|" phase"
argument_list|)
expr_stmt|;
return|return
literal|true
return|;
block|}
specifier|public
name|Iterable
argument_list|<
name|Map
operator|.
name|Entry
argument_list|<
name|Long
argument_list|,
name|ByteBuffer
argument_list|>
argument_list|>
name|getFileMetadata
parameter_list|(
name|List
argument_list|<
name|Long
argument_list|>
name|fileIds
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|getFileMetadata
argument_list|(
name|fileIds
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|TException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|Iterable
argument_list|<
name|Map
operator|.
name|Entry
argument_list|<
name|Long
argument_list|,
name|MetadataPpdResult
argument_list|>
argument_list|>
name|getFileMetadataByExpr
parameter_list|(
name|List
argument_list|<
name|Long
argument_list|>
name|fileIds
parameter_list|,
name|ByteBuffer
name|sarg
parameter_list|,
name|boolean
name|doGetFooters
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|getFileMetadataBySarg
argument_list|(
name|fileIds
argument_list|,
name|sarg
argument_list|,
name|doGetFooters
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|TException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|void
name|clearFileMetadata
parameter_list|(
name|List
argument_list|<
name|Long
argument_list|>
name|fileIds
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|getMSC
argument_list|()
operator|.
name|clearFileMetadata
argument_list|(
name|fileIds
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|TException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|void
name|putFileMetadata
parameter_list|(
name|List
argument_list|<
name|Long
argument_list|>
name|fileIds
parameter_list|,
name|List
argument_list|<
name|ByteBuffer
argument_list|>
name|metadata
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|getMSC
argument_list|()
operator|.
name|putFileMetadata
argument_list|(
name|fileIds
argument_list|,
name|metadata
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|TException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|void
name|cacheFileMetadata
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|tableName
parameter_list|,
name|String
name|partName
parameter_list|,
name|boolean
name|allParts
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|boolean
name|willCache
init|=
name|getMSC
argument_list|()
operator|.
name|cacheFileMetadata
argument_list|(
name|dbName
argument_list|,
name|tableName
argument_list|,
name|partName
argument_list|,
name|allParts
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|willCache
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Caching file metadata is not supported by metastore or for this file format"
argument_list|)
throw|;
block|}
block|}
catch|catch
parameter_list|(
name|TException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|void
name|dropConstraint
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|tableName
parameter_list|,
name|String
name|constraintName
parameter_list|)
throws|throws
name|HiveException
throws|,
name|NoSuchObjectException
block|{
try|try
block|{
name|getMSC
argument_list|()
operator|.
name|dropConstraint
argument_list|(
name|dbName
argument_list|,
name|tableName
argument_list|,
name|constraintName
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    * Get all primary key columns associated with the table.    *    * @param dbName Database Name    * @param tblName Table Name    * @return Primary Key associated with the table.    * @throws HiveException    */
specifier|public
name|PrimaryKeyInfo
name|getPrimaryKeys
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|tblName
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|List
argument_list|<
name|SQLPrimaryKey
argument_list|>
name|primaryKeys
init|=
name|getMSC
argument_list|()
operator|.
name|getPrimaryKeys
argument_list|(
operator|new
name|PrimaryKeysRequest
argument_list|(
name|dbName
argument_list|,
name|tblName
argument_list|)
argument_list|)
decl_stmt|;
return|return
operator|new
name|PrimaryKeyInfo
argument_list|(
name|primaryKeys
argument_list|,
name|tblName
argument_list|,
name|dbName
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    * Get all foreign keys associated with the table.    *    * @param dbName Database Name    * @param tblName Table Name    * @return Foreign keys associated with the table.    * @throws HiveException    */
specifier|public
name|ForeignKeyInfo
name|getForeignKeys
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|tblName
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|List
argument_list|<
name|SQLForeignKey
argument_list|>
name|foreignKeys
init|=
name|getMSC
argument_list|()
operator|.
name|getForeignKeys
argument_list|(
operator|new
name|ForeignKeysRequest
argument_list|(
literal|null
argument_list|,
literal|null
argument_list|,
name|dbName
argument_list|,
name|tblName
argument_list|)
argument_list|)
decl_stmt|;
return|return
operator|new
name|ForeignKeyInfo
argument_list|(
name|foreignKeys
argument_list|,
name|tblName
argument_list|,
name|dbName
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    * Get all unique constraints associated with the table.    *    * @param dbName Database Name    * @param tblName Table Name    * @return Unique constraints associated with the table.    * @throws HiveException    */
specifier|public
name|UniqueConstraint
name|getUniqueConstraints
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|tblName
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|List
argument_list|<
name|SQLUniqueConstraint
argument_list|>
name|uniqueConstraints
init|=
name|getMSC
argument_list|()
operator|.
name|getUniqueConstraints
argument_list|(
operator|new
name|UniqueConstraintsRequest
argument_list|(
name|dbName
argument_list|,
name|tblName
argument_list|)
argument_list|)
decl_stmt|;
return|return
operator|new
name|UniqueConstraint
argument_list|(
name|uniqueConstraints
argument_list|,
name|tblName
argument_list|,
name|dbName
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    * Get all not null constraints associated with the table.    *    * @param dbName Database Name    * @param tblName Table Name    * @return Not null constraints associated with the table.    * @throws HiveException    */
specifier|public
name|NotNullConstraint
name|getNotNullConstraints
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|tblName
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|List
argument_list|<
name|SQLNotNullConstraint
argument_list|>
name|notNullConstraints
init|=
name|getMSC
argument_list|()
operator|.
name|getNotNullConstraints
argument_list|(
operator|new
name|NotNullConstraintsRequest
argument_list|(
name|dbName
argument_list|,
name|tblName
argument_list|)
argument_list|)
decl_stmt|;
return|return
operator|new
name|NotNullConstraint
argument_list|(
name|notNullConstraints
argument_list|,
name|tblName
argument_list|,
name|dbName
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|void
name|addPrimaryKey
parameter_list|(
name|List
argument_list|<
name|SQLPrimaryKey
argument_list|>
name|primaryKeyCols
parameter_list|)
throws|throws
name|HiveException
throws|,
name|NoSuchObjectException
block|{
try|try
block|{
name|getMSC
argument_list|()
operator|.
name|addPrimaryKey
argument_list|(
name|primaryKeyCols
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|void
name|addForeignKey
parameter_list|(
name|List
argument_list|<
name|SQLForeignKey
argument_list|>
name|foreignKeyCols
parameter_list|)
throws|throws
name|HiveException
throws|,
name|NoSuchObjectException
block|{
try|try
block|{
name|getMSC
argument_list|()
operator|.
name|addForeignKey
argument_list|(
name|foreignKeyCols
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|void
name|addUniqueConstraint
parameter_list|(
name|List
argument_list|<
name|SQLUniqueConstraint
argument_list|>
name|uniqueConstraintCols
parameter_list|)
throws|throws
name|HiveException
throws|,
name|NoSuchObjectException
block|{
try|try
block|{
name|getMSC
argument_list|()
operator|.
name|addUniqueConstraint
argument_list|(
name|uniqueConstraintCols
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|void
name|addNotNullConstraint
parameter_list|(
name|List
argument_list|<
name|SQLNotNullConstraint
argument_list|>
name|notNullConstraintCols
parameter_list|)
throws|throws
name|HiveException
throws|,
name|NoSuchObjectException
block|{
try|try
block|{
name|getMSC
argument_list|()
operator|.
name|addNotNullConstraint
argument_list|(
name|notNullConstraintCols
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
block|}
end_class

begin_empty_stmt
empty_stmt|;
end_empty_stmt

end_unit

