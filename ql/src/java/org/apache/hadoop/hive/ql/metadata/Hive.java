begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/**  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
package|;
end_package

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|hive_metastoreConstants
operator|.
name|META_TABLE_STORAGE
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde
operator|.
name|serdeConstants
operator|.
name|COLLECTION_DELIM
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde
operator|.
name|serdeConstants
operator|.
name|ESCAPE_CHAR
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde
operator|.
name|serdeConstants
operator|.
name|FIELD_DELIM
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde
operator|.
name|serdeConstants
operator|.
name|LINE_DELIM
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde
operator|.
name|serdeConstants
operator|.
name|MAPKEY_DELIM
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde
operator|.
name|serdeConstants
operator|.
name|SERIALIZATION_FORMAT
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde
operator|.
name|serdeConstants
operator|.
name|STRING_TYPE_NAME
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|FileNotFoundException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Arrays
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashSet
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Iterator
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|LinkedHashMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|LinkedHashSet
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|LinkedList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
operator|.
name|Entry
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Set
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|Log
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|LogFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configuration
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileStatus
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|FileUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|HiveStatsUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|ObjectPair
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|StatsSetupConst
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|classification
operator|.
name|InterfaceAudience
operator|.
name|LimitedPrivate
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|classification
operator|.
name|InterfaceStability
operator|.
name|Unstable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
operator|.
name|ConfVars
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|HiveMetaException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|HiveMetaHook
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|HiveMetaHookLoader
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|IMetaStoreClient
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|MetaStoreUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|PartitionDropOptions
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|RetryingMetaStoreClient
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|TableType
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|Warehouse
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|AggrStats
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|AlreadyExistsException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|ColumnStatistics
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|ColumnStatisticsObj
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|CompactionType
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Database
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|FieldSchema
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|FireEventRequest
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|FireEventRequestData
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Function
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|GetOpenTxnsInfoResponse
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|GetRoleGrantsForPrincipalRequest
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|GetRoleGrantsForPrincipalResponse
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|HiveObjectPrivilege
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|HiveObjectRef
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|HiveObjectType
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Index
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|InsertEventRequestData
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|InvalidOperationException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|MetaException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|NoSuchObjectException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Order
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|PrincipalPrivilegeSet
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|PrincipalType
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|PrivilegeBag
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Role
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|RolePrincipalGrant
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|SerDeInfo
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|SetPartitionsStatsRequest
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|ShowCompactResponse
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|SkewedInfo
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|StorageDescriptor
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|hive_metastoreConstants
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|ErrorMsg
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|FunctionRegistry
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|FunctionTask
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|FunctionUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|Utilities
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|index
operator|.
name|HiveIndexHandler
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|AcidUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|optimizer
operator|.
name|listbucketingpruner
operator|.
name|ListBucketingPrunerUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|AddPartitionDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|DropTableDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|ExprNodeGenericFuncDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|session
operator|.
name|CreateTableAutomaticGrant
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|session
operator|.
name|SessionState
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|Deserializer
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|SerDeException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|lazy
operator|.
name|LazySimpleSerDe
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|shims
operator|.
name|HadoopShims
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|shims
operator|.
name|ShimLoader
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|InputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|security
operator|.
name|UserGroupInformation
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|StringUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|thrift
operator|.
name|TException
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|collect
operator|.
name|Sets
import|;
end_import

begin_comment
comment|/**  * This class has functions that implement meta data/DDL operations using calls  * to the metastore.  * It has a metastore client instance it uses to communicate with the metastore.  *  * It is a thread local variable, and the instances is accessed using static  * get methods in this class.  */
end_comment

begin_class
annotation|@
name|SuppressWarnings
argument_list|(
block|{
literal|"deprecation"
block|,
literal|"rawtypes"
block|}
argument_list|)
specifier|public
class|class
name|Hive
block|{
specifier|static
specifier|final
specifier|private
name|Log
name|LOG
init|=
name|LogFactory
operator|.
name|getLog
argument_list|(
literal|"hive.ql.metadata.Hive"
argument_list|)
decl_stmt|;
specifier|private
name|HiveConf
name|conf
init|=
literal|null
decl_stmt|;
specifier|private
name|IMetaStoreClient
name|metaStoreClient
decl_stmt|;
specifier|private
name|UserGroupInformation
name|owner
decl_stmt|;
comment|// metastore calls timing information
specifier|private
specifier|final
name|Map
argument_list|<
name|String
argument_list|,
name|Long
argument_list|>
name|metaCallTimeMap
init|=
operator|new
name|HashMap
argument_list|<
name|String
argument_list|,
name|Long
argument_list|>
argument_list|()
decl_stmt|;
specifier|private
specifier|static
name|ThreadLocal
argument_list|<
name|Hive
argument_list|>
name|hiveDB
init|=
operator|new
name|ThreadLocal
argument_list|<
name|Hive
argument_list|>
argument_list|()
block|{
annotation|@
name|Override
specifier|protected
specifier|synchronized
name|Hive
name|initialValue
parameter_list|()
block|{
return|return
literal|null
return|;
block|}
annotation|@
name|Override
specifier|public
specifier|synchronized
name|void
name|remove
parameter_list|()
block|{
if|if
condition|(
name|this
operator|.
name|get
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|this
operator|.
name|get
argument_list|()
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
name|super
operator|.
name|remove
argument_list|()
expr_stmt|;
block|}
block|}
decl_stmt|;
comment|// register all permanent functions. need improvement
static|static
block|{
try|try
block|{
name|reloadFunctions
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Failed to access metastore. This class should not accessed in runtime."
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
block|}
specifier|public
specifier|static
name|void
name|reloadFunctions
parameter_list|()
throws|throws
name|HiveException
block|{
name|Hive
name|db
init|=
name|Hive
operator|.
name|get
argument_list|()
decl_stmt|;
for|for
control|(
name|String
name|dbName
range|:
name|db
operator|.
name|getAllDatabases
argument_list|()
control|)
block|{
for|for
control|(
name|String
name|functionName
range|:
name|db
operator|.
name|getFunctions
argument_list|(
name|dbName
argument_list|,
literal|"*"
argument_list|)
control|)
block|{
name|Function
name|function
init|=
name|db
operator|.
name|getFunction
argument_list|(
name|dbName
argument_list|,
name|functionName
argument_list|)
decl_stmt|;
try|try
block|{
name|FunctionRegistry
operator|.
name|registerPermanentFunction
argument_list|(
name|FunctionUtils
operator|.
name|qualifyFunctionName
argument_list|(
name|functionName
argument_list|,
name|dbName
argument_list|)
argument_list|,
name|function
operator|.
name|getClassName
argument_list|()
argument_list|,
literal|false
argument_list|,
name|FunctionTask
operator|.
name|toFunctionResource
argument_list|(
name|function
operator|.
name|getResourceUris
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Failed to register persistent function "
operator|+
name|functionName
operator|+
literal|":"
operator|+
name|function
operator|.
name|getClassName
argument_list|()
operator|+
literal|". Ignore and continue."
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
specifier|public
specifier|static
name|Hive
name|get
parameter_list|(
name|Configuration
name|c
parameter_list|,
name|Class
argument_list|<
name|?
argument_list|>
name|clazz
parameter_list|)
throws|throws
name|HiveException
block|{
return|return
name|get
argument_list|(
name|c
operator|instanceof
name|HiveConf
condition|?
operator|(
name|HiveConf
operator|)
name|c
else|:
operator|new
name|HiveConf
argument_list|(
name|c
argument_list|,
name|clazz
argument_list|)
argument_list|)
return|;
block|}
comment|/**    * Gets hive object for the current thread. If one is not initialized then a    * new one is created If the new configuration is different in metadata conf    * vars then a new one is created.    *    * @param c    *          new Hive Configuration    * @return Hive object for current thread    * @throws HiveException    *    */
specifier|public
specifier|static
name|Hive
name|get
parameter_list|(
name|HiveConf
name|c
parameter_list|)
throws|throws
name|HiveException
block|{
name|Hive
name|db
init|=
name|hiveDB
operator|.
name|get
argument_list|()
decl_stmt|;
if|if
condition|(
name|db
operator|==
literal|null
operator|||
operator|(
name|db
operator|.
name|metaStoreClient
operator|!=
literal|null
operator|&&
operator|!
name|db
operator|.
name|metaStoreClient
operator|.
name|isCompatibleWith
argument_list|(
name|c
argument_list|)
operator|)
condition|)
block|{
return|return
name|get
argument_list|(
name|c
argument_list|,
literal|true
argument_list|)
return|;
block|}
name|db
operator|.
name|conf
operator|=
name|c
expr_stmt|;
return|return
name|db
return|;
block|}
comment|/**    * get a connection to metastore. see get(HiveConf) function for comments    *    * @param c    *          new conf    * @param needsRefresh    *          if true then creates a new one    * @return The connection to the metastore    * @throws HiveException    */
specifier|public
specifier|static
name|Hive
name|get
parameter_list|(
name|HiveConf
name|c
parameter_list|,
name|boolean
name|needsRefresh
parameter_list|)
throws|throws
name|HiveException
block|{
name|Hive
name|db
init|=
name|hiveDB
operator|.
name|get
argument_list|()
decl_stmt|;
if|if
condition|(
name|db
operator|==
literal|null
operator|||
name|needsRefresh
operator|||
operator|!
name|db
operator|.
name|isCurrentUserOwner
argument_list|()
condition|)
block|{
if|if
condition|(
name|db
operator|!=
literal|null
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Creating new db. db = "
operator|+
name|db
operator|+
literal|", needsRefresh = "
operator|+
name|needsRefresh
operator|+
literal|", db.isCurrentUserOwner = "
operator|+
name|db
operator|.
name|isCurrentUserOwner
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|closeCurrent
argument_list|()
expr_stmt|;
name|c
operator|.
name|set
argument_list|(
literal|"fs.scheme.class"
argument_list|,
literal|"dfs"
argument_list|)
expr_stmt|;
name|Hive
name|newdb
init|=
operator|new
name|Hive
argument_list|(
name|c
argument_list|)
decl_stmt|;
name|hiveDB
operator|.
name|set
argument_list|(
name|newdb
argument_list|)
expr_stmt|;
return|return
name|newdb
return|;
block|}
name|db
operator|.
name|conf
operator|=
name|c
expr_stmt|;
return|return
name|db
return|;
block|}
specifier|public
specifier|static
name|Hive
name|get
parameter_list|()
throws|throws
name|HiveException
block|{
name|Hive
name|db
init|=
name|hiveDB
operator|.
name|get
argument_list|()
decl_stmt|;
if|if
condition|(
name|db
operator|!=
literal|null
operator|&&
operator|!
name|db
operator|.
name|isCurrentUserOwner
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Creating new db. db.isCurrentUserOwner = "
operator|+
name|db
operator|.
name|isCurrentUserOwner
argument_list|()
argument_list|)
expr_stmt|;
name|db
operator|.
name|close
argument_list|()
expr_stmt|;
name|db
operator|=
literal|null
expr_stmt|;
block|}
if|if
condition|(
name|db
operator|==
literal|null
condition|)
block|{
name|SessionState
name|session
init|=
name|SessionState
operator|.
name|get
argument_list|()
decl_stmt|;
name|db
operator|=
operator|new
name|Hive
argument_list|(
name|session
operator|==
literal|null
condition|?
operator|new
name|HiveConf
argument_list|(
name|Hive
operator|.
name|class
argument_list|)
else|:
name|session
operator|.
name|getConf
argument_list|()
argument_list|)
expr_stmt|;
name|hiveDB
operator|.
name|set
argument_list|(
name|db
argument_list|)
expr_stmt|;
block|}
return|return
name|db
return|;
block|}
specifier|public
specifier|static
name|void
name|set
parameter_list|(
name|Hive
name|hive
parameter_list|)
block|{
name|hiveDB
operator|.
name|set
argument_list|(
name|hive
argument_list|)
expr_stmt|;
block|}
specifier|public
specifier|static
name|void
name|closeCurrent
parameter_list|()
block|{
name|hiveDB
operator|.
name|remove
argument_list|()
expr_stmt|;
block|}
comment|/**    * Hive    *    * @param c    *    */
specifier|private
name|Hive
parameter_list|(
name|HiveConf
name|c
parameter_list|)
throws|throws
name|HiveException
block|{
name|conf
operator|=
name|c
expr_stmt|;
block|}
specifier|private
name|boolean
name|isCurrentUserOwner
parameter_list|()
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|owner
operator|==
literal|null
operator|||
name|owner
operator|.
name|equals
argument_list|(
name|UserGroupInformation
operator|.
name|getCurrentUser
argument_list|()
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Error getting current user: "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    * closes the connection to metastore for the calling thread    */
specifier|private
name|void
name|close
parameter_list|()
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Closing current thread's connection to Hive Metastore."
argument_list|)
expr_stmt|;
if|if
condition|(
name|metaStoreClient
operator|!=
literal|null
condition|)
block|{
name|metaStoreClient
operator|.
name|close
argument_list|()
expr_stmt|;
name|metaStoreClient
operator|=
literal|null
expr_stmt|;
block|}
block|}
comment|/**    * Create a database    * @param db    * @param ifNotExist if true, will ignore AlreadyExistsException exception    * @throws AlreadyExistsException    * @throws HiveException    */
specifier|public
name|void
name|createDatabase
parameter_list|(
name|Database
name|db
parameter_list|,
name|boolean
name|ifNotExist
parameter_list|)
throws|throws
name|AlreadyExistsException
throws|,
name|HiveException
block|{
try|try
block|{
name|getMSC
argument_list|()
operator|.
name|createDatabase
argument_list|(
name|db
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|AlreadyExistsException
name|e
parameter_list|)
block|{
if|if
condition|(
operator|!
name|ifNotExist
condition|)
block|{
throw|throw
name|e
throw|;
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    * Create a Database. Raise an error if a database with the same name already exists.    * @param db    * @throws AlreadyExistsException    * @throws HiveException    */
specifier|public
name|void
name|createDatabase
parameter_list|(
name|Database
name|db
parameter_list|)
throws|throws
name|AlreadyExistsException
throws|,
name|HiveException
block|{
name|createDatabase
argument_list|(
name|db
argument_list|,
literal|false
argument_list|)
expr_stmt|;
block|}
comment|/**    * Drop a database.    * @param name    * @throws NoSuchObjectException    * @throws HiveException    * @see org.apache.hadoop.hive.metastore.HiveMetaStoreClient#dropDatabase(java.lang.String)    */
specifier|public
name|void
name|dropDatabase
parameter_list|(
name|String
name|name
parameter_list|)
throws|throws
name|HiveException
throws|,
name|NoSuchObjectException
block|{
name|dropDatabase
argument_list|(
name|name
argument_list|,
literal|true
argument_list|,
literal|false
argument_list|,
literal|false
argument_list|)
expr_stmt|;
block|}
comment|/**    * Drop a database    * @param name    * @param deleteData    * @param ignoreUnknownDb if true, will ignore NoSuchObjectException    * @throws HiveException    * @throws NoSuchObjectException    */
specifier|public
name|void
name|dropDatabase
parameter_list|(
name|String
name|name
parameter_list|,
name|boolean
name|deleteData
parameter_list|,
name|boolean
name|ignoreUnknownDb
parameter_list|)
throws|throws
name|HiveException
throws|,
name|NoSuchObjectException
block|{
name|dropDatabase
argument_list|(
name|name
argument_list|,
name|deleteData
argument_list|,
name|ignoreUnknownDb
argument_list|,
literal|false
argument_list|)
expr_stmt|;
block|}
comment|/**    * Drop a database    * @param name    * @param deleteData    * @param ignoreUnknownDb if true, will ignore NoSuchObjectException    * @param cascade         if true, delete all tables on the DB if exists. Otherwise, the query    *                        will fail if table still exists.    * @throws HiveException    * @throws NoSuchObjectException    */
specifier|public
name|void
name|dropDatabase
parameter_list|(
name|String
name|name
parameter_list|,
name|boolean
name|deleteData
parameter_list|,
name|boolean
name|ignoreUnknownDb
parameter_list|,
name|boolean
name|cascade
parameter_list|)
throws|throws
name|HiveException
throws|,
name|NoSuchObjectException
block|{
try|try
block|{
name|getMSC
argument_list|()
operator|.
name|dropDatabase
argument_list|(
name|name
argument_list|,
name|deleteData
argument_list|,
name|ignoreUnknownDb
argument_list|,
name|cascade
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|NoSuchObjectException
name|e
parameter_list|)
block|{
throw|throw
name|e
throw|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    * Creates a table metadata and the directory for the table data    *    * @param tableName    *          name of the table    * @param columns    *          list of fields of the table    * @param partCols    *          partition keys of the table    * @param fileInputFormat    *          Class of the input format of the table data file    * @param fileOutputFormat    *          Class of the output format of the table data file    * @throws HiveException    *           thrown if the args are invalid or if the metadata or the data    *           directory couldn't be created    */
specifier|public
name|void
name|createTable
parameter_list|(
name|String
name|tableName
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|columns
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|partCols
parameter_list|,
name|Class
argument_list|<
name|?
extends|extends
name|InputFormat
argument_list|>
name|fileInputFormat
parameter_list|,
name|Class
argument_list|<
name|?
argument_list|>
name|fileOutputFormat
parameter_list|)
throws|throws
name|HiveException
block|{
name|this
operator|.
name|createTable
argument_list|(
name|tableName
argument_list|,
name|columns
argument_list|,
name|partCols
argument_list|,
name|fileInputFormat
argument_list|,
name|fileOutputFormat
argument_list|,
operator|-
literal|1
argument_list|,
literal|null
argument_list|)
expr_stmt|;
block|}
comment|/**    * Creates a table metadata and the directory for the table data    *    * @param tableName    *          name of the table    * @param columns    *          list of fields of the table    * @param partCols    *          partition keys of the table    * @param fileInputFormat    *          Class of the input format of the table data file    * @param fileOutputFormat    *          Class of the output format of the table data file    * @param bucketCount    *          number of buckets that each partition (or the table itself) should    *          be divided into    * @throws HiveException    *           thrown if the args are invalid or if the metadata or the data    *           directory couldn't be created    */
specifier|public
name|void
name|createTable
parameter_list|(
name|String
name|tableName
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|columns
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|partCols
parameter_list|,
name|Class
argument_list|<
name|?
extends|extends
name|InputFormat
argument_list|>
name|fileInputFormat
parameter_list|,
name|Class
argument_list|<
name|?
argument_list|>
name|fileOutputFormat
parameter_list|,
name|int
name|bucketCount
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|bucketCols
parameter_list|)
throws|throws
name|HiveException
block|{
name|createTable
argument_list|(
name|tableName
argument_list|,
name|columns
argument_list|,
name|partCols
argument_list|,
name|fileInputFormat
argument_list|,
name|fileOutputFormat
argument_list|,
name|bucketCount
argument_list|,
name|bucketCols
argument_list|,
literal|null
argument_list|)
expr_stmt|;
block|}
comment|/**    * Create a table metadata and the directory for the table data    * @param tableName table name    * @param columns list of fields of the table    * @param partCols partition keys of the table    * @param fileInputFormat Class of the input format of the table data file    * @param fileOutputFormat Class of the output format of the table data file    * @param bucketCount number of buckets that each partition (or the table itself) should be    *                    divided into    * @param bucketCols Bucket columns    * @param parameters Parameters for the table    * @throws HiveException    */
specifier|public
name|void
name|createTable
parameter_list|(
name|String
name|tableName
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|columns
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|partCols
parameter_list|,
name|Class
argument_list|<
name|?
extends|extends
name|InputFormat
argument_list|>
name|fileInputFormat
parameter_list|,
name|Class
argument_list|<
name|?
argument_list|>
name|fileOutputFormat
parameter_list|,
name|int
name|bucketCount
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|bucketCols
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|parameters
parameter_list|)
throws|throws
name|HiveException
block|{
if|if
condition|(
name|columns
operator|==
literal|null
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"columns not specified for table "
operator|+
name|tableName
argument_list|)
throw|;
block|}
name|Table
name|tbl
init|=
name|newTable
argument_list|(
name|tableName
argument_list|)
decl_stmt|;
name|tbl
operator|.
name|setInputFormatClass
argument_list|(
name|fileInputFormat
operator|.
name|getName
argument_list|()
argument_list|)
expr_stmt|;
name|tbl
operator|.
name|setOutputFormatClass
argument_list|(
name|fileOutputFormat
operator|.
name|getName
argument_list|()
argument_list|)
expr_stmt|;
for|for
control|(
name|String
name|col
range|:
name|columns
control|)
block|{
name|FieldSchema
name|field
init|=
operator|new
name|FieldSchema
argument_list|(
name|col
argument_list|,
name|STRING_TYPE_NAME
argument_list|,
literal|"default"
argument_list|)
decl_stmt|;
name|tbl
operator|.
name|getCols
argument_list|()
operator|.
name|add
argument_list|(
name|field
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|partCols
operator|!=
literal|null
condition|)
block|{
for|for
control|(
name|String
name|partCol
range|:
name|partCols
control|)
block|{
name|FieldSchema
name|part
init|=
operator|new
name|FieldSchema
argument_list|()
decl_stmt|;
name|part
operator|.
name|setName
argument_list|(
name|partCol
argument_list|)
expr_stmt|;
name|part
operator|.
name|setType
argument_list|(
name|STRING_TYPE_NAME
argument_list|)
expr_stmt|;
comment|// default partition key
name|tbl
operator|.
name|getPartCols
argument_list|()
operator|.
name|add
argument_list|(
name|part
argument_list|)
expr_stmt|;
block|}
block|}
name|tbl
operator|.
name|setSerializationLib
argument_list|(
name|LazySimpleSerDe
operator|.
name|class
operator|.
name|getName
argument_list|()
argument_list|)
expr_stmt|;
name|tbl
operator|.
name|setNumBuckets
argument_list|(
name|bucketCount
argument_list|)
expr_stmt|;
name|tbl
operator|.
name|setBucketCols
argument_list|(
name|bucketCols
argument_list|)
expr_stmt|;
if|if
condition|(
name|parameters
operator|!=
literal|null
condition|)
block|{
name|tbl
operator|.
name|setParamters
argument_list|(
name|parameters
argument_list|)
expr_stmt|;
block|}
name|createTable
argument_list|(
name|tbl
argument_list|)
expr_stmt|;
block|}
comment|/**    * Updates the existing table metadata with the new metadata.    *    * @param tblName    *          name of the existing table    * @param newTbl    *          new name of the table. could be the old name    * @throws InvalidOperationException    *           if the changes in metadata is not acceptable    * @throws TException    */
specifier|public
name|void
name|alterTable
parameter_list|(
name|String
name|tblName
parameter_list|,
name|Table
name|newTbl
parameter_list|)
throws|throws
name|InvalidOperationException
throws|,
name|HiveException
block|{
name|alterTable
argument_list|(
name|tblName
argument_list|,
name|newTbl
argument_list|,
literal|false
argument_list|)
expr_stmt|;
block|}
specifier|public
name|void
name|alterTable
parameter_list|(
name|String
name|tblName
parameter_list|,
name|Table
name|newTbl
parameter_list|,
name|boolean
name|cascade
parameter_list|)
throws|throws
name|InvalidOperationException
throws|,
name|HiveException
block|{
name|String
index|[]
name|names
init|=
name|Utilities
operator|.
name|getDbTableName
argument_list|(
name|tblName
argument_list|)
decl_stmt|;
try|try
block|{
comment|// Remove the DDL_TIME so it gets refreshed
if|if
condition|(
name|newTbl
operator|.
name|getParameters
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|newTbl
operator|.
name|getParameters
argument_list|()
operator|.
name|remove
argument_list|(
name|hive_metastoreConstants
operator|.
name|DDL_TIME
argument_list|)
expr_stmt|;
block|}
name|newTbl
operator|.
name|checkValidity
argument_list|()
expr_stmt|;
name|getMSC
argument_list|()
operator|.
name|alter_table
argument_list|(
name|names
index|[
literal|0
index|]
argument_list|,
name|names
index|[
literal|1
index|]
argument_list|,
name|newTbl
operator|.
name|getTTable
argument_list|()
argument_list|,
name|cascade
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|MetaException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Unable to alter table. "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
catch|catch
parameter_list|(
name|TException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Unable to alter table. "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|void
name|alterIndex
parameter_list|(
name|String
name|baseTableName
parameter_list|,
name|String
name|indexName
parameter_list|,
name|Index
name|newIdx
parameter_list|)
throws|throws
name|InvalidOperationException
throws|,
name|HiveException
block|{
name|String
index|[]
name|names
init|=
name|Utilities
operator|.
name|getDbTableName
argument_list|(
name|baseTableName
argument_list|)
decl_stmt|;
name|alterIndex
argument_list|(
name|names
index|[
literal|0
index|]
argument_list|,
name|names
index|[
literal|1
index|]
argument_list|,
name|indexName
argument_list|,
name|newIdx
argument_list|)
expr_stmt|;
block|}
comment|/**    * Updates the existing index metadata with the new metadata.    *    * @param idxName    *          name of the existing index    * @param newIdx    *          new name of the index. could be the old name    * @throws InvalidOperationException    *           if the changes in metadata is not acceptable    * @throws TException    */
specifier|public
name|void
name|alterIndex
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|baseTblName
parameter_list|,
name|String
name|idxName
parameter_list|,
name|Index
name|newIdx
parameter_list|)
throws|throws
name|InvalidOperationException
throws|,
name|HiveException
block|{
try|try
block|{
name|getMSC
argument_list|()
operator|.
name|alter_index
argument_list|(
name|dbName
argument_list|,
name|baseTblName
argument_list|,
name|idxName
argument_list|,
name|newIdx
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|MetaException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Unable to alter index. "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
catch|catch
parameter_list|(
name|TException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Unable to alter index. "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    * Updates the existing partition metadata with the new metadata.    *    * @param tblName    *          name of the existing table    * @param newPart    *          new partition    * @throws InvalidOperationException    *           if the changes in metadata is not acceptable    * @throws TException    */
specifier|public
name|void
name|alterPartition
parameter_list|(
name|String
name|tblName
parameter_list|,
name|Partition
name|newPart
parameter_list|)
throws|throws
name|InvalidOperationException
throws|,
name|HiveException
block|{
name|String
index|[]
name|names
init|=
name|Utilities
operator|.
name|getDbTableName
argument_list|(
name|tblName
argument_list|)
decl_stmt|;
name|alterPartition
argument_list|(
name|names
index|[
literal|0
index|]
argument_list|,
name|names
index|[
literal|1
index|]
argument_list|,
name|newPart
argument_list|)
expr_stmt|;
block|}
comment|/**    * Updates the existing partition metadata with the new metadata.    *    * @param dbName    *          name of the exiting table's database    * @param tblName    *          name of the existing table    * @param newPart    *          new partition    * @throws InvalidOperationException    *           if the changes in metadata is not acceptable    * @throws TException    */
specifier|public
name|void
name|alterPartition
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|tblName
parameter_list|,
name|Partition
name|newPart
parameter_list|)
throws|throws
name|InvalidOperationException
throws|,
name|HiveException
block|{
try|try
block|{
comment|// Remove the DDL time so that it gets refreshed
if|if
condition|(
name|newPart
operator|.
name|getParameters
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|newPart
operator|.
name|getParameters
argument_list|()
operator|.
name|remove
argument_list|(
name|hive_metastoreConstants
operator|.
name|DDL_TIME
argument_list|)
expr_stmt|;
block|}
name|newPart
operator|.
name|checkValidity
argument_list|()
expr_stmt|;
name|getMSC
argument_list|()
operator|.
name|alter_partition
argument_list|(
name|dbName
argument_list|,
name|tblName
argument_list|,
name|newPart
operator|.
name|getTPartition
argument_list|()
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|MetaException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Unable to alter partition. "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
catch|catch
parameter_list|(
name|TException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Unable to alter partition. "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    * Updates the existing table metadata with the new metadata.    *    * @param tblName    *          name of the existing table    * @param newParts    *          new partitions    * @throws InvalidOperationException    *           if the changes in metadata is not acceptable    * @throws TException    */
specifier|public
name|void
name|alterPartitions
parameter_list|(
name|String
name|tblName
parameter_list|,
name|List
argument_list|<
name|Partition
argument_list|>
name|newParts
parameter_list|)
throws|throws
name|InvalidOperationException
throws|,
name|HiveException
block|{
name|String
index|[]
name|names
init|=
name|Utilities
operator|.
name|getDbTableName
argument_list|(
name|tblName
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
argument_list|>
name|newTParts
init|=
operator|new
name|ArrayList
argument_list|<
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
argument_list|>
argument_list|()
decl_stmt|;
try|try
block|{
comment|// Remove the DDL time so that it gets refreshed
for|for
control|(
name|Partition
name|tmpPart
range|:
name|newParts
control|)
block|{
if|if
condition|(
name|tmpPart
operator|.
name|getParameters
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|tmpPart
operator|.
name|getParameters
argument_list|()
operator|.
name|remove
argument_list|(
name|hive_metastoreConstants
operator|.
name|DDL_TIME
argument_list|)
expr_stmt|;
block|}
name|newTParts
operator|.
name|add
argument_list|(
name|tmpPart
operator|.
name|getTPartition
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|getMSC
argument_list|()
operator|.
name|alter_partitions
argument_list|(
name|names
index|[
literal|0
index|]
argument_list|,
name|names
index|[
literal|1
index|]
argument_list|,
name|newTParts
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|MetaException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Unable to alter partition. "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
catch|catch
parameter_list|(
name|TException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Unable to alter partition. "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    * Rename a old partition to new partition    *    * @param tbl    *          existing table    * @param oldPartSpec    *          spec of old partition    * @param newPart    *          new partition    * @throws InvalidOperationException    *           if the changes in metadata is not acceptable    * @throws TException    */
specifier|public
name|void
name|renamePartition
parameter_list|(
name|Table
name|tbl
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|oldPartSpec
parameter_list|,
name|Partition
name|newPart
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|newPartSpec
init|=
name|newPart
operator|.
name|getSpec
argument_list|()
decl_stmt|;
if|if
condition|(
name|oldPartSpec
operator|.
name|keySet
argument_list|()
operator|.
name|size
argument_list|()
operator|!=
name|tbl
operator|.
name|getPartCols
argument_list|()
operator|.
name|size
argument_list|()
operator|||
name|newPartSpec
operator|.
name|keySet
argument_list|()
operator|.
name|size
argument_list|()
operator|!=
name|tbl
operator|.
name|getPartCols
argument_list|()
operator|.
name|size
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Unable to rename partition to the same name: number of partition cols don't match. "
argument_list|)
throw|;
block|}
if|if
condition|(
operator|!
name|oldPartSpec
operator|.
name|keySet
argument_list|()
operator|.
name|equals
argument_list|(
name|newPartSpec
operator|.
name|keySet
argument_list|()
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Unable to rename partition to the same name: old and new partition cols don't match. "
argument_list|)
throw|;
block|}
name|List
argument_list|<
name|String
argument_list|>
name|pvals
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|FieldSchema
name|field
range|:
name|tbl
operator|.
name|getPartCols
argument_list|()
control|)
block|{
name|String
name|val
init|=
name|oldPartSpec
operator|.
name|get
argument_list|(
name|field
operator|.
name|getName
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|val
operator|==
literal|null
operator|||
name|val
operator|.
name|length
argument_list|()
operator|==
literal|0
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"get partition: Value for key "
operator|+
name|field
operator|.
name|getName
argument_list|()
operator|+
literal|" is null or empty"
argument_list|)
throw|;
block|}
elseif|else
if|if
condition|(
name|val
operator|!=
literal|null
condition|)
block|{
name|pvals
operator|.
name|add
argument_list|(
name|val
argument_list|)
expr_stmt|;
block|}
block|}
name|getMSC
argument_list|()
operator|.
name|renamePartition
argument_list|(
name|tbl
operator|.
name|getDbName
argument_list|()
argument_list|,
name|tbl
operator|.
name|getTableName
argument_list|()
argument_list|,
name|pvals
argument_list|,
name|newPart
operator|.
name|getTPartition
argument_list|()
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|InvalidOperationException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Unable to rename partition. "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
catch|catch
parameter_list|(
name|MetaException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Unable to rename partition. "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
catch|catch
parameter_list|(
name|TException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Unable to rename partition. "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|void
name|alterDatabase
parameter_list|(
name|String
name|dbName
parameter_list|,
name|Database
name|db
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|getMSC
argument_list|()
operator|.
name|alterDatabase
argument_list|(
name|dbName
argument_list|,
name|db
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|MetaException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Unable to alter database "
operator|+
name|dbName
operator|+
literal|". "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
catch|catch
parameter_list|(
name|NoSuchObjectException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Database "
operator|+
name|dbName
operator|+
literal|" does not exists."
argument_list|,
name|e
argument_list|)
throw|;
block|}
catch|catch
parameter_list|(
name|TException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Unable to alter database "
operator|+
name|dbName
operator|+
literal|". "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    * Creates the table with the give objects    *    * @param tbl    *          a table object    * @throws HiveException    */
specifier|public
name|void
name|createTable
parameter_list|(
name|Table
name|tbl
parameter_list|)
throws|throws
name|HiveException
block|{
name|createTable
argument_list|(
name|tbl
argument_list|,
literal|false
argument_list|)
expr_stmt|;
block|}
comment|/**    * Creates the table with the give objects    *    * @param tbl    *          a table object    * @param ifNotExists    *          if true, ignore AlreadyExistsException    * @throws HiveException    */
specifier|public
name|void
name|createTable
parameter_list|(
name|Table
name|tbl
parameter_list|,
name|boolean
name|ifNotExists
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
if|if
condition|(
name|tbl
operator|.
name|getDbName
argument_list|()
operator|==
literal|null
operator|||
literal|""
operator|.
name|equals
argument_list|(
name|tbl
operator|.
name|getDbName
argument_list|()
operator|.
name|trim
argument_list|()
argument_list|)
condition|)
block|{
name|tbl
operator|.
name|setDbName
argument_list|(
name|SessionState
operator|.
name|get
argument_list|()
operator|.
name|getCurrentDatabase
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|tbl
operator|.
name|getCols
argument_list|()
operator|.
name|size
argument_list|()
operator|==
literal|0
operator|||
name|tbl
operator|.
name|getSd
argument_list|()
operator|.
name|getColsSize
argument_list|()
operator|==
literal|0
condition|)
block|{
name|tbl
operator|.
name|setFields
argument_list|(
name|MetaStoreUtils
operator|.
name|getFieldsFromDeserializer
argument_list|(
name|tbl
operator|.
name|getTableName
argument_list|()
argument_list|,
name|tbl
operator|.
name|getDeserializer
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|tbl
operator|.
name|checkValidity
argument_list|()
expr_stmt|;
if|if
condition|(
name|tbl
operator|.
name|getParameters
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|tbl
operator|.
name|getParameters
argument_list|()
operator|.
name|remove
argument_list|(
name|hive_metastoreConstants
operator|.
name|DDL_TIME
argument_list|)
expr_stmt|;
block|}
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Table
name|tTbl
init|=
name|tbl
operator|.
name|getTTable
argument_list|()
decl_stmt|;
name|PrincipalPrivilegeSet
name|principalPrivs
init|=
operator|new
name|PrincipalPrivilegeSet
argument_list|()
decl_stmt|;
name|SessionState
name|ss
init|=
name|SessionState
operator|.
name|get
argument_list|()
decl_stmt|;
if|if
condition|(
name|ss
operator|!=
literal|null
condition|)
block|{
name|CreateTableAutomaticGrant
name|grants
init|=
name|ss
operator|.
name|getCreateTableGrants
argument_list|()
decl_stmt|;
if|if
condition|(
name|grants
operator|!=
literal|null
condition|)
block|{
name|principalPrivs
operator|.
name|setUserPrivileges
argument_list|(
name|grants
operator|.
name|getUserGrants
argument_list|()
argument_list|)
expr_stmt|;
name|principalPrivs
operator|.
name|setGroupPrivileges
argument_list|(
name|grants
operator|.
name|getGroupGrants
argument_list|()
argument_list|)
expr_stmt|;
name|principalPrivs
operator|.
name|setRolePrivileges
argument_list|(
name|grants
operator|.
name|getRoleGrants
argument_list|()
argument_list|)
expr_stmt|;
name|tTbl
operator|.
name|setPrivileges
argument_list|(
name|principalPrivs
argument_list|)
expr_stmt|;
block|}
block|}
name|getMSC
argument_list|()
operator|.
name|createTable
argument_list|(
name|tTbl
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|AlreadyExistsException
name|e
parameter_list|)
block|{
if|if
condition|(
operator|!
name|ifNotExists
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    *    * @param tableName    *          table name    * @param indexName    *          index name    * @param indexHandlerClass    *          index handler class    * @param indexedCols    *          index columns    * @param indexTblName    *          index table's name    * @param deferredRebuild    *          referred build index table's data    * @param inputFormat    *          input format    * @param outputFormat    *          output format    * @param serde    * @param storageHandler    *          index table's storage handler    * @param location    *          location    * @param idxProps    *          idx    * @param serdeProps    *          serde properties    * @param collItemDelim    * @param fieldDelim    * @param fieldEscape    * @param lineDelim    * @param mapKeyDelim    * @throws HiveException    */
specifier|public
name|void
name|createIndex
parameter_list|(
name|String
name|tableName
parameter_list|,
name|String
name|indexName
parameter_list|,
name|String
name|indexHandlerClass
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|indexedCols
parameter_list|,
name|String
name|indexTblName
parameter_list|,
name|boolean
name|deferredRebuild
parameter_list|,
name|String
name|inputFormat
parameter_list|,
name|String
name|outputFormat
parameter_list|,
name|String
name|serde
parameter_list|,
name|String
name|storageHandler
parameter_list|,
name|String
name|location
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|idxProps
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|tblProps
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|serdeProps
parameter_list|,
name|String
name|collItemDelim
parameter_list|,
name|String
name|fieldDelim
parameter_list|,
name|String
name|fieldEscape
parameter_list|,
name|String
name|lineDelim
parameter_list|,
name|String
name|mapKeyDelim
parameter_list|,
name|String
name|indexComment
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|String
name|tdname
init|=
name|Utilities
operator|.
name|getDatabaseName
argument_list|(
name|tableName
argument_list|)
decl_stmt|;
name|String
name|idname
init|=
name|Utilities
operator|.
name|getDatabaseName
argument_list|(
name|indexTblName
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|idname
operator|.
name|equals
argument_list|(
name|tdname
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Index on different database ("
operator|+
name|idname
operator|+
literal|") from base table ("
operator|+
name|tdname
operator|+
literal|") is not supported."
argument_list|)
throw|;
block|}
name|Index
name|old_index
init|=
literal|null
decl_stmt|;
try|try
block|{
name|old_index
operator|=
name|getIndex
argument_list|(
name|tableName
argument_list|,
name|indexName
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{       }
if|if
condition|(
name|old_index
operator|!=
literal|null
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Index "
operator|+
name|indexName
operator|+
literal|" already exists on table "
operator|+
name|tableName
argument_list|)
throw|;
block|}
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Table
name|baseTbl
init|=
name|getTable
argument_list|(
name|tableName
argument_list|)
operator|.
name|getTTable
argument_list|()
decl_stmt|;
if|if
condition|(
name|baseTbl
operator|.
name|getTableType
argument_list|()
operator|==
name|TableType
operator|.
name|VIRTUAL_VIEW
operator|.
name|toString
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"tableName="
operator|+
name|tableName
operator|+
literal|" is a VIRTUAL VIEW. Index on VIRTUAL VIEW is not supported."
argument_list|)
throw|;
block|}
if|if
condition|(
name|baseTbl
operator|.
name|isTemporary
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"tableName="
operator|+
name|tableName
operator|+
literal|" is a TEMPORARY TABLE. Index on TEMPORARY TABLE is not supported."
argument_list|)
throw|;
block|}
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Table
name|temp
init|=
literal|null
decl_stmt|;
try|try
block|{
name|temp
operator|=
name|getTable
argument_list|(
name|indexTblName
argument_list|)
operator|.
name|getTTable
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{       }
if|if
condition|(
name|temp
operator|!=
literal|null
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Table name "
operator|+
name|indexTblName
operator|+
literal|" already exists. Choose another name."
argument_list|)
throw|;
block|}
name|SerDeInfo
name|serdeInfo
init|=
operator|new
name|SerDeInfo
argument_list|()
decl_stmt|;
name|serdeInfo
operator|.
name|setName
argument_list|(
name|indexTblName
argument_list|)
expr_stmt|;
if|if
condition|(
name|serde
operator|!=
literal|null
condition|)
block|{
name|serdeInfo
operator|.
name|setSerializationLib
argument_list|(
name|serde
argument_list|)
expr_stmt|;
block|}
else|else
block|{
if|if
condition|(
name|storageHandler
operator|==
literal|null
condition|)
block|{
name|serdeInfo
operator|.
name|setSerializationLib
argument_list|(
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|lazy
operator|.
name|LazySimpleSerDe
operator|.
name|class
operator|.
name|getName
argument_list|()
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|HiveStorageHandler
name|sh
init|=
name|HiveUtils
operator|.
name|getStorageHandler
argument_list|(
name|getConf
argument_list|()
argument_list|,
name|storageHandler
argument_list|)
decl_stmt|;
name|String
name|serDeClassName
init|=
name|sh
operator|.
name|getSerDeClass
argument_list|()
operator|.
name|getName
argument_list|()
decl_stmt|;
name|serdeInfo
operator|.
name|setSerializationLib
argument_list|(
name|serDeClassName
argument_list|)
expr_stmt|;
block|}
block|}
name|serdeInfo
operator|.
name|setParameters
argument_list|(
operator|new
name|HashMap
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|fieldDelim
operator|!=
literal|null
condition|)
block|{
name|serdeInfo
operator|.
name|getParameters
argument_list|()
operator|.
name|put
argument_list|(
name|FIELD_DELIM
argument_list|,
name|fieldDelim
argument_list|)
expr_stmt|;
name|serdeInfo
operator|.
name|getParameters
argument_list|()
operator|.
name|put
argument_list|(
name|SERIALIZATION_FORMAT
argument_list|,
name|fieldDelim
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|fieldEscape
operator|!=
literal|null
condition|)
block|{
name|serdeInfo
operator|.
name|getParameters
argument_list|()
operator|.
name|put
argument_list|(
name|ESCAPE_CHAR
argument_list|,
name|fieldEscape
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|collItemDelim
operator|!=
literal|null
condition|)
block|{
name|serdeInfo
operator|.
name|getParameters
argument_list|()
operator|.
name|put
argument_list|(
name|COLLECTION_DELIM
argument_list|,
name|collItemDelim
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|mapKeyDelim
operator|!=
literal|null
condition|)
block|{
name|serdeInfo
operator|.
name|getParameters
argument_list|()
operator|.
name|put
argument_list|(
name|MAPKEY_DELIM
argument_list|,
name|mapKeyDelim
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|lineDelim
operator|!=
literal|null
condition|)
block|{
name|serdeInfo
operator|.
name|getParameters
argument_list|()
operator|.
name|put
argument_list|(
name|LINE_DELIM
argument_list|,
name|lineDelim
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|serdeProps
operator|!=
literal|null
condition|)
block|{
name|Iterator
argument_list|<
name|Entry
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
argument_list|>
name|iter
init|=
name|serdeProps
operator|.
name|entrySet
argument_list|()
operator|.
name|iterator
argument_list|()
decl_stmt|;
while|while
condition|(
name|iter
operator|.
name|hasNext
argument_list|()
condition|)
block|{
name|Entry
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|m
init|=
name|iter
operator|.
name|next
argument_list|()
decl_stmt|;
name|serdeInfo
operator|.
name|getParameters
argument_list|()
operator|.
name|put
argument_list|(
name|m
operator|.
name|getKey
argument_list|()
argument_list|,
name|m
operator|.
name|getValue
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
name|List
argument_list|<
name|FieldSchema
argument_list|>
name|indexTblCols
init|=
operator|new
name|ArrayList
argument_list|<
name|FieldSchema
argument_list|>
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|Order
argument_list|>
name|sortCols
init|=
operator|new
name|ArrayList
argument_list|<
name|Order
argument_list|>
argument_list|()
decl_stmt|;
name|int
name|k
init|=
literal|0
decl_stmt|;
name|Table
name|metaBaseTbl
init|=
operator|new
name|Table
argument_list|(
name|baseTbl
argument_list|)
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|metaBaseTbl
operator|.
name|getCols
argument_list|()
operator|.
name|size
argument_list|()
condition|;
name|i
operator|++
control|)
block|{
name|FieldSchema
name|col
init|=
name|metaBaseTbl
operator|.
name|getCols
argument_list|()
operator|.
name|get
argument_list|(
name|i
argument_list|)
decl_stmt|;
if|if
condition|(
name|indexedCols
operator|.
name|contains
argument_list|(
name|col
operator|.
name|getName
argument_list|()
argument_list|)
condition|)
block|{
name|indexTblCols
operator|.
name|add
argument_list|(
name|col
argument_list|)
expr_stmt|;
name|sortCols
operator|.
name|add
argument_list|(
operator|new
name|Order
argument_list|(
name|col
operator|.
name|getName
argument_list|()
argument_list|,
literal|1
argument_list|)
argument_list|)
expr_stmt|;
name|k
operator|++
expr_stmt|;
block|}
block|}
if|if
condition|(
name|k
operator|!=
name|indexedCols
operator|.
name|size
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
literal|"Check the index columns, they should appear in the table being indexed."
argument_list|)
throw|;
block|}
name|int
name|time
init|=
call|(
name|int
call|)
argument_list|(
name|System
operator|.
name|currentTimeMillis
argument_list|()
operator|/
literal|1000
argument_list|)
decl_stmt|;
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Table
name|tt
init|=
literal|null
decl_stmt|;
name|HiveIndexHandler
name|indexHandler
init|=
name|HiveUtils
operator|.
name|getIndexHandler
argument_list|(
name|this
operator|.
name|getConf
argument_list|()
argument_list|,
name|indexHandlerClass
argument_list|)
decl_stmt|;
name|String
name|itname
init|=
name|Utilities
operator|.
name|getTableName
argument_list|(
name|indexTblName
argument_list|)
decl_stmt|;
if|if
condition|(
name|indexHandler
operator|.
name|usesIndexTable
argument_list|()
condition|)
block|{
name|tt
operator|=
operator|new
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|Table
argument_list|(
name|idname
argument_list|,
name|itname
argument_list|)
operator|.
name|getTTable
argument_list|()
expr_stmt|;
name|List
argument_list|<
name|FieldSchema
argument_list|>
name|partKeys
init|=
name|baseTbl
operator|.
name|getPartitionKeys
argument_list|()
decl_stmt|;
name|tt
operator|.
name|setPartitionKeys
argument_list|(
name|partKeys
argument_list|)
expr_stmt|;
name|tt
operator|.
name|setTableType
argument_list|(
name|TableType
operator|.
name|INDEX_TABLE
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|tblProps
operator|!=
literal|null
condition|)
block|{
for|for
control|(
name|Entry
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|prop
range|:
name|tblProps
operator|.
name|entrySet
argument_list|()
control|)
block|{
name|tt
operator|.
name|putToParameters
argument_list|(
name|prop
operator|.
name|getKey
argument_list|()
argument_list|,
name|prop
operator|.
name|getValue
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
name|SessionState
name|ss
init|=
name|SessionState
operator|.
name|get
argument_list|()
decl_stmt|;
name|CreateTableAutomaticGrant
name|grants
decl_stmt|;
if|if
condition|(
name|ss
operator|!=
literal|null
operator|&&
operator|(
operator|(
name|grants
operator|=
name|ss
operator|.
name|getCreateTableGrants
argument_list|()
operator|)
operator|!=
literal|null
operator|)
condition|)
block|{
name|PrincipalPrivilegeSet
name|principalPrivs
init|=
operator|new
name|PrincipalPrivilegeSet
argument_list|()
decl_stmt|;
name|principalPrivs
operator|.
name|setUserPrivileges
argument_list|(
name|grants
operator|.
name|getUserGrants
argument_list|()
argument_list|)
expr_stmt|;
name|principalPrivs
operator|.
name|setGroupPrivileges
argument_list|(
name|grants
operator|.
name|getGroupGrants
argument_list|()
argument_list|)
expr_stmt|;
name|principalPrivs
operator|.
name|setRolePrivileges
argument_list|(
name|grants
operator|.
name|getRoleGrants
argument_list|()
argument_list|)
expr_stmt|;
name|tt
operator|.
name|setPrivileges
argument_list|(
name|principalPrivs
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
operator|!
name|deferredRebuild
condition|)
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
literal|"Please specify deferred rebuild using \" WITH DEFERRED REBUILD \"."
argument_list|)
throw|;
block|}
name|StorageDescriptor
name|indexSd
init|=
operator|new
name|StorageDescriptor
argument_list|(
name|indexTblCols
argument_list|,
name|location
argument_list|,
name|inputFormat
argument_list|,
name|outputFormat
argument_list|,
literal|false
comment|/*compressed - not used*/
argument_list|,
operator|-
literal|1
comment|/*numBuckets - default is -1 when the table has no buckets*/
argument_list|,
name|serdeInfo
argument_list|,
literal|null
comment|/*bucketCols*/
argument_list|,
name|sortCols
argument_list|,
literal|null
comment|/*parameters*/
argument_list|)
decl_stmt|;
name|String
name|ttname
init|=
name|Utilities
operator|.
name|getTableName
argument_list|(
name|tableName
argument_list|)
decl_stmt|;
name|Index
name|indexDesc
init|=
operator|new
name|Index
argument_list|(
name|indexName
argument_list|,
name|indexHandlerClass
argument_list|,
name|tdname
argument_list|,
name|ttname
argument_list|,
name|time
argument_list|,
name|time
argument_list|,
name|itname
argument_list|,
name|indexSd
argument_list|,
operator|new
name|HashMap
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
argument_list|()
argument_list|,
name|deferredRebuild
argument_list|)
decl_stmt|;
if|if
condition|(
name|indexComment
operator|!=
literal|null
condition|)
block|{
name|indexDesc
operator|.
name|getParameters
argument_list|()
operator|.
name|put
argument_list|(
literal|"comment"
argument_list|,
name|indexComment
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|idxProps
operator|!=
literal|null
condition|)
block|{
name|indexDesc
operator|.
name|getParameters
argument_list|()
operator|.
name|putAll
argument_list|(
name|idxProps
argument_list|)
expr_stmt|;
block|}
name|indexHandler
operator|.
name|analyzeIndexDefinition
argument_list|(
name|baseTbl
argument_list|,
name|indexDesc
argument_list|,
name|tt
argument_list|)
expr_stmt|;
name|this
operator|.
name|getMSC
argument_list|()
operator|.
name|createIndex
argument_list|(
name|indexDesc
argument_list|,
name|tt
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|Index
name|getIndex
parameter_list|(
name|String
name|baseTableName
parameter_list|,
name|String
name|indexName
parameter_list|)
throws|throws
name|HiveException
block|{
name|String
index|[]
name|names
init|=
name|Utilities
operator|.
name|getDbTableName
argument_list|(
name|baseTableName
argument_list|)
decl_stmt|;
return|return
name|this
operator|.
name|getIndex
argument_list|(
name|names
index|[
literal|0
index|]
argument_list|,
name|names
index|[
literal|1
index|]
argument_list|,
name|indexName
argument_list|)
return|;
block|}
specifier|public
name|Index
name|getIndex
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|baseTableName
parameter_list|,
name|String
name|indexName
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|this
operator|.
name|getMSC
argument_list|()
operator|.
name|getIndex
argument_list|(
name|dbName
argument_list|,
name|baseTableName
argument_list|,
name|indexName
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|boolean
name|dropIndex
parameter_list|(
name|String
name|baseTableName
parameter_list|,
name|String
name|index_name
parameter_list|,
name|boolean
name|throwException
parameter_list|,
name|boolean
name|deleteData
parameter_list|)
throws|throws
name|HiveException
block|{
name|String
index|[]
name|names
init|=
name|Utilities
operator|.
name|getDbTableName
argument_list|(
name|baseTableName
argument_list|)
decl_stmt|;
return|return
name|dropIndex
argument_list|(
name|names
index|[
literal|0
index|]
argument_list|,
name|names
index|[
literal|1
index|]
argument_list|,
name|index_name
argument_list|,
name|throwException
argument_list|,
name|deleteData
argument_list|)
return|;
block|}
specifier|public
name|boolean
name|dropIndex
parameter_list|(
name|String
name|db_name
parameter_list|,
name|String
name|tbl_name
parameter_list|,
name|String
name|index_name
parameter_list|,
name|boolean
name|throwException
parameter_list|,
name|boolean
name|deleteData
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|dropIndex
argument_list|(
name|db_name
argument_list|,
name|tbl_name
argument_list|,
name|index_name
argument_list|,
name|deleteData
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|NoSuchObjectException
name|e
parameter_list|)
block|{
if|if
condition|(
name|throwException
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Index "
operator|+
name|index_name
operator|+
literal|" doesn't exist. "
argument_list|,
name|e
argument_list|)
throw|;
block|}
return|return
literal|false
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    * Drops table along with the data in it. If the table doesn't exist then it    * is a no-op. If ifPurge option is specified it is passed to the    * hdfs command that removes table data from warehouse to make it skip trash.    *    * @param tableName    *          table to drop    * @param ifPurge    *          completely purge the table (skipping trash) while removing data from warehouse    * @throws HiveException    *           thrown if the drop fails    */
specifier|public
name|void
name|dropTable
parameter_list|(
name|String
name|tableName
parameter_list|,
name|boolean
name|ifPurge
parameter_list|)
throws|throws
name|HiveException
block|{
name|String
index|[]
name|names
init|=
name|Utilities
operator|.
name|getDbTableName
argument_list|(
name|tableName
argument_list|)
decl_stmt|;
name|dropTable
argument_list|(
name|names
index|[
literal|0
index|]
argument_list|,
name|names
index|[
literal|1
index|]
argument_list|,
literal|true
argument_list|,
literal|true
argument_list|,
name|ifPurge
argument_list|)
expr_stmt|;
block|}
comment|/**    * Drops table along with the data in it. If the table doesn't exist then it    * is a no-op    *    * @param tableName    *          table to drop    * @throws HiveException    *           thrown if the drop fails    */
specifier|public
name|void
name|dropTable
parameter_list|(
name|String
name|tableName
parameter_list|)
throws|throws
name|HiveException
block|{
name|dropTable
argument_list|(
name|tableName
argument_list|,
literal|false
argument_list|)
expr_stmt|;
block|}
comment|/**    * Drops table along with the data in it. If the table doesn't exist then it    * is a no-op    *    * @param dbName    *          database where the table lives    * @param tableName    *          table to drop    * @throws HiveException    *           thrown if the drop fails    */
specifier|public
name|void
name|dropTable
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|tableName
parameter_list|)
throws|throws
name|HiveException
block|{
name|dropTable
argument_list|(
name|dbName
argument_list|,
name|tableName
argument_list|,
literal|true
argument_list|,
literal|true
argument_list|,
literal|false
argument_list|)
expr_stmt|;
block|}
comment|/**    * Drops the table.    *    * @param dbName    * @param tableName    * @param deleteData    *          deletes the underlying data along with metadata    * @param ignoreUnknownTab    *          an exception is thrown if this is false and the table doesn't exist    * @throws HiveException    */
specifier|public
name|void
name|dropTable
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|tableName
parameter_list|,
name|boolean
name|deleteData
parameter_list|,
name|boolean
name|ignoreUnknownTab
parameter_list|)
throws|throws
name|HiveException
block|{
name|dropTable
argument_list|(
name|dbName
argument_list|,
name|tableName
argument_list|,
name|deleteData
argument_list|,
name|ignoreUnknownTab
argument_list|,
literal|false
argument_list|)
expr_stmt|;
block|}
comment|/**    * Drops the table.    *    * @param dbName    * @param tableName    * @param deleteData    *          deletes the underlying data along with metadata    * @param ignoreUnknownTab    *          an exception is thrown if this is false and the table doesn't exist    * @param ifPurge    *          completely purge the table skipping trash while removing data from warehouse    * @throws HiveException    */
specifier|public
name|void
name|dropTable
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|tableName
parameter_list|,
name|boolean
name|deleteData
parameter_list|,
name|boolean
name|ignoreUnknownTab
parameter_list|,
name|boolean
name|ifPurge
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|getMSC
argument_list|()
operator|.
name|dropTable
argument_list|(
name|dbName
argument_list|,
name|tableName
argument_list|,
name|deleteData
argument_list|,
name|ignoreUnknownTab
argument_list|,
name|ifPurge
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|NoSuchObjectException
name|e
parameter_list|)
block|{
if|if
condition|(
operator|!
name|ignoreUnknownTab
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|HiveConf
name|getConf
parameter_list|()
block|{
return|return
operator|(
name|conf
operator|)
return|;
block|}
comment|/**    * Returns metadata for the table named tableName    * @param tableName the name of the table    * @return the table metadata    * @throws HiveException if there's an internal error or if the    * table doesn't exist    */
specifier|public
name|Table
name|getTable
parameter_list|(
specifier|final
name|String
name|tableName
parameter_list|)
throws|throws
name|HiveException
block|{
return|return
name|this
operator|.
name|getTable
argument_list|(
name|tableName
argument_list|,
literal|true
argument_list|)
return|;
block|}
comment|/**    * Returns metadata for the table named tableName    * @param tableName the name of the table    * @param throwException controls whether an exception is thrown or a returns a null    * @return the table metadata    * @throws HiveException if there's an internal error or if the    * table doesn't exist    */
specifier|public
name|Table
name|getTable
parameter_list|(
specifier|final
name|String
name|tableName
parameter_list|,
name|boolean
name|throwException
parameter_list|)
throws|throws
name|HiveException
block|{
name|String
index|[]
name|names
init|=
name|Utilities
operator|.
name|getDbTableName
argument_list|(
name|tableName
argument_list|)
decl_stmt|;
return|return
name|this
operator|.
name|getTable
argument_list|(
name|names
index|[
literal|0
index|]
argument_list|,
name|names
index|[
literal|1
index|]
argument_list|,
name|throwException
argument_list|)
return|;
block|}
comment|/**    * Returns metadata of the table    *    * @param dbName    *          the name of the database    * @param tableName    *          the name of the table    * @return the table    * @exception HiveException    *              if there's an internal error or if the table doesn't exist    */
specifier|public
name|Table
name|getTable
parameter_list|(
specifier|final
name|String
name|dbName
parameter_list|,
specifier|final
name|String
name|tableName
parameter_list|)
throws|throws
name|HiveException
block|{
if|if
condition|(
name|tableName
operator|.
name|contains
argument_list|(
literal|"."
argument_list|)
condition|)
block|{
name|String
index|[]
name|names
init|=
name|Utilities
operator|.
name|getDbTableName
argument_list|(
name|tableName
argument_list|)
decl_stmt|;
return|return
name|this
operator|.
name|getTable
argument_list|(
name|names
index|[
literal|0
index|]
argument_list|,
name|names
index|[
literal|1
index|]
argument_list|,
literal|true
argument_list|)
return|;
block|}
else|else
block|{
return|return
name|this
operator|.
name|getTable
argument_list|(
name|dbName
argument_list|,
name|tableName
argument_list|,
literal|true
argument_list|)
return|;
block|}
block|}
comment|/**    * Returns metadata of the table    *    * @param dbName    *          the name of the database    * @param tableName    *          the name of the table    * @param throwException    *          controls whether an exception is thrown or a returns a null    * @return the table or if throwException is false a null value.    * @throws HiveException    */
specifier|public
name|Table
name|getTable
parameter_list|(
specifier|final
name|String
name|dbName
parameter_list|,
specifier|final
name|String
name|tableName
parameter_list|,
name|boolean
name|throwException
parameter_list|)
throws|throws
name|HiveException
block|{
if|if
condition|(
name|tableName
operator|==
literal|null
operator|||
name|tableName
operator|.
name|equals
argument_list|(
literal|""
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"empty table creation??"
argument_list|)
throw|;
block|}
comment|// Get the table from metastore
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Table
name|tTable
init|=
literal|null
decl_stmt|;
try|try
block|{
name|tTable
operator|=
name|getMSC
argument_list|()
operator|.
name|getTable
argument_list|(
name|dbName
argument_list|,
name|tableName
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|NoSuchObjectException
name|e
parameter_list|)
block|{
if|if
condition|(
name|throwException
condition|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Table "
operator|+
name|tableName
operator|+
literal|" not found: "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|InvalidTableException
argument_list|(
name|tableName
argument_list|)
throw|;
block|}
return|return
literal|null
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Unable to fetch table "
operator|+
name|tableName
operator|+
literal|". "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
comment|// For non-views, we need to do some extra fixes
if|if
condition|(
operator|!
name|TableType
operator|.
name|VIRTUAL_VIEW
operator|.
name|toString
argument_list|()
operator|.
name|equals
argument_list|(
name|tTable
operator|.
name|getTableType
argument_list|()
argument_list|)
condition|)
block|{
comment|// Fix the non-printable chars
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|parameters
init|=
name|tTable
operator|.
name|getSd
argument_list|()
operator|.
name|getParameters
argument_list|()
decl_stmt|;
name|String
name|sf
init|=
name|parameters
operator|.
name|get
argument_list|(
name|SERIALIZATION_FORMAT
argument_list|)
decl_stmt|;
if|if
condition|(
name|sf
operator|!=
literal|null
condition|)
block|{
name|char
index|[]
name|b
init|=
name|sf
operator|.
name|toCharArray
argument_list|()
decl_stmt|;
if|if
condition|(
operator|(
name|b
operator|.
name|length
operator|==
literal|1
operator|)
operator|&&
operator|(
name|b
index|[
literal|0
index|]
operator|<
literal|10
operator|)
condition|)
block|{
comment|// ^A, ^B, ^C, ^D, \t
name|parameters
operator|.
name|put
argument_list|(
name|SERIALIZATION_FORMAT
argument_list|,
name|Integer
operator|.
name|toString
argument_list|(
name|b
index|[
literal|0
index|]
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
comment|// Use LazySimpleSerDe for MetadataTypedColumnsetSerDe.
comment|// NOTE: LazySimpleSerDe does not support tables with a single column of
comment|// col
comment|// of type "array<string>". This happens when the table is created using
comment|// an
comment|// earlier version of Hive.
if|if
condition|(
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|MetadataTypedColumnsetSerDe
operator|.
name|class
operator|.
name|getName
argument_list|()
operator|.
name|equals
argument_list|(
name|tTable
operator|.
name|getSd
argument_list|()
operator|.
name|getSerdeInfo
argument_list|()
operator|.
name|getSerializationLib
argument_list|()
argument_list|)
operator|&&
name|tTable
operator|.
name|getSd
argument_list|()
operator|.
name|getColsSize
argument_list|()
operator|>
literal|0
operator|&&
name|tTable
operator|.
name|getSd
argument_list|()
operator|.
name|getCols
argument_list|()
operator|.
name|get
argument_list|(
literal|0
argument_list|)
operator|.
name|getType
argument_list|()
operator|.
name|indexOf
argument_list|(
literal|'<'
argument_list|)
operator|==
operator|-
literal|1
condition|)
block|{
name|tTable
operator|.
name|getSd
argument_list|()
operator|.
name|getSerdeInfo
argument_list|()
operator|.
name|setSerializationLib
argument_list|(
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|lazy
operator|.
name|LazySimpleSerDe
operator|.
name|class
operator|.
name|getName
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
return|return
operator|new
name|Table
argument_list|(
name|tTable
argument_list|)
return|;
block|}
comment|/**    * Get all table names for the current database.    * @return List of table names    * @throws HiveException    */
specifier|public
name|List
argument_list|<
name|String
argument_list|>
name|getAllTables
parameter_list|()
throws|throws
name|HiveException
block|{
return|return
name|getAllTables
argument_list|(
name|SessionState
operator|.
name|get
argument_list|()
operator|.
name|getCurrentDatabase
argument_list|()
argument_list|)
return|;
block|}
comment|/**    * Get all table names for the specified database.    * @param dbName    * @return List of table names    * @throws HiveException    */
specifier|public
name|List
argument_list|<
name|String
argument_list|>
name|getAllTables
parameter_list|(
name|String
name|dbName
parameter_list|)
throws|throws
name|HiveException
block|{
return|return
name|getTablesByPattern
argument_list|(
name|dbName
argument_list|,
literal|".*"
argument_list|)
return|;
block|}
comment|/**    * Returns all existing tables from default database which match the given    * pattern. The matching occurs as per Java regular expressions    *    * @param tablePattern    *          java re pattern    * @return list of table names    * @throws HiveException    */
specifier|public
name|List
argument_list|<
name|String
argument_list|>
name|getTablesByPattern
parameter_list|(
name|String
name|tablePattern
parameter_list|)
throws|throws
name|HiveException
block|{
return|return
name|getTablesByPattern
argument_list|(
name|SessionState
operator|.
name|get
argument_list|()
operator|.
name|getCurrentDatabase
argument_list|()
argument_list|,
name|tablePattern
argument_list|)
return|;
block|}
comment|/**    * Returns all existing tables from the specified database which match the given    * pattern. The matching occurs as per Java regular expressions.    * @param dbName    * @param tablePattern    * @return list of table names    * @throws HiveException    */
specifier|public
name|List
argument_list|<
name|String
argument_list|>
name|getTablesByPattern
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|tablePattern
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|getTables
argument_list|(
name|dbName
argument_list|,
name|tablePattern
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    * Returns all existing tables from the given database which match the given    * pattern. The matching occurs as per Java regular expressions    *    * @param database    *          the database name    * @param tablePattern    *          java re pattern    * @return list of table names    * @throws HiveException    */
specifier|public
name|List
argument_list|<
name|String
argument_list|>
name|getTablesForDb
parameter_list|(
name|String
name|database
parameter_list|,
name|String
name|tablePattern
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|getTables
argument_list|(
name|database
argument_list|,
name|tablePattern
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    * Get all existing database names.    *    * @return List of database names.    * @throws HiveException    */
specifier|public
name|List
argument_list|<
name|String
argument_list|>
name|getAllDatabases
parameter_list|()
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|getAllDatabases
argument_list|()
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    * Get all existing databases that match the given    * pattern. The matching occurs as per Java regular expressions    *    * @param databasePattern    *          java re pattern    * @return list of database names    * @throws HiveException    */
specifier|public
name|List
argument_list|<
name|String
argument_list|>
name|getDatabasesByPattern
parameter_list|(
name|String
name|databasePattern
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|getDatabases
argument_list|(
name|databasePattern
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|boolean
name|grantPrivileges
parameter_list|(
name|PrivilegeBag
name|privileges
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|grant_privileges
argument_list|(
name|privileges
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    * @param privileges    *          a bag of privileges    * @return true on success    * @throws HiveException    */
specifier|public
name|boolean
name|revokePrivileges
parameter_list|(
name|PrivilegeBag
name|privileges
parameter_list|,
name|boolean
name|grantOption
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|revoke_privileges
argument_list|(
name|privileges
argument_list|,
name|grantOption
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    * Query metadata to see if a database with the given name already exists.    *    * @param dbName    * @return true if a database with the given name already exists, false if    *         does not exist.    * @throws HiveException    */
specifier|public
name|boolean
name|databaseExists
parameter_list|(
name|String
name|dbName
parameter_list|)
throws|throws
name|HiveException
block|{
return|return
name|getDatabase
argument_list|(
name|dbName
argument_list|)
operator|!=
literal|null
return|;
block|}
comment|/**    * Get the database by name.    * @param dbName the name of the database.    * @return a Database object if this database exists, null otherwise.    * @throws HiveException    */
specifier|public
name|Database
name|getDatabase
parameter_list|(
name|String
name|dbName
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|getDatabase
argument_list|(
name|dbName
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|NoSuchObjectException
name|e
parameter_list|)
block|{
return|return
literal|null
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    * Get the Database object for current database    * @return a Database object if this database exists, null otherwise.    * @throws HiveException    */
specifier|public
name|Database
name|getDatabaseCurrent
parameter_list|()
throws|throws
name|HiveException
block|{
name|String
name|currentDb
init|=
name|SessionState
operator|.
name|get
argument_list|()
operator|.
name|getCurrentDatabase
argument_list|()
decl_stmt|;
return|return
name|getDatabase
argument_list|(
name|currentDb
argument_list|)
return|;
block|}
specifier|public
name|void
name|loadPartition
parameter_list|(
name|Path
name|loadPath
parameter_list|,
name|String
name|tableName
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|partSpec
parameter_list|,
name|boolean
name|replace
parameter_list|,
name|boolean
name|holdDDLTime
parameter_list|,
name|boolean
name|inheritTableSpecs
parameter_list|,
name|boolean
name|isSkewedStoreAsSubdir
parameter_list|,
name|boolean
name|isSrcLocal
parameter_list|,
name|boolean
name|isAcid
parameter_list|)
throws|throws
name|HiveException
block|{
name|Table
name|tbl
init|=
name|getTable
argument_list|(
name|tableName
argument_list|)
decl_stmt|;
name|loadPartition
argument_list|(
name|loadPath
argument_list|,
name|tbl
argument_list|,
name|partSpec
argument_list|,
name|replace
argument_list|,
name|holdDDLTime
argument_list|,
name|inheritTableSpecs
argument_list|,
name|isSkewedStoreAsSubdir
argument_list|,
name|isSrcLocal
argument_list|,
name|isAcid
argument_list|)
expr_stmt|;
block|}
comment|/**    * Load a directory into a Hive Table Partition - Alters existing content of    * the partition with the contents of loadPath. - If the partition does not    * exist - one is created - files in loadPath are moved into Hive. But the    * directory itself is not removed.    *    * @param loadPath    *          Directory containing files to load into Table    * @param  tbl    *          name of table to be loaded.    * @param partSpec    *          defines which partition needs to be loaded    * @param replace    *          if true - replace files in the partition, otherwise add files to    *          the partition    * @param holdDDLTime if true, force [re]create the partition    * @param inheritTableSpecs if true, on [re]creating the partition, take the    *          location/inputformat/outputformat/serde details from table spec    * @param isSrcLocal    *          If the source directory is LOCAL    * @param isAcid true if this is an ACID operation    */
specifier|public
name|Partition
name|loadPartition
parameter_list|(
name|Path
name|loadPath
parameter_list|,
name|Table
name|tbl
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|partSpec
parameter_list|,
name|boolean
name|replace
parameter_list|,
name|boolean
name|holdDDLTime
parameter_list|,
name|boolean
name|inheritTableSpecs
parameter_list|,
name|boolean
name|isSkewedStoreAsSubdir
parameter_list|,
name|boolean
name|isSrcLocal
parameter_list|,
name|boolean
name|isAcid
parameter_list|)
throws|throws
name|HiveException
block|{
name|Path
name|tblDataLocationPath
init|=
name|tbl
operator|.
name|getDataLocation
argument_list|()
decl_stmt|;
name|Partition
name|newTPart
init|=
literal|null
decl_stmt|;
try|try
block|{
comment|/**        * Move files before creating the partition since down stream processes        * check for existence of partition in metadata before accessing the data.        * If partition is created before data is moved, downstream waiting        * processes might move forward with partial data        */
name|Partition
name|oldPart
init|=
name|getPartition
argument_list|(
name|tbl
argument_list|,
name|partSpec
argument_list|,
literal|false
argument_list|)
decl_stmt|;
name|Path
name|oldPartPath
init|=
literal|null
decl_stmt|;
if|if
condition|(
name|oldPart
operator|!=
literal|null
condition|)
block|{
name|oldPartPath
operator|=
name|oldPart
operator|.
name|getDataLocation
argument_list|()
expr_stmt|;
block|}
name|Path
name|newPartPath
init|=
literal|null
decl_stmt|;
if|if
condition|(
name|inheritTableSpecs
condition|)
block|{
name|Path
name|partPath
init|=
operator|new
name|Path
argument_list|(
name|tbl
operator|.
name|getDataLocation
argument_list|()
argument_list|,
name|Warehouse
operator|.
name|makePartPath
argument_list|(
name|partSpec
argument_list|)
argument_list|)
decl_stmt|;
name|newPartPath
operator|=
operator|new
name|Path
argument_list|(
name|tblDataLocationPath
operator|.
name|toUri
argument_list|()
operator|.
name|getScheme
argument_list|()
argument_list|,
name|tblDataLocationPath
operator|.
name|toUri
argument_list|()
operator|.
name|getAuthority
argument_list|()
argument_list|,
name|partPath
operator|.
name|toUri
argument_list|()
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|oldPart
operator|!=
literal|null
condition|)
block|{
comment|/*            * If we are moving the partition across filesystem boundaries            * inherit from the table properties. Otherwise (same filesystem) use the            * original partition location.            *            * See: HIVE-1707 and HIVE-2117 for background            */
name|FileSystem
name|oldPartPathFS
init|=
name|oldPartPath
operator|.
name|getFileSystem
argument_list|(
name|getConf
argument_list|()
argument_list|)
decl_stmt|;
name|FileSystem
name|loadPathFS
init|=
name|loadPath
operator|.
name|getFileSystem
argument_list|(
name|getConf
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|FileUtils
operator|.
name|equalsFileSystem
argument_list|(
name|oldPartPathFS
argument_list|,
name|loadPathFS
argument_list|)
condition|)
block|{
name|newPartPath
operator|=
name|oldPartPath
expr_stmt|;
block|}
block|}
block|}
else|else
block|{
name|newPartPath
operator|=
name|oldPartPath
expr_stmt|;
block|}
name|List
argument_list|<
name|Path
argument_list|>
name|newFiles
init|=
literal|null
decl_stmt|;
if|if
condition|(
name|replace
condition|)
block|{
name|Hive
operator|.
name|replaceFiles
argument_list|(
name|tbl
operator|.
name|getPath
argument_list|()
argument_list|,
name|loadPath
argument_list|,
name|newPartPath
argument_list|,
name|oldPartPath
argument_list|,
name|getConf
argument_list|()
argument_list|,
name|isSrcLocal
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|newFiles
operator|=
operator|new
name|ArrayList
argument_list|<
name|Path
argument_list|>
argument_list|()
expr_stmt|;
name|FileSystem
name|fs
init|=
name|tbl
operator|.
name|getDataLocation
argument_list|()
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|Hive
operator|.
name|copyFiles
argument_list|(
name|conf
argument_list|,
name|loadPath
argument_list|,
name|newPartPath
argument_list|,
name|fs
argument_list|,
name|isSrcLocal
argument_list|,
name|isAcid
argument_list|,
name|newFiles
argument_list|)
expr_stmt|;
block|}
name|boolean
name|forceCreate
init|=
operator|(
operator|!
name|holdDDLTime
operator|)
condition|?
literal|true
else|:
literal|false
decl_stmt|;
name|newTPart
operator|=
name|getPartition
argument_list|(
name|tbl
argument_list|,
name|partSpec
argument_list|,
name|forceCreate
argument_list|,
name|newPartPath
operator|.
name|toString
argument_list|()
argument_list|,
name|inheritTableSpecs
argument_list|,
name|newFiles
argument_list|)
expr_stmt|;
comment|// recreate the partition if it existed before
if|if
condition|(
operator|!
name|holdDDLTime
condition|)
block|{
if|if
condition|(
name|isSkewedStoreAsSubdir
condition|)
block|{
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
name|newCreatedTpart
init|=
name|newTPart
operator|.
name|getTPartition
argument_list|()
decl_stmt|;
name|SkewedInfo
name|skewedInfo
init|=
name|newCreatedTpart
operator|.
name|getSd
argument_list|()
operator|.
name|getSkewedInfo
argument_list|()
decl_stmt|;
comment|/* Construct list bucketing location mappings from sub-directory name. */
name|Map
argument_list|<
name|List
argument_list|<
name|String
argument_list|>
argument_list|,
name|String
argument_list|>
name|skewedColValueLocationMaps
init|=
name|constructListBucketingLocationMap
argument_list|(
name|newPartPath
argument_list|,
name|skewedInfo
argument_list|)
decl_stmt|;
comment|/* Add list bucketing location mappings. */
name|skewedInfo
operator|.
name|setSkewedColValueLocationMaps
argument_list|(
name|skewedColValueLocationMaps
argument_list|)
expr_stmt|;
name|newCreatedTpart
operator|.
name|getSd
argument_list|()
operator|.
name|setSkewedInfo
argument_list|(
name|skewedInfo
argument_list|)
expr_stmt|;
name|alterPartition
argument_list|(
name|tbl
operator|.
name|getDbName
argument_list|()
argument_list|,
name|tbl
operator|.
name|getTableName
argument_list|()
argument_list|,
operator|new
name|Partition
argument_list|(
name|tbl
argument_list|,
name|newCreatedTpart
argument_list|)
argument_list|)
expr_stmt|;
name|newTPart
operator|=
name|getPartition
argument_list|(
name|tbl
argument_list|,
name|partSpec
argument_list|,
literal|true
argument_list|,
name|newPartPath
operator|.
name|toString
argument_list|()
argument_list|,
name|inheritTableSpecs
argument_list|,
name|newFiles
argument_list|)
expr_stmt|;
return|return
operator|new
name|Partition
argument_list|(
name|tbl
argument_list|,
name|newCreatedTpart
argument_list|)
return|;
block|}
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
catch|catch
parameter_list|(
name|MetaException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
catch|catch
parameter_list|(
name|InvalidOperationException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
return|return
name|newTPart
return|;
block|}
comment|/**  * Walk through sub-directory tree to construct list bucketing location map.  *  * @param fSta  * @param fSys  * @param skewedColValueLocationMaps  * @param newPartPath  * @param skewedInfo  * @throws IOException  */
specifier|private
name|void
name|walkDirTree
parameter_list|(
name|FileStatus
name|fSta
parameter_list|,
name|FileSystem
name|fSys
parameter_list|,
name|Map
argument_list|<
name|List
argument_list|<
name|String
argument_list|>
argument_list|,
name|String
argument_list|>
name|skewedColValueLocationMaps
parameter_list|,
name|Path
name|newPartPath
parameter_list|,
name|SkewedInfo
name|skewedInfo
parameter_list|)
throws|throws
name|IOException
block|{
comment|/* Base Case. It's leaf. */
if|if
condition|(
operator|!
name|fSta
operator|.
name|isDir
argument_list|()
condition|)
block|{
comment|/* construct one location map if not exists. */
name|constructOneLBLocationMap
argument_list|(
name|fSta
argument_list|,
name|skewedColValueLocationMaps
argument_list|,
name|newPartPath
argument_list|,
name|skewedInfo
argument_list|)
expr_stmt|;
return|return;
block|}
comment|/* dfs. */
name|FileStatus
index|[]
name|children
init|=
name|fSys
operator|.
name|listStatus
argument_list|(
name|fSta
operator|.
name|getPath
argument_list|()
argument_list|,
name|FileUtils
operator|.
name|HIDDEN_FILES_PATH_FILTER
argument_list|)
decl_stmt|;
if|if
condition|(
name|children
operator|!=
literal|null
condition|)
block|{
for|for
control|(
name|FileStatus
name|child
range|:
name|children
control|)
block|{
name|walkDirTree
argument_list|(
name|child
argument_list|,
name|fSys
argument_list|,
name|skewedColValueLocationMaps
argument_list|,
name|newPartPath
argument_list|,
name|skewedInfo
argument_list|)
expr_stmt|;
block|}
block|}
block|}
comment|/**  * Construct a list bucketing location map  * @param fSta  * @param skewedColValueLocationMaps  * @param newPartPath  * @param skewedInfo  */
specifier|private
name|void
name|constructOneLBLocationMap
parameter_list|(
name|FileStatus
name|fSta
parameter_list|,
name|Map
argument_list|<
name|List
argument_list|<
name|String
argument_list|>
argument_list|,
name|String
argument_list|>
name|skewedColValueLocationMaps
parameter_list|,
name|Path
name|newPartPath
parameter_list|,
name|SkewedInfo
name|skewedInfo
parameter_list|)
block|{
name|Path
name|lbdPath
init|=
name|fSta
operator|.
name|getPath
argument_list|()
operator|.
name|getParent
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|skewedValue
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
name|String
name|lbDirName
init|=
name|FileUtils
operator|.
name|unescapePathName
argument_list|(
name|lbdPath
operator|.
name|toString
argument_list|()
argument_list|)
decl_stmt|;
name|String
name|partDirName
init|=
name|FileUtils
operator|.
name|unescapePathName
argument_list|(
name|newPartPath
operator|.
name|toString
argument_list|()
argument_list|)
decl_stmt|;
name|String
name|lbDirSuffix
init|=
name|lbDirName
operator|.
name|replace
argument_list|(
name|partDirName
argument_list|,
literal|""
argument_list|)
decl_stmt|;
name|String
index|[]
name|dirNames
init|=
name|lbDirSuffix
operator|.
name|split
argument_list|(
name|Path
operator|.
name|SEPARATOR
argument_list|)
decl_stmt|;
for|for
control|(
name|String
name|dirName
range|:
name|dirNames
control|)
block|{
if|if
condition|(
operator|(
name|dirName
operator|!=
literal|null
operator|)
operator|&&
operator|(
name|dirName
operator|.
name|length
argument_list|()
operator|>
literal|0
operator|)
condition|)
block|{
comment|// Construct skewed-value to location map except default directory.
comment|// why? query logic knows default-dir structure and don't need to get from map
if|if
condition|(
operator|!
name|dirName
operator|.
name|equalsIgnoreCase
argument_list|(
name|ListBucketingPrunerUtils
operator|.
name|HIVE_LIST_BUCKETING_DEFAULT_DIR_NAME
argument_list|)
condition|)
block|{
name|String
index|[]
name|kv
init|=
name|dirName
operator|.
name|split
argument_list|(
literal|"="
argument_list|)
decl_stmt|;
if|if
condition|(
name|kv
operator|.
name|length
operator|==
literal|2
condition|)
block|{
name|skewedValue
operator|.
name|add
argument_list|(
name|kv
index|[
literal|1
index|]
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
if|if
condition|(
operator|(
name|skewedValue
operator|.
name|size
argument_list|()
operator|>
literal|0
operator|)
operator|&&
operator|(
name|skewedValue
operator|.
name|size
argument_list|()
operator|==
name|skewedInfo
operator|.
name|getSkewedColNames
argument_list|()
operator|.
name|size
argument_list|()
operator|)
operator|&&
operator|!
name|skewedColValueLocationMaps
operator|.
name|containsKey
argument_list|(
name|skewedValue
argument_list|)
condition|)
block|{
name|skewedColValueLocationMaps
operator|.
name|put
argument_list|(
name|skewedValue
argument_list|,
name|lbdPath
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * Construct location map from path    *    * @param newPartPath    * @param skewedInfo    * @return    * @throws IOException    * @throws FileNotFoundException    */
specifier|private
name|Map
argument_list|<
name|List
argument_list|<
name|String
argument_list|>
argument_list|,
name|String
argument_list|>
name|constructListBucketingLocationMap
parameter_list|(
name|Path
name|newPartPath
parameter_list|,
name|SkewedInfo
name|skewedInfo
parameter_list|)
throws|throws
name|IOException
throws|,
name|FileNotFoundException
block|{
name|Map
argument_list|<
name|List
argument_list|<
name|String
argument_list|>
argument_list|,
name|String
argument_list|>
name|skewedColValueLocationMaps
init|=
operator|new
name|HashMap
argument_list|<
name|List
argument_list|<
name|String
argument_list|>
argument_list|,
name|String
argument_list|>
argument_list|()
decl_stmt|;
name|FileSystem
name|fSys
init|=
name|newPartPath
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|walkDirTree
argument_list|(
name|fSys
operator|.
name|getFileStatus
argument_list|(
name|newPartPath
argument_list|)
argument_list|,
name|fSys
argument_list|,
name|skewedColValueLocationMaps
argument_list|,
name|newPartPath
argument_list|,
name|skewedInfo
argument_list|)
expr_stmt|;
return|return
name|skewedColValueLocationMaps
return|;
block|}
comment|/**    * Given a source directory name of the load path, load all dynamically generated partitions    * into the specified table and return a list of strings that represent the dynamic partition    * paths.    * @param loadPath    * @param tableName    * @param partSpec    * @param replace    * @param numDP number of dynamic partitions    * @param holdDDLTime    * @param listBucketingEnabled    * @param isAcid true if this is an ACID operation    * @param txnId txnId, can be 0 unless isAcid == true    * @return partition map details (PartitionSpec and Partition)    * @throws HiveException    */
specifier|public
name|Map
argument_list|<
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
argument_list|,
name|Partition
argument_list|>
name|loadDynamicPartitions
parameter_list|(
name|Path
name|loadPath
parameter_list|,
name|String
name|tableName
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|partSpec
parameter_list|,
name|boolean
name|replace
parameter_list|,
name|int
name|numDP
parameter_list|,
name|boolean
name|holdDDLTime
parameter_list|,
name|boolean
name|listBucketingEnabled
parameter_list|,
name|boolean
name|isAcid
parameter_list|,
name|long
name|txnId
parameter_list|)
throws|throws
name|HiveException
block|{
name|Set
argument_list|<
name|Path
argument_list|>
name|validPartitions
init|=
operator|new
name|HashSet
argument_list|<
name|Path
argument_list|>
argument_list|()
decl_stmt|;
try|try
block|{
name|Map
argument_list|<
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
argument_list|,
name|Partition
argument_list|>
name|partitionsMap
init|=
operator|new
name|LinkedHashMap
argument_list|<
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
argument_list|,
name|Partition
argument_list|>
argument_list|()
decl_stmt|;
name|FileSystem
name|fs
init|=
name|loadPath
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|FileStatus
index|[]
name|leafStatus
init|=
name|HiveStatsUtils
operator|.
name|getFileStatusRecurse
argument_list|(
name|loadPath
argument_list|,
name|numDP
operator|+
literal|1
argument_list|,
name|fs
argument_list|)
decl_stmt|;
comment|// Check for empty partitions
for|for
control|(
name|FileStatus
name|s
range|:
name|leafStatus
control|)
block|{
comment|// Check if the hadoop version supports sub-directories for tables/partitions
if|if
condition|(
name|s
operator|.
name|isDir
argument_list|()
operator|&&
operator|!
name|conf
operator|.
name|getBoolVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_HADOOP_SUPPORTS_SUBDIRECTORIES
argument_list|)
condition|)
block|{
comment|// No leaves in this directory
name|LOG
operator|.
name|info
argument_list|(
literal|"NOT moving empty directory: "
operator|+
name|s
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
block|}
else|else
block|{
try|try
block|{
name|validatePartitionNameCharacters
argument_list|(
name|Warehouse
operator|.
name|getPartValuesFromPartName
argument_list|(
name|s
operator|.
name|getPath
argument_list|()
operator|.
name|getParent
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|MetaException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
name|validPartitions
operator|.
name|add
argument_list|(
name|s
operator|.
name|getPath
argument_list|()
operator|.
name|getParent
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
name|validPartitions
operator|.
name|size
argument_list|()
operator|==
literal|0
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"No partition is generated by dynamic partitioning"
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|validPartitions
operator|.
name|size
argument_list|()
operator|>
name|conf
operator|.
name|getIntVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|DYNAMICPARTITIONMAXPARTS
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Number of dynamic partitions created is "
operator|+
name|validPartitions
operator|.
name|size
argument_list|()
operator|+
literal|", which is more than "
operator|+
name|conf
operator|.
name|getIntVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|DYNAMICPARTITIONMAXPARTS
argument_list|)
operator|+
literal|". To solve this try to set "
operator|+
name|HiveConf
operator|.
name|ConfVars
operator|.
name|DYNAMICPARTITIONMAXPARTS
operator|.
name|varname
operator|+
literal|" to at least "
operator|+
name|validPartitions
operator|.
name|size
argument_list|()
operator|+
literal|'.'
argument_list|)
throw|;
block|}
name|Table
name|tbl
init|=
name|getTable
argument_list|(
name|tableName
argument_list|)
decl_stmt|;
comment|// for each dynamically created DP directory, construct a full partition spec
comment|// and load the partition based on that
name|Iterator
argument_list|<
name|Path
argument_list|>
name|iter
init|=
name|validPartitions
operator|.
name|iterator
argument_list|()
decl_stmt|;
while|while
condition|(
name|iter
operator|.
name|hasNext
argument_list|()
condition|)
block|{
comment|// get the dynamically created directory
name|Path
name|partPath
init|=
name|iter
operator|.
name|next
argument_list|()
decl_stmt|;
assert|assert
name|fs
operator|.
name|getFileStatus
argument_list|(
name|partPath
argument_list|)
operator|.
name|isDir
argument_list|()
operator|:
literal|"partitions "
operator|+
name|partPath
operator|+
literal|" is not a directory !"
assert|;
comment|// generate a full partition specification
name|LinkedHashMap
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|fullPartSpec
init|=
operator|new
name|LinkedHashMap
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
argument_list|(
name|partSpec
argument_list|)
decl_stmt|;
name|Warehouse
operator|.
name|makeSpecFromName
argument_list|(
name|fullPartSpec
argument_list|,
name|partPath
argument_list|)
expr_stmt|;
name|Partition
name|newPartition
init|=
name|loadPartition
argument_list|(
name|partPath
argument_list|,
name|tbl
argument_list|,
name|fullPartSpec
argument_list|,
name|replace
argument_list|,
name|holdDDLTime
argument_list|,
literal|true
argument_list|,
name|listBucketingEnabled
argument_list|,
literal|false
argument_list|,
name|isAcid
argument_list|)
decl_stmt|;
name|partitionsMap
operator|.
name|put
argument_list|(
name|fullPartSpec
argument_list|,
name|newPartition
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"New loading path = "
operator|+
name|partPath
operator|+
literal|" with partSpec "
operator|+
name|fullPartSpec
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|isAcid
condition|)
block|{
name|List
argument_list|<
name|String
argument_list|>
name|partNames
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|(
name|partitionsMap
operator|.
name|size
argument_list|()
argument_list|)
decl_stmt|;
for|for
control|(
name|Partition
name|p
range|:
name|partitionsMap
operator|.
name|values
argument_list|()
control|)
block|{
name|partNames
operator|.
name|add
argument_list|(
name|p
operator|.
name|getName
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|metaStoreClient
operator|.
name|addDynamicPartitions
argument_list|(
name|txnId
argument_list|,
name|tbl
operator|.
name|getDbName
argument_list|()
argument_list|,
name|tbl
operator|.
name|getTableName
argument_list|()
argument_list|,
name|partNames
argument_list|)
expr_stmt|;
block|}
return|return
name|partitionsMap
return|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
catch|catch
parameter_list|(
name|TException
name|te
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|te
argument_list|)
throw|;
block|}
block|}
comment|/**    * Load a directory into a Hive Table. - Alters existing content of table with    * the contents of loadPath. - If table does not exist - an exception is    * thrown - files in loadPath are moved into Hive. But the directory itself is    * not removed.    *    * @param loadPath    *          Directory containing files to load into Table    * @param tableName    *          name of table to be loaded.    * @param replace    *          if true - replace files in the table, otherwise add files to table    * @param holdDDLTime    * @param isSrcLocal    *          If the source directory is LOCAL    * @param isSkewedStoreAsSubdir    *          if list bucketing enabled    * @param isAcid true if this is an ACID based write    */
specifier|public
name|void
name|loadTable
parameter_list|(
name|Path
name|loadPath
parameter_list|,
name|String
name|tableName
parameter_list|,
name|boolean
name|replace
parameter_list|,
name|boolean
name|holdDDLTime
parameter_list|,
name|boolean
name|isSrcLocal
parameter_list|,
name|boolean
name|isSkewedStoreAsSubdir
parameter_list|,
name|boolean
name|isAcid
parameter_list|)
throws|throws
name|HiveException
block|{
name|List
argument_list|<
name|Path
argument_list|>
name|newFiles
init|=
operator|new
name|ArrayList
argument_list|<
name|Path
argument_list|>
argument_list|()
decl_stmt|;
name|Table
name|tbl
init|=
name|getTable
argument_list|(
name|tableName
argument_list|)
decl_stmt|;
name|HiveConf
name|sessionConf
init|=
name|SessionState
operator|.
name|getSessionConf
argument_list|()
decl_stmt|;
if|if
condition|(
name|replace
condition|)
block|{
name|Path
name|tableDest
init|=
name|tbl
operator|.
name|getPath
argument_list|()
decl_stmt|;
name|replaceFiles
argument_list|(
name|tableDest
argument_list|,
name|loadPath
argument_list|,
name|tableDest
argument_list|,
name|tableDest
argument_list|,
name|sessionConf
argument_list|,
name|isSrcLocal
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|FileSystem
name|fs
decl_stmt|;
try|try
block|{
name|fs
operator|=
name|tbl
operator|.
name|getDataLocation
argument_list|()
operator|.
name|getFileSystem
argument_list|(
name|sessionConf
argument_list|)
expr_stmt|;
name|copyFiles
argument_list|(
name|sessionConf
argument_list|,
name|loadPath
argument_list|,
name|tbl
operator|.
name|getPath
argument_list|()
argument_list|,
name|fs
argument_list|,
name|isSrcLocal
argument_list|,
name|isAcid
argument_list|,
name|newFiles
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"addFiles: filesystem error in check phase"
argument_list|,
name|e
argument_list|)
throw|;
block|}
name|tbl
operator|.
name|getParameters
argument_list|()
operator|.
name|put
argument_list|(
name|StatsSetupConst
operator|.
name|STATS_GENERATED_VIA_STATS_TASK
argument_list|,
literal|"true"
argument_list|)
expr_stmt|;
block|}
try|try
block|{
if|if
condition|(
name|isSkewedStoreAsSubdir
condition|)
block|{
name|SkewedInfo
name|skewedInfo
init|=
name|tbl
operator|.
name|getSkewedInfo
argument_list|()
decl_stmt|;
comment|// Construct list bucketing location mappings from sub-directory name.
name|Map
argument_list|<
name|List
argument_list|<
name|String
argument_list|>
argument_list|,
name|String
argument_list|>
name|skewedColValueLocationMaps
init|=
name|constructListBucketingLocationMap
argument_list|(
name|tbl
operator|.
name|getPath
argument_list|()
argument_list|,
name|skewedInfo
argument_list|)
decl_stmt|;
comment|// Add list bucketing location mappings.
name|skewedInfo
operator|.
name|setSkewedColValueLocationMaps
argument_list|(
name|skewedColValueLocationMaps
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
if|if
condition|(
operator|!
name|holdDDLTime
condition|)
block|{
try|try
block|{
name|alterTable
argument_list|(
name|tableName
argument_list|,
name|tbl
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|InvalidOperationException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
name|fireInsertEvent
argument_list|(
name|tbl
argument_list|,
literal|null
argument_list|,
name|newFiles
argument_list|)
expr_stmt|;
block|}
comment|/**    * Creates a partition.    *    * @param tbl    *          table for which partition needs to be created    * @param partSpec    *          partition keys and their values    * @return created partition object    * @throws HiveException    *           if table doesn't exist or partition already exists    */
specifier|public
name|Partition
name|createPartition
parameter_list|(
name|Table
name|tbl
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|partSpec
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
operator|new
name|Partition
argument_list|(
name|tbl
argument_list|,
name|getMSC
argument_list|()
operator|.
name|add_partition
argument_list|(
name|Partition
operator|.
name|createMetaPartitionObject
argument_list|(
name|tbl
argument_list|,
name|partSpec
argument_list|,
literal|null
argument_list|)
argument_list|)
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|List
argument_list|<
name|Partition
argument_list|>
name|createPartitions
parameter_list|(
name|AddPartitionDesc
name|addPartitionDesc
parameter_list|)
throws|throws
name|HiveException
block|{
name|Table
name|tbl
init|=
name|getTable
argument_list|(
name|addPartitionDesc
operator|.
name|getDbName
argument_list|()
argument_list|,
name|addPartitionDesc
operator|.
name|getTableName
argument_list|()
argument_list|)
decl_stmt|;
name|int
name|size
init|=
name|addPartitionDesc
operator|.
name|getPartitionCount
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
argument_list|>
name|in
init|=
operator|new
name|ArrayList
argument_list|<
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
argument_list|>
argument_list|(
name|size
argument_list|)
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|size
condition|;
operator|++
name|i
control|)
block|{
name|in
operator|.
name|add
argument_list|(
name|convertAddSpecToMetaPartition
argument_list|(
name|tbl
argument_list|,
name|addPartitionDesc
operator|.
name|getPartition
argument_list|(
name|i
argument_list|)
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|List
argument_list|<
name|Partition
argument_list|>
name|out
init|=
operator|new
name|ArrayList
argument_list|<
name|Partition
argument_list|>
argument_list|()
decl_stmt|;
try|try
block|{
if|if
condition|(
operator|!
name|addPartitionDesc
operator|.
name|getReplaceMode
argument_list|()
condition|)
block|{
comment|// TODO: normally, the result is not necessary; might make sense to pass false
for|for
control|(
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
name|outPart
range|:
name|getMSC
argument_list|()
operator|.
name|add_partitions
argument_list|(
name|in
argument_list|,
name|addPartitionDesc
operator|.
name|isIfNotExists
argument_list|()
argument_list|,
literal|true
argument_list|)
control|)
block|{
name|out
operator|.
name|add
argument_list|(
operator|new
name|Partition
argument_list|(
name|tbl
argument_list|,
name|outPart
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
name|getMSC
argument_list|()
operator|.
name|alter_partitions
argument_list|(
name|addPartitionDesc
operator|.
name|getDbName
argument_list|()
argument_list|,
name|addPartitionDesc
operator|.
name|getTableName
argument_list|()
argument_list|,
name|in
argument_list|)
expr_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|part_names
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
name|p
range|:
name|in
control|)
block|{
name|part_names
operator|.
name|add
argument_list|(
name|Warehouse
operator|.
name|makePartName
argument_list|(
name|tbl
operator|.
name|getPartitionKeys
argument_list|()
argument_list|,
name|p
operator|.
name|getValues
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
for|for
control|(
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
name|outPart
range|:
name|getMSC
argument_list|()
operator|.
name|getPartitionsByNames
argument_list|(
name|addPartitionDesc
operator|.
name|getDbName
argument_list|()
argument_list|,
name|addPartitionDesc
operator|.
name|getTableName
argument_list|()
argument_list|,
name|part_names
argument_list|)
control|)
block|{
name|out
operator|.
name|add
argument_list|(
operator|new
name|Partition
argument_list|(
name|tbl
argument_list|,
name|outPart
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
return|return
name|out
return|;
block|}
specifier|private
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
name|convertAddSpecToMetaPartition
parameter_list|(
name|Table
name|tbl
parameter_list|,
name|AddPartitionDesc
operator|.
name|OnePartitionDesc
name|addSpec
parameter_list|)
throws|throws
name|HiveException
block|{
name|Path
name|location
init|=
name|addSpec
operator|.
name|getLocation
argument_list|()
operator|!=
literal|null
condition|?
operator|new
name|Path
argument_list|(
name|tbl
operator|.
name|getPath
argument_list|()
argument_list|,
name|addSpec
operator|.
name|getLocation
argument_list|()
argument_list|)
else|:
literal|null
decl_stmt|;
if|if
condition|(
name|location
operator|!=
literal|null
operator|&&
operator|!
name|Utilities
operator|.
name|isDefaultNameNode
argument_list|(
name|conf
argument_list|)
condition|)
block|{
comment|// Ensure that it is a full qualified path (in most cases it will be since tbl.getPath() is full qualified)
name|location
operator|=
operator|new
name|Path
argument_list|(
name|Utilities
operator|.
name|getQualifiedPath
argument_list|(
name|conf
argument_list|,
name|location
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
name|part
init|=
name|Partition
operator|.
name|createMetaPartitionObject
argument_list|(
name|tbl
argument_list|,
name|addSpec
operator|.
name|getPartSpec
argument_list|()
argument_list|,
name|location
argument_list|)
decl_stmt|;
if|if
condition|(
name|addSpec
operator|.
name|getPartParams
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|part
operator|.
name|setParameters
argument_list|(
name|addSpec
operator|.
name|getPartParams
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|addSpec
operator|.
name|getInputFormat
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|part
operator|.
name|getSd
argument_list|()
operator|.
name|setInputFormat
argument_list|(
name|addSpec
operator|.
name|getInputFormat
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|addSpec
operator|.
name|getOutputFormat
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|part
operator|.
name|getSd
argument_list|()
operator|.
name|setOutputFormat
argument_list|(
name|addSpec
operator|.
name|getOutputFormat
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|addSpec
operator|.
name|getNumBuckets
argument_list|()
operator|!=
operator|-
literal|1
condition|)
block|{
name|part
operator|.
name|getSd
argument_list|()
operator|.
name|setNumBuckets
argument_list|(
name|addSpec
operator|.
name|getNumBuckets
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|addSpec
operator|.
name|getCols
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|part
operator|.
name|getSd
argument_list|()
operator|.
name|setCols
argument_list|(
name|addSpec
operator|.
name|getCols
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|addSpec
operator|.
name|getSerializationLib
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|part
operator|.
name|getSd
argument_list|()
operator|.
name|getSerdeInfo
argument_list|()
operator|.
name|setSerializationLib
argument_list|(
name|addSpec
operator|.
name|getSerializationLib
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|addSpec
operator|.
name|getSerdeParams
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|part
operator|.
name|getSd
argument_list|()
operator|.
name|getSerdeInfo
argument_list|()
operator|.
name|setParameters
argument_list|(
name|addSpec
operator|.
name|getSerdeParams
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|addSpec
operator|.
name|getBucketCols
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|part
operator|.
name|getSd
argument_list|()
operator|.
name|setBucketCols
argument_list|(
name|addSpec
operator|.
name|getBucketCols
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|addSpec
operator|.
name|getSortCols
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|part
operator|.
name|getSd
argument_list|()
operator|.
name|setSortCols
argument_list|(
name|addSpec
operator|.
name|getSortCols
argument_list|()
argument_list|)
expr_stmt|;
block|}
return|return
name|part
return|;
block|}
specifier|public
name|Partition
name|getPartition
parameter_list|(
name|Table
name|tbl
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|partSpec
parameter_list|,
name|boolean
name|forceCreate
parameter_list|)
throws|throws
name|HiveException
block|{
return|return
name|getPartition
argument_list|(
name|tbl
argument_list|,
name|partSpec
argument_list|,
name|forceCreate
argument_list|,
literal|null
argument_list|,
literal|true
argument_list|,
literal|null
argument_list|)
return|;
block|}
comment|/**    * Returns partition metadata    *    * @param tbl    *          the partition's table    * @param partSpec    *          partition keys and values    * @param forceCreate    *          if this is true and partition doesn't exist then a partition is    *          created    * @param partPath the path where the partition data is located    * @param inheritTableSpecs whether to copy over the table specs for if/of/serde    * @return result partition object or null if there is no partition    * @throws HiveException    */
specifier|public
name|Partition
name|getPartition
parameter_list|(
name|Table
name|tbl
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|partSpec
parameter_list|,
name|boolean
name|forceCreate
parameter_list|,
name|String
name|partPath
parameter_list|,
name|boolean
name|inheritTableSpecs
parameter_list|)
throws|throws
name|HiveException
block|{
return|return
name|getPartition
argument_list|(
name|tbl
argument_list|,
name|partSpec
argument_list|,
name|forceCreate
argument_list|,
name|partPath
argument_list|,
name|inheritTableSpecs
argument_list|,
literal|null
argument_list|)
return|;
block|}
comment|/**    * Returns partition metadata    *    * @param tbl    *          the partition's table    * @param partSpec    *          partition keys and values    * @param forceCreate    *          if this is true and partition doesn't exist then a partition is    *          created    * @param partPath the path where the partition data is located    * @param inheritTableSpecs whether to copy over the table specs for if/of/serde    * @param newFiles An optional list of new files that were moved into this partition.  If    *                 non-null these will be included in the DML event sent to the metastore.    * @return result partition object or null if there is no partition    * @throws HiveException    */
specifier|public
name|Partition
name|getPartition
parameter_list|(
name|Table
name|tbl
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|partSpec
parameter_list|,
name|boolean
name|forceCreate
parameter_list|,
name|String
name|partPath
parameter_list|,
name|boolean
name|inheritTableSpecs
parameter_list|,
name|List
argument_list|<
name|Path
argument_list|>
name|newFiles
parameter_list|)
throws|throws
name|HiveException
block|{
name|tbl
operator|.
name|validatePartColumnNames
argument_list|(
name|partSpec
argument_list|,
literal|true
argument_list|)
expr_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|pvals
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|FieldSchema
name|field
range|:
name|tbl
operator|.
name|getPartCols
argument_list|()
control|)
block|{
name|String
name|val
init|=
name|partSpec
operator|.
name|get
argument_list|(
name|field
operator|.
name|getName
argument_list|()
argument_list|)
decl_stmt|;
comment|// enable dynamic partitioning
if|if
condition|(
operator|(
name|val
operator|==
literal|null
operator|&&
operator|!
name|HiveConf
operator|.
name|getBoolVar
argument_list|(
name|conf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|DYNAMICPARTITIONING
argument_list|)
operator|)
operator|||
operator|(
name|val
operator|!=
literal|null
operator|&&
name|val
operator|.
name|length
argument_list|()
operator|==
literal|0
operator|)
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"get partition: Value for key "
operator|+
name|field
operator|.
name|getName
argument_list|()
operator|+
literal|" is null or empty"
argument_list|)
throw|;
block|}
elseif|else
if|if
condition|(
name|val
operator|!=
literal|null
condition|)
block|{
name|pvals
operator|.
name|add
argument_list|(
name|val
argument_list|)
expr_stmt|;
block|}
block|}
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
name|tpart
init|=
literal|null
decl_stmt|;
try|try
block|{
name|tpart
operator|=
name|getMSC
argument_list|()
operator|.
name|getPartitionWithAuthInfo
argument_list|(
name|tbl
operator|.
name|getDbName
argument_list|()
argument_list|,
name|tbl
operator|.
name|getTableName
argument_list|()
argument_list|,
name|pvals
argument_list|,
name|getUserName
argument_list|()
argument_list|,
name|getGroupNames
argument_list|()
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|NoSuchObjectException
name|nsoe
parameter_list|)
block|{
comment|// this means no partition exists for the given partition
comment|// key value pairs - thrift cannot handle null return values, hence
comment|// getPartition() throws NoSuchObjectException to indicate null partition
name|tpart
operator|=
literal|null
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
try|try
block|{
if|if
condition|(
name|forceCreate
condition|)
block|{
if|if
condition|(
name|tpart
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"creating partition for table "
operator|+
name|tbl
operator|.
name|getTableName
argument_list|()
operator|+
literal|" with partition spec : "
operator|+
name|partSpec
argument_list|)
expr_stmt|;
try|try
block|{
name|tpart
operator|=
name|getMSC
argument_list|()
operator|.
name|appendPartition
argument_list|(
name|tbl
operator|.
name|getDbName
argument_list|()
argument_list|,
name|tbl
operator|.
name|getTableName
argument_list|()
argument_list|,
name|pvals
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|AlreadyExistsException
name|aee
parameter_list|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Caught already exists exception, trying to alter partition instead"
argument_list|)
expr_stmt|;
name|tpart
operator|=
name|getMSC
argument_list|()
operator|.
name|getPartitionWithAuthInfo
argument_list|(
name|tbl
operator|.
name|getDbName
argument_list|()
argument_list|,
name|tbl
operator|.
name|getTableName
argument_list|()
argument_list|,
name|pvals
argument_list|,
name|getUserName
argument_list|()
argument_list|,
name|getGroupNames
argument_list|()
argument_list|)
expr_stmt|;
name|alterPartitionSpec
argument_list|(
name|tbl
argument_list|,
name|partSpec
argument_list|,
name|tpart
argument_list|,
name|inheritTableSpecs
argument_list|,
name|partPath
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
if|if
condition|(
name|CheckJDOException
operator|.
name|isJDODataStoreException
argument_list|(
name|e
argument_list|)
condition|)
block|{
comment|// Using utility method above, so that JDODataStoreException doesn't
comment|// have to be used here. This helps avoid adding jdo dependency for
comment|// hcatalog client uses
name|LOG
operator|.
name|debug
argument_list|(
literal|"Caught JDO exception, trying to alter partition instead"
argument_list|)
expr_stmt|;
name|tpart
operator|=
name|getMSC
argument_list|()
operator|.
name|getPartitionWithAuthInfo
argument_list|(
name|tbl
operator|.
name|getDbName
argument_list|()
argument_list|,
name|tbl
operator|.
name|getTableName
argument_list|()
argument_list|,
name|pvals
argument_list|,
name|getUserName
argument_list|()
argument_list|,
name|getGroupNames
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|tpart
operator|==
literal|null
condition|)
block|{
comment|// This means the exception was caused by something other than a race condition
comment|// in creating the partition, since the partition still doesn't exist.
throw|throw
name|e
throw|;
block|}
name|alterPartitionSpec
argument_list|(
name|tbl
argument_list|,
name|partSpec
argument_list|,
name|tpart
argument_list|,
name|inheritTableSpecs
argument_list|,
name|partPath
argument_list|)
expr_stmt|;
block|}
else|else
block|{
throw|throw
name|e
throw|;
block|}
block|}
block|}
else|else
block|{
name|alterPartitionSpec
argument_list|(
name|tbl
argument_list|,
name|partSpec
argument_list|,
name|tpart
argument_list|,
name|inheritTableSpecs
argument_list|,
name|partPath
argument_list|)
expr_stmt|;
name|fireInsertEvent
argument_list|(
name|tbl
argument_list|,
name|partSpec
argument_list|,
name|newFiles
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
name|tpart
operator|==
literal|null
condition|)
block|{
return|return
literal|null
return|;
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
return|return
operator|new
name|Partition
argument_list|(
name|tbl
argument_list|,
name|tpart
argument_list|)
return|;
block|}
specifier|private
name|void
name|alterPartitionSpec
parameter_list|(
name|Table
name|tbl
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|partSpec
parameter_list|,
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
name|tpart
parameter_list|,
name|boolean
name|inheritTableSpecs
parameter_list|,
name|String
name|partPath
parameter_list|)
throws|throws
name|HiveException
throws|,
name|InvalidOperationException
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"altering partition for table "
operator|+
name|tbl
operator|.
name|getTableName
argument_list|()
operator|+
literal|" with partition spec : "
operator|+
name|partSpec
argument_list|)
expr_stmt|;
if|if
condition|(
name|inheritTableSpecs
condition|)
block|{
name|tpart
operator|.
name|getSd
argument_list|()
operator|.
name|setOutputFormat
argument_list|(
name|tbl
operator|.
name|getTTable
argument_list|()
operator|.
name|getSd
argument_list|()
operator|.
name|getOutputFormat
argument_list|()
argument_list|)
expr_stmt|;
name|tpart
operator|.
name|getSd
argument_list|()
operator|.
name|setInputFormat
argument_list|(
name|tbl
operator|.
name|getTTable
argument_list|()
operator|.
name|getSd
argument_list|()
operator|.
name|getInputFormat
argument_list|()
argument_list|)
expr_stmt|;
name|tpart
operator|.
name|getSd
argument_list|()
operator|.
name|getSerdeInfo
argument_list|()
operator|.
name|setSerializationLib
argument_list|(
name|tbl
operator|.
name|getSerializationLib
argument_list|()
argument_list|)
expr_stmt|;
name|tpart
operator|.
name|getSd
argument_list|()
operator|.
name|getSerdeInfo
argument_list|()
operator|.
name|setParameters
argument_list|(
name|tbl
operator|.
name|getTTable
argument_list|()
operator|.
name|getSd
argument_list|()
operator|.
name|getSerdeInfo
argument_list|()
operator|.
name|getParameters
argument_list|()
argument_list|)
expr_stmt|;
name|tpart
operator|.
name|getSd
argument_list|()
operator|.
name|setBucketCols
argument_list|(
name|tbl
operator|.
name|getBucketCols
argument_list|()
argument_list|)
expr_stmt|;
name|tpart
operator|.
name|getSd
argument_list|()
operator|.
name|setNumBuckets
argument_list|(
name|tbl
operator|.
name|getNumBuckets
argument_list|()
argument_list|)
expr_stmt|;
name|tpart
operator|.
name|getSd
argument_list|()
operator|.
name|setSortCols
argument_list|(
name|tbl
operator|.
name|getSortCols
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|partPath
operator|==
literal|null
operator|||
name|partPath
operator|.
name|trim
argument_list|()
operator|.
name|equals
argument_list|(
literal|""
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"new partition path should not be null or empty."
argument_list|)
throw|;
block|}
name|tpart
operator|.
name|getSd
argument_list|()
operator|.
name|setLocation
argument_list|(
name|partPath
argument_list|)
expr_stmt|;
name|tpart
operator|.
name|getParameters
argument_list|()
operator|.
name|put
argument_list|(
name|StatsSetupConst
operator|.
name|STATS_GENERATED_VIA_STATS_TASK
argument_list|,
literal|"true"
argument_list|)
expr_stmt|;
name|String
name|fullName
init|=
name|tbl
operator|.
name|getTableName
argument_list|()
decl_stmt|;
if|if
condition|(
operator|!
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|lang
operator|.
name|StringUtils
operator|.
name|isEmpty
argument_list|(
name|tbl
operator|.
name|getDbName
argument_list|()
argument_list|)
condition|)
block|{
name|fullName
operator|=
name|tbl
operator|.
name|getDbName
argument_list|()
operator|+
literal|"."
operator|+
name|tbl
operator|.
name|getTableName
argument_list|()
expr_stmt|;
block|}
name|alterPartition
argument_list|(
name|fullName
argument_list|,
operator|new
name|Partition
argument_list|(
name|tbl
argument_list|,
name|tpart
argument_list|)
argument_list|)
expr_stmt|;
block|}
specifier|private
name|void
name|fireInsertEvent
parameter_list|(
name|Table
name|tbl
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|partitionSpec
parameter_list|,
name|List
argument_list|<
name|Path
argument_list|>
name|newFiles
parameter_list|)
throws|throws
name|HiveException
block|{
if|if
condition|(
name|conf
operator|.
name|getBoolVar
argument_list|(
name|ConfVars
operator|.
name|FIRE_EVENTS_FOR_DML
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Firing dml insert event"
argument_list|)
expr_stmt|;
if|if
condition|(
name|tbl
operator|.
name|isTemporary
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Not firing dml insert event as "
operator|+
name|tbl
operator|.
name|getTableName
argument_list|()
operator|+
literal|" is temporary"
argument_list|)
expr_stmt|;
return|return;
block|}
name|FireEventRequestData
name|data
init|=
operator|new
name|FireEventRequestData
argument_list|()
decl_stmt|;
name|InsertEventRequestData
name|insertData
init|=
operator|new
name|InsertEventRequestData
argument_list|()
decl_stmt|;
name|data
operator|.
name|setInsertData
argument_list|(
name|insertData
argument_list|)
expr_stmt|;
if|if
condition|(
name|newFiles
operator|!=
literal|null
operator|&&
name|newFiles
operator|.
name|size
argument_list|()
operator|>
literal|0
condition|)
block|{
for|for
control|(
name|Path
name|p
range|:
name|newFiles
control|)
block|{
name|insertData
operator|.
name|addToFilesAdded
argument_list|(
name|p
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
name|FireEventRequest
name|rqst
init|=
operator|new
name|FireEventRequest
argument_list|(
literal|true
argument_list|,
name|data
argument_list|)
decl_stmt|;
name|rqst
operator|.
name|setDbName
argument_list|(
name|tbl
operator|.
name|getDbName
argument_list|()
argument_list|)
expr_stmt|;
name|rqst
operator|.
name|setTableName
argument_list|(
name|tbl
operator|.
name|getTableName
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|partitionSpec
operator|!=
literal|null
operator|&&
name|partitionSpec
operator|.
name|size
argument_list|()
operator|>
literal|0
condition|)
block|{
name|List
argument_list|<
name|String
argument_list|>
name|partVals
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|(
name|partitionSpec
operator|.
name|size
argument_list|()
argument_list|)
decl_stmt|;
for|for
control|(
name|FieldSchema
name|fs
range|:
name|tbl
operator|.
name|getPartitionKeys
argument_list|()
control|)
block|{
name|partVals
operator|.
name|add
argument_list|(
name|partitionSpec
operator|.
name|get
argument_list|(
name|fs
operator|.
name|getName
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|rqst
operator|.
name|setPartitionVals
argument_list|(
name|partVals
argument_list|)
expr_stmt|;
block|}
try|try
block|{
name|getMSC
argument_list|()
operator|.
name|fireListenerEvent
argument_list|(
name|rqst
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|TException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
block|}
specifier|public
name|boolean
name|dropPartition
parameter_list|(
name|String
name|tblName
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|part_vals
parameter_list|,
name|boolean
name|deleteData
parameter_list|)
throws|throws
name|HiveException
block|{
name|String
index|[]
name|names
init|=
name|Utilities
operator|.
name|getDbTableName
argument_list|(
name|tblName
argument_list|)
decl_stmt|;
return|return
name|dropPartition
argument_list|(
name|names
index|[
literal|0
index|]
argument_list|,
name|names
index|[
literal|1
index|]
argument_list|,
name|part_vals
argument_list|,
name|deleteData
argument_list|)
return|;
block|}
specifier|public
name|boolean
name|dropPartition
parameter_list|(
name|String
name|db_name
parameter_list|,
name|String
name|tbl_name
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|part_vals
parameter_list|,
name|boolean
name|deleteData
parameter_list|)
throws|throws
name|HiveException
block|{
return|return
name|dropPartition
argument_list|(
name|db_name
argument_list|,
name|tbl_name
argument_list|,
name|part_vals
argument_list|,
name|PartitionDropOptions
operator|.
name|instance
argument_list|()
operator|.
name|deleteData
argument_list|(
name|deleteData
argument_list|)
argument_list|)
return|;
block|}
specifier|public
name|boolean
name|dropPartition
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|tableName
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|partVals
parameter_list|,
name|PartitionDropOptions
name|options
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|dropPartition
argument_list|(
name|dbName
argument_list|,
name|tableName
argument_list|,
name|partVals
argument_list|,
name|options
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|NoSuchObjectException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Partition or table doesn't exist."
argument_list|,
name|e
argument_list|)
throw|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|List
argument_list|<
name|Partition
argument_list|>
name|dropPartitions
parameter_list|(
name|String
name|tblName
parameter_list|,
name|List
argument_list|<
name|DropTableDesc
operator|.
name|PartSpec
argument_list|>
name|partSpecs
parameter_list|,
name|boolean
name|deleteData
parameter_list|,
name|boolean
name|ignoreProtection
parameter_list|,
name|boolean
name|ifExists
parameter_list|)
throws|throws
name|HiveException
block|{
name|String
index|[]
name|names
init|=
name|Utilities
operator|.
name|getDbTableName
argument_list|(
name|tblName
argument_list|)
decl_stmt|;
return|return
name|dropPartitions
argument_list|(
name|names
index|[
literal|0
index|]
argument_list|,
name|names
index|[
literal|1
index|]
argument_list|,
name|partSpecs
argument_list|,
name|deleteData
argument_list|,
name|ignoreProtection
argument_list|,
name|ifExists
argument_list|)
return|;
block|}
specifier|public
name|List
argument_list|<
name|Partition
argument_list|>
name|dropPartitions
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|tblName
parameter_list|,
name|List
argument_list|<
name|DropTableDesc
operator|.
name|PartSpec
argument_list|>
name|partSpecs
parameter_list|,
name|boolean
name|deleteData
parameter_list|,
name|boolean
name|ignoreProtection
parameter_list|,
name|boolean
name|ifExists
parameter_list|)
throws|throws
name|HiveException
block|{
return|return
name|dropPartitions
argument_list|(
name|dbName
argument_list|,
name|tblName
argument_list|,
name|partSpecs
argument_list|,
name|PartitionDropOptions
operator|.
name|instance
argument_list|()
operator|.
name|deleteData
argument_list|(
name|deleteData
argument_list|)
operator|.
name|ignoreProtection
argument_list|(
name|ignoreProtection
argument_list|)
operator|.
name|ifExists
argument_list|(
name|ifExists
argument_list|)
argument_list|)
return|;
block|}
specifier|public
name|List
argument_list|<
name|Partition
argument_list|>
name|dropPartitions
parameter_list|(
name|String
name|tblName
parameter_list|,
name|List
argument_list|<
name|DropTableDesc
operator|.
name|PartSpec
argument_list|>
name|partSpecs
parameter_list|,
name|PartitionDropOptions
name|dropOptions
parameter_list|)
throws|throws
name|HiveException
block|{
name|String
index|[]
name|names
init|=
name|Utilities
operator|.
name|getDbTableName
argument_list|(
name|tblName
argument_list|)
decl_stmt|;
return|return
name|dropPartitions
argument_list|(
name|names
index|[
literal|0
index|]
argument_list|,
name|names
index|[
literal|1
index|]
argument_list|,
name|partSpecs
argument_list|,
name|dropOptions
argument_list|)
return|;
block|}
specifier|public
name|List
argument_list|<
name|Partition
argument_list|>
name|dropPartitions
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|tblName
parameter_list|,
name|List
argument_list|<
name|DropTableDesc
operator|.
name|PartSpec
argument_list|>
name|partSpecs
parameter_list|,
name|PartitionDropOptions
name|dropOptions
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|Table
name|tbl
init|=
name|getTable
argument_list|(
name|dbName
argument_list|,
name|tblName
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|ObjectPair
argument_list|<
name|Integer
argument_list|,
name|byte
index|[]
argument_list|>
argument_list|>
name|partExprs
init|=
operator|new
name|ArrayList
argument_list|<
name|ObjectPair
argument_list|<
name|Integer
argument_list|,
name|byte
index|[]
argument_list|>
argument_list|>
argument_list|(
name|partSpecs
operator|.
name|size
argument_list|()
argument_list|)
decl_stmt|;
for|for
control|(
name|DropTableDesc
operator|.
name|PartSpec
name|partSpec
range|:
name|partSpecs
control|)
block|{
name|partExprs
operator|.
name|add
argument_list|(
operator|new
name|ObjectPair
argument_list|<
name|Integer
argument_list|,
name|byte
index|[]
argument_list|>
argument_list|(
name|partSpec
operator|.
name|getPrefixLength
argument_list|()
argument_list|,
name|Utilities
operator|.
name|serializeExpressionToKryo
argument_list|(
name|partSpec
operator|.
name|getPartSpec
argument_list|()
argument_list|)
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|List
argument_list|<
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
argument_list|>
name|tParts
init|=
name|getMSC
argument_list|()
operator|.
name|dropPartitions
argument_list|(
name|dbName
argument_list|,
name|tblName
argument_list|,
name|partExprs
argument_list|,
name|dropOptions
argument_list|)
decl_stmt|;
return|return
name|convertFromMetastore
argument_list|(
name|tbl
argument_list|,
name|tParts
argument_list|,
literal|null
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|NoSuchObjectException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Partition or table doesn't exist."
argument_list|,
name|e
argument_list|)
throw|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|List
argument_list|<
name|String
argument_list|>
name|getPartitionNames
parameter_list|(
name|String
name|tblName
parameter_list|,
name|short
name|max
parameter_list|)
throws|throws
name|HiveException
block|{
name|String
index|[]
name|names
init|=
name|Utilities
operator|.
name|getDbTableName
argument_list|(
name|tblName
argument_list|)
decl_stmt|;
return|return
name|getPartitionNames
argument_list|(
name|names
index|[
literal|0
index|]
argument_list|,
name|names
index|[
literal|1
index|]
argument_list|,
name|max
argument_list|)
return|;
block|}
specifier|public
name|List
argument_list|<
name|String
argument_list|>
name|getPartitionNames
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|tblName
parameter_list|,
name|short
name|max
parameter_list|)
throws|throws
name|HiveException
block|{
name|List
argument_list|<
name|String
argument_list|>
name|names
init|=
literal|null
decl_stmt|;
try|try
block|{
name|names
operator|=
name|getMSC
argument_list|()
operator|.
name|listPartitionNames
argument_list|(
name|dbName
argument_list|,
name|tblName
argument_list|,
name|max
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
return|return
name|names
return|;
block|}
specifier|public
name|List
argument_list|<
name|String
argument_list|>
name|getPartitionNames
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|tblName
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|partSpec
parameter_list|,
name|short
name|max
parameter_list|)
throws|throws
name|HiveException
block|{
name|List
argument_list|<
name|String
argument_list|>
name|names
init|=
literal|null
decl_stmt|;
name|Table
name|t
init|=
name|getTable
argument_list|(
name|dbName
argument_list|,
name|tblName
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|pvals
init|=
name|MetaStoreUtils
operator|.
name|getPvals
argument_list|(
name|t
operator|.
name|getPartCols
argument_list|()
argument_list|,
name|partSpec
argument_list|)
decl_stmt|;
try|try
block|{
name|names
operator|=
name|getMSC
argument_list|()
operator|.
name|listPartitionNames
argument_list|(
name|dbName
argument_list|,
name|tblName
argument_list|,
name|pvals
argument_list|,
name|max
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
return|return
name|names
return|;
block|}
comment|/**    * get all the partitions that the table has    *    * @param tbl    *          object for which partition is needed    * @return list of partition objects    * @throws HiveException    */
specifier|public
name|List
argument_list|<
name|Partition
argument_list|>
name|getPartitions
parameter_list|(
name|Table
name|tbl
parameter_list|)
throws|throws
name|HiveException
block|{
if|if
condition|(
name|tbl
operator|.
name|isPartitioned
argument_list|()
condition|)
block|{
name|List
argument_list|<
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
argument_list|>
name|tParts
decl_stmt|;
try|try
block|{
name|tParts
operator|=
name|getMSC
argument_list|()
operator|.
name|listPartitionsWithAuthInfo
argument_list|(
name|tbl
operator|.
name|getDbName
argument_list|()
argument_list|,
name|tbl
operator|.
name|getTableName
argument_list|()
argument_list|,
operator|(
name|short
operator|)
operator|-
literal|1
argument_list|,
name|getUserName
argument_list|()
argument_list|,
name|getGroupNames
argument_list|()
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
name|List
argument_list|<
name|Partition
argument_list|>
name|parts
init|=
operator|new
name|ArrayList
argument_list|<
name|Partition
argument_list|>
argument_list|(
name|tParts
operator|.
name|size
argument_list|()
argument_list|)
decl_stmt|;
for|for
control|(
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
name|tpart
range|:
name|tParts
control|)
block|{
name|parts
operator|.
name|add
argument_list|(
operator|new
name|Partition
argument_list|(
name|tbl
argument_list|,
name|tpart
argument_list|)
argument_list|)
expr_stmt|;
block|}
return|return
name|parts
return|;
block|}
else|else
block|{
name|Partition
name|part
init|=
operator|new
name|Partition
argument_list|(
name|tbl
argument_list|)
decl_stmt|;
name|ArrayList
argument_list|<
name|Partition
argument_list|>
name|parts
init|=
operator|new
name|ArrayList
argument_list|<
name|Partition
argument_list|>
argument_list|(
literal|1
argument_list|)
decl_stmt|;
name|parts
operator|.
name|add
argument_list|(
name|part
argument_list|)
expr_stmt|;
return|return
name|parts
return|;
block|}
block|}
comment|/**    * Get all the partitions; unlike {@link #getPartitions(Table)}, does not include auth.    * @param tbl table for which partitions are needed    * @return list of partition objects    */
specifier|public
name|Set
argument_list|<
name|Partition
argument_list|>
name|getAllPartitionsOf
parameter_list|(
name|Table
name|tbl
parameter_list|)
throws|throws
name|HiveException
block|{
if|if
condition|(
operator|!
name|tbl
operator|.
name|isPartitioned
argument_list|()
condition|)
block|{
return|return
name|Sets
operator|.
name|newHashSet
argument_list|(
operator|new
name|Partition
argument_list|(
name|tbl
argument_list|)
argument_list|)
return|;
block|}
name|List
argument_list|<
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
argument_list|>
name|tParts
decl_stmt|;
try|try
block|{
name|tParts
operator|=
name|getMSC
argument_list|()
operator|.
name|listPartitions
argument_list|(
name|tbl
operator|.
name|getDbName
argument_list|()
argument_list|,
name|tbl
operator|.
name|getTableName
argument_list|()
argument_list|,
operator|(
name|short
operator|)
operator|-
literal|1
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
name|Set
argument_list|<
name|Partition
argument_list|>
name|parts
init|=
operator|new
name|LinkedHashSet
argument_list|<
name|Partition
argument_list|>
argument_list|(
name|tParts
operator|.
name|size
argument_list|()
argument_list|)
decl_stmt|;
for|for
control|(
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
name|tpart
range|:
name|tParts
control|)
block|{
name|parts
operator|.
name|add
argument_list|(
operator|new
name|Partition
argument_list|(
name|tbl
argument_list|,
name|tpart
argument_list|)
argument_list|)
expr_stmt|;
block|}
return|return
name|parts
return|;
block|}
comment|/**    * get all the partitions of the table that matches the given partial    * specification. partition columns whose value is can be anything should be    * an empty string.    *    * @param tbl    *          object for which partition is needed. Must be partitioned.    * @param limit number of partitions to return    * @return list of partition objects    * @throws HiveException    */
specifier|public
name|List
argument_list|<
name|Partition
argument_list|>
name|getPartitions
parameter_list|(
name|Table
name|tbl
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|partialPartSpec
parameter_list|,
name|short
name|limit
parameter_list|)
throws|throws
name|HiveException
block|{
if|if
condition|(
operator|!
name|tbl
operator|.
name|isPartitioned
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|ErrorMsg
operator|.
name|TABLE_NOT_PARTITIONED
argument_list|,
name|tbl
operator|.
name|getTableName
argument_list|()
argument_list|)
throw|;
block|}
name|List
argument_list|<
name|String
argument_list|>
name|partialPvals
init|=
name|MetaStoreUtils
operator|.
name|getPvals
argument_list|(
name|tbl
operator|.
name|getPartCols
argument_list|()
argument_list|,
name|partialPartSpec
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
argument_list|>
name|partitions
init|=
literal|null
decl_stmt|;
try|try
block|{
name|partitions
operator|=
name|getMSC
argument_list|()
operator|.
name|listPartitionsWithAuthInfo
argument_list|(
name|tbl
operator|.
name|getDbName
argument_list|()
argument_list|,
name|tbl
operator|.
name|getTableName
argument_list|()
argument_list|,
name|partialPvals
argument_list|,
name|limit
argument_list|,
name|getUserName
argument_list|()
argument_list|,
name|getGroupNames
argument_list|()
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
name|List
argument_list|<
name|Partition
argument_list|>
name|qlPartitions
init|=
operator|new
name|ArrayList
argument_list|<
name|Partition
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
name|p
range|:
name|partitions
control|)
block|{
name|qlPartitions
operator|.
name|add
argument_list|(
operator|new
name|Partition
argument_list|(
name|tbl
argument_list|,
name|p
argument_list|)
argument_list|)
expr_stmt|;
block|}
return|return
name|qlPartitions
return|;
block|}
comment|/**    * get all the partitions of the table that matches the given partial    * specification. partition columns whose value is can be anything should be    * an empty string.    *    * @param tbl    *          object for which partition is needed. Must be partitioned.    * @return list of partition objects    * @throws HiveException    */
specifier|public
name|List
argument_list|<
name|Partition
argument_list|>
name|getPartitions
parameter_list|(
name|Table
name|tbl
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|partialPartSpec
parameter_list|)
throws|throws
name|HiveException
block|{
return|return
name|getPartitions
argument_list|(
name|tbl
argument_list|,
name|partialPartSpec
argument_list|,
operator|(
name|short
operator|)
operator|-
literal|1
argument_list|)
return|;
block|}
comment|/**    * get all the partitions of the table that matches the given partial    * specification. partition columns whose value is can be anything should be    * an empty string.    *    * @param tbl    *          object for which partition is needed. Must be partitioned.    * @param partialPartSpec    *          partial partition specification (some subpartitions can be empty).    * @return list of partition objects    * @throws HiveException    */
specifier|public
name|List
argument_list|<
name|Partition
argument_list|>
name|getPartitionsByNames
parameter_list|(
name|Table
name|tbl
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|partialPartSpec
parameter_list|)
throws|throws
name|HiveException
block|{
if|if
condition|(
operator|!
name|tbl
operator|.
name|isPartitioned
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|ErrorMsg
operator|.
name|TABLE_NOT_PARTITIONED
argument_list|,
name|tbl
operator|.
name|getTableName
argument_list|()
argument_list|)
throw|;
block|}
name|List
argument_list|<
name|String
argument_list|>
name|names
init|=
name|getPartitionNames
argument_list|(
name|tbl
operator|.
name|getDbName
argument_list|()
argument_list|,
name|tbl
operator|.
name|getTableName
argument_list|()
argument_list|,
name|partialPartSpec
argument_list|,
operator|(
name|short
operator|)
operator|-
literal|1
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|Partition
argument_list|>
name|partitions
init|=
name|getPartitionsByNames
argument_list|(
name|tbl
argument_list|,
name|names
argument_list|)
decl_stmt|;
return|return
name|partitions
return|;
block|}
comment|/**    * Get all partitions of the table that matches the list of given partition names.    *    * @param tbl    *          object for which partition is needed. Must be partitioned.    * @param partNames    *          list of partition names    * @return list of partition objects    * @throws HiveException    */
specifier|public
name|List
argument_list|<
name|Partition
argument_list|>
name|getPartitionsByNames
parameter_list|(
name|Table
name|tbl
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|partNames
parameter_list|)
throws|throws
name|HiveException
block|{
if|if
condition|(
operator|!
name|tbl
operator|.
name|isPartitioned
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|ErrorMsg
operator|.
name|TABLE_NOT_PARTITIONED
argument_list|,
name|tbl
operator|.
name|getTableName
argument_list|()
argument_list|)
throw|;
block|}
name|List
argument_list|<
name|Partition
argument_list|>
name|partitions
init|=
operator|new
name|ArrayList
argument_list|<
name|Partition
argument_list|>
argument_list|(
name|partNames
operator|.
name|size
argument_list|()
argument_list|)
decl_stmt|;
name|int
name|batchSize
init|=
name|HiveConf
operator|.
name|getIntVar
argument_list|(
name|conf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|METASTORE_BATCH_RETRIEVE_MAX
argument_list|)
decl_stmt|;
comment|// TODO: might want to increase the default batch size. 1024 is viable; MS gets OOM if too high.
name|int
name|nParts
init|=
name|partNames
operator|.
name|size
argument_list|()
decl_stmt|;
name|int
name|nBatches
init|=
name|nParts
operator|/
name|batchSize
decl_stmt|;
try|try
block|{
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|nBatches
condition|;
operator|++
name|i
control|)
block|{
name|List
argument_list|<
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
argument_list|>
name|tParts
init|=
name|getMSC
argument_list|()
operator|.
name|getPartitionsByNames
argument_list|(
name|tbl
operator|.
name|getDbName
argument_list|()
argument_list|,
name|tbl
operator|.
name|getTableName
argument_list|()
argument_list|,
name|partNames
operator|.
name|subList
argument_list|(
name|i
operator|*
name|batchSize
argument_list|,
operator|(
name|i
operator|+
literal|1
operator|)
operator|*
name|batchSize
argument_list|)
argument_list|)
decl_stmt|;
if|if
condition|(
name|tParts
operator|!=
literal|null
condition|)
block|{
for|for
control|(
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
name|tpart
range|:
name|tParts
control|)
block|{
name|partitions
operator|.
name|add
argument_list|(
operator|new
name|Partition
argument_list|(
name|tbl
argument_list|,
name|tpart
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
block|}
if|if
condition|(
name|nParts
operator|>
name|nBatches
operator|*
name|batchSize
condition|)
block|{
name|List
argument_list|<
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
argument_list|>
name|tParts
init|=
name|getMSC
argument_list|()
operator|.
name|getPartitionsByNames
argument_list|(
name|tbl
operator|.
name|getDbName
argument_list|()
argument_list|,
name|tbl
operator|.
name|getTableName
argument_list|()
argument_list|,
name|partNames
operator|.
name|subList
argument_list|(
name|nBatches
operator|*
name|batchSize
argument_list|,
name|nParts
argument_list|)
argument_list|)
decl_stmt|;
if|if
condition|(
name|tParts
operator|!=
literal|null
condition|)
block|{
for|for
control|(
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
name|tpart
range|:
name|tParts
control|)
block|{
name|partitions
operator|.
name|add
argument_list|(
operator|new
name|Partition
argument_list|(
name|tbl
argument_list|,
name|tpart
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
return|return
name|partitions
return|;
block|}
comment|/**    * Get a list of Partitions by filter.    * @param tbl The table containing the partitions.    * @param filter A string represent partition predicates.    * @return a list of partitions satisfying the partition predicates.    * @throws HiveException    * @throws MetaException    * @throws NoSuchObjectException    * @throws TException    */
specifier|public
name|List
argument_list|<
name|Partition
argument_list|>
name|getPartitionsByFilter
parameter_list|(
name|Table
name|tbl
parameter_list|,
name|String
name|filter
parameter_list|)
throws|throws
name|HiveException
throws|,
name|MetaException
throws|,
name|NoSuchObjectException
throws|,
name|TException
block|{
if|if
condition|(
operator|!
name|tbl
operator|.
name|isPartitioned
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|ErrorMsg
operator|.
name|TABLE_NOT_PARTITIONED
argument_list|,
name|tbl
operator|.
name|getTableName
argument_list|()
argument_list|)
throw|;
block|}
name|List
argument_list|<
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
argument_list|>
name|tParts
init|=
name|getMSC
argument_list|()
operator|.
name|listPartitionsByFilter
argument_list|(
name|tbl
operator|.
name|getDbName
argument_list|()
argument_list|,
name|tbl
operator|.
name|getTableName
argument_list|()
argument_list|,
name|filter
argument_list|,
operator|(
name|short
operator|)
operator|-
literal|1
argument_list|)
decl_stmt|;
return|return
name|convertFromMetastore
argument_list|(
name|tbl
argument_list|,
name|tParts
argument_list|,
literal|null
argument_list|)
return|;
block|}
specifier|private
specifier|static
name|List
argument_list|<
name|Partition
argument_list|>
name|convertFromMetastore
parameter_list|(
name|Table
name|tbl
parameter_list|,
name|List
argument_list|<
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
argument_list|>
name|src
parameter_list|,
name|List
argument_list|<
name|Partition
argument_list|>
name|dest
parameter_list|)
throws|throws
name|HiveException
block|{
if|if
condition|(
name|src
operator|==
literal|null
condition|)
block|{
return|return
name|dest
return|;
block|}
if|if
condition|(
name|dest
operator|==
literal|null
condition|)
block|{
name|dest
operator|=
operator|new
name|ArrayList
argument_list|<
name|Partition
argument_list|>
argument_list|(
name|src
operator|.
name|size
argument_list|()
argument_list|)
expr_stmt|;
block|}
for|for
control|(
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
name|tPart
range|:
name|src
control|)
block|{
name|dest
operator|.
name|add
argument_list|(
operator|new
name|Partition
argument_list|(
name|tbl
argument_list|,
name|tPart
argument_list|)
argument_list|)
expr_stmt|;
block|}
return|return
name|dest
return|;
block|}
comment|/**    * Get a list of Partitions by expr.    * @param tbl The table containing the partitions.    * @param expr A serialized expression for partition predicates.    * @param conf Hive config.    * @param result the resulting list of partitions    * @return whether the resulting list contains partitions which may or may not match the expr    */
specifier|public
name|boolean
name|getPartitionsByExpr
parameter_list|(
name|Table
name|tbl
parameter_list|,
name|ExprNodeGenericFuncDesc
name|expr
parameter_list|,
name|HiveConf
name|conf
parameter_list|,
name|List
argument_list|<
name|Partition
argument_list|>
name|result
parameter_list|)
throws|throws
name|HiveException
throws|,
name|TException
block|{
assert|assert
name|result
operator|!=
literal|null
assert|;
name|byte
index|[]
name|exprBytes
init|=
name|Utilities
operator|.
name|serializeExpressionToKryo
argument_list|(
name|expr
argument_list|)
decl_stmt|;
name|String
name|defaultPartitionName
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|conf
argument_list|,
name|ConfVars
operator|.
name|DEFAULTPARTITIONNAME
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
argument_list|>
name|msParts
init|=
operator|new
name|ArrayList
argument_list|<
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
argument_list|>
argument_list|()
decl_stmt|;
name|boolean
name|hasUnknownParts
init|=
name|getMSC
argument_list|()
operator|.
name|listPartitionsByExpr
argument_list|(
name|tbl
operator|.
name|getDbName
argument_list|()
argument_list|,
name|tbl
operator|.
name|getTableName
argument_list|()
argument_list|,
name|exprBytes
argument_list|,
name|defaultPartitionName
argument_list|,
operator|(
name|short
operator|)
operator|-
literal|1
argument_list|,
name|msParts
argument_list|)
decl_stmt|;
name|convertFromMetastore
argument_list|(
name|tbl
argument_list|,
name|msParts
argument_list|,
name|result
argument_list|)
expr_stmt|;
return|return
name|hasUnknownParts
return|;
block|}
specifier|public
name|void
name|validatePartitionNameCharacters
parameter_list|(
name|List
argument_list|<
name|String
argument_list|>
name|partVals
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|getMSC
argument_list|()
operator|.
name|validatePartitionNameCharacters
argument_list|(
name|partVals
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|void
name|createRole
parameter_list|(
name|String
name|roleName
parameter_list|,
name|String
name|ownerName
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|getMSC
argument_list|()
operator|.
name|create_role
argument_list|(
operator|new
name|Role
argument_list|(
name|roleName
argument_list|,
operator|-
literal|1
argument_list|,
name|ownerName
argument_list|)
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|void
name|dropRole
parameter_list|(
name|String
name|roleName
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|getMSC
argument_list|()
operator|.
name|drop_role
argument_list|(
name|roleName
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    * Get all existing role names.    *    * @return List of role names.    * @throws HiveException    */
specifier|public
name|List
argument_list|<
name|String
argument_list|>
name|getAllRoleNames
parameter_list|()
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|listRoleNames
argument_list|()
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|List
argument_list|<
name|RolePrincipalGrant
argument_list|>
name|getRoleGrantInfoForPrincipal
parameter_list|(
name|String
name|principalName
parameter_list|,
name|PrincipalType
name|principalType
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|GetRoleGrantsForPrincipalRequest
name|req
init|=
operator|new
name|GetRoleGrantsForPrincipalRequest
argument_list|(
name|principalName
argument_list|,
name|principalType
argument_list|)
decl_stmt|;
name|GetRoleGrantsForPrincipalResponse
name|resp
init|=
name|getMSC
argument_list|()
operator|.
name|get_role_grants_for_principal
argument_list|(
name|req
argument_list|)
decl_stmt|;
return|return
name|resp
operator|.
name|getPrincipalGrants
argument_list|()
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|boolean
name|grantRole
parameter_list|(
name|String
name|roleName
parameter_list|,
name|String
name|userName
parameter_list|,
name|PrincipalType
name|principalType
parameter_list|,
name|String
name|grantor
parameter_list|,
name|PrincipalType
name|grantorType
parameter_list|,
name|boolean
name|grantOption
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|grant_role
argument_list|(
name|roleName
argument_list|,
name|userName
argument_list|,
name|principalType
argument_list|,
name|grantor
argument_list|,
name|grantorType
argument_list|,
name|grantOption
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|boolean
name|revokeRole
parameter_list|(
name|String
name|roleName
parameter_list|,
name|String
name|userName
parameter_list|,
name|PrincipalType
name|principalType
parameter_list|,
name|boolean
name|grantOption
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|revoke_role
argument_list|(
name|roleName
argument_list|,
name|userName
argument_list|,
name|principalType
argument_list|,
name|grantOption
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|List
argument_list|<
name|Role
argument_list|>
name|listRoles
parameter_list|(
name|String
name|userName
parameter_list|,
name|PrincipalType
name|principalType
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|list_roles
argument_list|(
name|userName
argument_list|,
name|principalType
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    * @param objectType    *          hive object type    * @param db_name    *          database name    * @param table_name    *          table name    * @param part_values    *          partition values    * @param column_name    *          column name    * @param user_name    *          user name    * @param group_names    *          group names    * @return the privilege set    * @throws HiveException    */
specifier|public
name|PrincipalPrivilegeSet
name|get_privilege_set
parameter_list|(
name|HiveObjectType
name|objectType
parameter_list|,
name|String
name|db_name
parameter_list|,
name|String
name|table_name
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|part_values
parameter_list|,
name|String
name|column_name
parameter_list|,
name|String
name|user_name
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|group_names
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|HiveObjectRef
name|hiveObj
init|=
operator|new
name|HiveObjectRef
argument_list|(
name|objectType
argument_list|,
name|db_name
argument_list|,
name|table_name
argument_list|,
name|part_values
argument_list|,
name|column_name
argument_list|)
decl_stmt|;
return|return
name|getMSC
argument_list|()
operator|.
name|get_privilege_set
argument_list|(
name|hiveObj
argument_list|,
name|user_name
argument_list|,
name|group_names
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    * @param objectType    *          hive object type    * @param principalName    * @param principalType    * @param dbName    * @param tableName    * @param partValues    * @param columnName    * @return list of privileges    * @throws HiveException    */
specifier|public
name|List
argument_list|<
name|HiveObjectPrivilege
argument_list|>
name|showPrivilegeGrant
parameter_list|(
name|HiveObjectType
name|objectType
parameter_list|,
name|String
name|principalName
parameter_list|,
name|PrincipalType
name|principalType
parameter_list|,
name|String
name|dbName
parameter_list|,
name|String
name|tableName
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|partValues
parameter_list|,
name|String
name|columnName
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|HiveObjectRef
name|hiveObj
init|=
operator|new
name|HiveObjectRef
argument_list|(
name|objectType
argument_list|,
name|dbName
argument_list|,
name|tableName
argument_list|,
name|partValues
argument_list|,
name|columnName
argument_list|)
decl_stmt|;
return|return
name|getMSC
argument_list|()
operator|.
name|list_privileges
argument_list|(
name|principalName
argument_list|,
name|principalType
argument_list|,
name|hiveObj
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
comment|// for each file or directory in 'srcs', make mapping for every file in src to safe name in dest
specifier|private
specifier|static
name|List
argument_list|<
name|List
argument_list|<
name|Path
index|[]
argument_list|>
argument_list|>
name|checkPaths
parameter_list|(
name|HiveConf
name|conf
parameter_list|,
name|FileSystem
name|fs
parameter_list|,
name|FileStatus
index|[]
name|srcs
parameter_list|,
name|FileSystem
name|srcFs
parameter_list|,
name|Path
name|destf
parameter_list|,
name|boolean
name|replace
parameter_list|)
throws|throws
name|HiveException
block|{
name|List
argument_list|<
name|List
argument_list|<
name|Path
index|[]
argument_list|>
argument_list|>
name|result
init|=
operator|new
name|ArrayList
argument_list|<
name|List
argument_list|<
name|Path
index|[]
argument_list|>
argument_list|>
argument_list|()
decl_stmt|;
try|try
block|{
name|FileStatus
name|destStatus
init|=
operator|!
name|replace
condition|?
name|FileUtils
operator|.
name|getFileStatusOrNull
argument_list|(
name|fs
argument_list|,
name|destf
argument_list|)
else|:
literal|null
decl_stmt|;
if|if
condition|(
name|destStatus
operator|!=
literal|null
operator|&&
operator|!
name|destStatus
operator|.
name|isDir
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"checkPaths: destination "
operator|+
name|destf
operator|+
literal|" should be a directory"
argument_list|)
throw|;
block|}
for|for
control|(
name|FileStatus
name|src
range|:
name|srcs
control|)
block|{
name|FileStatus
index|[]
name|items
decl_stmt|;
if|if
condition|(
name|src
operator|.
name|isDir
argument_list|()
condition|)
block|{
name|items
operator|=
name|srcFs
operator|.
name|listStatus
argument_list|(
name|src
operator|.
name|getPath
argument_list|()
argument_list|,
name|FileUtils
operator|.
name|HIDDEN_FILES_PATH_FILTER
argument_list|)
expr_stmt|;
name|Arrays
operator|.
name|sort
argument_list|(
name|items
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|items
operator|=
operator|new
name|FileStatus
index|[]
block|{
name|src
block|}
expr_stmt|;
block|}
name|List
argument_list|<
name|Path
index|[]
argument_list|>
name|srcToDest
init|=
operator|new
name|ArrayList
argument_list|<
name|Path
index|[]
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|FileStatus
name|item
range|:
name|items
control|)
block|{
name|Path
name|itemSource
init|=
name|item
operator|.
name|getPath
argument_list|()
decl_stmt|;
if|if
condition|(
name|Utilities
operator|.
name|isTempPath
argument_list|(
name|item
argument_list|)
condition|)
block|{
comment|// This check is redundant because temp files are removed by
comment|// execution layer before
comment|// calling loadTable/Partition. But leaving it in just in case.
name|srcFs
operator|.
name|delete
argument_list|(
name|itemSource
argument_list|,
literal|true
argument_list|)
expr_stmt|;
continue|continue;
block|}
if|if
condition|(
operator|!
name|conf
operator|.
name|getBoolVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_HADOOP_SUPPORTS_SUBDIRECTORIES
argument_list|)
operator|&&
operator|!
name|HiveConf
operator|.
name|getVar
argument_list|(
name|conf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|STAGINGDIR
argument_list|)
operator|.
name|equals
argument_list|(
name|itemSource
operator|.
name|getName
argument_list|()
argument_list|)
operator|&&
name|item
operator|.
name|isDir
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"checkPaths: "
operator|+
name|src
operator|.
name|getPath
argument_list|()
operator|+
literal|" has nested directory "
operator|+
name|itemSource
argument_list|)
throw|;
block|}
comment|// Strip off the file type, if any so we don't make:
comment|// 000000_0.gz -> 000000_0.gz_copy_1
name|String
name|name
init|=
name|itemSource
operator|.
name|getName
argument_list|()
decl_stmt|;
name|String
name|filetype
decl_stmt|;
name|int
name|index
init|=
name|name
operator|.
name|lastIndexOf
argument_list|(
literal|'.'
argument_list|)
decl_stmt|;
if|if
condition|(
name|index
operator|>=
literal|0
condition|)
block|{
name|filetype
operator|=
name|name
operator|.
name|substring
argument_list|(
name|index
argument_list|)
expr_stmt|;
name|name
operator|=
name|name
operator|.
name|substring
argument_list|(
literal|0
argument_list|,
name|index
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|filetype
operator|=
literal|""
expr_stmt|;
block|}
name|Path
name|itemDest
init|=
operator|new
name|Path
argument_list|(
name|destf
argument_list|,
name|itemSource
operator|.
name|getName
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|replace
condition|)
block|{
comment|// It's possible that the file we're copying may have the same
comment|// relative name as an existing file in the "destf" directory.
comment|// So let's make a quick check to see if we can rename any
comment|// potential offenders so as to allow them to move into the
comment|// "destf" directory. The scheme is dead simple: simply tack
comment|// on "_copy_N" where N starts at 1 and works its way up until
comment|// we find a free space.
comment|// removed source file staging.. it's more confusing when failed.
for|for
control|(
name|int
name|counter
init|=
literal|1
init|;
name|fs
operator|.
name|exists
argument_list|(
name|itemDest
argument_list|)
operator|||
name|destExists
argument_list|(
name|result
argument_list|,
name|itemDest
argument_list|)
condition|;
name|counter
operator|++
control|)
block|{
name|itemDest
operator|=
operator|new
name|Path
argument_list|(
name|destf
argument_list|,
name|name
operator|+
operator|(
literal|"_copy_"
operator|+
name|counter
operator|)
operator|+
name|filetype
argument_list|)
expr_stmt|;
block|}
block|}
name|srcToDest
operator|.
name|add
argument_list|(
operator|new
name|Path
index|[]
block|{
name|itemSource
block|,
name|itemDest
block|}
argument_list|)
expr_stmt|;
block|}
name|result
operator|.
name|add
argument_list|(
name|srcToDest
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"checkPaths: filesystem error in check phase. "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
return|return
name|result
return|;
block|}
specifier|private
specifier|static
name|boolean
name|destExists
parameter_list|(
name|List
argument_list|<
name|List
argument_list|<
name|Path
index|[]
argument_list|>
argument_list|>
name|result
parameter_list|,
name|Path
name|proposed
parameter_list|)
block|{
for|for
control|(
name|List
argument_list|<
name|Path
index|[]
argument_list|>
name|sdpairs
range|:
name|result
control|)
block|{
for|for
control|(
name|Path
index|[]
name|sdpair
range|:
name|sdpairs
control|)
block|{
if|if
condition|(
name|sdpair
index|[
literal|1
index|]
operator|.
name|equals
argument_list|(
name|proposed
argument_list|)
condition|)
block|{
return|return
literal|true
return|;
block|}
block|}
block|}
return|return
literal|false
return|;
block|}
specifier|private
specifier|static
name|boolean
name|isSubDir
parameter_list|(
name|Path
name|srcf
parameter_list|,
name|Path
name|destf
parameter_list|,
name|FileSystem
name|fs
parameter_list|,
name|boolean
name|isSrcLocal
parameter_list|)
block|{
if|if
condition|(
name|srcf
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"The source path is null for isSubDir method."
argument_list|)
expr_stmt|;
return|return
literal|false
return|;
block|}
name|String
name|fullF1
init|=
name|getQualifiedPathWithoutSchemeAndAuthority
argument_list|(
name|srcf
argument_list|,
name|fs
argument_list|)
decl_stmt|;
name|String
name|fullF2
init|=
name|getQualifiedPathWithoutSchemeAndAuthority
argument_list|(
name|destf
argument_list|,
name|fs
argument_list|)
decl_stmt|;
name|boolean
name|isInTest
init|=
name|Boolean
operator|.
name|valueOf
argument_list|(
name|HiveConf
operator|.
name|getBoolVar
argument_list|(
name|fs
operator|.
name|getConf
argument_list|()
argument_list|,
name|ConfVars
operator|.
name|HIVE_IN_TEST
argument_list|)
argument_list|)
decl_stmt|;
comment|// In the automation, the data warehouse is the local file system based.
name|LOG
operator|.
name|debug
argument_list|(
literal|"The source path is "
operator|+
name|fullF1
operator|+
literal|" and the destination path is "
operator|+
name|fullF2
argument_list|)
expr_stmt|;
if|if
condition|(
name|isInTest
condition|)
block|{
return|return
name|fullF1
operator|.
name|startsWith
argument_list|(
name|fullF2
argument_list|)
return|;
block|}
comment|// schema is diff, return false
name|String
name|schemaSrcf
init|=
name|srcf
operator|.
name|toUri
argument_list|()
operator|.
name|getScheme
argument_list|()
decl_stmt|;
name|String
name|schemaDestf
init|=
name|destf
operator|.
name|toUri
argument_list|()
operator|.
name|getScheme
argument_list|()
decl_stmt|;
comment|// if the schemaDestf is null, it means the destination is not in the local file system
if|if
condition|(
name|schemaDestf
operator|==
literal|null
operator|&&
name|isSrcLocal
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"The source file is in the local while the dest not."
argument_list|)
expr_stmt|;
return|return
literal|false
return|;
block|}
comment|// If both schema information are provided, they should be the same.
if|if
condition|(
name|schemaSrcf
operator|!=
literal|null
operator|&&
name|schemaDestf
operator|!=
literal|null
operator|&&
operator|!
name|schemaSrcf
operator|.
name|equals
argument_list|(
name|schemaDestf
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"The source path's schema is "
operator|+
name|schemaSrcf
operator|+
literal|" and the destination path's schema is "
operator|+
name|schemaDestf
operator|+
literal|"."
argument_list|)
expr_stmt|;
return|return
literal|false
return|;
block|}
name|LOG
operator|.
name|debug
argument_list|(
literal|"The source path is "
operator|+
name|fullF1
operator|+
literal|" and the destination path is "
operator|+
name|fullF2
argument_list|)
expr_stmt|;
return|return
name|fullF1
operator|.
name|startsWith
argument_list|(
name|fullF2
argument_list|)
return|;
block|}
specifier|private
specifier|static
name|String
name|getQualifiedPathWithoutSchemeAndAuthority
parameter_list|(
name|Path
name|srcf
parameter_list|,
name|FileSystem
name|fs
parameter_list|)
block|{
name|Path
name|currentWorkingDir
init|=
name|fs
operator|.
name|getWorkingDirectory
argument_list|()
decl_stmt|;
name|Path
name|path
init|=
name|srcf
operator|.
name|makeQualified
argument_list|(
name|srcf
operator|.
name|toUri
argument_list|()
argument_list|,
name|currentWorkingDir
argument_list|)
decl_stmt|;
return|return
name|ShimLoader
operator|.
name|getHadoopShims
argument_list|()
operator|.
name|getPathWithoutSchemeAndAuthority
argument_list|(
name|path
argument_list|)
operator|.
name|toString
argument_list|()
return|;
block|}
comment|//it is assumed that parent directory of the destf should already exist when this
comment|//method is called. when the replace value is true, this method works a little different
comment|//from mv command if the destf is a directory, it replaces the destf instead of moving under
comment|//the destf. in this case, the replaced destf still preserves the original destf's permission
specifier|public
specifier|static
name|boolean
name|moveFile
parameter_list|(
name|HiveConf
name|conf
parameter_list|,
name|Path
name|srcf
parameter_list|,
name|Path
name|destf
parameter_list|,
name|FileSystem
name|fs
parameter_list|,
name|boolean
name|replace
parameter_list|,
name|boolean
name|isSrcLocal
parameter_list|)
throws|throws
name|HiveException
block|{
name|boolean
name|success
init|=
literal|false
decl_stmt|;
comment|//needed for perm inheritance.
name|boolean
name|inheritPerms
init|=
name|HiveConf
operator|.
name|getBoolVar
argument_list|(
name|conf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_WAREHOUSE_SUBDIR_INHERIT_PERMS
argument_list|)
decl_stmt|;
name|HadoopShims
name|shims
init|=
name|ShimLoader
operator|.
name|getHadoopShims
argument_list|()
decl_stmt|;
name|HadoopShims
operator|.
name|HdfsFileStatus
name|destStatus
init|=
literal|null
decl_stmt|;
name|HadoopShims
operator|.
name|HdfsEncryptionShim
name|hdfsEncryptionShim
init|=
name|SessionState
operator|.
name|get
argument_list|()
operator|.
name|getHdfsEncryptionShim
argument_list|()
decl_stmt|;
comment|// If source path is a subdirectory of the destination path:
comment|//   ex: INSERT OVERWRITE DIRECTORY 'target/warehouse/dest4.out' SELECT src.value WHERE src.key>= 300;
comment|//   where the staging directory is a subdirectory of the destination directory
comment|// (1) Do not delete the dest dir before doing the move operation.
comment|// (2) It is assumed that subdir and dir are in same encryption zone.
comment|// (3) Move individual files from scr dir to dest dir.
name|boolean
name|destIsSubDir
init|=
name|isSubDir
argument_list|(
name|srcf
argument_list|,
name|destf
argument_list|,
name|fs
argument_list|,
name|isSrcLocal
argument_list|)
decl_stmt|;
try|try
block|{
if|if
condition|(
name|inheritPerms
operator|||
name|replace
condition|)
block|{
try|try
block|{
name|destStatus
operator|=
name|shims
operator|.
name|getFullFileStatus
argument_list|(
name|conf
argument_list|,
name|fs
argument_list|,
name|destf
operator|.
name|getParent
argument_list|()
argument_list|)
expr_stmt|;
comment|//if destf is an existing directory:
comment|//if replace is true, delete followed by rename(mv) is equivalent to replace
comment|//if replace is false, rename (mv) actually move the src under dest dir
comment|//if destf is an existing file, rename is actually a replace, and do not need
comment|// to delete the file first
if|if
condition|(
name|replace
operator|&&
operator|!
name|destIsSubDir
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"The path "
operator|+
name|destf
operator|.
name|toString
argument_list|()
operator|+
literal|" is deleted"
argument_list|)
expr_stmt|;
name|fs
operator|.
name|delete
argument_list|(
name|destf
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|FileNotFoundException
name|ignore
parameter_list|)
block|{
comment|//if dest dir does not exist, any re
if|if
condition|(
name|inheritPerms
condition|)
block|{
name|destStatus
operator|=
name|shims
operator|.
name|getFullFileStatus
argument_list|(
name|conf
argument_list|,
name|fs
argument_list|,
name|destf
operator|.
name|getParent
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
block|}
if|if
condition|(
operator|!
name|isSrcLocal
condition|)
block|{
comment|// For NOT local src file, rename the file
if|if
condition|(
name|hdfsEncryptionShim
operator|!=
literal|null
operator|&&
operator|(
name|hdfsEncryptionShim
operator|.
name|isPathEncrypted
argument_list|(
name|srcf
argument_list|)
operator|||
name|hdfsEncryptionShim
operator|.
name|isPathEncrypted
argument_list|(
name|destf
argument_list|)
operator|)
operator|&&
operator|!
name|hdfsEncryptionShim
operator|.
name|arePathsOnSameEncryptionZone
argument_list|(
name|srcf
argument_list|,
name|destf
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Copying source "
operator|+
name|srcf
operator|+
literal|" to "
operator|+
name|destf
operator|+
literal|" because HDFS encryption zones are different."
argument_list|)
expr_stmt|;
name|success
operator|=
name|FileUtils
operator|.
name|copy
argument_list|(
name|srcf
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
argument_list|,
name|srcf
argument_list|,
name|destf
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
argument_list|,
name|destf
argument_list|,
literal|true
argument_list|,
comment|// delete source
name|replace
argument_list|,
comment|// overwrite destination
name|conf
argument_list|)
expr_stmt|;
block|}
else|else
block|{
if|if
condition|(
name|destIsSubDir
condition|)
block|{
name|FileStatus
index|[]
name|srcs
init|=
name|fs
operator|.
name|listStatus
argument_list|(
name|srcf
argument_list|,
name|FileUtils
operator|.
name|HIDDEN_FILES_PATH_FILTER
argument_list|)
decl_stmt|;
for|for
control|(
name|FileStatus
name|status
range|:
name|srcs
control|)
block|{
name|success
operator|=
name|FileUtils
operator|.
name|copy
argument_list|(
name|srcf
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
argument_list|,
name|status
operator|.
name|getPath
argument_list|()
argument_list|,
name|destf
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
argument_list|,
name|destf
argument_list|,
literal|true
argument_list|,
comment|// delete source
name|replace
argument_list|,
comment|// overwrite destination
name|conf
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|success
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Unable to move source "
operator|+
name|status
operator|.
name|getPath
argument_list|()
operator|+
literal|" to destination "
operator|+
name|destf
argument_list|)
throw|;
block|}
block|}
block|}
else|else
block|{
name|success
operator|=
name|fs
operator|.
name|rename
argument_list|(
name|srcf
argument_list|,
name|destf
argument_list|)
expr_stmt|;
block|}
block|}
block|}
else|else
block|{
comment|// For local src file, copy to hdfs
name|fs
operator|.
name|copyFromLocalFile
argument_list|(
name|srcf
argument_list|,
name|destf
argument_list|)
expr_stmt|;
name|success
operator|=
literal|true
expr_stmt|;
block|}
name|LOG
operator|.
name|info
argument_list|(
operator|(
name|replace
condition|?
literal|"Replacing src:"
else|:
literal|"Renaming src: "
operator|)
operator|+
name|srcf
operator|.
name|toString
argument_list|()
operator|+
literal|", dest: "
operator|+
name|destf
operator|.
name|toString
argument_list|()
operator|+
literal|", Status:"
operator|+
name|success
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|ioe
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Unable to move source "
operator|+
name|srcf
operator|+
literal|" to destination "
operator|+
name|destf
argument_list|,
name|ioe
argument_list|)
throw|;
block|}
if|if
condition|(
name|success
operator|&&
name|inheritPerms
condition|)
block|{
try|try
block|{
name|ShimLoader
operator|.
name|getHadoopShims
argument_list|()
operator|.
name|setFullFileStatus
argument_list|(
name|conf
argument_list|,
name|destStatus
argument_list|,
name|fs
argument_list|,
name|destf
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Error setting permission of file "
operator|+
name|destf
operator|+
literal|": "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|success
return|;
block|}
comment|/**    * Copy files.  This handles building the mapping for buckets and such between the source and    * destination    * @param conf Configuration object    * @param srcf source directory, if bucketed should contain bucket files    * @param destf directory to move files into    * @param fs Filesystem    * @param isSrcLocal true if source is on local file system    * @param isAcid true if this is an ACID based write    * @param newFiles if this is non-null, a list of files that were created as a result of this    *                 move will be returned.    * @throws HiveException    */
specifier|static
specifier|protected
name|void
name|copyFiles
parameter_list|(
name|HiveConf
name|conf
parameter_list|,
name|Path
name|srcf
parameter_list|,
name|Path
name|destf
parameter_list|,
name|FileSystem
name|fs
parameter_list|,
name|boolean
name|isSrcLocal
parameter_list|,
name|boolean
name|isAcid
parameter_list|,
name|List
argument_list|<
name|Path
argument_list|>
name|newFiles
parameter_list|)
throws|throws
name|HiveException
block|{
name|boolean
name|inheritPerms
init|=
name|HiveConf
operator|.
name|getBoolVar
argument_list|(
name|conf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_WAREHOUSE_SUBDIR_INHERIT_PERMS
argument_list|)
decl_stmt|;
try|try
block|{
comment|// create the destination if it does not exist
if|if
condition|(
operator|!
name|fs
operator|.
name|exists
argument_list|(
name|destf
argument_list|)
condition|)
block|{
name|FileUtils
operator|.
name|mkdir
argument_list|(
name|fs
argument_list|,
name|destf
argument_list|,
name|inheritPerms
argument_list|,
name|conf
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"copyFiles: error while checking/creating destination directory!!!"
argument_list|,
name|e
argument_list|)
throw|;
block|}
name|FileStatus
index|[]
name|srcs
decl_stmt|;
name|FileSystem
name|srcFs
decl_stmt|;
try|try
block|{
name|srcFs
operator|=
name|srcf
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
expr_stmt|;
name|srcs
operator|=
name|srcFs
operator|.
name|globStatus
argument_list|(
name|srcf
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"addFiles: filesystem error in check phase. "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
if|if
condition|(
name|srcs
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"No sources specified to move: "
operator|+
name|srcf
argument_list|)
expr_stmt|;
return|return;
comment|// srcs = new FileStatus[0]; Why is this needed?
block|}
comment|// If we're moving files around for an ACID write then the rules and paths are all different.
comment|// You can blame this on Owen.
if|if
condition|(
name|isAcid
condition|)
block|{
name|moveAcidFiles
argument_list|(
name|srcFs
argument_list|,
name|srcs
argument_list|,
name|destf
argument_list|,
name|newFiles
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// check that source and target paths exist
name|List
argument_list|<
name|List
argument_list|<
name|Path
index|[]
argument_list|>
argument_list|>
name|result
init|=
name|checkPaths
argument_list|(
name|conf
argument_list|,
name|fs
argument_list|,
name|srcs
argument_list|,
name|srcFs
argument_list|,
name|destf
argument_list|,
literal|false
argument_list|)
decl_stmt|;
comment|// move it, move it
try|try
block|{
for|for
control|(
name|List
argument_list|<
name|Path
index|[]
argument_list|>
name|sdpairs
range|:
name|result
control|)
block|{
for|for
control|(
name|Path
index|[]
name|sdpair
range|:
name|sdpairs
control|)
block|{
if|if
condition|(
operator|!
name|moveFile
argument_list|(
name|conf
argument_list|,
name|sdpair
index|[
literal|0
index|]
argument_list|,
name|sdpair
index|[
literal|1
index|]
argument_list|,
name|fs
argument_list|,
literal|false
argument_list|,
name|isSrcLocal
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Cannot move "
operator|+
name|sdpair
index|[
literal|0
index|]
operator|+
literal|" to "
operator|+
name|sdpair
index|[
literal|1
index|]
argument_list|)
throw|;
block|}
if|if
condition|(
name|newFiles
operator|!=
literal|null
condition|)
name|newFiles
operator|.
name|add
argument_list|(
name|sdpair
index|[
literal|1
index|]
argument_list|)
expr_stmt|;
block|}
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"copyFiles: error while moving files!!! "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
block|}
specifier|private
specifier|static
name|void
name|moveAcidFiles
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|FileStatus
index|[]
name|stats
parameter_list|,
name|Path
name|dst
parameter_list|,
name|List
argument_list|<
name|Path
argument_list|>
name|newFiles
parameter_list|)
throws|throws
name|HiveException
block|{
comment|// The layout for ACID files is table|partname/base|delta/bucket
comment|// We will always only be writing delta files.  In the buckets created by FileSinkOperator
comment|// it will look like bucket/delta/bucket.  So we need to move that into the above structure.
comment|// For the first mover there will be no delta directory, so we can move the whole directory.
comment|// For everyone else we will need to just move the buckets under the existing delta
comment|// directory.
name|Set
argument_list|<
name|Path
argument_list|>
name|createdDeltaDirs
init|=
operator|new
name|HashSet
argument_list|<
name|Path
argument_list|>
argument_list|()
decl_stmt|;
comment|// Open the original path we've been given and find the list of original buckets
for|for
control|(
name|FileStatus
name|stat
range|:
name|stats
control|)
block|{
name|Path
name|srcPath
init|=
name|stat
operator|.
name|getPath
argument_list|()
decl_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Acid move Looking for original buckets in "
operator|+
name|srcPath
argument_list|)
expr_stmt|;
name|FileStatus
index|[]
name|origBucketStats
init|=
literal|null
decl_stmt|;
try|try
block|{
name|origBucketStats
operator|=
name|fs
operator|.
name|listStatus
argument_list|(
name|srcPath
argument_list|,
name|AcidUtils
operator|.
name|originalBucketFilter
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|String
name|msg
init|=
literal|"Unable to look for bucket files in src path "
operator|+
name|srcPath
operator|.
name|toUri
argument_list|()
operator|.
name|toString
argument_list|()
decl_stmt|;
name|LOG
operator|.
name|error
argument_list|(
name|msg
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|msg
argument_list|,
name|e
argument_list|)
throw|;
block|}
name|LOG
operator|.
name|debug
argument_list|(
literal|"Acid move found "
operator|+
name|origBucketStats
operator|.
name|length
operator|+
literal|" original buckets"
argument_list|)
expr_stmt|;
for|for
control|(
name|FileStatus
name|origBucketStat
range|:
name|origBucketStats
control|)
block|{
name|Path
name|origBucketPath
init|=
name|origBucketStat
operator|.
name|getPath
argument_list|()
decl_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Acid move looking for delta files in bucket "
operator|+
name|origBucketPath
argument_list|)
expr_stmt|;
name|FileStatus
index|[]
name|deltaStats
init|=
literal|null
decl_stmt|;
try|try
block|{
name|deltaStats
operator|=
name|fs
operator|.
name|listStatus
argument_list|(
name|origBucketPath
argument_list|,
name|AcidUtils
operator|.
name|deltaFileFilter
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Unable to look for delta files in original bucket "
operator|+
name|origBucketPath
operator|.
name|toUri
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
name|LOG
operator|.
name|debug
argument_list|(
literal|"Acid move found "
operator|+
name|deltaStats
operator|.
name|length
operator|+
literal|" delta files"
argument_list|)
expr_stmt|;
for|for
control|(
name|FileStatus
name|deltaStat
range|:
name|deltaStats
control|)
block|{
name|Path
name|deltaPath
init|=
name|deltaStat
operator|.
name|getPath
argument_list|()
decl_stmt|;
comment|// Create the delta directory.  Don't worry if it already exists,
comment|// as that likely means another task got to it first.  Then move each of the buckets.
comment|// it would be more efficient to try to move the delta with it's buckets but that is
comment|// harder to make race condition proof.
name|Path
name|deltaDest
init|=
operator|new
name|Path
argument_list|(
name|dst
argument_list|,
name|deltaPath
operator|.
name|getName
argument_list|()
argument_list|)
decl_stmt|;
try|try
block|{
if|if
condition|(
operator|!
name|createdDeltaDirs
operator|.
name|contains
argument_list|(
name|deltaDest
argument_list|)
condition|)
block|{
try|try
block|{
name|fs
operator|.
name|mkdirs
argument_list|(
name|deltaDest
argument_list|)
expr_stmt|;
name|createdDeltaDirs
operator|.
name|add
argument_list|(
name|deltaDest
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|swallowIt
parameter_list|)
block|{
comment|// Don't worry about this, as it likely just means it's already been created.
name|LOG
operator|.
name|info
argument_list|(
literal|"Unable to create delta directory "
operator|+
name|deltaDest
operator|+
literal|", assuming it already exists: "
operator|+
name|swallowIt
operator|.
name|getMessage
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
name|FileStatus
index|[]
name|bucketStats
init|=
name|fs
operator|.
name|listStatus
argument_list|(
name|deltaPath
argument_list|,
name|AcidUtils
operator|.
name|bucketFileFilter
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Acid move found "
operator|+
name|bucketStats
operator|.
name|length
operator|+
literal|" bucket files"
argument_list|)
expr_stmt|;
for|for
control|(
name|FileStatus
name|bucketStat
range|:
name|bucketStats
control|)
block|{
name|Path
name|bucketSrc
init|=
name|bucketStat
operator|.
name|getPath
argument_list|()
decl_stmt|;
name|Path
name|bucketDest
init|=
operator|new
name|Path
argument_list|(
name|deltaDest
argument_list|,
name|bucketSrc
operator|.
name|getName
argument_list|()
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Moving bucket "
operator|+
name|bucketSrc
operator|.
name|toUri
argument_list|()
operator|.
name|toString
argument_list|()
operator|+
literal|" to "
operator|+
name|bucketDest
operator|.
name|toUri
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
name|fs
operator|.
name|rename
argument_list|(
name|bucketSrc
argument_list|,
name|bucketDest
argument_list|)
expr_stmt|;
if|if
condition|(
name|newFiles
operator|!=
literal|null
condition|)
name|newFiles
operator|.
name|add
argument_list|(
name|bucketDest
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Error moving acid files "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
block|}
block|}
block|}
comment|/**    * Replaces files in the partition with new data set specified by srcf. Works    * by renaming directory of srcf to the destination file.    * srcf, destf, and tmppath should resident in the same DFS, but the oldPath can be in a    * different DFS.    *    * @param tablePath path of the table.  Used to identify permission inheritance.    * @param srcf    *          Source directory to be renamed to tmppath. It should be a    *          leaf directory where the final data files reside. However it    *          could potentially contain subdirectories as well.    * @param destf    *          The directory where the final data needs to go    * @param oldPath    *          The directory where the old data location, need to be cleaned up.  Most of time, will be the same    *          as destf, unless its across FileSystem boundaries.    * @param isSrcLocal    *          If the source directory is LOCAL    */
specifier|protected
specifier|static
name|void
name|replaceFiles
parameter_list|(
name|Path
name|tablePath
parameter_list|,
name|Path
name|srcf
parameter_list|,
name|Path
name|destf
parameter_list|,
name|Path
name|oldPath
parameter_list|,
name|HiveConf
name|conf
parameter_list|,
name|boolean
name|isSrcLocal
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|FileSystem
name|destFs
init|=
name|destf
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|boolean
name|inheritPerms
init|=
name|HiveConf
operator|.
name|getBoolVar
argument_list|(
name|conf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_WAREHOUSE_SUBDIR_INHERIT_PERMS
argument_list|)
decl_stmt|;
comment|// check if srcf contains nested sub-directories
name|FileStatus
index|[]
name|srcs
decl_stmt|;
name|FileSystem
name|srcFs
decl_stmt|;
try|try
block|{
name|srcFs
operator|=
name|srcf
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
expr_stmt|;
name|srcs
operator|=
name|srcFs
operator|.
name|globStatus
argument_list|(
name|srcf
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Getting globStatus "
operator|+
name|srcf
operator|.
name|toString
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
if|if
condition|(
name|srcs
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"No sources specified to move: "
operator|+
name|srcf
argument_list|)
expr_stmt|;
return|return;
block|}
name|List
argument_list|<
name|List
argument_list|<
name|Path
index|[]
argument_list|>
argument_list|>
name|result
init|=
name|checkPaths
argument_list|(
name|conf
argument_list|,
name|destFs
argument_list|,
name|srcs
argument_list|,
name|srcFs
argument_list|,
name|destf
argument_list|,
literal|true
argument_list|)
decl_stmt|;
if|if
condition|(
name|oldPath
operator|!=
literal|null
condition|)
block|{
try|try
block|{
name|FileSystem
name|fs2
init|=
name|oldPath
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
if|if
condition|(
name|fs2
operator|.
name|exists
argument_list|(
name|oldPath
argument_list|)
condition|)
block|{
comment|// Do not delete oldPath if:
comment|//  - destf is subdir of oldPath
comment|//if ( !(fs2.equals(destf.getFileSystem(conf))&& FileUtils.isSubDir(oldPath, destf, fs2)))
if|if
condition|(
name|FileUtils
operator|.
name|isSubDir
argument_list|(
name|oldPath
argument_list|,
name|destf
argument_list|,
name|fs2
argument_list|)
condition|)
block|{
name|FileUtils
operator|.
name|trashFilesUnderDir
argument_list|(
name|fs2
argument_list|,
name|oldPath
argument_list|,
name|conf
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|inheritPerms
condition|)
block|{
name|inheritFromTable
argument_list|(
name|tablePath
argument_list|,
name|destf
argument_list|,
name|conf
argument_list|,
name|destFs
argument_list|)
expr_stmt|;
block|}
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
comment|//swallow the exception
name|LOG
operator|.
name|warn
argument_list|(
literal|"Directory "
operator|+
name|oldPath
operator|.
name|toString
argument_list|()
operator|+
literal|" cannot be removed: "
operator|+
name|e
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
block|}
comment|// rename src directory to destf
if|if
condition|(
name|srcs
operator|.
name|length
operator|==
literal|1
operator|&&
name|srcs
index|[
literal|0
index|]
operator|.
name|isDir
argument_list|()
condition|)
block|{
comment|// rename can fail if the parent doesn't exist
name|Path
name|destfp
init|=
name|destf
operator|.
name|getParent
argument_list|()
decl_stmt|;
if|if
condition|(
operator|!
name|destFs
operator|.
name|exists
argument_list|(
name|destfp
argument_list|)
condition|)
block|{
name|boolean
name|success
init|=
name|destFs
operator|.
name|mkdirs
argument_list|(
name|destfp
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|success
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Error creating directory "
operator|+
name|destf
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|inheritPerms
operator|&&
name|success
condition|)
block|{
name|inheritFromTable
argument_list|(
name|tablePath
argument_list|,
name|destfp
argument_list|,
name|conf
argument_list|,
name|destFs
argument_list|)
expr_stmt|;
block|}
block|}
comment|// Copy/move each file under the source directory to avoid to delete the destination
comment|// directory if it is the root of an HDFS encryption zone.
for|for
control|(
name|List
argument_list|<
name|Path
index|[]
argument_list|>
name|sdpairs
range|:
name|result
control|)
block|{
for|for
control|(
name|Path
index|[]
name|sdpair
range|:
name|sdpairs
control|)
block|{
name|Path
name|destParent
init|=
name|sdpair
index|[
literal|1
index|]
operator|.
name|getParent
argument_list|()
decl_stmt|;
name|FileSystem
name|destParentFs
init|=
name|destParent
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|destParentFs
operator|.
name|isDirectory
argument_list|(
name|destParent
argument_list|)
condition|)
block|{
name|boolean
name|success
init|=
name|destFs
operator|.
name|mkdirs
argument_list|(
name|destParent
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|success
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Error creating directory "
operator|+
name|destParent
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|inheritPerms
operator|&&
name|success
condition|)
block|{
name|inheritFromTable
argument_list|(
name|tablePath
argument_list|,
name|destParent
argument_list|,
name|conf
argument_list|,
name|destFs
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
operator|!
name|moveFile
argument_list|(
name|conf
argument_list|,
name|sdpair
index|[
literal|0
index|]
argument_list|,
name|sdpair
index|[
literal|1
index|]
argument_list|,
name|destFs
argument_list|,
literal|true
argument_list|,
name|isSrcLocal
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Unable to move file/directory from "
operator|+
name|sdpair
index|[
literal|0
index|]
operator|+
literal|" to "
operator|+
name|sdpair
index|[
literal|1
index|]
argument_list|)
throw|;
block|}
block|}
block|}
block|}
else|else
block|{
comment|// srcf is a file or pattern containing wildcards
if|if
condition|(
operator|!
name|destFs
operator|.
name|exists
argument_list|(
name|destf
argument_list|)
condition|)
block|{
name|boolean
name|success
init|=
name|destFs
operator|.
name|mkdirs
argument_list|(
name|destf
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|success
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Error creating directory "
operator|+
name|destf
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|inheritPerms
operator|&&
name|success
condition|)
block|{
name|inheritFromTable
argument_list|(
name|tablePath
argument_list|,
name|destf
argument_list|,
name|conf
argument_list|,
name|destFs
argument_list|)
expr_stmt|;
block|}
block|}
comment|// srcs must be a list of files -- ensured by LoadSemanticAnalyzer
for|for
control|(
name|List
argument_list|<
name|Path
index|[]
argument_list|>
name|sdpairs
range|:
name|result
control|)
block|{
for|for
control|(
name|Path
index|[]
name|sdpair
range|:
name|sdpairs
control|)
block|{
if|if
condition|(
operator|!
name|moveFile
argument_list|(
name|conf
argument_list|,
name|sdpair
index|[
literal|0
index|]
argument_list|,
name|sdpair
index|[
literal|1
index|]
argument_list|,
name|destFs
argument_list|,
literal|true
argument_list|,
name|isSrcLocal
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Error moving: "
operator|+
name|sdpair
index|[
literal|0
index|]
operator|+
literal|" into: "
operator|+
name|sdpair
index|[
literal|1
index|]
argument_list|)
throw|;
block|}
block|}
block|}
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    * This method sets all paths from tablePath to destf (including destf) to have same permission as tablePath.    * @param tablePath path of table    * @param destf path of table-subdir.    * @param conf    * @param fs    */
specifier|private
specifier|static
name|void
name|inheritFromTable
parameter_list|(
name|Path
name|tablePath
parameter_list|,
name|Path
name|destf
parameter_list|,
name|HiveConf
name|conf
parameter_list|,
name|FileSystem
name|fs
parameter_list|)
block|{
if|if
condition|(
operator|!
name|FileUtils
operator|.
name|isSubDir
argument_list|(
name|destf
argument_list|,
name|tablePath
argument_list|,
name|fs
argument_list|)
condition|)
block|{
comment|//partition may not be under the parent.
return|return;
block|}
name|HadoopShims
name|shims
init|=
name|ShimLoader
operator|.
name|getHadoopShims
argument_list|()
decl_stmt|;
comment|//Calculate all the paths from the table dir, to destf
comment|//At end of this loop, currPath is table dir, and pathsToSet contain list of all those paths.
name|Path
name|currPath
init|=
name|destf
decl_stmt|;
name|List
argument_list|<
name|Path
argument_list|>
name|pathsToSet
init|=
operator|new
name|LinkedList
argument_list|<
name|Path
argument_list|>
argument_list|()
decl_stmt|;
while|while
condition|(
operator|!
name|currPath
operator|.
name|equals
argument_list|(
name|tablePath
argument_list|)
condition|)
block|{
name|pathsToSet
operator|.
name|add
argument_list|(
name|currPath
argument_list|)
expr_stmt|;
name|currPath
operator|=
name|currPath
operator|.
name|getParent
argument_list|()
expr_stmt|;
block|}
try|try
block|{
name|HadoopShims
operator|.
name|HdfsFileStatus
name|fullFileStatus
init|=
name|shims
operator|.
name|getFullFileStatus
argument_list|(
name|conf
argument_list|,
name|fs
argument_list|,
name|currPath
argument_list|)
decl_stmt|;
for|for
control|(
name|Path
name|pathToSet
range|:
name|pathsToSet
control|)
block|{
name|shims
operator|.
name|setFullFileStatus
argument_list|(
name|conf
argument_list|,
name|fullFileStatus
argument_list|,
name|fs
argument_list|,
name|pathToSet
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Error setting permissions or group of "
operator|+
name|destf
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
block|}
specifier|public
specifier|static
name|boolean
name|isHadoop1
parameter_list|()
block|{
return|return
name|ShimLoader
operator|.
name|getMajorVersion
argument_list|()
operator|.
name|startsWith
argument_list|(
literal|"0.20"
argument_list|)
return|;
block|}
specifier|public
name|void
name|exchangeTablePartitions
parameter_list|(
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|partitionSpecs
parameter_list|,
name|String
name|sourceDb
parameter_list|,
name|String
name|sourceTable
parameter_list|,
name|String
name|destDb
parameter_list|,
name|String
name|destinationTableName
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|getMSC
argument_list|()
operator|.
name|exchange_partition
argument_list|(
name|partitionSpecs
argument_list|,
name|sourceDb
argument_list|,
name|sourceTable
argument_list|,
name|destDb
argument_list|,
name|destinationTableName
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|ex
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|ex
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|ex
argument_list|)
throw|;
block|}
block|}
comment|/**    * Creates a metastore client. Currently it creates only JDBC based client as    * File based store support is removed    *    * @returns a Meta Store Client    * @throws HiveMetaException    *           if a working client can't be created    */
specifier|private
name|IMetaStoreClient
name|createMetaStoreClient
parameter_list|()
throws|throws
name|MetaException
block|{
name|HiveMetaHookLoader
name|hookLoader
init|=
operator|new
name|HiveMetaHookLoader
argument_list|()
block|{
annotation|@
name|Override
specifier|public
name|HiveMetaHook
name|getHook
parameter_list|(
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Table
name|tbl
parameter_list|)
throws|throws
name|MetaException
block|{
try|try
block|{
if|if
condition|(
name|tbl
operator|==
literal|null
condition|)
block|{
return|return
literal|null
return|;
block|}
name|HiveStorageHandler
name|storageHandler
init|=
name|HiveUtils
operator|.
name|getStorageHandler
argument_list|(
name|conf
argument_list|,
name|tbl
operator|.
name|getParameters
argument_list|()
operator|.
name|get
argument_list|(
name|META_TABLE_STORAGE
argument_list|)
argument_list|)
decl_stmt|;
if|if
condition|(
name|storageHandler
operator|==
literal|null
condition|)
block|{
return|return
literal|null
return|;
block|}
return|return
name|storageHandler
operator|.
name|getMetaHook
argument_list|()
return|;
block|}
catch|catch
parameter_list|(
name|HiveException
name|ex
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|ex
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|MetaException
argument_list|(
literal|"Failed to load storage handler:  "
operator|+
name|ex
operator|.
name|getMessage
argument_list|()
argument_list|)
throw|;
block|}
block|}
block|}
decl_stmt|;
return|return
name|RetryingMetaStoreClient
operator|.
name|getProxy
argument_list|(
name|conf
argument_list|,
name|hookLoader
argument_list|,
name|metaCallTimeMap
argument_list|,
name|SessionHiveMetaStoreClient
operator|.
name|class
operator|.
name|getName
argument_list|()
argument_list|)
return|;
block|}
comment|/**    * @return the metastore client for the current thread    * @throws MetaException    */
annotation|@
name|LimitedPrivate
argument_list|(
name|value
operator|=
block|{
literal|"Hive"
block|}
argument_list|)
annotation|@
name|Unstable
specifier|public
name|IMetaStoreClient
name|getMSC
parameter_list|()
throws|throws
name|MetaException
block|{
if|if
condition|(
name|metaStoreClient
operator|==
literal|null
condition|)
block|{
try|try
block|{
name|owner
operator|=
name|UserGroupInformation
operator|.
name|getCurrentUser
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|String
name|msg
init|=
literal|"Error getting current user: "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
decl_stmt|;
name|LOG
operator|.
name|error
argument_list|(
name|msg
argument_list|,
name|e
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|MetaException
argument_list|(
name|msg
operator|+
literal|"\n"
operator|+
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
throw|;
block|}
name|metaStoreClient
operator|=
name|createMetaStoreClient
argument_list|()
expr_stmt|;
block|}
return|return
name|metaStoreClient
return|;
block|}
specifier|private
name|String
name|getUserName
parameter_list|()
block|{
return|return
name|SessionState
operator|.
name|getUserFromAuthenticator
argument_list|()
return|;
block|}
specifier|private
name|List
argument_list|<
name|String
argument_list|>
name|getGroupNames
parameter_list|()
block|{
name|SessionState
name|ss
init|=
name|SessionState
operator|.
name|get
argument_list|()
decl_stmt|;
if|if
condition|(
name|ss
operator|!=
literal|null
operator|&&
name|ss
operator|.
name|getAuthenticator
argument_list|()
operator|!=
literal|null
condition|)
block|{
return|return
name|ss
operator|.
name|getAuthenticator
argument_list|()
operator|.
name|getGroupNames
argument_list|()
return|;
block|}
return|return
literal|null
return|;
block|}
specifier|public
specifier|static
name|List
argument_list|<
name|FieldSchema
argument_list|>
name|getFieldsFromDeserializer
parameter_list|(
name|String
name|name
parameter_list|,
name|Deserializer
name|serde
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|MetaStoreUtils
operator|.
name|getFieldsFromDeserializer
argument_list|(
name|name
argument_list|,
name|serde
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|SerDeException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Error in getting fields from serde. "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
catch|catch
parameter_list|(
name|MetaException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Error in getting fields from serde."
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|List
argument_list|<
name|Index
argument_list|>
name|getIndexes
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|tblName
parameter_list|,
name|short
name|max
parameter_list|)
throws|throws
name|HiveException
block|{
name|List
argument_list|<
name|Index
argument_list|>
name|indexes
init|=
literal|null
decl_stmt|;
try|try
block|{
name|indexes
operator|=
name|getMSC
argument_list|()
operator|.
name|listIndexes
argument_list|(
name|dbName
argument_list|,
name|tblName
argument_list|,
name|max
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
return|return
name|indexes
return|;
block|}
specifier|public
name|boolean
name|updateTableColumnStatistics
parameter_list|(
name|ColumnStatistics
name|statsObj
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|updateTableColumnStatistics
argument_list|(
name|statsObj
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|boolean
name|updatePartitionColumnStatistics
parameter_list|(
name|ColumnStatistics
name|statsObj
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|updatePartitionColumnStatistics
argument_list|(
name|statsObj
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|boolean
name|setPartitionColumnStatistics
parameter_list|(
name|SetPartitionsStatsRequest
name|request
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|setPartitionColumnStatistics
argument_list|(
name|request
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|List
argument_list|<
name|ColumnStatisticsObj
argument_list|>
name|getTableColumnStatistics
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|tableName
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|colNames
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|getTableColumnStatistics
argument_list|(
name|dbName
argument_list|,
name|tableName
argument_list|,
name|colNames
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|Map
argument_list|<
name|String
argument_list|,
name|List
argument_list|<
name|ColumnStatisticsObj
argument_list|>
argument_list|>
name|getPartitionColumnStatistics
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|tableName
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|partNames
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|colNames
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|getPartitionColumnStatistics
argument_list|(
name|dbName
argument_list|,
name|tableName
argument_list|,
name|partNames
argument_list|,
name|colNames
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|AggrStats
name|getAggrColStatsFor
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|tblName
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|colNames
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|partName
parameter_list|)
block|{
try|try
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|getAggrColStatsFor
argument_list|(
name|dbName
argument_list|,
name|tblName
argument_list|,
name|colNames
argument_list|,
name|partName
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
return|return
literal|null
return|;
block|}
block|}
specifier|public
name|boolean
name|deleteTableColumnStatistics
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|tableName
parameter_list|,
name|String
name|colName
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|deleteTableColumnStatistics
argument_list|(
name|dbName
argument_list|,
name|tableName
argument_list|,
name|colName
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|boolean
name|deletePartitionColumnStatistics
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|tableName
parameter_list|,
name|String
name|partName
parameter_list|,
name|String
name|colName
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|deletePartitionColumnStatistics
argument_list|(
name|dbName
argument_list|,
name|tableName
argument_list|,
name|partName
argument_list|,
name|colName
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|Table
name|newTable
parameter_list|(
name|String
name|tableName
parameter_list|)
throws|throws
name|HiveException
block|{
name|String
index|[]
name|names
init|=
name|Utilities
operator|.
name|getDbTableName
argument_list|(
name|tableName
argument_list|)
decl_stmt|;
return|return
operator|new
name|Table
argument_list|(
name|names
index|[
literal|0
index|]
argument_list|,
name|names
index|[
literal|1
index|]
argument_list|)
return|;
block|}
specifier|public
name|String
name|getDelegationToken
parameter_list|(
name|String
name|owner
parameter_list|,
name|String
name|renewer
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|getDelegationToken
argument_list|(
name|owner
argument_list|,
name|renewer
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|void
name|cancelDelegationToken
parameter_list|(
name|String
name|tokenStrForm
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|getMSC
argument_list|()
operator|.
name|cancelDelegationToken
argument_list|(
name|tokenStrForm
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    * Enqueue a compaction request.    * @param dbname name of the database, if null default will be used.    * @param tableName name of the table, cannot be null    * @param partName name of the partition, if null table will be compacted (valid only for    *                 non-partitioned tables).    * @param compactType major or minor    * @throws HiveException    */
specifier|public
name|void
name|compact
parameter_list|(
name|String
name|dbname
parameter_list|,
name|String
name|tableName
parameter_list|,
name|String
name|partName
parameter_list|,
name|String
name|compactType
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|CompactionType
name|cr
init|=
literal|null
decl_stmt|;
if|if
condition|(
literal|"major"
operator|.
name|equals
argument_list|(
name|compactType
argument_list|)
condition|)
name|cr
operator|=
name|CompactionType
operator|.
name|MAJOR
expr_stmt|;
elseif|else
if|if
condition|(
literal|"minor"
operator|.
name|equals
argument_list|(
name|compactType
argument_list|)
condition|)
name|cr
operator|=
name|CompactionType
operator|.
name|MINOR
expr_stmt|;
else|else
throw|throw
operator|new
name|RuntimeException
argument_list|(
literal|"Unknown compaction type "
operator|+
name|compactType
argument_list|)
throw|;
name|getMSC
argument_list|()
operator|.
name|compact
argument_list|(
name|dbname
argument_list|,
name|tableName
argument_list|,
name|partName
argument_list|,
name|cr
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|ShowCompactResponse
name|showCompactions
parameter_list|()
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|showCompactions
argument_list|()
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|GetOpenTxnsInfoResponse
name|showTransactions
parameter_list|()
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|showTxns
argument_list|()
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
name|void
name|createFunction
parameter_list|(
name|Function
name|func
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|getMSC
argument_list|()
operator|.
name|createFunction
argument_list|(
name|func
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|TException
name|te
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|te
argument_list|)
throw|;
block|}
block|}
specifier|public
name|void
name|alterFunction
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|funcName
parameter_list|,
name|Function
name|newFunction
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|getMSC
argument_list|()
operator|.
name|alterFunction
argument_list|(
name|dbName
argument_list|,
name|funcName
argument_list|,
name|newFunction
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|TException
name|te
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|te
argument_list|)
throw|;
block|}
block|}
specifier|public
name|void
name|dropFunction
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|funcName
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|getMSC
argument_list|()
operator|.
name|dropFunction
argument_list|(
name|dbName
argument_list|,
name|funcName
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|TException
name|te
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|te
argument_list|)
throw|;
block|}
block|}
specifier|public
name|Function
name|getFunction
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|funcName
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|getFunction
argument_list|(
name|dbName
argument_list|,
name|funcName
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|TException
name|te
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|te
argument_list|)
throw|;
block|}
block|}
specifier|public
name|List
argument_list|<
name|String
argument_list|>
name|getFunctions
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|pattern
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|getFunctions
argument_list|(
name|dbName
argument_list|,
name|pattern
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|TException
name|te
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|te
argument_list|)
throw|;
block|}
block|}
specifier|public
name|void
name|setMetaConf
parameter_list|(
name|String
name|propName
parameter_list|,
name|String
name|propValue
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|getMSC
argument_list|()
operator|.
name|setMetaConf
argument_list|(
name|propName
argument_list|,
name|propValue
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|TException
name|te
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|te
argument_list|)
throw|;
block|}
block|}
specifier|public
name|String
name|getMetaConf
parameter_list|(
name|String
name|propName
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
return|return
name|getMSC
argument_list|()
operator|.
name|getMetaConf
argument_list|(
name|propName
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|TException
name|te
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|te
argument_list|)
throw|;
block|}
block|}
specifier|public
name|void
name|clearMetaCallTiming
parameter_list|()
block|{
name|metaCallTimeMap
operator|.
name|clear
argument_list|()
expr_stmt|;
block|}
specifier|public
name|void
name|dumpAndClearMetaCallTiming
parameter_list|(
name|String
name|phase
parameter_list|)
block|{
name|boolean
name|phaseInfoLogged
init|=
literal|false
decl_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|phaseInfoLogged
operator|=
name|logDumpPhase
argument_list|(
name|phase
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Total time spent in each metastore function (ms): "
operator|+
name|metaCallTimeMap
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|LOG
operator|.
name|isInfoEnabled
argument_list|()
condition|)
block|{
comment|// print information about calls that took longer time at INFO level
for|for
control|(
name|Entry
argument_list|<
name|String
argument_list|,
name|Long
argument_list|>
name|callTime
range|:
name|metaCallTimeMap
operator|.
name|entrySet
argument_list|()
control|)
block|{
comment|// dump information if call took more than 1 sec (1000ms)
if|if
condition|(
name|callTime
operator|.
name|getValue
argument_list|()
operator|>
literal|1000
condition|)
block|{
if|if
condition|(
operator|!
name|phaseInfoLogged
condition|)
block|{
name|phaseInfoLogged
operator|=
name|logDumpPhase
argument_list|(
name|phase
argument_list|)
expr_stmt|;
block|}
name|LOG
operator|.
name|info
argument_list|(
literal|"Total time spent in this metastore function was greater than 1000ms : "
operator|+
name|callTime
argument_list|)
expr_stmt|;
block|}
block|}
block|}
name|metaCallTimeMap
operator|.
name|clear
argument_list|()
expr_stmt|;
block|}
specifier|private
name|boolean
name|logDumpPhase
parameter_list|(
name|String
name|phase
parameter_list|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Dumping metastore api call timing information for : "
operator|+
name|phase
operator|+
literal|" phase"
argument_list|)
expr_stmt|;
return|return
literal|true
return|;
block|}
block|}
end_class

begin_empty_stmt
empty_stmt|;
end_empty_stmt

end_unit

