begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/**  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|persistence
package|;
end_package

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|File
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|FileOutputStream
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|InputStream
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|ObjectOutputStream
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|OutputStream
import|;
end_import

begin_import
import|import
name|java
operator|.
name|nio
operator|.
name|file
operator|.
name|Files
import|;
end_import

begin_import
import|import
name|java
operator|.
name|nio
operator|.
name|file
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Collections
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configuration
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|FileUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|ExprNodeEvaluator
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|JoinUtil
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|JoinUtil
operator|.
name|JoinResult
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|SerializationUtilities
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|persistence
operator|.
name|MapJoinBytesTableContainer
operator|.
name|KeyValueHelper
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|vector
operator|.
name|VectorHashKeyWrapper
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|vector
operator|.
name|VectorHashKeyWrapperBatch
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|vector
operator|.
name|expressions
operator|.
name|VectorExpressionWriter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|vector
operator|.
name|mapjoin
operator|.
name|VectorMapJoinRowBytesContainer
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|HiveKey
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|HiveException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|ByteStream
operator|.
name|Output
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|SerDe
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|SerDeException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|WriteBuffers
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|binarysortable
operator|.
name|BinarySortableSerDe
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|lazy
operator|.
name|ByteArrayRef
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|lazybinary
operator|.
name|LazyBinaryFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|lazybinary
operator|.
name|LazyBinaryStruct
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|lazybinary
operator|.
name|objectinspector
operator|.
name|LazyBinaryStructObjectInspector
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|objectinspector
operator|.
name|ObjectInspector
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|BytesWritable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|Writable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hive
operator|.
name|common
operator|.
name|util
operator|.
name|BloomFilter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hive
operator|.
name|common
operator|.
name|util
operator|.
name|HashCodeUtil
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|Logger
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|LoggerFactory
import|;
end_import

begin_import
import|import
name|com
operator|.
name|esotericsoftware
operator|.
name|kryo
operator|.
name|Kryo
import|;
end_import

begin_comment
comment|/**  * Hash table container that can have many partitions -- each partition has its own hashmap,  * as well as row container for small table and big table.  *  * The purpose is to distribute rows into multiple partitions so that when the entire small table  * cannot fit into memory, we are still able to perform hash join, by processing them recursively.  *  * Partitions that can fit in memory will be processed first, and then every spilled partition will  * be restored and processed one by one.  */
end_comment

begin_class
specifier|public
class|class
name|HybridHashTableContainer
implements|implements
name|MapJoinTableContainer
implements|,
name|MapJoinTableContainerDirectAccess
block|{
specifier|private
specifier|static
specifier|final
name|Logger
name|LOG
init|=
name|LoggerFactory
operator|.
name|getLogger
argument_list|(
name|HybridHashTableContainer
operator|.
name|class
argument_list|)
decl_stmt|;
specifier|private
specifier|final
name|HashPartition
index|[]
name|hashPartitions
decl_stmt|;
comment|// an array of partitions holding the triplets
specifier|private
name|int
name|totalInMemRowCount
init|=
literal|0
decl_stmt|;
comment|// total number of small table rows in memory
specifier|private
name|long
name|memoryThreshold
decl_stmt|;
comment|// the max memory limit that can be allocated
specifier|private
name|long
name|memoryUsed
decl_stmt|;
comment|// the actual memory used
specifier|private
specifier|final
name|long
name|tableRowSize
decl_stmt|;
comment|// row size of the small table
specifier|private
name|boolean
name|isSpilled
decl_stmt|;
comment|// whether there's any spilled partition
specifier|private
name|int
name|toSpillPartitionId
decl_stmt|;
comment|// the partition into which to spill the big table row;
comment|// This may change after every setMapJoinKey call
specifier|private
name|int
name|numPartitionsSpilled
decl_stmt|;
comment|// number of spilled partitions
specifier|private
name|boolean
name|lastPartitionInMem
decl_stmt|;
comment|// only one (last one) partition is left in memory
specifier|private
specifier|final
name|int
name|memoryCheckFrequency
decl_stmt|;
comment|// how often (# of rows apart) to check if memory is full
specifier|private
specifier|final
name|HybridHashTableConf
name|nwayConf
decl_stmt|;
comment|// configuration for n-way join
specifier|private
name|int
name|writeBufferSize
decl_stmt|;
comment|// write buffer size for BytesBytesMultiHashMap
comment|/** The OI used to deserialize values. We never deserialize keys. */
specifier|private
name|LazyBinaryStructObjectInspector
name|internalValueOi
decl_stmt|;
specifier|private
name|boolean
index|[]
name|sortableSortOrders
decl_stmt|;
specifier|private
name|byte
index|[]
name|nullMarkers
decl_stmt|;
specifier|private
name|byte
index|[]
name|notNullMarkers
decl_stmt|;
specifier|private
name|MapJoinBytesTableContainer
operator|.
name|KeyValueHelper
name|writeHelper
decl_stmt|;
specifier|private
specifier|final
name|MapJoinBytesTableContainer
operator|.
name|DirectKeyValueWriter
name|directWriteHelper
decl_stmt|;
comment|/*    * this is not a real bloom filter, but is a cheap version of the 1-memory    * access bloom filters    *    * In several cases, we'll have map-join spills because the value columns are    * a few hundred columns of Text each, while there are very few keys in total    * (a few thousand).    *    * This is a cheap exit option to prevent spilling the big-table in such a    * scenario.    */
specifier|private
specifier|transient
specifier|final
name|BloomFilter
name|bloom1
decl_stmt|;
specifier|private
specifier|final
name|List
argument_list|<
name|Object
argument_list|>
name|EMPTY_LIST
init|=
operator|new
name|ArrayList
argument_list|<
name|Object
argument_list|>
argument_list|(
literal|0
argument_list|)
decl_stmt|;
specifier|private
specifier|final
name|String
name|spillLocalDirs
decl_stmt|;
comment|/**    * This class encapsulates the triplet together since they are closely related to each other    * The triplet: hashmap (either in memory or on disk), small table container, big table container    */
specifier|public
specifier|static
class|class
name|HashPartition
block|{
name|BytesBytesMultiHashMap
name|hashMap
decl_stmt|;
comment|// In memory hashMap
name|KeyValueContainer
name|sidefileKVContainer
decl_stmt|;
comment|// Stores small table key/value pairs
name|ObjectContainer
name|matchfileObjContainer
decl_stmt|;
comment|// Stores big table rows
name|VectorMapJoinRowBytesContainer
name|matchfileRowBytesContainer
decl_stmt|;
comment|// Stores big table rows as bytes for native vector map join.
name|Path
name|hashMapLocalPath
decl_stmt|;
comment|// Local file system path for spilled hashMap
name|boolean
name|hashMapOnDisk
decl_stmt|;
comment|// Status of hashMap. true: on disk, false: in memory
name|boolean
name|hashMapSpilledOnCreation
decl_stmt|;
comment|// When there's no enough memory, cannot create hashMap
name|int
name|initialCapacity
decl_stmt|;
comment|// Used to create an empty BytesBytesMultiHashMap
name|float
name|loadFactor
decl_stmt|;
comment|// Same as above
name|int
name|wbSize
decl_stmt|;
comment|// Same as above
name|int
name|rowsOnDisk
decl_stmt|;
comment|// How many rows saved to the on-disk hashmap (if on disk)
specifier|private
specifier|final
name|String
name|spillLocalDirs
decl_stmt|;
comment|/* It may happen that there's not enough memory to instantiate a hashmap for the partition.      * In that case, we don't create the hashmap, but pretend the hashmap is directly "spilled".      */
specifier|public
name|HashPartition
parameter_list|(
name|int
name|initialCapacity
parameter_list|,
name|float
name|loadFactor
parameter_list|,
name|int
name|wbSize
parameter_list|,
name|long
name|maxProbeSize
parameter_list|,
name|boolean
name|createHashMap
parameter_list|,
name|String
name|spillLocalDirs
parameter_list|)
block|{
if|if
condition|(
name|createHashMap
condition|)
block|{
comment|// Probe space should be at least equal to the size of our designated wbSize
name|maxProbeSize
operator|=
name|Math
operator|.
name|max
argument_list|(
name|maxProbeSize
argument_list|,
name|wbSize
argument_list|)
expr_stmt|;
name|hashMap
operator|=
operator|new
name|BytesBytesMultiHashMap
argument_list|(
name|initialCapacity
argument_list|,
name|loadFactor
argument_list|,
name|wbSize
argument_list|,
name|maxProbeSize
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|hashMapSpilledOnCreation
operator|=
literal|true
expr_stmt|;
name|hashMapOnDisk
operator|=
literal|true
expr_stmt|;
block|}
name|this
operator|.
name|spillLocalDirs
operator|=
name|spillLocalDirs
expr_stmt|;
name|this
operator|.
name|initialCapacity
operator|=
name|initialCapacity
expr_stmt|;
name|this
operator|.
name|loadFactor
operator|=
name|loadFactor
expr_stmt|;
name|this
operator|.
name|wbSize
operator|=
name|wbSize
expr_stmt|;
block|}
comment|/* Get the in memory hashmap */
specifier|public
name|BytesBytesMultiHashMap
name|getHashMapFromMemory
parameter_list|()
block|{
return|return
name|hashMap
return|;
block|}
comment|/* Restore the hashmap from disk by deserializing it.      * Currently Kryo is used for this purpose.      */
specifier|public
name|BytesBytesMultiHashMap
name|getHashMapFromDisk
parameter_list|(
name|int
name|rowCount
parameter_list|)
throws|throws
name|IOException
throws|,
name|ClassNotFoundException
block|{
if|if
condition|(
name|hashMapSpilledOnCreation
condition|)
block|{
return|return
operator|new
name|BytesBytesMultiHashMap
argument_list|(
name|rowCount
argument_list|,
name|loadFactor
argument_list|,
name|wbSize
argument_list|,
operator|-
literal|1
argument_list|)
return|;
block|}
else|else
block|{
name|InputStream
name|inputStream
init|=
name|Files
operator|.
name|newInputStream
argument_list|(
name|hashMapLocalPath
argument_list|)
decl_stmt|;
name|com
operator|.
name|esotericsoftware
operator|.
name|kryo
operator|.
name|io
operator|.
name|Input
name|input
init|=
operator|new
name|com
operator|.
name|esotericsoftware
operator|.
name|kryo
operator|.
name|io
operator|.
name|Input
argument_list|(
name|inputStream
argument_list|)
decl_stmt|;
name|Kryo
name|kryo
init|=
name|SerializationUtilities
operator|.
name|borrowKryo
argument_list|()
decl_stmt|;
name|BytesBytesMultiHashMap
name|restoredHashMap
init|=
literal|null
decl_stmt|;
try|try
block|{
name|restoredHashMap
operator|=
name|kryo
operator|.
name|readObject
argument_list|(
name|input
argument_list|,
name|BytesBytesMultiHashMap
operator|.
name|class
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
name|SerializationUtilities
operator|.
name|releaseKryo
argument_list|(
name|kryo
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|rowCount
operator|>
literal|0
condition|)
block|{
name|restoredHashMap
operator|.
name|expandAndRehashToTarget
argument_list|(
name|rowCount
argument_list|)
expr_stmt|;
block|}
comment|// some bookkeeping
name|rowsOnDisk
operator|=
literal|0
expr_stmt|;
name|hashMapOnDisk
operator|=
literal|false
expr_stmt|;
name|input
operator|.
name|close
argument_list|()
expr_stmt|;
name|inputStream
operator|.
name|close
argument_list|()
expr_stmt|;
name|Files
operator|.
name|delete
argument_list|(
name|hashMapLocalPath
argument_list|)
expr_stmt|;
return|return
name|restoredHashMap
return|;
block|}
block|}
comment|/* Get the small table key/value container */
specifier|public
name|KeyValueContainer
name|getSidefileKVContainer
parameter_list|()
block|{
if|if
condition|(
name|sidefileKVContainer
operator|==
literal|null
condition|)
block|{
name|sidefileKVContainer
operator|=
operator|new
name|KeyValueContainer
argument_list|(
name|spillLocalDirs
argument_list|)
expr_stmt|;
block|}
return|return
name|sidefileKVContainer
return|;
block|}
comment|/* Get the big table row container */
specifier|public
name|ObjectContainer
name|getMatchfileObjContainer
parameter_list|()
block|{
if|if
condition|(
name|matchfileObjContainer
operator|==
literal|null
condition|)
block|{
name|matchfileObjContainer
operator|=
operator|new
name|ObjectContainer
argument_list|(
name|spillLocalDirs
argument_list|)
expr_stmt|;
block|}
return|return
name|matchfileObjContainer
return|;
block|}
comment|/* Get the big table row bytes container for native vector map join */
specifier|public
name|VectorMapJoinRowBytesContainer
name|getMatchfileRowBytesContainer
parameter_list|()
block|{
if|if
condition|(
name|matchfileRowBytesContainer
operator|==
literal|null
condition|)
block|{
name|matchfileRowBytesContainer
operator|=
operator|new
name|VectorMapJoinRowBytesContainer
argument_list|(
name|spillLocalDirs
argument_list|)
expr_stmt|;
block|}
return|return
name|matchfileRowBytesContainer
return|;
block|}
comment|/* Check if hashmap is on disk or in memory */
specifier|public
name|boolean
name|isHashMapOnDisk
parameter_list|()
block|{
return|return
name|hashMapOnDisk
return|;
block|}
specifier|public
name|void
name|clear
parameter_list|()
block|{
if|if
condition|(
name|hashMap
operator|!=
literal|null
condition|)
block|{
name|hashMap
operator|.
name|clear
argument_list|()
expr_stmt|;
name|hashMap
operator|=
literal|null
expr_stmt|;
block|}
if|if
condition|(
name|hashMapLocalPath
operator|!=
literal|null
condition|)
block|{
try|try
block|{
name|Files
operator|.
name|delete
argument_list|(
name|hashMapLocalPath
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Throwable
name|ignored
parameter_list|)
block|{         }
name|hashMapLocalPath
operator|=
literal|null
expr_stmt|;
name|rowsOnDisk
operator|=
literal|0
expr_stmt|;
name|hashMapOnDisk
operator|=
literal|false
expr_stmt|;
block|}
if|if
condition|(
name|sidefileKVContainer
operator|!=
literal|null
condition|)
block|{
name|sidefileKVContainer
operator|.
name|clear
argument_list|()
expr_stmt|;
name|sidefileKVContainer
operator|=
literal|null
expr_stmt|;
block|}
if|if
condition|(
name|matchfileObjContainer
operator|!=
literal|null
condition|)
block|{
name|matchfileObjContainer
operator|.
name|clear
argument_list|()
expr_stmt|;
name|matchfileObjContainer
operator|=
literal|null
expr_stmt|;
block|}
if|if
condition|(
name|matchfileRowBytesContainer
operator|!=
literal|null
condition|)
block|{
name|matchfileRowBytesContainer
operator|.
name|clear
argument_list|()
expr_stmt|;
name|matchfileRowBytesContainer
operator|=
literal|null
expr_stmt|;
block|}
block|}
specifier|public
name|int
name|size
parameter_list|()
block|{
if|if
condition|(
name|isHashMapOnDisk
argument_list|()
condition|)
block|{
comment|// Rows are in a combination of the on-disk hashmap and the sidefile
return|return
name|rowsOnDisk
operator|+
operator|(
name|sidefileKVContainer
operator|!=
literal|null
condition|?
name|sidefileKVContainer
operator|.
name|size
argument_list|()
else|:
literal|0
operator|)
return|;
block|}
else|else
block|{
comment|// All rows should be in the in-memory hashmap
return|return
name|hashMap
operator|.
name|size
argument_list|()
return|;
block|}
block|}
block|}
specifier|public
name|HybridHashTableContainer
parameter_list|(
name|Configuration
name|hconf
parameter_list|,
name|long
name|keyCount
parameter_list|,
name|long
name|memoryAvailable
parameter_list|,
name|long
name|estimatedTableSize
parameter_list|,
name|HybridHashTableConf
name|nwayConf
parameter_list|)
throws|throws
name|SerDeException
throws|,
name|IOException
block|{
name|this
argument_list|(
name|HiveConf
operator|.
name|getFloatVar
argument_list|(
name|hconf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEHASHTABLEKEYCOUNTADJUSTMENT
argument_list|)
argument_list|,
name|HiveConf
operator|.
name|getIntVar
argument_list|(
name|hconf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEHASHTABLETHRESHOLD
argument_list|)
argument_list|,
name|HiveConf
operator|.
name|getFloatVar
argument_list|(
name|hconf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEHASHTABLELOADFACTOR
argument_list|)
argument_list|,
name|HiveConf
operator|.
name|getIntVar
argument_list|(
name|hconf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEHYBRIDGRACEHASHJOINMEMCHECKFREQ
argument_list|)
argument_list|,
name|HiveConf
operator|.
name|getIntVar
argument_list|(
name|hconf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEHYBRIDGRACEHASHJOINMINWBSIZE
argument_list|)
argument_list|,
name|HiveConf
operator|.
name|getIntVar
argument_list|(
name|hconf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEHASHTABLEWBSIZE
argument_list|)
argument_list|,
name|HiveConf
operator|.
name|getIntVar
argument_list|(
name|hconf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEHYBRIDGRACEHASHJOINMINNUMPARTITIONS
argument_list|)
argument_list|,
name|HiveConf
operator|.
name|getFloatVar
argument_list|(
name|hconf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEMAPJOINOPTIMIZEDTABLEPROBEPERCENT
argument_list|)
argument_list|,
name|estimatedTableSize
argument_list|,
name|keyCount
argument_list|,
name|memoryAvailable
argument_list|,
name|nwayConf
argument_list|,
name|RowContainer
operator|.
name|getLocalDirsForSpillFiles
argument_list|(
name|hconf
argument_list|)
argument_list|)
expr_stmt|;
block|}
specifier|private
name|HybridHashTableContainer
parameter_list|(
name|float
name|keyCountAdj
parameter_list|,
name|int
name|threshold
parameter_list|,
name|float
name|loadFactor
parameter_list|,
name|int
name|memCheckFreq
parameter_list|,
name|int
name|minWbSize
parameter_list|,
name|int
name|maxWbSize
parameter_list|,
name|int
name|minNumParts
parameter_list|,
name|float
name|probePercent
parameter_list|,
name|long
name|estimatedTableSize
parameter_list|,
name|long
name|keyCount
parameter_list|,
name|long
name|memoryAvailable
parameter_list|,
name|HybridHashTableConf
name|nwayConf
parameter_list|,
name|String
name|spillLocalDirs
parameter_list|)
throws|throws
name|SerDeException
throws|,
name|IOException
block|{
name|directWriteHelper
operator|=
operator|new
name|MapJoinBytesTableContainer
operator|.
name|DirectKeyValueWriter
argument_list|()
expr_stmt|;
name|int
name|newKeyCount
init|=
name|HashMapWrapper
operator|.
name|calculateTableSize
argument_list|(
name|keyCountAdj
argument_list|,
name|threshold
argument_list|,
name|loadFactor
argument_list|,
name|keyCount
argument_list|)
decl_stmt|;
name|memoryThreshold
operator|=
name|memoryAvailable
expr_stmt|;
name|tableRowSize
operator|=
name|estimatedTableSize
operator|/
operator|(
name|keyCount
operator|!=
literal|0
condition|?
name|keyCount
else|:
literal|1
operator|)
expr_stmt|;
name|memoryCheckFrequency
operator|=
name|memCheckFreq
expr_stmt|;
name|this
operator|.
name|spillLocalDirs
operator|=
name|spillLocalDirs
expr_stmt|;
name|this
operator|.
name|nwayConf
operator|=
name|nwayConf
expr_stmt|;
name|int
name|numPartitions
decl_stmt|;
if|if
condition|(
name|nwayConf
operator|==
literal|null
condition|)
block|{
comment|// binary join
name|numPartitions
operator|=
name|calcNumPartitions
argument_list|(
name|memoryThreshold
argument_list|,
name|estimatedTableSize
argument_list|,
name|minNumParts
argument_list|,
name|minWbSize
argument_list|)
expr_stmt|;
name|writeBufferSize
operator|=
call|(
name|int
call|)
argument_list|(
name|estimatedTableSize
operator|/
name|numPartitions
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// n-way join
comment|// It has been calculated in HashTableLoader earlier, so just need to retrieve that number
name|numPartitions
operator|=
name|nwayConf
operator|.
name|getNumberOfPartitions
argument_list|()
expr_stmt|;
if|if
condition|(
name|nwayConf
operator|.
name|getLoadedContainerList
argument_list|()
operator|.
name|size
argument_list|()
operator|==
literal|0
condition|)
block|{
comment|// n-way: first small table
name|writeBufferSize
operator|=
call|(
name|int
call|)
argument_list|(
name|estimatedTableSize
operator|/
name|numPartitions
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// n-way: all later small tables
while|while
condition|(
name|memoryThreshold
operator|<
name|numPartitions
operator|*
name|minWbSize
condition|)
block|{
comment|// Spill previously loaded tables to make more room
name|long
name|memFreed
init|=
name|nwayConf
operator|.
name|spill
argument_list|()
decl_stmt|;
if|if
condition|(
name|memFreed
operator|==
literal|0
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Available memory is not enough to create HybridHashTableContainers"
operator|+
literal|" consistently!"
argument_list|)
expr_stmt|;
break|break;
block|}
else|else
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Total available memory was: "
operator|+
name|memoryThreshold
argument_list|)
expr_stmt|;
name|memoryThreshold
operator|+=
name|memFreed
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Total available memory is: "
operator|+
name|memoryThreshold
argument_list|)
expr_stmt|;
block|}
block|}
name|writeBufferSize
operator|=
call|(
name|int
call|)
argument_list|(
name|memoryThreshold
operator|/
name|numPartitions
argument_list|)
expr_stmt|;
block|}
block|}
comment|// Round to power of 2 here, as is required by WriteBuffers
name|writeBufferSize
operator|=
name|Integer
operator|.
name|bitCount
argument_list|(
name|writeBufferSize
argument_list|)
operator|==
literal|1
condition|?
name|writeBufferSize
else|:
name|Integer
operator|.
name|highestOneBit
argument_list|(
name|writeBufferSize
argument_list|)
expr_stmt|;
comment|// Cap WriteBufferSize to avoid large preallocations
comment|// We also want to limit the size of writeBuffer, because we normally have 16 partitions, that
comment|// makes spilling prediction (isMemoryFull) to be too defensive which results in unnecessary spilling
name|writeBufferSize
operator|=
name|writeBufferSize
operator|<
name|minWbSize
condition|?
name|minWbSize
else|:
name|Math
operator|.
name|min
argument_list|(
name|maxWbSize
operator|/
name|numPartitions
argument_list|,
name|writeBufferSize
argument_list|)
expr_stmt|;
name|this
operator|.
name|bloom1
operator|=
operator|new
name|BloomFilter
argument_list|(
name|newKeyCount
argument_list|)
expr_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isInfoEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
name|String
operator|.
name|format
argument_list|(
literal|"Using a bloom-1 filter %d keys of size %d bytes"
argument_list|,
name|newKeyCount
argument_list|,
name|bloom1
operator|.
name|sizeInBytes
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Write buffer size: "
operator|+
name|writeBufferSize
argument_list|)
expr_stmt|;
block|}
name|hashPartitions
operator|=
operator|new
name|HashPartition
index|[
name|numPartitions
index|]
expr_stmt|;
name|int
name|numPartitionsSpilledOnCreation
init|=
literal|0
decl_stmt|;
name|memoryUsed
operator|=
literal|0
expr_stmt|;
name|int
name|initialCapacity
init|=
name|Math
operator|.
name|max
argument_list|(
name|newKeyCount
operator|/
name|numPartitions
argument_list|,
name|threshold
operator|/
name|numPartitions
argument_list|)
decl_stmt|;
comment|// maxCapacity should be calculated based on a percentage of memoryThreshold, which is to divide
comment|// row size using long size
name|float
name|probePercentage
init|=
operator|(
name|float
operator|)
literal|8
operator|/
operator|(
name|tableRowSize
operator|+
literal|8
operator|)
decl_stmt|;
comment|// long_size / tableRowSize + long_size
if|if
condition|(
name|probePercentage
operator|==
literal|1
condition|)
block|{
name|probePercentage
operator|=
name|probePercent
expr_stmt|;
block|}
name|int
name|maxCapacity
init|=
call|(
name|int
call|)
argument_list|(
name|memoryThreshold
operator|*
name|probePercentage
argument_list|)
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|numPartitions
condition|;
name|i
operator|++
control|)
block|{
if|if
condition|(
name|this
operator|.
name|nwayConf
operator|==
literal|null
operator|||
comment|// binary join
name|nwayConf
operator|.
name|getLoadedContainerList
argument_list|()
operator|.
name|size
argument_list|()
operator|==
literal|0
condition|)
block|{
comment|// n-way join, first (biggest) small table
if|if
condition|(
name|i
operator|==
literal|0
condition|)
block|{
comment|// We unconditionally create a hashmap for the first hash partition
name|hashPartitions
index|[
name|i
index|]
operator|=
operator|new
name|HashPartition
argument_list|(
name|initialCapacity
argument_list|,
name|loadFactor
argument_list|,
name|writeBufferSize
argument_list|,
name|maxCapacity
argument_list|,
literal|true
argument_list|,
name|spillLocalDirs
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// To check whether we have enough memory to allocate for another hash partition,
comment|// we need to get the size of the first hash partition to get an idea.
name|hashPartitions
index|[
name|i
index|]
operator|=
operator|new
name|HashPartition
argument_list|(
name|initialCapacity
argument_list|,
name|loadFactor
argument_list|,
name|writeBufferSize
argument_list|,
name|maxCapacity
argument_list|,
name|memoryUsed
operator|+
name|hashPartitions
index|[
literal|0
index|]
operator|.
name|hashMap
operator|.
name|memorySize
argument_list|()
operator|<
name|memoryThreshold
argument_list|,
name|spillLocalDirs
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
comment|// n-way join, all later small tables
comment|// For all later small tables, follow the same pattern of the previously loaded tables.
if|if
condition|(
name|this
operator|.
name|nwayConf
operator|.
name|doSpillOnCreation
argument_list|(
name|i
argument_list|)
condition|)
block|{
name|hashPartitions
index|[
name|i
index|]
operator|=
operator|new
name|HashPartition
argument_list|(
name|initialCapacity
argument_list|,
name|loadFactor
argument_list|,
name|writeBufferSize
argument_list|,
name|maxCapacity
argument_list|,
literal|false
argument_list|,
name|spillLocalDirs
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|hashPartitions
index|[
name|i
index|]
operator|=
operator|new
name|HashPartition
argument_list|(
name|initialCapacity
argument_list|,
name|loadFactor
argument_list|,
name|writeBufferSize
argument_list|,
name|maxCapacity
argument_list|,
literal|true
argument_list|,
name|spillLocalDirs
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
name|isHashMapSpilledOnCreation
argument_list|(
name|i
argument_list|)
condition|)
block|{
name|numPartitionsSpilledOnCreation
operator|++
expr_stmt|;
name|numPartitionsSpilled
operator|++
expr_stmt|;
name|this
operator|.
name|setSpill
argument_list|(
literal|true
argument_list|)
expr_stmt|;
if|if
condition|(
name|this
operator|.
name|nwayConf
operator|!=
literal|null
operator|&&
name|this
operator|.
name|nwayConf
operator|.
name|getNextSpillPartition
argument_list|()
operator|==
name|numPartitions
operator|-
literal|1
condition|)
block|{
name|this
operator|.
name|nwayConf
operator|.
name|setNextSpillPartition
argument_list|(
name|i
operator|-
literal|1
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
name|memoryUsed
operator|+=
name|hashPartitions
index|[
name|i
index|]
operator|.
name|hashMap
operator|.
name|memorySize
argument_list|()
expr_stmt|;
block|}
block|}
if|if
condition|(
name|writeBufferSize
operator|*
operator|(
name|numPartitions
operator|-
name|numPartitionsSpilledOnCreation
operator|)
operator|>
name|memoryThreshold
condition|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"There is not enough memory to allocate "
operator|+
operator|(
name|numPartitions
operator|-
name|numPartitionsSpilledOnCreation
operator|)
operator|+
literal|" hash partitions."
argument_list|)
expr_stmt|;
block|}
assert|assert
name|numPartitionsSpilledOnCreation
operator|!=
name|numPartitions
operator|:
literal|"All partitions are directly spilled!"
operator|+
literal|" It is not supported now."
assert|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Number of partitions created: "
operator|+
name|numPartitions
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Number of partitions spilled directly to disk on creation: "
operator|+
name|numPartitionsSpilledOnCreation
argument_list|)
expr_stmt|;
comment|// Append this container to the loaded list
if|if
condition|(
name|this
operator|.
name|nwayConf
operator|!=
literal|null
condition|)
block|{
name|this
operator|.
name|nwayConf
operator|.
name|getLoadedContainerList
argument_list|()
operator|.
name|add
argument_list|(
name|this
argument_list|)
expr_stmt|;
block|}
block|}
specifier|public
name|MapJoinBytesTableContainer
operator|.
name|KeyValueHelper
name|getWriteHelper
parameter_list|()
block|{
return|return
name|writeHelper
return|;
block|}
specifier|public
name|HashPartition
index|[]
name|getHashPartitions
parameter_list|()
block|{
return|return
name|hashPartitions
return|;
block|}
specifier|public
name|long
name|getMemoryThreshold
parameter_list|()
block|{
return|return
name|memoryThreshold
return|;
block|}
comment|/**    * Get the current memory usage by recalculating it.    * @return current memory usage    */
specifier|public
name|long
name|refreshMemoryUsed
parameter_list|()
block|{
name|long
name|memUsed
init|=
literal|0
decl_stmt|;
for|for
control|(
name|HashPartition
name|hp
range|:
name|hashPartitions
control|)
block|{
if|if
condition|(
name|hp
operator|.
name|hashMap
operator|!=
literal|null
condition|)
block|{
name|memUsed
operator|+=
name|hp
operator|.
name|hashMap
operator|.
name|memorySize
argument_list|()
expr_stmt|;
block|}
else|else
block|{
comment|// also include the still-in-memory sidefile, before it has been truely spilled
if|if
condition|(
name|hp
operator|.
name|sidefileKVContainer
operator|!=
literal|null
condition|)
block|{
name|memUsed
operator|+=
name|hp
operator|.
name|sidefileKVContainer
operator|.
name|numRowsInReadBuffer
argument_list|()
operator|*
name|tableRowSize
expr_stmt|;
block|}
block|}
block|}
return|return
name|memoryUsed
operator|=
name|memUsed
return|;
block|}
specifier|public
name|LazyBinaryStructObjectInspector
name|getInternalValueOi
parameter_list|()
block|{
return|return
name|internalValueOi
return|;
block|}
specifier|public
name|boolean
index|[]
name|getSortableSortOrders
parameter_list|()
block|{
return|return
name|sortableSortOrders
return|;
block|}
specifier|public
name|byte
index|[]
name|getNullMarkers
parameter_list|()
block|{
return|return
name|nullMarkers
return|;
block|}
specifier|public
name|byte
index|[]
name|getNotNullMarkers
parameter_list|()
block|{
return|return
name|notNullMarkers
return|;
block|}
comment|/* For a given row, put it into proper partition based on its hash value.    * When memory threshold is reached, the biggest hash table in memory will be spilled to disk.    * If the hash table of a specific partition is already on disk, all later rows will be put into    * a row container for later use.    */
annotation|@
name|SuppressWarnings
argument_list|(
literal|"deprecation"
argument_list|)
annotation|@
name|Override
specifier|public
name|MapJoinKey
name|putRow
parameter_list|(
name|Writable
name|currentKey
parameter_list|,
name|Writable
name|currentValue
parameter_list|)
throws|throws
name|SerDeException
throws|,
name|HiveException
throws|,
name|IOException
block|{
name|writeHelper
operator|.
name|setKeyValue
argument_list|(
name|currentKey
argument_list|,
name|currentValue
argument_list|)
expr_stmt|;
return|return
name|internalPutRow
argument_list|(
name|writeHelper
argument_list|,
name|currentKey
argument_list|,
name|currentValue
argument_list|)
return|;
block|}
specifier|private
name|MapJoinKey
name|internalPutRow
parameter_list|(
name|KeyValueHelper
name|keyValueHelper
parameter_list|,
name|Writable
name|currentKey
parameter_list|,
name|Writable
name|currentValue
parameter_list|)
throws|throws
name|SerDeException
throws|,
name|IOException
block|{
name|boolean
name|putToSidefile
init|=
literal|false
decl_stmt|;
comment|// by default we put row into partition in memory
comment|// Next, put row into corresponding hash partition
name|int
name|keyHash
init|=
name|keyValueHelper
operator|.
name|getHashFromKey
argument_list|()
decl_stmt|;
name|int
name|partitionId
init|=
name|keyHash
operator|&
operator|(
name|hashPartitions
operator|.
name|length
operator|-
literal|1
operator|)
decl_stmt|;
name|HashPartition
name|hashPartition
init|=
name|hashPartitions
index|[
name|partitionId
index|]
decl_stmt|;
name|bloom1
operator|.
name|addLong
argument_list|(
name|keyHash
argument_list|)
expr_stmt|;
if|if
condition|(
name|isOnDisk
argument_list|(
name|partitionId
argument_list|)
operator|||
name|isHashMapSpilledOnCreation
argument_list|(
name|partitionId
argument_list|)
condition|)
block|{
comment|// destination on disk
name|putToSidefile
operator|=
literal|true
expr_stmt|;
block|}
else|else
block|{
comment|// destination in memory
if|if
condition|(
operator|!
name|lastPartitionInMem
operator|&&
comment|// If this is the only partition in memory, proceed without check
operator|(
name|hashPartition
operator|.
name|size
argument_list|()
operator|==
literal|0
operator|||
comment|// Destination partition being empty indicates a write buffer
comment|// will be allocated, thus need to check if memory is full
operator|(
name|totalInMemRowCount
operator|&
operator|(
name|this
operator|.
name|memoryCheckFrequency
operator|-
literal|1
operator|)
operator|)
operator|==
literal|0
operator|)
condition|)
block|{
comment|// check periodically
if|if
condition|(
name|isMemoryFull
argument_list|()
condition|)
block|{
if|if
condition|(
operator|(
name|numPartitionsSpilled
operator|==
name|hashPartitions
operator|.
name|length
operator|-
literal|1
operator|)
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"This LAST partition in memory won't be spilled!"
argument_list|)
expr_stmt|;
name|lastPartitionInMem
operator|=
literal|true
expr_stmt|;
block|}
else|else
block|{
if|if
condition|(
name|nwayConf
operator|==
literal|null
condition|)
block|{
comment|// binary join
name|int
name|biggest
init|=
name|biggestPartition
argument_list|()
decl_stmt|;
name|spillPartition
argument_list|(
name|biggest
argument_list|)
expr_stmt|;
name|this
operator|.
name|setSpill
argument_list|(
literal|true
argument_list|)
expr_stmt|;
if|if
condition|(
name|partitionId
operator|==
name|biggest
condition|)
block|{
comment|// destination hash partition has just be spilled
name|putToSidefile
operator|=
literal|true
expr_stmt|;
block|}
block|}
else|else
block|{
comment|// n-way join
name|LOG
operator|.
name|info
argument_list|(
literal|"N-way spilling: spill tail partition from previously loaded small tables"
argument_list|)
expr_stmt|;
name|int
name|biggest
init|=
name|nwayConf
operator|.
name|getNextSpillPartition
argument_list|()
decl_stmt|;
name|memoryThreshold
operator|+=
name|nwayConf
operator|.
name|spill
argument_list|()
expr_stmt|;
if|if
condition|(
name|biggest
operator|!=
literal|0
operator|&&
name|partitionId
operator|==
name|biggest
condition|)
block|{
comment|// destination hash partition has just be spilled
name|putToSidefile
operator|=
literal|true
expr_stmt|;
block|}
name|LOG
operator|.
name|info
argument_list|(
literal|"Memory threshold has been increased to: "
operator|+
name|memoryThreshold
argument_list|)
expr_stmt|;
block|}
name|numPartitionsSpilled
operator|++
expr_stmt|;
block|}
block|}
block|}
block|}
comment|// Now we know where to put row
if|if
condition|(
name|putToSidefile
condition|)
block|{
name|KeyValueContainer
name|kvContainer
init|=
name|hashPartition
operator|.
name|getSidefileKVContainer
argument_list|()
decl_stmt|;
name|kvContainer
operator|.
name|add
argument_list|(
operator|(
name|HiveKey
operator|)
name|currentKey
argument_list|,
operator|(
name|BytesWritable
operator|)
name|currentValue
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|hashPartition
operator|.
name|hashMap
operator|.
name|put
argument_list|(
name|keyValueHelper
argument_list|,
name|keyHash
argument_list|)
expr_stmt|;
comment|// Pass along hashcode to avoid recalculation
name|totalInMemRowCount
operator|++
expr_stmt|;
block|}
return|return
literal|null
return|;
comment|// there's no key to return
block|}
comment|/**    * Check if the hash table of a specified partition is on disk (or "spilled" on creation)    * @param partitionId partition number    * @return true if on disk, false if in memory    */
specifier|public
name|boolean
name|isOnDisk
parameter_list|(
name|int
name|partitionId
parameter_list|)
block|{
return|return
name|hashPartitions
index|[
name|partitionId
index|]
operator|.
name|hashMapOnDisk
return|;
block|}
comment|/**    * Check if the hash table of a specified partition has been "spilled" to disk when it was created.    * In fact, in other words, check if a hashmap does exist or not.    * @param partitionId hashMap ID    * @return true if it was not created at all, false if there is a hash table existing there    */
specifier|public
name|boolean
name|isHashMapSpilledOnCreation
parameter_list|(
name|int
name|partitionId
parameter_list|)
block|{
return|return
name|hashPartitions
index|[
name|partitionId
index|]
operator|.
name|hashMapSpilledOnCreation
return|;
block|}
comment|/**    * Check if the memory threshold is about to be reached.    * Since all the write buffer will be lazily allocated in BytesBytesMultiHashMap, we need to    * consider those as well.    * @return true if memory is full, false if not    */
specifier|private
name|boolean
name|isMemoryFull
parameter_list|()
block|{
name|int
name|numPartitionsInMem
init|=
literal|0
decl_stmt|;
for|for
control|(
name|HashPartition
name|hp
range|:
name|hashPartitions
control|)
block|{
if|if
condition|(
operator|!
name|hp
operator|.
name|isHashMapOnDisk
argument_list|()
condition|)
block|{
name|numPartitionsInMem
operator|++
expr_stmt|;
block|}
block|}
return|return
name|refreshMemoryUsed
argument_list|()
operator|+
name|writeBufferSize
operator|*
name|numPartitionsInMem
operator|>=
name|memoryThreshold
return|;
block|}
comment|/**    * Find the partition with biggest hashtable in memory at this moment    * @return the biggest partition number    */
specifier|private
name|int
name|biggestPartition
parameter_list|()
block|{
name|int
name|res
init|=
operator|-
literal|1
decl_stmt|;
name|int
name|maxSize
init|=
literal|0
decl_stmt|;
comment|// If a partition has been spilled to disk, its size will be 0, i.e. it won't be picked
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|hashPartitions
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|int
name|size
decl_stmt|;
if|if
condition|(
name|isOnDisk
argument_list|(
name|i
argument_list|)
condition|)
block|{
continue|continue;
block|}
else|else
block|{
name|size
operator|=
name|hashPartitions
index|[
name|i
index|]
operator|.
name|hashMap
operator|.
name|getNumValues
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
name|size
operator|>
name|maxSize
condition|)
block|{
name|maxSize
operator|=
name|size
expr_stmt|;
name|res
operator|=
name|i
expr_stmt|;
block|}
block|}
comment|// It can happen that although there're some partitions in memory, but their sizes are all 0.
comment|// In that case we just pick one and spill.
if|if
condition|(
name|res
operator|==
operator|-
literal|1
condition|)
block|{
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|hashPartitions
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
if|if
condition|(
operator|!
name|isOnDisk
argument_list|(
name|i
argument_list|)
condition|)
block|{
return|return
name|i
return|;
block|}
block|}
block|}
return|return
name|res
return|;
block|}
comment|/**    * Move the hashtable of a specified partition from memory into local file system    * @param partitionId the hashtable to be moved    * @return amount of memory freed    */
specifier|public
name|long
name|spillPartition
parameter_list|(
name|int
name|partitionId
parameter_list|)
throws|throws
name|IOException
block|{
name|HashPartition
name|partition
init|=
name|hashPartitions
index|[
name|partitionId
index|]
decl_stmt|;
name|int
name|inMemRowCount
init|=
name|partition
operator|.
name|hashMap
operator|.
name|getNumValues
argument_list|()
decl_stmt|;
if|if
condition|(
name|inMemRowCount
operator|==
literal|0
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Trying to spill an empty hash partition! It may be due to "
operator|+
literal|"hive.auto.convert.join.noconditionaltask.size being set too low."
argument_list|)
expr_stmt|;
block|}
name|File
name|file
init|=
name|FileUtils
operator|.
name|createLocalDirsTempFile
argument_list|(
name|spillLocalDirs
argument_list|,
literal|"partition-"
operator|+
name|partitionId
operator|+
literal|"-"
argument_list|,
literal|null
argument_list|,
literal|false
argument_list|)
decl_stmt|;
name|OutputStream
name|outputStream
init|=
operator|new
name|FileOutputStream
argument_list|(
name|file
argument_list|,
literal|false
argument_list|)
decl_stmt|;
name|com
operator|.
name|esotericsoftware
operator|.
name|kryo
operator|.
name|io
operator|.
name|Output
name|output
init|=
operator|new
name|com
operator|.
name|esotericsoftware
operator|.
name|kryo
operator|.
name|io
operator|.
name|Output
argument_list|(
name|outputStream
argument_list|)
decl_stmt|;
name|Kryo
name|kryo
init|=
name|SerializationUtilities
operator|.
name|borrowKryo
argument_list|()
decl_stmt|;
try|try
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Trying to spill hash partition "
operator|+
name|partitionId
operator|+
literal|" ..."
argument_list|)
expr_stmt|;
name|kryo
operator|.
name|writeObject
argument_list|(
name|output
argument_list|,
name|partition
operator|.
name|hashMap
argument_list|)
expr_stmt|;
comment|// use Kryo to serialize hashmap
name|output
operator|.
name|close
argument_list|()
expr_stmt|;
name|outputStream
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
finally|finally
block|{
name|SerializationUtilities
operator|.
name|releaseKryo
argument_list|(
name|kryo
argument_list|)
expr_stmt|;
block|}
name|partition
operator|.
name|hashMapLocalPath
operator|=
name|file
operator|.
name|toPath
argument_list|()
expr_stmt|;
name|partition
operator|.
name|hashMapOnDisk
operator|=
literal|true
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Spilling hash partition "
operator|+
name|partitionId
operator|+
literal|" (Rows: "
operator|+
name|inMemRowCount
operator|+
literal|", Mem size: "
operator|+
name|partition
operator|.
name|hashMap
operator|.
name|memorySize
argument_list|()
operator|+
literal|"): "
operator|+
name|file
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Memory usage before spilling: "
operator|+
name|memoryUsed
argument_list|)
expr_stmt|;
name|long
name|memFreed
init|=
name|partition
operator|.
name|hashMap
operator|.
name|memorySize
argument_list|()
decl_stmt|;
name|memoryUsed
operator|-=
name|memFreed
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Memory usage after spilling: "
operator|+
name|memoryUsed
argument_list|)
expr_stmt|;
name|partition
operator|.
name|rowsOnDisk
operator|=
name|inMemRowCount
expr_stmt|;
name|totalInMemRowCount
operator|-=
name|inMemRowCount
expr_stmt|;
name|partition
operator|.
name|hashMap
operator|.
name|clear
argument_list|()
expr_stmt|;
return|return
name|memFreed
return|;
block|}
comment|/**    * Calculate how many partitions are needed.    * For n-way join, we only do this calculation once in the HashTableLoader, for the biggest small    * table. Other small tables will use the same number. They may need to adjust (usually reduce)    * their individual write buffer size in order not to exceed memory threshold.    * @param memoryThreshold memory threshold for the given table    * @param dataSize total data size for the table    * @param minNumParts minimum required number of partitions    * @param minWbSize minimum required write buffer size    * @return number of partitions needed    */
specifier|public
specifier|static
name|int
name|calcNumPartitions
parameter_list|(
name|long
name|memoryThreshold
parameter_list|,
name|long
name|dataSize
parameter_list|,
name|int
name|minNumParts
parameter_list|,
name|int
name|minWbSize
parameter_list|)
throws|throws
name|IOException
block|{
name|int
name|numPartitions
init|=
name|minNumParts
decl_stmt|;
if|if
condition|(
name|memoryThreshold
operator|<
name|minNumParts
operator|*
name|minWbSize
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Available memory is not enough to create a HybridHashTableContainer!"
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|memoryThreshold
operator|<
name|dataSize
condition|)
block|{
while|while
condition|(
name|dataSize
operator|/
name|numPartitions
operator|>
name|memoryThreshold
condition|)
block|{
name|numPartitions
operator|*=
literal|2
expr_stmt|;
block|}
block|}
name|LOG
operator|.
name|info
argument_list|(
literal|"Total available memory: "
operator|+
name|memoryThreshold
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Estimated small table size: "
operator|+
name|dataSize
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Number of hash partitions to be created: "
operator|+
name|numPartitions
argument_list|)
expr_stmt|;
return|return
name|numPartitions
return|;
block|}
comment|/* Get number of partitions */
specifier|public
name|int
name|getNumPartitions
parameter_list|()
block|{
return|return
name|hashPartitions
operator|.
name|length
return|;
block|}
comment|/* Get total number of rows from all in memory partitions */
specifier|public
name|int
name|getTotalInMemRowCount
parameter_list|()
block|{
return|return
name|totalInMemRowCount
return|;
block|}
comment|/* Set total number of rows from all in memory partitions */
specifier|public
name|void
name|setTotalInMemRowCount
parameter_list|(
name|int
name|totalInMemRowCount
parameter_list|)
block|{
name|this
operator|.
name|totalInMemRowCount
operator|=
name|totalInMemRowCount
expr_stmt|;
block|}
comment|/* Get row size of small table */
specifier|public
name|long
name|getTableRowSize
parameter_list|()
block|{
return|return
name|tableRowSize
return|;
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|hasSpill
parameter_list|()
block|{
return|return
name|isSpilled
return|;
block|}
specifier|public
name|void
name|setSpill
parameter_list|(
name|boolean
name|isSpilled
parameter_list|)
block|{
name|this
operator|.
name|isSpilled
operator|=
name|isSpilled
expr_stmt|;
block|}
comment|/**    * Gets the partition Id into which to spill the big table row    * @return partition Id    */
specifier|public
name|int
name|getToSpillPartitionId
parameter_list|()
block|{
return|return
name|toSpillPartitionId
return|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|clear
parameter_list|()
block|{
for|for
control|(
name|HashPartition
name|hp
range|:
name|hashPartitions
control|)
block|{
if|if
condition|(
name|hp
operator|!=
literal|null
condition|)
block|{
name|hp
operator|.
name|clear
argument_list|()
expr_stmt|;
block|}
block|}
name|memoryUsed
operator|=
literal|0
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|MapJoinKey
name|getAnyKey
parameter_list|()
block|{
return|return
literal|null
return|;
comment|// This table has no keys.
block|}
annotation|@
name|Override
specifier|public
name|ReusableGetAdaptor
name|createGetter
parameter_list|(
name|MapJoinKey
name|keyTypeFromLoader
parameter_list|)
block|{
if|if
condition|(
name|keyTypeFromLoader
operator|!=
literal|null
condition|)
block|{
throw|throw
operator|new
name|AssertionError
argument_list|(
literal|"No key expected from loader but got "
operator|+
name|keyTypeFromLoader
argument_list|)
throw|;
block|}
return|return
operator|new
name|GetAdaptor
argument_list|()
return|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|seal
parameter_list|()
block|{
for|for
control|(
name|HashPartition
name|hp
range|:
name|hashPartitions
control|)
block|{
comment|// Only seal those partitions that haven't been spilled and cleared,
comment|// because once a hashMap is cleared, it will become unusable
if|if
condition|(
name|hp
operator|.
name|hashMap
operator|!=
literal|null
operator|&&
name|hp
operator|.
name|hashMap
operator|.
name|size
argument_list|()
operator|!=
literal|0
condition|)
block|{
name|hp
operator|.
name|hashMap
operator|.
name|seal
argument_list|()
expr_stmt|;
block|}
block|}
block|}
comment|// Direct access interfaces.
annotation|@
name|Override
specifier|public
name|void
name|put
parameter_list|(
name|Writable
name|currentKey
parameter_list|,
name|Writable
name|currentValue
parameter_list|)
throws|throws
name|SerDeException
throws|,
name|IOException
block|{
name|directWriteHelper
operator|.
name|setKeyValue
argument_list|(
name|currentKey
argument_list|,
name|currentValue
argument_list|)
expr_stmt|;
name|internalPutRow
argument_list|(
name|directWriteHelper
argument_list|,
name|currentKey
argument_list|,
name|currentValue
argument_list|)
expr_stmt|;
block|}
comment|/** Implementation of ReusableGetAdaptor that has Output for key serialization; row    * container is also created once and reused for every row. */
specifier|private
class|class
name|GetAdaptor
implements|implements
name|ReusableGetAdaptor
implements|,
name|ReusableGetAdaptorDirectAccess
block|{
specifier|private
name|Object
index|[]
name|currentKey
decl_stmt|;
specifier|private
name|boolean
index|[]
name|nulls
decl_stmt|;
specifier|private
name|List
argument_list|<
name|ObjectInspector
argument_list|>
name|vectorKeyOIs
decl_stmt|;
specifier|private
specifier|final
name|ReusableRowContainer
name|currentValue
decl_stmt|;
specifier|private
specifier|final
name|Output
name|output
decl_stmt|;
specifier|public
name|GetAdaptor
parameter_list|()
block|{
name|currentValue
operator|=
operator|new
name|ReusableRowContainer
argument_list|()
expr_stmt|;
name|output
operator|=
operator|new
name|Output
argument_list|()
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|JoinUtil
operator|.
name|JoinResult
name|setFromVector
parameter_list|(
name|VectorHashKeyWrapper
name|kw
parameter_list|,
name|VectorExpressionWriter
index|[]
name|keyOutputWriters
parameter_list|,
name|VectorHashKeyWrapperBatch
name|keyWrapperBatch
parameter_list|)
throws|throws
name|HiveException
block|{
if|if
condition|(
name|nulls
operator|==
literal|null
condition|)
block|{
name|nulls
operator|=
operator|new
name|boolean
index|[
name|keyOutputWriters
operator|.
name|length
index|]
expr_stmt|;
name|currentKey
operator|=
operator|new
name|Object
index|[
name|keyOutputWriters
operator|.
name|length
index|]
expr_stmt|;
name|vectorKeyOIs
operator|=
operator|new
name|ArrayList
argument_list|<
name|ObjectInspector
argument_list|>
argument_list|()
expr_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|keyOutputWriters
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|vectorKeyOIs
operator|.
name|add
argument_list|(
name|keyOutputWriters
index|[
name|i
index|]
operator|.
name|getObjectInspector
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
assert|assert
name|nulls
operator|.
name|length
operator|==
name|keyOutputWriters
operator|.
name|length
assert|;
block|}
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|keyOutputWriters
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|currentKey
index|[
name|i
index|]
operator|=
name|keyWrapperBatch
operator|.
name|getWritableKeyValue
argument_list|(
name|kw
argument_list|,
name|i
argument_list|,
name|keyOutputWriters
index|[
name|i
index|]
argument_list|)
expr_stmt|;
name|nulls
index|[
name|i
index|]
operator|=
name|currentKey
index|[
name|i
index|]
operator|==
literal|null
expr_stmt|;
block|}
return|return
name|currentValue
operator|.
name|setFromOutput
argument_list|(
name|MapJoinKey
operator|.
name|serializeRow
argument_list|(
name|output
argument_list|,
name|currentKey
argument_list|,
name|vectorKeyOIs
argument_list|,
name|sortableSortOrders
argument_list|,
name|nullMarkers
argument_list|,
name|notNullMarkers
argument_list|)
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|public
name|JoinUtil
operator|.
name|JoinResult
name|setFromRow
parameter_list|(
name|Object
name|row
parameter_list|,
name|List
argument_list|<
name|ExprNodeEvaluator
argument_list|>
name|fields
parameter_list|,
name|List
argument_list|<
name|ObjectInspector
argument_list|>
name|ois
parameter_list|)
throws|throws
name|HiveException
block|{
if|if
condition|(
name|nulls
operator|==
literal|null
condition|)
block|{
name|nulls
operator|=
operator|new
name|boolean
index|[
name|fields
operator|.
name|size
argument_list|()
index|]
expr_stmt|;
name|currentKey
operator|=
operator|new
name|Object
index|[
name|fields
operator|.
name|size
argument_list|()
index|]
expr_stmt|;
block|}
for|for
control|(
name|int
name|keyIndex
init|=
literal|0
init|;
name|keyIndex
operator|<
name|fields
operator|.
name|size
argument_list|()
condition|;
operator|++
name|keyIndex
control|)
block|{
name|currentKey
index|[
name|keyIndex
index|]
operator|=
name|fields
operator|.
name|get
argument_list|(
name|keyIndex
argument_list|)
operator|.
name|evaluate
argument_list|(
name|row
argument_list|)
expr_stmt|;
name|nulls
index|[
name|keyIndex
index|]
operator|=
name|currentKey
index|[
name|keyIndex
index|]
operator|==
literal|null
expr_stmt|;
block|}
return|return
name|currentValue
operator|.
name|setFromOutput
argument_list|(
name|MapJoinKey
operator|.
name|serializeRow
argument_list|(
name|output
argument_list|,
name|currentKey
argument_list|,
name|ois
argument_list|,
name|sortableSortOrders
argument_list|,
name|nullMarkers
argument_list|,
name|notNullMarkers
argument_list|)
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|public
name|JoinUtil
operator|.
name|JoinResult
name|setFromOther
parameter_list|(
name|ReusableGetAdaptor
name|other
parameter_list|)
throws|throws
name|HiveException
block|{
assert|assert
name|other
operator|instanceof
name|GetAdaptor
assert|;
name|GetAdaptor
name|other2
init|=
operator|(
name|GetAdaptor
operator|)
name|other
decl_stmt|;
name|nulls
operator|=
name|other2
operator|.
name|nulls
expr_stmt|;
name|currentKey
operator|=
name|other2
operator|.
name|currentKey
expr_stmt|;
return|return
name|currentValue
operator|.
name|setFromOutput
argument_list|(
name|other2
operator|.
name|output
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|hasAnyNulls
parameter_list|(
name|int
name|fieldCount
parameter_list|,
name|boolean
index|[]
name|nullsafes
parameter_list|)
block|{
if|if
condition|(
name|nulls
operator|==
literal|null
operator|||
name|nulls
operator|.
name|length
operator|==
literal|0
condition|)
return|return
literal|false
return|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|nulls
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
if|if
condition|(
name|nulls
index|[
name|i
index|]
operator|&&
operator|(
name|nullsafes
operator|==
literal|null
operator|||
operator|!
name|nullsafes
index|[
name|i
index|]
operator|)
condition|)
block|{
return|return
literal|true
return|;
block|}
block|}
return|return
literal|false
return|;
block|}
annotation|@
name|Override
specifier|public
name|MapJoinRowContainer
name|getCurrentRows
parameter_list|()
block|{
return|return
operator|!
name|currentValue
operator|.
name|hasRows
argument_list|()
condition|?
literal|null
else|:
name|currentValue
return|;
block|}
annotation|@
name|Override
specifier|public
name|Object
index|[]
name|getCurrentKey
parameter_list|()
block|{
return|return
name|currentKey
return|;
block|}
comment|// Direct access interfaces.
annotation|@
name|Override
specifier|public
name|JoinUtil
operator|.
name|JoinResult
name|setDirect
parameter_list|(
name|byte
index|[]
name|bytes
parameter_list|,
name|int
name|offset
parameter_list|,
name|int
name|length
parameter_list|,
name|BytesBytesMultiHashMap
operator|.
name|Result
name|hashMapResult
parameter_list|)
block|{
return|return
name|currentValue
operator|.
name|setDirect
argument_list|(
name|bytes
argument_list|,
name|offset
argument_list|,
name|length
argument_list|,
name|hashMapResult
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|public
name|int
name|directSpillPartitionId
parameter_list|()
block|{
return|return
name|currentValue
operator|.
name|directSpillPartitionId
argument_list|()
return|;
block|}
block|}
comment|/** Row container that gets and deserializes the rows on demand from bytes provided. */
specifier|private
class|class
name|ReusableRowContainer
implements|implements
name|MapJoinRowContainer
implements|,
name|AbstractRowContainer
operator|.
name|RowIterator
argument_list|<
name|List
argument_list|<
name|Object
argument_list|>
argument_list|>
block|{
specifier|private
name|byte
name|aliasFilter
decl_stmt|;
specifier|private
specifier|final
name|BytesBytesMultiHashMap
operator|.
name|Result
name|hashMapResult
decl_stmt|;
comment|/**      * Sometimes, when container is empty in multi-table mapjoin, we need to add a dummy row.      * This container does not normally support adding rows; this is for the dummy row.      */
specifier|private
name|List
argument_list|<
name|Object
argument_list|>
name|dummyRow
init|=
literal|null
decl_stmt|;
specifier|private
specifier|final
name|ByteArrayRef
name|uselessIndirection
decl_stmt|;
comment|// LBStruct needs ByteArrayRef
specifier|private
specifier|final
name|LazyBinaryStruct
name|valueStruct
decl_stmt|;
specifier|private
specifier|final
name|boolean
name|needsComplexObjectFixup
decl_stmt|;
specifier|private
specifier|final
name|ArrayList
argument_list|<
name|Object
argument_list|>
name|complexObjectArrayBuffer
decl_stmt|;
specifier|private
name|int
name|partitionId
decl_stmt|;
comment|// Current hashMap in use
specifier|public
name|ReusableRowContainer
parameter_list|()
block|{
if|if
condition|(
name|internalValueOi
operator|!=
literal|null
condition|)
block|{
name|valueStruct
operator|=
operator|(
name|LazyBinaryStruct
operator|)
name|LazyBinaryFactory
operator|.
name|createLazyBinaryObject
argument_list|(
name|internalValueOi
argument_list|)
expr_stmt|;
name|needsComplexObjectFixup
operator|=
name|MapJoinBytesTableContainer
operator|.
name|hasComplexObjects
argument_list|(
name|internalValueOi
argument_list|)
expr_stmt|;
if|if
condition|(
name|needsComplexObjectFixup
condition|)
block|{
name|complexObjectArrayBuffer
operator|=
operator|new
name|ArrayList
argument_list|<
name|Object
argument_list|>
argument_list|(
name|Collections
operator|.
name|nCopies
argument_list|(
name|internalValueOi
operator|.
name|getAllStructFieldRefs
argument_list|()
operator|.
name|size
argument_list|()
argument_list|,
literal|null
argument_list|)
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|complexObjectArrayBuffer
operator|=
literal|null
expr_stmt|;
block|}
block|}
else|else
block|{
name|valueStruct
operator|=
literal|null
expr_stmt|;
comment|// No rows?
name|needsComplexObjectFixup
operator|=
literal|false
expr_stmt|;
name|complexObjectArrayBuffer
operator|=
literal|null
expr_stmt|;
block|}
name|uselessIndirection
operator|=
operator|new
name|ByteArrayRef
argument_list|()
expr_stmt|;
name|hashMapResult
operator|=
operator|new
name|BytesBytesMultiHashMap
operator|.
name|Result
argument_list|()
expr_stmt|;
name|clearRows
argument_list|()
expr_stmt|;
block|}
comment|/* Determine if there is a match between big table row and the corresponding hashtable      * Three states can be returned:      * MATCH: a match is found      * NOMATCH: no match is found from the specified partition      * SPILL: the specified partition has been spilled to disk and is not available;      *        the evaluation for this big table row will be postponed.      */
specifier|public
name|JoinUtil
operator|.
name|JoinResult
name|setFromOutput
parameter_list|(
name|Output
name|output
parameter_list|)
throws|throws
name|HiveException
block|{
name|int
name|keyHash
init|=
name|HashCodeUtil
operator|.
name|murmurHash
argument_list|(
name|output
operator|.
name|getData
argument_list|()
argument_list|,
literal|0
argument_list|,
name|output
operator|.
name|getLength
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|bloom1
operator|.
name|testLong
argument_list|(
name|keyHash
argument_list|)
condition|)
block|{
comment|/*          * if the keyHash is missing in the bloom filter, then the value cannot          * exist in any of the spilled partition - return NOMATCH          */
name|dummyRow
operator|=
literal|null
expr_stmt|;
name|aliasFilter
operator|=
operator|(
name|byte
operator|)
literal|0xff
expr_stmt|;
name|hashMapResult
operator|.
name|forget
argument_list|()
expr_stmt|;
return|return
name|JoinResult
operator|.
name|NOMATCH
return|;
block|}
name|partitionId
operator|=
name|keyHash
operator|&
operator|(
name|hashPartitions
operator|.
name|length
operator|-
literal|1
operator|)
expr_stmt|;
comment|// If the target hash table is on disk, spill this row to disk as well to be processed later
if|if
condition|(
name|isOnDisk
argument_list|(
name|partitionId
argument_list|)
condition|)
block|{
name|toSpillPartitionId
operator|=
name|partitionId
expr_stmt|;
name|hashMapResult
operator|.
name|forget
argument_list|()
expr_stmt|;
return|return
name|JoinUtil
operator|.
name|JoinResult
operator|.
name|SPILL
return|;
block|}
else|else
block|{
name|aliasFilter
operator|=
name|hashPartitions
index|[
name|partitionId
index|]
operator|.
name|hashMap
operator|.
name|getValueResult
argument_list|(
name|output
operator|.
name|getData
argument_list|()
argument_list|,
literal|0
argument_list|,
name|output
operator|.
name|getLength
argument_list|()
argument_list|,
name|hashMapResult
argument_list|)
expr_stmt|;
name|dummyRow
operator|=
literal|null
expr_stmt|;
if|if
condition|(
name|hashMapResult
operator|.
name|hasRows
argument_list|()
condition|)
block|{
return|return
name|JoinUtil
operator|.
name|JoinResult
operator|.
name|MATCH
return|;
block|}
else|else
block|{
name|aliasFilter
operator|=
operator|(
name|byte
operator|)
literal|0xff
expr_stmt|;
return|return
name|JoinUtil
operator|.
name|JoinResult
operator|.
name|NOMATCH
return|;
block|}
block|}
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|hasRows
parameter_list|()
block|{
return|return
name|hashMapResult
operator|.
name|hasRows
argument_list|()
operator|||
operator|(
name|dummyRow
operator|!=
literal|null
operator|)
return|;
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|isSingleRow
parameter_list|()
block|{
if|if
condition|(
operator|!
name|hashMapResult
operator|.
name|hasRows
argument_list|()
condition|)
block|{
return|return
operator|(
name|dummyRow
operator|!=
literal|null
operator|)
return|;
block|}
return|return
name|hashMapResult
operator|.
name|isSingleRow
argument_list|()
return|;
block|}
comment|// Implementation of row container
annotation|@
name|Override
specifier|public
name|AbstractRowContainer
operator|.
name|RowIterator
argument_list|<
name|List
argument_list|<
name|Object
argument_list|>
argument_list|>
name|rowIter
parameter_list|()
throws|throws
name|HiveException
block|{
return|return
name|this
return|;
block|}
annotation|@
name|Override
specifier|public
name|int
name|rowCount
parameter_list|()
throws|throws
name|HiveException
block|{
comment|// For performance reasons we do not want to chase the values to the end to determine
comment|// the count.  Use hasRows and isSingleRow instead.
throw|throw
operator|new
name|UnsupportedOperationException
argument_list|(
literal|"Getting the row count not supported"
argument_list|)
throw|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|clearRows
parameter_list|()
block|{
comment|// Doesn't clear underlying hashtable
name|hashMapResult
operator|.
name|forget
argument_list|()
expr_stmt|;
name|dummyRow
operator|=
literal|null
expr_stmt|;
name|aliasFilter
operator|=
operator|(
name|byte
operator|)
literal|0xff
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|byte
name|getAliasFilter
parameter_list|()
throws|throws
name|HiveException
block|{
return|return
name|aliasFilter
return|;
block|}
annotation|@
name|Override
specifier|public
name|MapJoinRowContainer
name|copy
parameter_list|()
throws|throws
name|HiveException
block|{
return|return
name|this
return|;
comment|// Independent of hashtable and can be modified, no need to copy.
block|}
comment|// Implementation of row iterator
annotation|@
name|Override
specifier|public
name|List
argument_list|<
name|Object
argument_list|>
name|first
parameter_list|()
throws|throws
name|HiveException
block|{
comment|// A little strange that we forget the dummy row on read.
if|if
condition|(
name|dummyRow
operator|!=
literal|null
condition|)
block|{
name|List
argument_list|<
name|Object
argument_list|>
name|result
init|=
name|dummyRow
decl_stmt|;
name|dummyRow
operator|=
literal|null
expr_stmt|;
return|return
name|result
return|;
block|}
name|WriteBuffers
operator|.
name|ByteSegmentRef
name|byteSegmentRef
init|=
name|hashMapResult
operator|.
name|first
argument_list|()
decl_stmt|;
if|if
condition|(
name|byteSegmentRef
operator|==
literal|null
condition|)
block|{
return|return
literal|null
return|;
block|}
else|else
block|{
return|return
name|unpack
argument_list|(
name|byteSegmentRef
argument_list|)
return|;
block|}
block|}
annotation|@
name|Override
specifier|public
name|List
argument_list|<
name|Object
argument_list|>
name|next
parameter_list|()
throws|throws
name|HiveException
block|{
name|WriteBuffers
operator|.
name|ByteSegmentRef
name|byteSegmentRef
init|=
name|hashMapResult
operator|.
name|next
argument_list|()
decl_stmt|;
if|if
condition|(
name|byteSegmentRef
operator|==
literal|null
condition|)
block|{
return|return
literal|null
return|;
block|}
else|else
block|{
return|return
name|unpack
argument_list|(
name|byteSegmentRef
argument_list|)
return|;
block|}
block|}
specifier|private
name|List
argument_list|<
name|Object
argument_list|>
name|unpack
parameter_list|(
name|WriteBuffers
operator|.
name|ByteSegmentRef
name|ref
parameter_list|)
throws|throws
name|HiveException
block|{
if|if
condition|(
name|ref
operator|.
name|getLength
argument_list|()
operator|==
literal|0
condition|)
block|{
return|return
name|EMPTY_LIST
return|;
comment|// shortcut, 0 length means no fields
block|}
name|uselessIndirection
operator|.
name|setData
argument_list|(
name|ref
operator|.
name|getBytes
argument_list|()
argument_list|)
expr_stmt|;
name|valueStruct
operator|.
name|init
argument_list|(
name|uselessIndirection
argument_list|,
operator|(
name|int
operator|)
name|ref
operator|.
name|getOffset
argument_list|()
argument_list|,
name|ref
operator|.
name|getLength
argument_list|()
argument_list|)
expr_stmt|;
name|List
argument_list|<
name|Object
argument_list|>
name|result
decl_stmt|;
if|if
condition|(
operator|!
name|needsComplexObjectFixup
condition|)
block|{
comment|// Good performance for common case where small table has no complex objects.
name|result
operator|=
name|valueStruct
operator|.
name|getFieldsAsList
argument_list|()
expr_stmt|;
block|}
else|else
block|{
comment|// Convert the complex LazyBinary objects to standard (Java) objects so downstream
comment|// operators like FileSinkOperator can serialize complex objects in the form they expect
comment|// (i.e. Java objects).
name|result
operator|=
name|MapJoinBytesTableContainer
operator|.
name|getComplexFieldsAsList
argument_list|(
name|valueStruct
argument_list|,
name|complexObjectArrayBuffer
argument_list|,
name|internalValueOi
argument_list|)
expr_stmt|;
block|}
return|return
name|result
return|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|addRow
parameter_list|(
name|List
argument_list|<
name|Object
argument_list|>
name|t
parameter_list|)
block|{
if|if
condition|(
name|dummyRow
operator|!=
literal|null
operator|||
name|hashMapResult
operator|.
name|hasRows
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
literal|"Cannot add rows when not empty"
argument_list|)
throw|;
block|}
name|dummyRow
operator|=
name|t
expr_stmt|;
block|}
comment|// Various unsupported methods.
annotation|@
name|Override
specifier|public
name|void
name|addRow
parameter_list|(
name|Object
index|[]
name|value
parameter_list|)
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
name|this
operator|.
name|getClass
argument_list|()
operator|.
name|getCanonicalName
argument_list|()
operator|+
literal|" cannot add arrays"
argument_list|)
throw|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|write
parameter_list|(
name|MapJoinObjectSerDeContext
name|valueContext
parameter_list|,
name|ObjectOutputStream
name|out
parameter_list|)
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
name|this
operator|.
name|getClass
argument_list|()
operator|.
name|getCanonicalName
argument_list|()
operator|+
literal|" cannot be serialized"
argument_list|)
throw|;
block|}
comment|// Direct access.
specifier|public
name|JoinUtil
operator|.
name|JoinResult
name|setDirect
parameter_list|(
name|byte
index|[]
name|bytes
parameter_list|,
name|int
name|offset
parameter_list|,
name|int
name|length
parameter_list|,
name|BytesBytesMultiHashMap
operator|.
name|Result
name|hashMapResult
parameter_list|)
block|{
name|int
name|keyHash
init|=
name|HashCodeUtil
operator|.
name|murmurHash
argument_list|(
name|bytes
argument_list|,
name|offset
argument_list|,
name|length
argument_list|)
decl_stmt|;
name|partitionId
operator|=
name|keyHash
operator|&
operator|(
name|hashPartitions
operator|.
name|length
operator|-
literal|1
operator|)
expr_stmt|;
if|if
condition|(
operator|!
name|bloom1
operator|.
name|testLong
argument_list|(
name|keyHash
argument_list|)
condition|)
block|{
comment|/*          * if the keyHash is missing in the bloom filter, then the value cannot exist in any of the          * spilled partition - return NOMATCH          */
name|dummyRow
operator|=
literal|null
expr_stmt|;
name|aliasFilter
operator|=
operator|(
name|byte
operator|)
literal|0xff
expr_stmt|;
name|hashMapResult
operator|.
name|forget
argument_list|()
expr_stmt|;
return|return
name|JoinResult
operator|.
name|NOMATCH
return|;
block|}
comment|// If the target hash table is on disk, spill this row to disk as well to be processed later
if|if
condition|(
name|isOnDisk
argument_list|(
name|partitionId
argument_list|)
condition|)
block|{
return|return
name|JoinUtil
operator|.
name|JoinResult
operator|.
name|SPILL
return|;
block|}
else|else
block|{
name|aliasFilter
operator|=
name|hashPartitions
index|[
name|partitionId
index|]
operator|.
name|hashMap
operator|.
name|getValueResult
argument_list|(
name|bytes
argument_list|,
name|offset
argument_list|,
name|length
argument_list|,
name|hashMapResult
argument_list|)
expr_stmt|;
name|dummyRow
operator|=
literal|null
expr_stmt|;
if|if
condition|(
name|hashMapResult
operator|.
name|hasRows
argument_list|()
condition|)
block|{
return|return
name|JoinUtil
operator|.
name|JoinResult
operator|.
name|MATCH
return|;
block|}
else|else
block|{
name|aliasFilter
operator|=
operator|(
name|byte
operator|)
literal|0xff
expr_stmt|;
return|return
name|JoinUtil
operator|.
name|JoinResult
operator|.
name|NOMATCH
return|;
block|}
block|}
block|}
specifier|public
name|int
name|directSpillPartitionId
parameter_list|()
block|{
return|return
name|partitionId
return|;
block|}
block|}
annotation|@
name|Override
specifier|public
name|void
name|dumpMetrics
parameter_list|()
block|{
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|hashPartitions
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|HashPartition
name|hp
init|=
name|hashPartitions
index|[
name|i
index|]
decl_stmt|;
if|if
condition|(
name|hp
operator|.
name|hashMap
operator|!=
literal|null
condition|)
block|{
name|hp
operator|.
name|hashMap
operator|.
name|debugDumpMetrics
argument_list|()
expr_stmt|;
block|}
block|}
block|}
specifier|public
name|void
name|dumpStats
parameter_list|()
block|{
name|int
name|numPartitionsInMem
init|=
literal|0
decl_stmt|;
name|int
name|numPartitionsOnDisk
init|=
literal|0
decl_stmt|;
for|for
control|(
name|HashPartition
name|hp
range|:
name|hashPartitions
control|)
block|{
if|if
condition|(
name|hp
operator|.
name|isHashMapOnDisk
argument_list|()
condition|)
block|{
name|numPartitionsOnDisk
operator|++
expr_stmt|;
block|}
else|else
block|{
name|numPartitionsInMem
operator|++
expr_stmt|;
block|}
block|}
name|LOG
operator|.
name|info
argument_list|(
literal|"In memory partitions have been processed successfully: "
operator|+
name|numPartitionsInMem
operator|+
literal|" partitions in memory have been processed; "
operator|+
name|numPartitionsOnDisk
operator|+
literal|" partitions have been spilled to disk and will be processed next."
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|int
name|size
parameter_list|()
block|{
name|int
name|totalSize
init|=
literal|0
decl_stmt|;
for|for
control|(
name|HashPartition
name|hashPartition
range|:
name|hashPartitions
control|)
block|{
name|totalSize
operator|+=
name|hashPartition
operator|.
name|size
argument_list|()
expr_stmt|;
block|}
return|return
name|totalSize
return|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|setSerde
parameter_list|(
name|MapJoinObjectSerDeContext
name|keyCtx
parameter_list|,
name|MapJoinObjectSerDeContext
name|valCtx
parameter_list|)
throws|throws
name|SerDeException
block|{
name|SerDe
name|keySerde
init|=
name|keyCtx
operator|.
name|getSerDe
argument_list|()
decl_stmt|,
name|valSerde
init|=
name|valCtx
operator|.
name|getSerDe
argument_list|()
decl_stmt|;
if|if
condition|(
name|writeHelper
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Initializing container with "
operator|+
name|keySerde
operator|.
name|getClass
argument_list|()
operator|.
name|getName
argument_list|()
operator|+
literal|" and "
operator|+
name|valSerde
operator|.
name|getClass
argument_list|()
operator|.
name|getName
argument_list|()
argument_list|)
expr_stmt|;
comment|// We assume this hashtable is loaded only when tez is enabled
name|LazyBinaryStructObjectInspector
name|valSoi
init|=
operator|(
name|LazyBinaryStructObjectInspector
operator|)
name|valSerde
operator|.
name|getObjectInspector
argument_list|()
decl_stmt|;
name|writeHelper
operator|=
operator|new
name|MapJoinBytesTableContainer
operator|.
name|LazyBinaryKvWriter
argument_list|(
name|keySerde
argument_list|,
name|valSoi
argument_list|,
name|valCtx
operator|.
name|hasFilterTag
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|internalValueOi
operator|==
literal|null
condition|)
block|{
name|internalValueOi
operator|=
name|valSoi
expr_stmt|;
block|}
if|if
condition|(
name|sortableSortOrders
operator|==
literal|null
condition|)
block|{
name|sortableSortOrders
operator|=
operator|(
operator|(
name|BinarySortableSerDe
operator|)
name|keySerde
operator|)
operator|.
name|getSortOrders
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
name|nullMarkers
operator|==
literal|null
condition|)
block|{
name|nullMarkers
operator|=
operator|(
operator|(
name|BinarySortableSerDe
operator|)
name|keySerde
operator|)
operator|.
name|getNullMarkers
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
name|notNullMarkers
operator|==
literal|null
condition|)
block|{
name|notNullMarkers
operator|=
operator|(
operator|(
name|BinarySortableSerDe
operator|)
name|keySerde
operator|)
operator|.
name|getNotNullMarkers
argument_list|()
expr_stmt|;
block|}
block|}
block|}
block|}
end_class

end_unit

