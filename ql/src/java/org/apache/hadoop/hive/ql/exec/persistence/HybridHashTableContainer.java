begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/**  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|persistence
package|;
end_package

begin_import
import|import
name|com
operator|.
name|esotericsoftware
operator|.
name|kryo
operator|.
name|Kryo
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|Log
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|LogFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configuration
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|ExprNodeEvaluator
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|JoinUtil
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|Utilities
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|vector
operator|.
name|VectorHashKeyWrapper
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|vector
operator|.
name|VectorHashKeyWrapperBatch
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|vector
operator|.
name|expressions
operator|.
name|VectorExpressionWriter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|HiveKey
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|HiveException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|SerDe
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|SerDeException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|WriteBuffers
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|ByteStream
operator|.
name|Output
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|binarysortable
operator|.
name|BinarySortableSerDe
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|lazy
operator|.
name|ByteArrayRef
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|lazybinary
operator|.
name|objectinspector
operator|.
name|LazyBinaryStructObjectInspector
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|lazybinary
operator|.
name|LazyBinaryFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|lazybinary
operator|.
name|LazyBinaryStruct
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|objectinspector
operator|.
name|ObjectInspector
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|BytesWritable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|Writable
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|InputStream
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|ObjectOutputStream
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|OutputStream
import|;
end_import

begin_import
import|import
name|java
operator|.
name|nio
operator|.
name|file
operator|.
name|Files
import|;
end_import

begin_import
import|import
name|java
operator|.
name|nio
operator|.
name|file
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|*
import|;
end_import

begin_comment
comment|/**  * Hash table container that can have many partitions -- each partition has its own hashmap,  * as well as row container for small table and big table.  *  * The purpose is to distribute rows into multiple partitions so that when the entire small table  * cannot fit into memory, we are still able to perform hash join, by processing them recursively.  *  * Partitions that can fit in memory will be processed first, and then every spilled partition will  * be restored and processed one by one.  */
end_comment

begin_class
specifier|public
class|class
name|HybridHashTableContainer
implements|implements
name|MapJoinTableContainer
block|{
specifier|private
specifier|static
specifier|final
name|Log
name|LOG
init|=
name|LogFactory
operator|.
name|getLog
argument_list|(
name|HybridHashTableContainer
operator|.
name|class
argument_list|)
decl_stmt|;
specifier|private
name|HashPartition
index|[]
name|hashPartitions
decl_stmt|;
comment|// an array of partitions holding the triplets
specifier|private
name|int
name|totalInMemRowCount
init|=
literal|0
decl_stmt|;
comment|// total number of small table rows in memory
specifier|private
name|long
name|memoryThreshold
decl_stmt|;
comment|// the max memory limit allocated
specifier|private
name|long
name|tableRowSize
decl_stmt|;
comment|// row size of the small table
specifier|private
name|boolean
name|isSpilled
decl_stmt|;
comment|// whether there's any spilled partition
specifier|private
name|int
name|toSpillPartitionId
decl_stmt|;
comment|// the partition into which to spill the big table row;
comment|// This may change after every setMapJoinKey call
specifier|private
name|int
name|numPartitionsSpilled
decl_stmt|;
comment|// number of spilled partitions
specifier|private
name|boolean
name|lastPartitionInMem
decl_stmt|;
comment|// only one (last one) partition is left in memory
specifier|private
name|int
name|memoryCheckFrequency
decl_stmt|;
comment|// how often (# of rows apart) to check if memory is full
comment|/** The OI used to deserialize values. We never deserialize keys. */
specifier|private
name|LazyBinaryStructObjectInspector
name|internalValueOi
decl_stmt|;
specifier|private
name|boolean
index|[]
name|sortableSortOrders
decl_stmt|;
specifier|private
name|MapJoinBytesTableContainer
operator|.
name|KeyValueHelper
name|writeHelper
decl_stmt|;
specifier|private
specifier|final
name|List
argument_list|<
name|Object
argument_list|>
name|EMPTY_LIST
init|=
operator|new
name|ArrayList
argument_list|<
name|Object
argument_list|>
argument_list|(
literal|0
argument_list|)
decl_stmt|;
comment|/**    * This class encapsulates the triplet together since they are closely related to each other    * The triplet: hashmap (either in memory or on disk), small table container, big table container    */
specifier|public
specifier|static
class|class
name|HashPartition
block|{
name|BytesBytesMultiHashMap
name|hashMap
decl_stmt|;
comment|// In memory hashMap
name|KeyValueContainer
name|sidefileKVContainer
decl_stmt|;
comment|// Stores small table key/value pairs
name|ObjectContainer
name|matchfileObjContainer
decl_stmt|;
comment|// Stores big table rows
name|Path
name|hashMapLocalPath
decl_stmt|;
comment|// Local file system path for spilled hashMap
name|boolean
name|hashMapOnDisk
decl_stmt|;
comment|// Status of hashMap. true: on disk, false: in memory
name|boolean
name|hashMapSpilledOnCreation
decl_stmt|;
comment|// When there's no enough memory, cannot create hashMap
name|int
name|threshold
decl_stmt|;
comment|// Used to create an empty BytesBytesMultiHashMap
name|float
name|loadFactor
decl_stmt|;
comment|// Same as above
name|int
name|wbSize
decl_stmt|;
comment|// Same as above
comment|/* It may happen that there's not enough memory to instantiate a hashmap for the partition.      * In that case, we don't create the hashmap, but pretend the hashmap is directly "spilled".      */
specifier|public
name|HashPartition
parameter_list|(
name|int
name|threshold
parameter_list|,
name|float
name|loadFactor
parameter_list|,
name|int
name|wbSize
parameter_list|,
name|long
name|memUsage
parameter_list|,
name|boolean
name|createHashMap
parameter_list|)
block|{
if|if
condition|(
name|createHashMap
condition|)
block|{
name|hashMap
operator|=
operator|new
name|BytesBytesMultiHashMap
argument_list|(
name|threshold
argument_list|,
name|loadFactor
argument_list|,
name|wbSize
argument_list|,
name|memUsage
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|hashMapSpilledOnCreation
operator|=
literal|true
expr_stmt|;
name|hashMapOnDisk
operator|=
literal|true
expr_stmt|;
block|}
name|this
operator|.
name|threshold
operator|=
name|threshold
expr_stmt|;
name|this
operator|.
name|loadFactor
operator|=
name|loadFactor
expr_stmt|;
name|this
operator|.
name|wbSize
operator|=
name|wbSize
expr_stmt|;
block|}
comment|/* Get the in memory hashmap */
specifier|public
name|BytesBytesMultiHashMap
name|getHashMapFromMemory
parameter_list|()
block|{
return|return
name|hashMap
return|;
block|}
comment|/* Restore the hashmap from disk by deserializing it.      * Currently Kryo is used for this purpose.      */
specifier|public
name|BytesBytesMultiHashMap
name|getHashMapFromDisk
parameter_list|()
throws|throws
name|IOException
throws|,
name|ClassNotFoundException
block|{
if|if
condition|(
name|hashMapSpilledOnCreation
condition|)
block|{
return|return
operator|new
name|BytesBytesMultiHashMap
argument_list|(
name|threshold
argument_list|,
name|loadFactor
argument_list|,
name|wbSize
argument_list|,
operator|-
literal|1
argument_list|)
return|;
block|}
else|else
block|{
name|InputStream
name|inputStream
init|=
name|Files
operator|.
name|newInputStream
argument_list|(
name|hashMapLocalPath
argument_list|)
decl_stmt|;
name|com
operator|.
name|esotericsoftware
operator|.
name|kryo
operator|.
name|io
operator|.
name|Input
name|input
init|=
operator|new
name|com
operator|.
name|esotericsoftware
operator|.
name|kryo
operator|.
name|io
operator|.
name|Input
argument_list|(
name|inputStream
argument_list|)
decl_stmt|;
name|Kryo
name|kryo
init|=
name|Utilities
operator|.
name|runtimeSerializationKryo
operator|.
name|get
argument_list|()
decl_stmt|;
name|BytesBytesMultiHashMap
name|restoredHashMap
init|=
name|kryo
operator|.
name|readObject
argument_list|(
name|input
argument_list|,
name|BytesBytesMultiHashMap
operator|.
name|class
argument_list|)
decl_stmt|;
name|input
operator|.
name|close
argument_list|()
expr_stmt|;
name|inputStream
operator|.
name|close
argument_list|()
expr_stmt|;
name|Files
operator|.
name|delete
argument_list|(
name|hashMapLocalPath
argument_list|)
expr_stmt|;
return|return
name|restoredHashMap
return|;
block|}
block|}
comment|/* Get the small table key/value container */
specifier|public
name|KeyValueContainer
name|getSidefileKVContainer
parameter_list|()
block|{
if|if
condition|(
name|sidefileKVContainer
operator|==
literal|null
condition|)
block|{
name|sidefileKVContainer
operator|=
operator|new
name|KeyValueContainer
argument_list|()
expr_stmt|;
block|}
return|return
name|sidefileKVContainer
return|;
block|}
comment|/* Get the big table row container */
specifier|public
name|ObjectContainer
name|getMatchfileObjContainer
parameter_list|()
block|{
if|if
condition|(
name|matchfileObjContainer
operator|==
literal|null
condition|)
block|{
name|matchfileObjContainer
operator|=
operator|new
name|ObjectContainer
argument_list|()
expr_stmt|;
block|}
return|return
name|matchfileObjContainer
return|;
block|}
comment|/* Check if hashmap is on disk or in memory */
specifier|public
name|boolean
name|isHashMapOnDisk
parameter_list|()
block|{
return|return
name|hashMapOnDisk
return|;
block|}
block|}
specifier|public
name|HybridHashTableContainer
parameter_list|(
name|Configuration
name|hconf
parameter_list|,
name|long
name|keyCount
parameter_list|,
name|long
name|memUsage
parameter_list|,
name|long
name|tableSize
parameter_list|)
throws|throws
name|SerDeException
block|{
name|this
argument_list|(
name|HiveConf
operator|.
name|getIntVar
argument_list|(
name|hconf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEHASHTABLETHRESHOLD
argument_list|)
argument_list|,
name|HiveConf
operator|.
name|getFloatVar
argument_list|(
name|hconf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEHASHTABLELOADFACTOR
argument_list|)
argument_list|,
name|HiveConf
operator|.
name|getIntVar
argument_list|(
name|hconf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEHASHTABLEWBSIZE
argument_list|)
argument_list|,
name|HiveConf
operator|.
name|getLongVar
argument_list|(
name|hconf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVECONVERTJOINNOCONDITIONALTASKTHRESHOLD
argument_list|)
argument_list|,
name|HiveConf
operator|.
name|getIntVar
argument_list|(
name|hconf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEHYBRIDGRACEHASHJOINMEMCHECKFREQ
argument_list|)
argument_list|,
name|tableSize
argument_list|,
name|keyCount
argument_list|,
name|memUsage
argument_list|)
expr_stmt|;
block|}
specifier|private
name|HybridHashTableContainer
parameter_list|(
name|int
name|threshold
parameter_list|,
name|float
name|loadFactor
parameter_list|,
name|int
name|wbSize
parameter_list|,
name|long
name|noConditionalTaskThreshold
parameter_list|,
name|int
name|memCheckFreq
parameter_list|,
name|long
name|tableSize
parameter_list|,
name|long
name|keyCount
parameter_list|,
name|long
name|memUsage
parameter_list|)
throws|throws
name|SerDeException
block|{
name|memoryThreshold
operator|=
name|noConditionalTaskThreshold
expr_stmt|;
name|tableRowSize
operator|=
name|tableSize
operator|/
name|keyCount
expr_stmt|;
name|memoryCheckFrequency
operator|=
name|memCheckFreq
expr_stmt|;
name|int
name|numPartitions
init|=
name|calcNumPartitions
argument_list|(
name|tableSize
argument_list|,
name|wbSize
argument_list|)
decl_stmt|;
comment|// estimate # of partitions to create
name|hashPartitions
operator|=
operator|new
name|HashPartition
index|[
name|numPartitions
index|]
expr_stmt|;
name|int
name|numPartitionsSpilledOnCreation
init|=
literal|0
decl_stmt|;
name|long
name|memoryAllocated
init|=
literal|0
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|numPartitions
condition|;
name|i
operator|++
control|)
block|{
if|if
condition|(
name|i
operator|==
literal|0
condition|)
block|{
comment|// We unconditionally create a hashmap for the first hash partition
name|hashPartitions
index|[
name|i
index|]
operator|=
operator|new
name|HashPartition
argument_list|(
name|threshold
argument_list|,
name|loadFactor
argument_list|,
name|wbSize
argument_list|,
name|memUsage
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|hashPartitions
index|[
name|i
index|]
operator|=
operator|new
name|HashPartition
argument_list|(
name|threshold
argument_list|,
name|loadFactor
argument_list|,
name|wbSize
argument_list|,
name|memUsage
argument_list|,
name|memoryAllocated
operator|+
name|wbSize
operator|<
name|memoryThreshold
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|isHashMapSpilledOnCreation
argument_list|(
name|i
argument_list|)
condition|)
block|{
name|numPartitionsSpilledOnCreation
operator|++
expr_stmt|;
name|numPartitionsSpilled
operator|++
expr_stmt|;
block|}
else|else
block|{
name|memoryAllocated
operator|+=
name|hashPartitions
index|[
name|i
index|]
operator|.
name|hashMap
operator|.
name|memorySize
argument_list|()
expr_stmt|;
block|}
block|}
assert|assert
name|numPartitionsSpilledOnCreation
operator|!=
name|numPartitions
operator|:
literal|"All partitions are directly spilled!"
operator|+
literal|" It is not supported now."
assert|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Number of partitions created: "
operator|+
name|numPartitions
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Number of partitions spilled directly to disk on creation: "
operator|+
name|numPartitionsSpilledOnCreation
argument_list|)
expr_stmt|;
block|}
specifier|public
name|MapJoinBytesTableContainer
operator|.
name|KeyValueHelper
name|getWriteHelper
parameter_list|()
block|{
return|return
name|writeHelper
return|;
block|}
specifier|public
name|HashPartition
index|[]
name|getHashPartitions
parameter_list|()
block|{
return|return
name|hashPartitions
return|;
block|}
specifier|public
name|long
name|getMemoryThreshold
parameter_list|()
block|{
return|return
name|memoryThreshold
return|;
block|}
specifier|public
name|LazyBinaryStructObjectInspector
name|getInternalValueOi
parameter_list|()
block|{
return|return
name|internalValueOi
return|;
block|}
specifier|public
name|boolean
index|[]
name|getSortableSortOrders
parameter_list|()
block|{
return|return
name|sortableSortOrders
return|;
block|}
comment|/* For a given row, put it into proper partition based on its hash value.    * When memory threshold is reached, the biggest hash table in memory will be spilled to disk.    * If the hash table of a specific partition is already on disk, all later rows will be put into    * a row container for later use.    */
annotation|@
name|SuppressWarnings
argument_list|(
literal|"deprecation"
argument_list|)
annotation|@
name|Override
specifier|public
name|MapJoinKey
name|putRow
parameter_list|(
name|MapJoinObjectSerDeContext
name|keyContext
parameter_list|,
name|Writable
name|currentKey
parameter_list|,
name|MapJoinObjectSerDeContext
name|valueContext
parameter_list|,
name|Writable
name|currentValue
parameter_list|)
throws|throws
name|SerDeException
throws|,
name|HiveException
throws|,
name|IOException
block|{
name|SerDe
name|keySerde
init|=
name|keyContext
operator|.
name|getSerDe
argument_list|()
decl_stmt|,
name|valSerde
init|=
name|valueContext
operator|.
name|getSerDe
argument_list|()
decl_stmt|;
if|if
condition|(
name|writeHelper
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Initializing container with "
operator|+
name|keySerde
operator|.
name|getClass
argument_list|()
operator|.
name|getName
argument_list|()
operator|+
literal|" and "
operator|+
name|valSerde
operator|.
name|getClass
argument_list|()
operator|.
name|getName
argument_list|()
argument_list|)
expr_stmt|;
comment|// We assume this hashtable is loaded only when tez is enabled
name|LazyBinaryStructObjectInspector
name|valSoi
init|=
operator|(
name|LazyBinaryStructObjectInspector
operator|)
name|valSerde
operator|.
name|getObjectInspector
argument_list|()
decl_stmt|;
name|writeHelper
operator|=
operator|new
name|MapJoinBytesTableContainer
operator|.
name|LazyBinaryKvWriter
argument_list|(
name|keySerde
argument_list|,
name|valSoi
argument_list|,
name|valueContext
operator|.
name|hasFilterTag
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|internalValueOi
operator|==
literal|null
condition|)
block|{
name|internalValueOi
operator|=
name|valSoi
expr_stmt|;
block|}
if|if
condition|(
name|sortableSortOrders
operator|==
literal|null
condition|)
block|{
name|sortableSortOrders
operator|=
operator|(
operator|(
name|BinarySortableSerDe
operator|)
name|keySerde
operator|)
operator|.
name|getSortOrders
argument_list|()
expr_stmt|;
block|}
block|}
name|writeHelper
operator|.
name|setKeyValue
argument_list|(
name|currentKey
argument_list|,
name|currentValue
argument_list|)
expr_stmt|;
comment|// Next, put row into corresponding hash partition
name|int
name|keyHash
init|=
name|writeHelper
operator|.
name|getHashFromKey
argument_list|()
decl_stmt|;
name|int
name|partitionId
init|=
name|keyHash
operator|&
operator|(
name|hashPartitions
operator|.
name|length
operator|-
literal|1
operator|)
decl_stmt|;
name|HashPartition
name|hashPartition
init|=
name|hashPartitions
index|[
name|partitionId
index|]
decl_stmt|;
if|if
condition|(
name|isOnDisk
argument_list|(
name|partitionId
argument_list|)
operator|||
name|isHashMapSpilledOnCreation
argument_list|(
name|partitionId
argument_list|)
condition|)
block|{
name|KeyValueContainer
name|kvContainer
init|=
name|hashPartition
operator|.
name|getSidefileKVContainer
argument_list|()
decl_stmt|;
name|kvContainer
operator|.
name|add
argument_list|(
operator|(
name|HiveKey
operator|)
name|currentKey
argument_list|,
operator|(
name|BytesWritable
operator|)
name|currentValue
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|hashPartition
operator|.
name|hashMap
operator|.
name|put
argument_list|(
name|writeHelper
argument_list|,
name|keyHash
argument_list|)
expr_stmt|;
comment|// Pass along hashcode to avoid recalculation
name|totalInMemRowCount
operator|++
expr_stmt|;
if|if
condition|(
operator|(
name|totalInMemRowCount
operator|&
operator|(
name|this
operator|.
name|memoryCheckFrequency
operator|-
literal|1
operator|)
operator|)
operator|==
literal|0
operator|&&
comment|// check periodically
operator|!
name|lastPartitionInMem
condition|)
block|{
comment|// If this is the only partition in memory, proceed without check
if|if
condition|(
name|isMemoryFull
argument_list|()
condition|)
block|{
if|if
condition|(
operator|(
name|numPartitionsSpilled
operator|==
name|hashPartitions
operator|.
name|length
operator|-
literal|1
operator|)
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"This LAST partition in memory won't be spilled!"
argument_list|)
expr_stmt|;
name|lastPartitionInMem
operator|=
literal|true
expr_stmt|;
block|}
else|else
block|{
name|int
name|biggest
init|=
name|biggestPartition
argument_list|()
decl_stmt|;
name|numPartitionsSpilled
operator|++
expr_stmt|;
name|spillPartition
argument_list|(
name|biggest
argument_list|)
expr_stmt|;
name|this
operator|.
name|setSpill
argument_list|(
literal|true
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
return|return
literal|null
return|;
comment|// there's no key to return
block|}
comment|/**    * Check if the hash table of a specified partition is on disk (or "spilled" on creation)    * @param partitionId partition number    * @return true if on disk, false if in memory    */
specifier|public
name|boolean
name|isOnDisk
parameter_list|(
name|int
name|partitionId
parameter_list|)
block|{
return|return
name|hashPartitions
index|[
name|partitionId
index|]
operator|.
name|hashMapOnDisk
return|;
block|}
comment|/**    * Check if the hash table of a specified partition has been "spilled" to disk when it was created.    * In fact, in other words, check if a hashmap does exist or not.    * @param partitionId hashMap ID    * @return true if it was not created at all, false if there is a hash table existing there    */
specifier|public
name|boolean
name|isHashMapSpilledOnCreation
parameter_list|(
name|int
name|partitionId
parameter_list|)
block|{
return|return
name|hashPartitions
index|[
name|partitionId
index|]
operator|.
name|hashMapSpilledOnCreation
return|;
block|}
comment|/**    * Check if the memory threshold is reached    * @return true if memory is full, false if not    */
specifier|private
name|boolean
name|isMemoryFull
parameter_list|()
block|{
name|long
name|size
init|=
literal|0
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|hashPartitions
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
if|if
condition|(
operator|!
name|isOnDisk
argument_list|(
name|i
argument_list|)
condition|)
block|{
name|size
operator|+=
name|hashPartitions
index|[
name|i
index|]
operator|.
name|hashMap
operator|.
name|memorySize
argument_list|()
expr_stmt|;
block|}
block|}
return|return
name|size
operator|>=
name|memoryThreshold
return|;
block|}
comment|/**    * Find the partition with biggest hashtable in memory at this moment    * @return the biggest partition number    */
specifier|private
name|int
name|biggestPartition
parameter_list|()
block|{
name|int
name|res
init|=
literal|0
decl_stmt|;
name|int
name|maxSize
init|=
literal|0
decl_stmt|;
comment|// If a partition has been spilled to disk, its size will be 0, i.e. it won't be picked
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|hashPartitions
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|int
name|size
decl_stmt|;
if|if
condition|(
name|isOnDisk
argument_list|(
name|i
argument_list|)
condition|)
block|{
continue|continue;
block|}
else|else
block|{
name|size
operator|=
name|hashPartitions
index|[
name|i
index|]
operator|.
name|hashMap
operator|.
name|getNumValues
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
name|size
operator|>
name|maxSize
condition|)
block|{
name|maxSize
operator|=
name|size
expr_stmt|;
name|res
operator|=
name|i
expr_stmt|;
block|}
block|}
return|return
name|res
return|;
block|}
comment|/**    * Move the hashtable of a specified partition from memory into local file system    * @param partitionId the hashtable to be moved    */
specifier|private
name|void
name|spillPartition
parameter_list|(
name|int
name|partitionId
parameter_list|)
throws|throws
name|IOException
block|{
name|HashPartition
name|partition
init|=
name|hashPartitions
index|[
name|partitionId
index|]
decl_stmt|;
name|int
name|inMemRowCount
init|=
name|partition
operator|.
name|hashMap
operator|.
name|getNumValues
argument_list|()
decl_stmt|;
name|long
name|inMemSize
init|=
name|partition
operator|.
name|hashMap
operator|.
name|memorySize
argument_list|()
decl_stmt|;
name|Path
name|path
init|=
name|Files
operator|.
name|createTempFile
argument_list|(
literal|"partition-"
operator|+
name|partitionId
operator|+
literal|"-"
argument_list|,
literal|null
argument_list|)
decl_stmt|;
name|OutputStream
name|outputStream
init|=
name|Files
operator|.
name|newOutputStream
argument_list|(
name|path
argument_list|)
decl_stmt|;
name|com
operator|.
name|esotericsoftware
operator|.
name|kryo
operator|.
name|io
operator|.
name|Output
name|output
init|=
operator|new
name|com
operator|.
name|esotericsoftware
operator|.
name|kryo
operator|.
name|io
operator|.
name|Output
argument_list|(
name|outputStream
argument_list|)
decl_stmt|;
name|Kryo
name|kryo
init|=
name|Utilities
operator|.
name|runtimeSerializationKryo
operator|.
name|get
argument_list|()
decl_stmt|;
name|kryo
operator|.
name|writeObject
argument_list|(
name|output
argument_list|,
name|partition
operator|.
name|hashMap
argument_list|)
expr_stmt|;
comment|// use Kryo to serialize hashmap
name|output
operator|.
name|close
argument_list|()
expr_stmt|;
name|outputStream
operator|.
name|close
argument_list|()
expr_stmt|;
name|partition
operator|.
name|hashMapLocalPath
operator|=
name|path
expr_stmt|;
name|partition
operator|.
name|hashMapOnDisk
operator|=
literal|true
expr_stmt|;
name|long
name|size
init|=
literal|0
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|hashPartitions
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
if|if
condition|(
operator|!
name|isOnDisk
argument_list|(
name|i
argument_list|)
condition|)
block|{
name|size
operator|+=
name|hashPartitions
index|[
name|i
index|]
operator|.
name|hashMap
operator|.
name|memorySize
argument_list|()
expr_stmt|;
block|}
block|}
name|LOG
operator|.
name|info
argument_list|(
literal|"Spilling hash partition "
operator|+
name|partitionId
operator|+
literal|" (Rows: "
operator|+
name|inMemRowCount
operator|+
literal|", Mem size: "
operator|+
name|inMemSize
operator|+
literal|"): "
operator|+
name|path
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Memory usage before spilling: "
operator|+
name|size
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Memory usage after spilling: "
operator|+
operator|(
name|size
operator|-
name|inMemSize
operator|)
argument_list|)
expr_stmt|;
name|totalInMemRowCount
operator|-=
name|inMemRowCount
expr_stmt|;
name|partition
operator|.
name|hashMap
operator|.
name|clear
argument_list|()
expr_stmt|;
block|}
comment|/**    * Calculate how many partitions are needed. This is an estimation.    * @param dataSize total data size for the table    * @param wbSize write buffer size    * @return number of partitions needed    */
specifier|private
name|int
name|calcNumPartitions
parameter_list|(
name|long
name|dataSize
parameter_list|,
name|int
name|wbSize
parameter_list|)
block|{
if|if
condition|(
name|memoryThreshold
operator|<
name|wbSize
condition|)
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
literal|"Available memory is less than hashtable writebuffer size!"
operator|+
literal|" Try increasing hive.auto.convert.join.noconditionaltask.size."
argument_list|)
throw|;
block|}
name|int
name|lowerLimit
init|=
literal|2
decl_stmt|;
name|int
name|numPartitions
init|=
operator|(
name|int
operator|)
name|Math
operator|.
name|ceil
argument_list|(
name|dataSize
operator|/
name|wbSize
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Total available memory: "
operator|+
name|memoryThreshold
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Estimated small table size: "
operator|+
name|dataSize
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Write buffer size: "
operator|+
name|wbSize
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Initial number of partitions: "
operator|+
name|numPartitions
argument_list|)
expr_stmt|;
if|if
condition|(
name|numPartitions
operator|<
name|lowerLimit
condition|)
block|{
return|return
name|lowerLimit
return|;
block|}
elseif|else
if|if
condition|(
name|dataSize
operator|>
name|memoryThreshold
condition|)
block|{
name|numPartitions
operator|=
call|(
name|int
call|)
argument_list|(
name|memoryThreshold
operator|/
name|wbSize
argument_list|)
expr_stmt|;
block|}
comment|// Make sure numPartitions is power of 2, to make N& (M - 1) easy when calculating partition No.
name|numPartitions
operator|=
operator|(
name|Long
operator|.
name|bitCount
argument_list|(
name|numPartitions
argument_list|)
operator|==
literal|1
operator|)
condition|?
name|numPartitions
else|:
name|Integer
operator|.
name|highestOneBit
argument_list|(
name|numPartitions
argument_list|)
operator|<<
literal|1
expr_stmt|;
while|while
condition|(
name|dataSize
operator|/
name|numPartitions
operator|>
name|memoryThreshold
condition|)
block|{
name|numPartitions
operator|*=
literal|2
expr_stmt|;
block|}
name|LOG
operator|.
name|info
argument_list|(
literal|"Number of hash partitions to be created: "
operator|+
name|numPartitions
argument_list|)
expr_stmt|;
return|return
name|numPartitions
return|;
block|}
comment|/* Get total number of rows from all in memory partitions */
specifier|public
name|int
name|getTotalInMemRowCount
parameter_list|()
block|{
return|return
name|totalInMemRowCount
return|;
block|}
comment|/* Set total number of rows from all in memory partitions */
specifier|public
name|void
name|setTotalInMemRowCount
parameter_list|(
name|int
name|totalInMemRowCount
parameter_list|)
block|{
name|this
operator|.
name|totalInMemRowCount
operator|=
name|totalInMemRowCount
expr_stmt|;
block|}
comment|/* Get row size of small table */
specifier|public
name|long
name|getTableRowSize
parameter_list|()
block|{
return|return
name|tableRowSize
return|;
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|hasSpill
parameter_list|()
block|{
return|return
name|isSpilled
return|;
block|}
specifier|public
name|void
name|setSpill
parameter_list|(
name|boolean
name|isSpilled
parameter_list|)
block|{
name|this
operator|.
name|isSpilled
operator|=
name|isSpilled
expr_stmt|;
block|}
comment|/**    * Gets the partition Id into which to spill the big table row    * @return partition Id    */
specifier|public
name|int
name|getToSpillPartitionId
parameter_list|()
block|{
return|return
name|toSpillPartitionId
return|;
block|}
comment|/* Clean up in memory hashtables */
annotation|@
name|Override
specifier|public
name|void
name|clear
parameter_list|()
block|{
for|for
control|(
name|HashPartition
name|hp
range|:
name|hashPartitions
control|)
block|{
if|if
condition|(
name|hp
operator|.
name|hashMap
operator|!=
literal|null
condition|)
block|{
name|hp
operator|.
name|hashMap
operator|.
name|clear
argument_list|()
expr_stmt|;
block|}
block|}
block|}
annotation|@
name|Override
specifier|public
name|MapJoinKey
name|getAnyKey
parameter_list|()
block|{
return|return
literal|null
return|;
comment|// This table has no keys.
block|}
annotation|@
name|Override
specifier|public
name|ReusableGetAdaptor
name|createGetter
parameter_list|(
name|MapJoinKey
name|keyTypeFromLoader
parameter_list|)
block|{
if|if
condition|(
name|keyTypeFromLoader
operator|!=
literal|null
condition|)
block|{
throw|throw
operator|new
name|AssertionError
argument_list|(
literal|"No key expected from loader but got "
operator|+
name|keyTypeFromLoader
argument_list|)
throw|;
block|}
return|return
operator|new
name|GetAdaptor
argument_list|()
return|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|seal
parameter_list|()
block|{
for|for
control|(
name|HashPartition
name|hp
range|:
name|hashPartitions
control|)
block|{
comment|// Only seal those partitions that haven't been spilled and cleared,
comment|// because once a hashMap is cleared, it will become unusable
if|if
condition|(
name|hp
operator|.
name|hashMap
operator|!=
literal|null
operator|&&
name|hp
operator|.
name|hashMap
operator|.
name|size
argument_list|()
operator|!=
literal|0
condition|)
block|{
name|hp
operator|.
name|hashMap
operator|.
name|seal
argument_list|()
expr_stmt|;
block|}
block|}
block|}
comment|/** Implementation of ReusableGetAdaptor that has Output for key serialization; row    * container is also created once and reused for every row. */
specifier|private
class|class
name|GetAdaptor
implements|implements
name|ReusableGetAdaptor
block|{
specifier|private
name|Object
index|[]
name|currentKey
decl_stmt|;
specifier|private
name|boolean
index|[]
name|nulls
decl_stmt|;
specifier|private
name|List
argument_list|<
name|ObjectInspector
argument_list|>
name|vectorKeyOIs
decl_stmt|;
specifier|private
specifier|final
name|ReusableRowContainer
name|currentValue
decl_stmt|;
specifier|private
specifier|final
name|Output
name|output
decl_stmt|;
specifier|public
name|GetAdaptor
parameter_list|()
block|{
name|currentValue
operator|=
operator|new
name|ReusableRowContainer
argument_list|()
expr_stmt|;
name|output
operator|=
operator|new
name|Output
argument_list|()
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|JoinUtil
operator|.
name|JoinResult
name|setFromVector
parameter_list|(
name|VectorHashKeyWrapper
name|kw
parameter_list|,
name|VectorExpressionWriter
index|[]
name|keyOutputWriters
parameter_list|,
name|VectorHashKeyWrapperBatch
name|keyWrapperBatch
parameter_list|)
throws|throws
name|HiveException
block|{
if|if
condition|(
name|nulls
operator|==
literal|null
condition|)
block|{
name|nulls
operator|=
operator|new
name|boolean
index|[
name|keyOutputWriters
operator|.
name|length
index|]
expr_stmt|;
name|currentKey
operator|=
operator|new
name|Object
index|[
name|keyOutputWriters
operator|.
name|length
index|]
expr_stmt|;
name|vectorKeyOIs
operator|=
operator|new
name|ArrayList
argument_list|<
name|ObjectInspector
argument_list|>
argument_list|()
expr_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|keyOutputWriters
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|vectorKeyOIs
operator|.
name|add
argument_list|(
name|keyOutputWriters
index|[
name|i
index|]
operator|.
name|getObjectInspector
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
assert|assert
name|nulls
operator|.
name|length
operator|==
name|keyOutputWriters
operator|.
name|length
assert|;
block|}
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|keyOutputWriters
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|currentKey
index|[
name|i
index|]
operator|=
name|keyWrapperBatch
operator|.
name|getWritableKeyValue
argument_list|(
name|kw
argument_list|,
name|i
argument_list|,
name|keyOutputWriters
index|[
name|i
index|]
argument_list|)
expr_stmt|;
name|nulls
index|[
name|i
index|]
operator|=
name|currentKey
index|[
name|i
index|]
operator|==
literal|null
expr_stmt|;
block|}
return|return
name|currentValue
operator|.
name|setFromOutput
argument_list|(
name|MapJoinKey
operator|.
name|serializeRow
argument_list|(
name|output
argument_list|,
name|currentKey
argument_list|,
name|vectorKeyOIs
argument_list|,
name|sortableSortOrders
argument_list|)
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|public
name|JoinUtil
operator|.
name|JoinResult
name|setFromRow
parameter_list|(
name|Object
name|row
parameter_list|,
name|List
argument_list|<
name|ExprNodeEvaluator
argument_list|>
name|fields
parameter_list|,
name|List
argument_list|<
name|ObjectInspector
argument_list|>
name|ois
parameter_list|)
throws|throws
name|HiveException
block|{
if|if
condition|(
name|nulls
operator|==
literal|null
condition|)
block|{
name|nulls
operator|=
operator|new
name|boolean
index|[
name|fields
operator|.
name|size
argument_list|()
index|]
expr_stmt|;
name|currentKey
operator|=
operator|new
name|Object
index|[
name|fields
operator|.
name|size
argument_list|()
index|]
expr_stmt|;
block|}
for|for
control|(
name|int
name|keyIndex
init|=
literal|0
init|;
name|keyIndex
operator|<
name|fields
operator|.
name|size
argument_list|()
condition|;
operator|++
name|keyIndex
control|)
block|{
name|currentKey
index|[
name|keyIndex
index|]
operator|=
name|fields
operator|.
name|get
argument_list|(
name|keyIndex
argument_list|)
operator|.
name|evaluate
argument_list|(
name|row
argument_list|)
expr_stmt|;
name|nulls
index|[
name|keyIndex
index|]
operator|=
name|currentKey
index|[
name|keyIndex
index|]
operator|==
literal|null
expr_stmt|;
block|}
return|return
name|currentValue
operator|.
name|setFromOutput
argument_list|(
name|MapJoinKey
operator|.
name|serializeRow
argument_list|(
name|output
argument_list|,
name|currentKey
argument_list|,
name|ois
argument_list|,
name|sortableSortOrders
argument_list|)
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|public
name|JoinUtil
operator|.
name|JoinResult
name|setFromOther
parameter_list|(
name|ReusableGetAdaptor
name|other
parameter_list|)
throws|throws
name|HiveException
block|{
assert|assert
name|other
operator|instanceof
name|GetAdaptor
assert|;
name|GetAdaptor
name|other2
init|=
operator|(
name|GetAdaptor
operator|)
name|other
decl_stmt|;
name|nulls
operator|=
name|other2
operator|.
name|nulls
expr_stmt|;
name|currentKey
operator|=
name|other2
operator|.
name|currentKey
expr_stmt|;
return|return
name|currentValue
operator|.
name|setFromOutput
argument_list|(
name|other2
operator|.
name|output
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|hasAnyNulls
parameter_list|(
name|int
name|fieldCount
parameter_list|,
name|boolean
index|[]
name|nullsafes
parameter_list|)
block|{
if|if
condition|(
name|nulls
operator|==
literal|null
operator|||
name|nulls
operator|.
name|length
operator|==
literal|0
condition|)
return|return
literal|false
return|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|nulls
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
if|if
condition|(
name|nulls
index|[
name|i
index|]
operator|&&
operator|(
name|nullsafes
operator|==
literal|null
operator|||
operator|!
name|nullsafes
index|[
name|i
index|]
operator|)
condition|)
block|{
return|return
literal|true
return|;
block|}
block|}
return|return
literal|false
return|;
block|}
annotation|@
name|Override
specifier|public
name|MapJoinRowContainer
name|getCurrentRows
parameter_list|()
block|{
return|return
name|currentValue
operator|.
name|isEmpty
argument_list|()
condition|?
literal|null
else|:
name|currentValue
return|;
block|}
annotation|@
name|Override
specifier|public
name|Object
index|[]
name|getCurrentKey
parameter_list|()
block|{
return|return
name|currentKey
return|;
block|}
block|}
comment|/** Row container that gets and deserializes the rows on demand from bytes provided. */
specifier|private
class|class
name|ReusableRowContainer
implements|implements
name|MapJoinRowContainer
implements|,
name|AbstractRowContainer
operator|.
name|RowIterator
argument_list|<
name|List
argument_list|<
name|Object
argument_list|>
argument_list|>
block|{
specifier|private
name|byte
name|aliasFilter
decl_stmt|;
specifier|private
name|List
argument_list|<
name|WriteBuffers
operator|.
name|ByteSegmentRef
argument_list|>
name|refs
decl_stmt|;
specifier|private
name|int
name|currentRow
decl_stmt|;
comment|/**      * Sometimes, when container is empty in multi-table mapjoin, we need to add a dummy row.      * This container does not normally support adding rows; this is for the dummy row.      */
specifier|private
name|List
argument_list|<
name|Object
argument_list|>
name|dummyRow
init|=
literal|null
decl_stmt|;
specifier|private
specifier|final
name|ByteArrayRef
name|uselessIndirection
decl_stmt|;
comment|// LBStruct needs ByteArrayRef
specifier|private
specifier|final
name|LazyBinaryStruct
name|valueStruct
decl_stmt|;
specifier|private
name|int
name|partitionId
decl_stmt|;
comment|// Current hashMap in use
specifier|public
name|ReusableRowContainer
parameter_list|()
block|{
if|if
condition|(
name|internalValueOi
operator|!=
literal|null
condition|)
block|{
name|valueStruct
operator|=
operator|(
name|LazyBinaryStruct
operator|)
name|LazyBinaryFactory
operator|.
name|createLazyBinaryObject
argument_list|(
name|internalValueOi
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|valueStruct
operator|=
literal|null
expr_stmt|;
comment|// No rows?
block|}
name|uselessIndirection
operator|=
operator|new
name|ByteArrayRef
argument_list|()
expr_stmt|;
name|clearRows
argument_list|()
expr_stmt|;
block|}
comment|/* Determine if there is a match between big table row and the corresponding hashtable      * Three states can be returned:      * MATCH: a match is found      * NOMATCH: no match is found from the specified partition      * SPILL: the specified partition has been spilled to disk and is not available;      *        the evaluation for this big table row will be postponed.      */
specifier|public
name|JoinUtil
operator|.
name|JoinResult
name|setFromOutput
parameter_list|(
name|Output
name|output
parameter_list|)
throws|throws
name|HiveException
block|{
if|if
condition|(
name|refs
operator|==
literal|null
condition|)
block|{
name|refs
operator|=
operator|new
name|ArrayList
argument_list|<
name|WriteBuffers
operator|.
name|ByteSegmentRef
argument_list|>
argument_list|(
literal|0
argument_list|)
expr_stmt|;
block|}
name|int
name|keyHash
init|=
name|WriteBuffers
operator|.
name|murmurHash
argument_list|(
name|output
operator|.
name|getData
argument_list|()
argument_list|,
literal|0
argument_list|,
name|output
operator|.
name|getLength
argument_list|()
argument_list|)
decl_stmt|;
name|partitionId
operator|=
name|keyHash
operator|&
operator|(
name|hashPartitions
operator|.
name|length
operator|-
literal|1
operator|)
expr_stmt|;
comment|// If the target hash table is on disk, spill this row to disk as well to be processed later
if|if
condition|(
name|isOnDisk
argument_list|(
name|partitionId
argument_list|)
condition|)
block|{
name|toSpillPartitionId
operator|=
name|partitionId
expr_stmt|;
name|refs
operator|.
name|clear
argument_list|()
expr_stmt|;
return|return
name|JoinUtil
operator|.
name|JoinResult
operator|.
name|SPILL
return|;
block|}
else|else
block|{
name|byte
name|aliasFilter
init|=
name|hashPartitions
index|[
name|partitionId
index|]
operator|.
name|hashMap
operator|.
name|getValueRefs
argument_list|(
name|output
operator|.
name|getData
argument_list|()
argument_list|,
name|output
operator|.
name|getLength
argument_list|()
argument_list|,
name|refs
argument_list|)
decl_stmt|;
name|this
operator|.
name|aliasFilter
operator|=
name|refs
operator|.
name|isEmpty
argument_list|()
condition|?
operator|(
name|byte
operator|)
literal|0xff
else|:
name|aliasFilter
expr_stmt|;
name|this
operator|.
name|dummyRow
operator|=
literal|null
expr_stmt|;
if|if
condition|(
name|refs
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
return|return
name|JoinUtil
operator|.
name|JoinResult
operator|.
name|NOMATCH
return|;
block|}
else|else
block|{
return|return
name|JoinUtil
operator|.
name|JoinResult
operator|.
name|MATCH
return|;
block|}
block|}
block|}
specifier|public
name|boolean
name|isEmpty
parameter_list|()
block|{
return|return
name|refs
operator|.
name|isEmpty
argument_list|()
operator|&&
operator|(
name|dummyRow
operator|==
literal|null
operator|)
return|;
block|}
comment|// Implementation of row container
annotation|@
name|Override
specifier|public
name|RowIterator
argument_list|<
name|List
argument_list|<
name|Object
argument_list|>
argument_list|>
name|rowIter
parameter_list|()
throws|throws
name|HiveException
block|{
name|currentRow
operator|=
operator|-
literal|1
expr_stmt|;
return|return
name|this
return|;
block|}
annotation|@
name|Override
specifier|public
name|int
name|rowCount
parameter_list|()
throws|throws
name|HiveException
block|{
return|return
name|dummyRow
operator|!=
literal|null
condition|?
literal|1
else|:
name|refs
operator|.
name|size
argument_list|()
return|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|clearRows
parameter_list|()
block|{
comment|// Doesn't clear underlying hashtable
if|if
condition|(
name|refs
operator|!=
literal|null
condition|)
block|{
name|refs
operator|.
name|clear
argument_list|()
expr_stmt|;
block|}
name|dummyRow
operator|=
literal|null
expr_stmt|;
name|currentRow
operator|=
operator|-
literal|1
expr_stmt|;
name|aliasFilter
operator|=
operator|(
name|byte
operator|)
literal|0xff
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|byte
name|getAliasFilter
parameter_list|()
throws|throws
name|HiveException
block|{
return|return
name|aliasFilter
return|;
block|}
annotation|@
name|Override
specifier|public
name|MapJoinRowContainer
name|copy
parameter_list|()
throws|throws
name|HiveException
block|{
return|return
name|this
return|;
comment|// Independent of hashtable and can be modified, no need to copy.
block|}
comment|// Implementation of row iterator
annotation|@
name|Override
specifier|public
name|List
argument_list|<
name|Object
argument_list|>
name|first
parameter_list|()
throws|throws
name|HiveException
block|{
name|currentRow
operator|=
literal|0
expr_stmt|;
return|return
name|next
argument_list|()
return|;
block|}
annotation|@
name|Override
specifier|public
name|List
argument_list|<
name|Object
argument_list|>
name|next
parameter_list|()
throws|throws
name|HiveException
block|{
if|if
condition|(
name|dummyRow
operator|!=
literal|null
condition|)
block|{
name|List
argument_list|<
name|Object
argument_list|>
name|result
init|=
name|dummyRow
decl_stmt|;
name|dummyRow
operator|=
literal|null
expr_stmt|;
return|return
name|result
return|;
block|}
if|if
condition|(
name|currentRow
operator|<
literal|0
operator|||
name|refs
operator|.
name|size
argument_list|()
operator|<
name|currentRow
condition|)
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"No rows"
argument_list|)
throw|;
if|if
condition|(
name|refs
operator|.
name|size
argument_list|()
operator|==
name|currentRow
condition|)
return|return
literal|null
return|;
name|WriteBuffers
operator|.
name|ByteSegmentRef
name|ref
init|=
name|refs
operator|.
name|get
argument_list|(
name|currentRow
operator|++
argument_list|)
decl_stmt|;
if|if
condition|(
name|ref
operator|.
name|getLength
argument_list|()
operator|==
literal|0
condition|)
block|{
return|return
name|EMPTY_LIST
return|;
comment|// shortcut, 0 length means no fields
block|}
if|if
condition|(
name|ref
operator|.
name|getBytes
argument_list|()
operator|==
literal|null
condition|)
block|{
comment|// partitionId is derived from previously calculated value in setFromOutput()
name|hashPartitions
index|[
name|partitionId
index|]
operator|.
name|hashMap
operator|.
name|populateValue
argument_list|(
name|ref
argument_list|)
expr_stmt|;
block|}
name|uselessIndirection
operator|.
name|setData
argument_list|(
name|ref
operator|.
name|getBytes
argument_list|()
argument_list|)
expr_stmt|;
name|valueStruct
operator|.
name|init
argument_list|(
name|uselessIndirection
argument_list|,
operator|(
name|int
operator|)
name|ref
operator|.
name|getOffset
argument_list|()
argument_list|,
name|ref
operator|.
name|getLength
argument_list|()
argument_list|)
expr_stmt|;
return|return
name|valueStruct
operator|.
name|getFieldsAsList
argument_list|()
return|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|addRow
parameter_list|(
name|List
argument_list|<
name|Object
argument_list|>
name|t
parameter_list|)
block|{
if|if
condition|(
name|dummyRow
operator|!=
literal|null
operator|||
operator|!
name|refs
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
literal|"Cannot add rows when not empty"
argument_list|)
throw|;
block|}
name|dummyRow
operator|=
name|t
expr_stmt|;
block|}
comment|// Various unsupported methods.
annotation|@
name|Override
specifier|public
name|void
name|addRow
parameter_list|(
name|Object
index|[]
name|value
parameter_list|)
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
name|this
operator|.
name|getClass
argument_list|()
operator|.
name|getCanonicalName
argument_list|()
operator|+
literal|" cannot add arrays"
argument_list|)
throw|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|write
parameter_list|(
name|MapJoinObjectSerDeContext
name|valueContext
parameter_list|,
name|ObjectOutputStream
name|out
parameter_list|)
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
name|this
operator|.
name|getClass
argument_list|()
operator|.
name|getCanonicalName
argument_list|()
operator|+
literal|" cannot be serialized"
argument_list|)
throw|;
block|}
block|}
annotation|@
name|Override
specifier|public
name|void
name|dumpMetrics
parameter_list|()
block|{
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|hashPartitions
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|HashPartition
name|hp
init|=
name|hashPartitions
index|[
name|i
index|]
decl_stmt|;
if|if
condition|(
name|hp
operator|.
name|hashMap
operator|!=
literal|null
condition|)
block|{
name|hp
operator|.
name|hashMap
operator|.
name|debugDumpMetrics
argument_list|()
expr_stmt|;
block|}
block|}
block|}
specifier|public
name|void
name|dumpStats
parameter_list|()
block|{
name|int
name|numPartitionsInMem
init|=
literal|0
decl_stmt|;
name|int
name|numPartitionsOnDisk
init|=
literal|0
decl_stmt|;
for|for
control|(
name|HashPartition
name|hp
range|:
name|hashPartitions
control|)
block|{
if|if
condition|(
name|hp
operator|.
name|isHashMapOnDisk
argument_list|()
condition|)
block|{
name|numPartitionsOnDisk
operator|++
expr_stmt|;
block|}
else|else
block|{
name|numPartitionsInMem
operator|++
expr_stmt|;
block|}
block|}
name|LOG
operator|.
name|info
argument_list|(
literal|"In memory partitions have been processed successfully: "
operator|+
name|numPartitionsInMem
operator|+
literal|" partitions in memory have been processed; "
operator|+
name|numPartitionsOnDisk
operator|+
literal|" partitions have been spilled to disk and will be processed next."
argument_list|)
expr_stmt|;
block|}
block|}
end_class

end_unit

