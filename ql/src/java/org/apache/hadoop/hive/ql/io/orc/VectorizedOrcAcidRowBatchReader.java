begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/*  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|orc
package|;
end_package

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|annotations
operator|.
name|VisibleForTesting
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configuration
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileStatus
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|ValidReaderWriteIdList
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|ValidWriteIdList
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
operator|.
name|ConfVars
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|hive_metastoreConstants
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|Utilities
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|vector
operator|.
name|ColumnVector
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|vector
operator|.
name|LongColumnVector
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|vector
operator|.
name|StructColumnVector
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|vector
operator|.
name|VectorizedRowBatch
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|vector
operator|.
name|VectorizedRowBatchCtx
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|AcidOutputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|AcidUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|BucketCodec
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|RecordIdentifier
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|sarg
operator|.
name|PredicateLeaf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|sarg
operator|.
name|SearchArgument
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|sarg
operator|.
name|SearchArgumentFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|VirtualColumn
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|shims
operator|.
name|HadoopShims
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|NullWritable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|JobConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|Reporter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|orc
operator|.
name|ColumnStatistics
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|orc
operator|.
name|IntegerColumnStatistics
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|orc
operator|.
name|OrcConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|orc
operator|.
name|StripeInformation
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|orc
operator|.
name|StripeStatistics
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|orc
operator|.
name|impl
operator|.
name|OrcAcidUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|Logger
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|LoggerFactory
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Arrays
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|BitSet
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
operator|.
name|Entry
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|TreeMap
import|;
end_import

begin_comment
comment|/**  * A fast vectorized batch reader class for ACID. Insert events are read directly  * from the base files/insert_only deltas in vectorized row batches. The deleted  * rows can then be easily indicated via the 'selected' field of the vectorized row batch.  * Refer HIVE-14233 for more details.  */
end_comment

begin_class
specifier|public
class|class
name|VectorizedOrcAcidRowBatchReader
implements|implements
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|RecordReader
argument_list|<
name|NullWritable
argument_list|,
name|VectorizedRowBatch
argument_list|>
block|{
specifier|private
specifier|static
specifier|final
name|Logger
name|LOG
init|=
name|LoggerFactory
operator|.
name|getLogger
argument_list|(
name|VectorizedOrcAcidRowBatchReader
operator|.
name|class
argument_list|)
decl_stmt|;
specifier|private
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|RecordReader
argument_list|<
name|NullWritable
argument_list|,
name|VectorizedRowBatch
argument_list|>
name|baseReader
decl_stmt|;
specifier|private
specifier|final
name|VectorizedRowBatchCtx
name|rbCtx
decl_stmt|;
specifier|private
name|VectorizedRowBatch
name|vectorizedRowBatchBase
decl_stmt|;
specifier|private
name|long
name|offset
decl_stmt|;
specifier|private
name|long
name|length
decl_stmt|;
specifier|protected
name|float
name|progress
init|=
literal|0.0f
decl_stmt|;
specifier|protected
name|Object
index|[]
name|partitionValues
decl_stmt|;
specifier|private
name|boolean
name|addPartitionCols
init|=
literal|true
decl_stmt|;
comment|/**    * true means there is no OrcRecordUpdater.ROW column    * (i.e. the struct wrapping user columns) in {@link #vectorizedRowBatchBase}.    */
specifier|private
specifier|final
name|boolean
name|isFlatPayload
decl_stmt|;
specifier|private
specifier|final
name|ValidWriteIdList
name|validWriteIdList
decl_stmt|;
specifier|private
specifier|final
name|DeleteEventRegistry
name|deleteEventRegistry
decl_stmt|;
comment|/**    * {@link RecordIdentifier}/{@link VirtualColumn#ROWID} information    */
specifier|private
specifier|final
name|StructColumnVector
name|recordIdColumnVector
decl_stmt|;
specifier|private
specifier|final
name|Reader
operator|.
name|Options
name|readerOptions
decl_stmt|;
specifier|private
specifier|final
name|boolean
name|isOriginal
decl_stmt|;
comment|/**    * something further in the data pipeline wants {@link VirtualColumn#ROWID}    */
specifier|private
specifier|final
name|boolean
name|rowIdProjected
decl_stmt|;
comment|/**    * if false, we don't need any acid medadata columns from the file because we    * know all data in the split is valid (wrt to visible writeIDs/delete events)    * and ROW_ID is not needed higher up in the operator pipeline    */
specifier|private
specifier|final
name|boolean
name|includeAcidColumns
decl_stmt|;
comment|/**    * partition/table root    */
specifier|private
specifier|final
name|Path
name|rootPath
decl_stmt|;
comment|/**    * for reading "original" files    */
specifier|private
specifier|final
name|OrcSplit
operator|.
name|OffsetAndBucketProperty
name|syntheticProps
decl_stmt|;
comment|/**    * To have access to {@link RecordReader#getRowNumber()} in the underlying    * file which we need to generate synthetic ROW_IDs for original files    */
specifier|private
name|RecordReader
name|innerReader
decl_stmt|;
comment|/**    * min/max ROW__ID for the split (if available) so that we can limit the    * number of delete events to load in memory    */
specifier|private
specifier|final
name|OrcRawRecordMerger
operator|.
name|KeyInterval
name|keyInterval
decl_stmt|;
comment|/**    * {@link SearchArgument} pushed down to delete_deltaS    */
specifier|private
name|SearchArgument
name|deleteEventSarg
init|=
literal|null
decl_stmt|;
comment|//OrcInputFormat c'tor
name|VectorizedOrcAcidRowBatchReader
parameter_list|(
name|OrcSplit
name|inputSplit
parameter_list|,
name|JobConf
name|conf
parameter_list|,
name|Reporter
name|reporter
parameter_list|)
throws|throws
name|IOException
block|{
name|this
argument_list|(
name|inputSplit
argument_list|,
name|conf
argument_list|,
name|reporter
argument_list|,
literal|null
argument_list|)
expr_stmt|;
block|}
annotation|@
name|VisibleForTesting
name|VectorizedOrcAcidRowBatchReader
parameter_list|(
name|OrcSplit
name|inputSplit
parameter_list|,
name|JobConf
name|conf
parameter_list|,
name|Reporter
name|reporter
parameter_list|,
name|VectorizedRowBatchCtx
name|rbCtx
parameter_list|)
throws|throws
name|IOException
block|{
name|this
argument_list|(
name|conf
argument_list|,
name|inputSplit
argument_list|,
name|reporter
argument_list|,
name|rbCtx
operator|==
literal|null
condition|?
name|Utilities
operator|.
name|getVectorizedRowBatchCtx
argument_list|(
name|conf
argument_list|)
else|:
name|rbCtx
argument_list|,
literal|false
argument_list|)
expr_stmt|;
specifier|final
name|Reader
name|reader
init|=
name|OrcInputFormat
operator|.
name|createOrcReaderForSplit
argument_list|(
name|conf
argument_list|,
name|inputSplit
argument_list|)
decl_stmt|;
comment|// Careful with the range here now, we do not want to read the whole base file like deltas.
name|innerReader
operator|=
name|reader
operator|.
name|rowsOptions
argument_list|(
name|readerOptions
operator|.
name|range
argument_list|(
name|offset
argument_list|,
name|length
argument_list|)
argument_list|,
name|conf
argument_list|)
expr_stmt|;
name|baseReader
operator|=
operator|new
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|RecordReader
argument_list|<
name|NullWritable
argument_list|,
name|VectorizedRowBatch
argument_list|>
argument_list|()
block|{
annotation|@
name|Override
specifier|public
name|boolean
name|next
parameter_list|(
name|NullWritable
name|key
parameter_list|,
name|VectorizedRowBatch
name|value
parameter_list|)
throws|throws
name|IOException
block|{
return|return
name|innerReader
operator|.
name|nextBatch
argument_list|(
name|value
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|public
name|NullWritable
name|createKey
parameter_list|()
block|{
return|return
name|NullWritable
operator|.
name|get
argument_list|()
return|;
block|}
annotation|@
name|Override
specifier|public
name|VectorizedRowBatch
name|createValue
parameter_list|()
block|{
return|return
name|rbCtx
operator|.
name|createVectorizedRowBatch
argument_list|()
return|;
block|}
annotation|@
name|Override
specifier|public
name|long
name|getPos
parameter_list|()
throws|throws
name|IOException
block|{
return|return
literal|0
return|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|close
parameter_list|()
throws|throws
name|IOException
block|{
name|innerReader
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|float
name|getProgress
parameter_list|()
throws|throws
name|IOException
block|{
return|return
name|innerReader
operator|.
name|getProgress
argument_list|()
return|;
block|}
block|}
expr_stmt|;
specifier|final
name|boolean
name|useDecimal64ColumnVectors
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|conf
argument_list|,
name|ConfVars
operator|.
name|HIVE_VECTORIZED_INPUT_FORMAT_SUPPORTS_ENABLED
argument_list|)
operator|.
name|equalsIgnoreCase
argument_list|(
literal|"decimal_64"
argument_list|)
decl_stmt|;
if|if
condition|(
name|useDecimal64ColumnVectors
condition|)
block|{
name|this
operator|.
name|vectorizedRowBatchBase
operator|=
operator|(
operator|(
name|RecordReaderImpl
operator|)
name|innerReader
operator|)
operator|.
name|createRowBatch
argument_list|(
literal|true
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|this
operator|.
name|vectorizedRowBatchBase
operator|=
operator|(
operator|(
name|RecordReaderImpl
operator|)
name|innerReader
operator|)
operator|.
name|createRowBatch
argument_list|(
literal|false
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * LLAP IO c'tor    */
specifier|public
name|VectorizedOrcAcidRowBatchReader
parameter_list|(
name|OrcSplit
name|inputSplit
parameter_list|,
name|JobConf
name|conf
parameter_list|,
name|Reporter
name|reporter
parameter_list|,
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|RecordReader
argument_list|<
name|NullWritable
argument_list|,
name|VectorizedRowBatch
argument_list|>
name|baseReader
parameter_list|,
name|VectorizedRowBatchCtx
name|rbCtx
parameter_list|,
name|boolean
name|isFlatPayload
parameter_list|)
throws|throws
name|IOException
block|{
name|this
argument_list|(
name|conf
argument_list|,
name|inputSplit
argument_list|,
name|reporter
argument_list|,
name|rbCtx
argument_list|,
name|isFlatPayload
argument_list|)
expr_stmt|;
if|if
condition|(
name|baseReader
operator|!=
literal|null
condition|)
block|{
name|setBaseAndInnerReader
argument_list|(
name|baseReader
argument_list|)
expr_stmt|;
block|}
block|}
specifier|private
name|VectorizedOrcAcidRowBatchReader
parameter_list|(
name|JobConf
name|conf
parameter_list|,
name|OrcSplit
name|orcSplit
parameter_list|,
name|Reporter
name|reporter
parameter_list|,
name|VectorizedRowBatchCtx
name|rowBatchCtx
parameter_list|,
name|boolean
name|isFlatPayload
parameter_list|)
throws|throws
name|IOException
block|{
name|this
operator|.
name|isFlatPayload
operator|=
name|isFlatPayload
expr_stmt|;
name|this
operator|.
name|rbCtx
operator|=
name|rowBatchCtx
expr_stmt|;
specifier|final
name|boolean
name|isAcidRead
init|=
name|AcidUtils
operator|.
name|isFullAcidScan
argument_list|(
name|conf
argument_list|)
decl_stmt|;
specifier|final
name|AcidUtils
operator|.
name|AcidOperationalProperties
name|acidOperationalProperties
init|=
name|AcidUtils
operator|.
name|getAcidOperationalProperties
argument_list|(
name|conf
argument_list|)
decl_stmt|;
comment|// This type of VectorizedOrcAcidRowBatchReader can only be created when split-update is
comment|// enabled for an ACID case and the file format is ORC.
name|boolean
name|isReadNotAllowed
init|=
operator|!
name|isAcidRead
operator|||
operator|!
name|acidOperationalProperties
operator|.
name|isSplitUpdate
argument_list|()
decl_stmt|;
if|if
condition|(
name|isReadNotAllowed
condition|)
block|{
name|OrcInputFormat
operator|.
name|raiseAcidTablesMustBeReadWithAcidReaderException
argument_list|(
name|conf
argument_list|)
expr_stmt|;
block|}
name|reporter
operator|.
name|setStatus
argument_list|(
name|orcSplit
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
name|readerOptions
operator|=
name|OrcInputFormat
operator|.
name|createOptionsForReader
argument_list|(
name|conf
argument_list|)
expr_stmt|;
name|this
operator|.
name|offset
operator|=
name|orcSplit
operator|.
name|getStart
argument_list|()
expr_stmt|;
name|this
operator|.
name|length
operator|=
name|orcSplit
operator|.
name|getLength
argument_list|()
expr_stmt|;
name|int
name|partitionColumnCount
init|=
operator|(
name|rbCtx
operator|!=
literal|null
operator|)
condition|?
name|rbCtx
operator|.
name|getPartitionColumnCount
argument_list|()
else|:
literal|0
decl_stmt|;
if|if
condition|(
name|partitionColumnCount
operator|>
literal|0
condition|)
block|{
name|partitionValues
operator|=
operator|new
name|Object
index|[
name|partitionColumnCount
index|]
expr_stmt|;
name|VectorizedRowBatchCtx
operator|.
name|getPartitionValues
argument_list|(
name|rbCtx
argument_list|,
name|conf
argument_list|,
name|orcSplit
argument_list|,
name|partitionValues
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|partitionValues
operator|=
literal|null
expr_stmt|;
block|}
name|String
name|txnString
init|=
name|conf
operator|.
name|get
argument_list|(
name|ValidWriteIdList
operator|.
name|VALID_WRITEIDS_KEY
argument_list|)
decl_stmt|;
name|this
operator|.
name|validWriteIdList
operator|=
operator|(
name|txnString
operator|==
literal|null
operator|)
condition|?
operator|new
name|ValidReaderWriteIdList
argument_list|()
else|:
operator|new
name|ValidReaderWriteIdList
argument_list|(
name|txnString
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Read ValidWriteIdList: "
operator|+
name|this
operator|.
name|validWriteIdList
operator|.
name|toString
argument_list|()
operator|+
literal|":"
operator|+
name|orcSplit
argument_list|)
expr_stmt|;
name|this
operator|.
name|syntheticProps
operator|=
name|orcSplit
operator|.
name|getSyntheticAcidProps
argument_list|()
expr_stmt|;
comment|// Clone readerOptions for deleteEvents.
name|Reader
operator|.
name|Options
name|deleteEventReaderOptions
init|=
name|readerOptions
operator|.
name|clone
argument_list|()
decl_stmt|;
comment|// Set the range on the deleteEventReaderOptions to 0 to INTEGER_MAX because
comment|// we always want to read all the delete delta files.
name|deleteEventReaderOptions
operator|.
name|range
argument_list|(
literal|0
argument_list|,
name|Long
operator|.
name|MAX_VALUE
argument_list|)
expr_stmt|;
name|keyInterval
operator|=
name|findMinMaxKeys
argument_list|(
name|orcSplit
argument_list|,
name|conf
argument_list|,
name|deleteEventReaderOptions
argument_list|)
expr_stmt|;
name|DeleteEventRegistry
name|der
decl_stmt|;
try|try
block|{
comment|// See if we can load all the relevant delete events from all the
comment|// delete deltas in memory...
name|der
operator|=
operator|new
name|ColumnizedDeleteEventRegistry
argument_list|(
name|conf
argument_list|,
name|orcSplit
argument_list|,
name|deleteEventReaderOptions
argument_list|,
name|keyInterval
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|DeleteEventsOverflowMemoryException
name|e
parameter_list|)
block|{
comment|// If not, then create a set of hanging readers that do sort-merge to find the next smallest
comment|// delete event on-demand. Caps the memory consumption to (some_const * no. of readers).
name|der
operator|=
operator|new
name|SortMergedDeleteEventRegistry
argument_list|(
name|conf
argument_list|,
name|orcSplit
argument_list|,
name|deleteEventReaderOptions
argument_list|)
expr_stmt|;
block|}
name|this
operator|.
name|deleteEventRegistry
operator|=
name|der
expr_stmt|;
name|isOriginal
operator|=
name|orcSplit
operator|.
name|isOriginal
argument_list|()
expr_stmt|;
if|if
condition|(
name|isOriginal
condition|)
block|{
name|recordIdColumnVector
operator|=
operator|new
name|StructColumnVector
argument_list|(
name|VectorizedRowBatch
operator|.
name|DEFAULT_SIZE
argument_list|,
operator|new
name|LongColumnVector
argument_list|()
argument_list|,
operator|new
name|LongColumnVector
argument_list|()
argument_list|,
operator|new
name|LongColumnVector
argument_list|()
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// Will swap in the Vectors from underlying row batch.
name|recordIdColumnVector
operator|=
operator|new
name|StructColumnVector
argument_list|(
name|VectorizedRowBatch
operator|.
name|DEFAULT_SIZE
argument_list|,
literal|null
argument_list|,
literal|null
argument_list|,
literal|null
argument_list|)
expr_stmt|;
block|}
name|rowIdProjected
operator|=
name|areRowIdsProjected
argument_list|(
name|rbCtx
argument_list|)
expr_stmt|;
name|rootPath
operator|=
name|orcSplit
operator|.
name|getRootDir
argument_list|()
expr_stmt|;
comment|/**      * This could be optimized by moving dir type/write id based checks are      * done during split generation (i.e. per file not per split) and the      * DeleteEventRegistry is checked here since some splits from the same      * file may have relevant deletes and other may not.      */
if|if
condition|(
name|conf
operator|.
name|getBoolean
argument_list|(
name|ConfVars
operator|.
name|OPTIMIZE_ACID_META_COLUMNS
operator|.
name|varname
argument_list|,
literal|true
argument_list|)
condition|)
block|{
comment|/*figure out if we can skip reading acid metadata columns:        * isOriginal - don't have meta columns - nothing to skip        * there no relevant delete events&& ROW__ID is not needed higher up        * (e.g. this is not a delete statement)*/
if|if
condition|(
operator|!
name|isOriginal
operator|&&
name|deleteEventRegistry
operator|.
name|isEmpty
argument_list|()
operator|&&
operator|!
name|rowIdProjected
condition|)
block|{
name|Path
name|parent
init|=
name|orcSplit
operator|.
name|getPath
argument_list|()
operator|.
name|getParent
argument_list|()
decl_stmt|;
while|while
condition|(
name|parent
operator|!=
literal|null
operator|&&
operator|!
name|rootPath
operator|.
name|equals
argument_list|(
name|parent
argument_list|)
condition|)
block|{
if|if
condition|(
name|parent
operator|.
name|getName
argument_list|()
operator|.
name|startsWith
argument_list|(
name|AcidUtils
operator|.
name|BASE_PREFIX
argument_list|)
condition|)
block|{
comment|/**              * The assumption here is that any base_x is filtered out by              * {@link AcidUtils#getAcidState(Path, Configuration, ValidWriteIdList)}              * so if we see it here it's valid.              * {@link AcidUtils#isValidBase(long, ValidWriteIdList, Path, FileSystem)}              * can check but it makes a {@link FileSystem} call.              */
name|readerOptions
operator|.
name|includeAcidColumns
argument_list|(
literal|false
argument_list|)
expr_stmt|;
break|break;
block|}
else|else
block|{
name|AcidUtils
operator|.
name|ParsedDelta
name|pd
init|=
name|AcidUtils
operator|.
name|parsedDelta
argument_list|(
name|parent
argument_list|,
name|isOriginal
argument_list|)
decl_stmt|;
if|if
condition|(
name|validWriteIdList
operator|.
name|isWriteIdRangeValid
argument_list|(
name|pd
operator|.
name|getMinWriteId
argument_list|()
argument_list|,
name|pd
operator|.
name|getMaxWriteId
argument_list|()
argument_list|)
operator|==
name|ValidWriteIdList
operator|.
name|RangeResponse
operator|.
name|ALL
condition|)
block|{
comment|//all write IDs in range are committed (and visible in current
comment|// snapshot)
name|readerOptions
operator|.
name|includeAcidColumns
argument_list|(
literal|false
argument_list|)
expr_stmt|;
break|break;
block|}
block|}
name|parent
operator|=
name|parent
operator|.
name|getParent
argument_list|()
expr_stmt|;
block|}
block|}
block|}
name|includeAcidColumns
operator|=
name|readerOptions
operator|.
name|getIncludeAcidColumns
argument_list|()
expr_stmt|;
comment|//default is true
block|}
comment|/**    * Generates a SearchArgument to push down to delete_delta files.    *    *    * Note that bucket is a bit packed int, so even thought all delete events    * for a given split have the same bucket ID but not the same "bucket" value    * {@link BucketCodec}    */
specifier|private
name|void
name|setSARG
parameter_list|(
name|OrcRawRecordMerger
operator|.
name|KeyInterval
name|keyInterval
parameter_list|,
name|Reader
operator|.
name|Options
name|deleteEventReaderOptions
parameter_list|,
name|long
name|minBucketProp
parameter_list|,
name|long
name|maxBucketProp
parameter_list|,
name|long
name|minRowId
parameter_list|,
name|long
name|maxRowId
parameter_list|)
block|{
name|SearchArgument
operator|.
name|Builder
name|b
init|=
literal|null
decl_stmt|;
if|if
condition|(
name|keyInterval
operator|.
name|getMinKey
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|RecordIdentifier
name|k
init|=
name|keyInterval
operator|.
name|getMinKey
argument_list|()
decl_stmt|;
name|b
operator|=
name|SearchArgumentFactory
operator|.
name|newBuilder
argument_list|()
expr_stmt|;
name|b
operator|.
name|startAnd
argument_list|()
comment|//not(ot< 7) -> ot>=7
operator|.
name|startNot
argument_list|()
operator|.
name|lessThan
argument_list|(
name|OrcRecordUpdater
operator|.
name|ORIGINAL_WRITEID_FIELD_NAME
argument_list|,
name|PredicateLeaf
operator|.
name|Type
operator|.
name|LONG
argument_list|,
name|k
operator|.
name|getWriteId
argument_list|()
argument_list|)
operator|.
name|end
argument_list|()
expr_stmt|;
name|b
operator|.
name|startNot
argument_list|()
operator|.
name|lessThan
argument_list|(
name|OrcRecordUpdater
operator|.
name|BUCKET_FIELD_NAME
argument_list|,
name|PredicateLeaf
operator|.
name|Type
operator|.
name|LONG
argument_list|,
name|minBucketProp
argument_list|)
operator|.
name|end
argument_list|()
expr_stmt|;
name|b
operator|.
name|startNot
argument_list|()
operator|.
name|lessThan
argument_list|(
name|OrcRecordUpdater
operator|.
name|ROW_ID_FIELD_NAME
argument_list|,
name|PredicateLeaf
operator|.
name|Type
operator|.
name|LONG
argument_list|,
name|minRowId
argument_list|)
operator|.
name|end
argument_list|()
expr_stmt|;
name|b
operator|.
name|end
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
name|keyInterval
operator|.
name|getMaxKey
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|RecordIdentifier
name|k
init|=
name|keyInterval
operator|.
name|getMaxKey
argument_list|()
decl_stmt|;
if|if
condition|(
name|b
operator|==
literal|null
condition|)
block|{
name|b
operator|=
name|SearchArgumentFactory
operator|.
name|newBuilder
argument_list|()
expr_stmt|;
block|}
name|b
operator|.
name|startAnd
argument_list|()
operator|.
name|lessThanEquals
argument_list|(
name|OrcRecordUpdater
operator|.
name|ORIGINAL_WRITEID_FIELD_NAME
argument_list|,
name|PredicateLeaf
operator|.
name|Type
operator|.
name|LONG
argument_list|,
name|k
operator|.
name|getWriteId
argument_list|()
argument_list|)
expr_stmt|;
name|b
operator|.
name|lessThanEquals
argument_list|(
name|OrcRecordUpdater
operator|.
name|BUCKET_FIELD_NAME
argument_list|,
name|PredicateLeaf
operator|.
name|Type
operator|.
name|LONG
argument_list|,
name|maxBucketProp
argument_list|)
expr_stmt|;
name|b
operator|.
name|lessThanEquals
argument_list|(
name|OrcRecordUpdater
operator|.
name|ROW_ID_FIELD_NAME
argument_list|,
name|PredicateLeaf
operator|.
name|Type
operator|.
name|LONG
argument_list|,
name|maxRowId
argument_list|)
expr_stmt|;
name|b
operator|.
name|end
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
name|b
operator|!=
literal|null
condition|)
block|{
name|deleteEventSarg
operator|=
name|b
operator|.
name|build
argument_list|()
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"deleteReader SARG("
operator|+
name|deleteEventSarg
operator|+
literal|") "
argument_list|)
expr_stmt|;
name|deleteEventReaderOptions
operator|.
name|searchArgument
argument_list|(
name|deleteEventSarg
argument_list|,
operator|new
name|String
index|[]
block|{
name|OrcRecordUpdater
operator|.
name|ORIGINAL_WRITEID_FIELD_NAME
block|,
name|OrcRecordUpdater
operator|.
name|BUCKET_FIELD_NAME
block|,
name|OrcRecordUpdater
operator|.
name|ROW_ID_FIELD_NAME
block|}
argument_list|)
expr_stmt|;
return|return;
block|}
name|deleteEventReaderOptions
operator|.
name|searchArgument
argument_list|(
literal|null
argument_list|,
literal|null
argument_list|)
expr_stmt|;
block|}
specifier|public
name|boolean
name|includeAcidColumns
parameter_list|()
block|{
return|return
name|this
operator|.
name|includeAcidColumns
return|;
block|}
specifier|public
name|void
name|setBaseAndInnerReader
parameter_list|(
specifier|final
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|RecordReader
argument_list|<
name|NullWritable
argument_list|,
name|VectorizedRowBatch
argument_list|>
name|baseReader
parameter_list|)
block|{
name|this
operator|.
name|baseReader
operator|=
name|baseReader
expr_stmt|;
name|this
operator|.
name|innerReader
operator|=
literal|null
expr_stmt|;
name|this
operator|.
name|vectorizedRowBatchBase
operator|=
name|baseReader
operator|.
name|createValue
argument_list|()
expr_stmt|;
block|}
comment|/**    * A given ORC reader will always process one or more whole stripes but the    * split boundaries may not line up with stripe boundaries if the InputFormat    * doesn't understand ORC specifics. So first we need to figure out which    * stripe(s) we are reading.    *    * Suppose txn1 writes 100K rows    * and txn2 writes 100 rows so we have events    * {1,0,0}....{1,0,100K},{2,0,0}...{2,0,100} in 2 files    * After compaction we may have 2 stripes    * {1,0,0}...{1,0,90K},{1,0,90001}...{2,0,100}    *    * Now suppose there is a delete stmt that deletes every row.  So when we load    * the 2nd stripe, if we just look at stripe {@link ColumnStatistics},    * minKey={1,0,100} and maxKey={2,0,90001}, all but the 1st 100 delete events    * will get loaded.  But with {@link OrcRecordUpdater#ACID_KEY_INDEX_NAME},    * minKey={1,0,90001} and maxKey={2,0,100} so we only load about 10K deletes.    *    * Also, even with Query Based compactor (once we have it), FileSinkOperator    * uses OrcRecordWriter to write to file, so we should have the    * hive.acid.index in place.    *    * If reading the 1st stripe, we don't have the start event, so we'll get it    * from stats, which will strictly speaking be accurate only wrt writeId and    * bucket but that is good enough.    *    * @return empty<code>KeyInterval</code> if KeyInterval could not be    * determined    */
specifier|private
name|OrcRawRecordMerger
operator|.
name|KeyInterval
name|findMinMaxKeys
parameter_list|(
name|OrcSplit
name|orcSplit
parameter_list|,
name|Configuration
name|conf
parameter_list|,
name|Reader
operator|.
name|Options
name|deleteEventReaderOptions
parameter_list|)
throws|throws
name|IOException
block|{
specifier|final
name|boolean
name|noDeleteDeltas
init|=
name|getDeleteDeltaDirsFromSplit
argument_list|(
name|orcSplit
argument_list|)
operator|.
name|length
operator|==
literal|0
decl_stmt|;
if|if
condition|(
operator|!
name|HiveConf
operator|.
name|getBoolVar
argument_list|(
name|conf
argument_list|,
name|ConfVars
operator|.
name|FILTER_DELETE_EVENTS
argument_list|)
operator|||
name|noDeleteDeltas
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"findMinMaxKeys() "
operator|+
name|ConfVars
operator|.
name|FILTER_DELETE_EVENTS
operator|+
literal|"=false"
argument_list|)
expr_stmt|;
return|return
operator|new
name|OrcRawRecordMerger
operator|.
name|KeyInterval
argument_list|(
literal|null
argument_list|,
literal|null
argument_list|)
return|;
block|}
comment|//todo: since we already have OrcSplit.orcTail, should somehow use it to
comment|// get the acid.index, stats, etc rather than fetching the footer again
comment|// though it seems that orcTail is mostly null....
name|Reader
name|reader
init|=
name|OrcFile
operator|.
name|createReader
argument_list|(
name|orcSplit
operator|.
name|getPath
argument_list|()
argument_list|,
name|OrcFile
operator|.
name|readerOptions
argument_list|(
name|conf
argument_list|)
argument_list|)
decl_stmt|;
if|if
condition|(
name|orcSplit
operator|.
name|isOriginal
argument_list|()
condition|)
block|{
comment|/**        * Among originals we may have files with _copy_N suffix.  To properly        * generate a synthetic ROW___ID for them we need        * {@link OffsetAndBucketProperty} which could be an expensive computation        * if there are lots of copy_N files for a given bucketId. But unless        * there are delete events, we often don't need synthetic ROW__IDs at all.        * Kind of chicken-and-egg - deal with this later.        * See {@link OrcRawRecordMerger#discoverOriginalKeyBounds(Reader, int,        * Reader.Options, Configuration, OrcRawRecordMerger.Options)}*/
name|LOG
operator|.
name|debug
argument_list|(
literal|"findMinMaxKeys(original split)"
argument_list|)
expr_stmt|;
return|return
name|findOriginalMinMaxKeys
argument_list|(
name|orcSplit
argument_list|,
name|reader
argument_list|,
name|deleteEventReaderOptions
argument_list|)
return|;
block|}
name|List
argument_list|<
name|StripeInformation
argument_list|>
name|stripes
init|=
name|reader
operator|.
name|getStripes
argument_list|()
decl_stmt|;
specifier|final
name|long
name|splitStart
init|=
name|orcSplit
operator|.
name|getStart
argument_list|()
decl_stmt|;
specifier|final
name|long
name|splitEnd
init|=
name|splitStart
operator|+
name|orcSplit
operator|.
name|getLength
argument_list|()
decl_stmt|;
name|int
name|firstStripeIndex
init|=
operator|-
literal|1
decl_stmt|;
name|int
name|lastStripeIndex
init|=
operator|-
literal|1
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|stripes
operator|.
name|size
argument_list|()
condition|;
name|i
operator|++
control|)
block|{
name|StripeInformation
name|stripe
init|=
name|stripes
operator|.
name|get
argument_list|(
name|i
argument_list|)
decl_stmt|;
name|long
name|stripeEnd
init|=
name|stripe
operator|.
name|getOffset
argument_list|()
operator|+
name|stripe
operator|.
name|getLength
argument_list|()
decl_stmt|;
if|if
condition|(
name|firstStripeIndex
operator|==
operator|-
literal|1
operator|&&
name|stripe
operator|.
name|getOffset
argument_list|()
operator|>=
name|splitStart
condition|)
block|{
name|firstStripeIndex
operator|=
name|i
expr_stmt|;
block|}
if|if
condition|(
name|lastStripeIndex
operator|==
operator|-
literal|1
operator|&&
name|splitEnd
operator|<=
name|stripeEnd
condition|)
block|{
name|lastStripeIndex
operator|=
name|i
expr_stmt|;
block|}
block|}
if|if
condition|(
name|lastStripeIndex
operator|==
operator|-
literal|1
condition|)
block|{
comment|//split goes to the EOF which is> end of stripe since file has a footer
assert|assert
name|stripes
operator|.
name|get
argument_list|(
name|stripes
operator|.
name|size
argument_list|()
operator|-
literal|1
argument_list|)
operator|.
name|getOffset
argument_list|()
operator|+
name|stripes
operator|.
name|get
argument_list|(
name|stripes
operator|.
name|size
argument_list|()
operator|-
literal|1
argument_list|)
operator|.
name|getLength
argument_list|()
operator|<
name|splitEnd
assert|;
name|lastStripeIndex
operator|=
name|stripes
operator|.
name|size
argument_list|()
operator|-
literal|1
expr_stmt|;
block|}
if|if
condition|(
name|firstStripeIndex
operator|>
name|lastStripeIndex
operator|||
name|firstStripeIndex
operator|==
operator|-
literal|1
condition|)
block|{
comment|/**        * If the firstStripeIndex was set after the lastStripeIndex the split lies entirely within a single stripe.        * In case the split lies entirely within the last stripe, the firstStripeIndex will never be found, hence the        * second condition.        * In this case, the reader for this split will not read any data.        * See {@link org.apache.orc.impl.RecordReaderImpl#RecordReaderImpl        * Create a KeyInterval such that no delete delta records are loaded into memory in the deleteEventRegistry.        */
name|long
name|minRowId
init|=
literal|1
decl_stmt|;
name|long
name|maxRowId
init|=
literal|0
decl_stmt|;
name|int
name|minBucketProp
init|=
literal|1
decl_stmt|;
name|int
name|maxBucketProp
init|=
literal|0
decl_stmt|;
name|OrcRawRecordMerger
operator|.
name|KeyInterval
name|keyIntervalTmp
init|=
operator|new
name|OrcRawRecordMerger
operator|.
name|KeyInterval
argument_list|(
operator|new
name|RecordIdentifier
argument_list|(
literal|1
argument_list|,
name|minBucketProp
argument_list|,
name|minRowId
argument_list|)
argument_list|,
operator|new
name|RecordIdentifier
argument_list|(
literal|0
argument_list|,
name|maxBucketProp
argument_list|,
name|maxRowId
argument_list|)
argument_list|)
decl_stmt|;
name|setSARG
argument_list|(
name|keyIntervalTmp
argument_list|,
name|deleteEventReaderOptions
argument_list|,
name|minBucketProp
argument_list|,
name|maxBucketProp
argument_list|,
name|minRowId
argument_list|,
name|maxRowId
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"findMinMaxKeys(): "
operator|+
name|keyIntervalTmp
operator|+
literal|" stripes("
operator|+
name|firstStripeIndex
operator|+
literal|","
operator|+
name|lastStripeIndex
operator|+
literal|")"
argument_list|)
expr_stmt|;
return|return
name|keyIntervalTmp
return|;
block|}
if|if
condition|(
name|firstStripeIndex
operator|==
operator|-
literal|1
operator|||
name|lastStripeIndex
operator|==
operator|-
literal|1
condition|)
block|{
comment|//this should not happen but... if we don't know which stripe(s) are
comment|//involved we can't figure out min/max bounds
name|LOG
operator|.
name|warn
argument_list|(
literal|"Could not find stripe ("
operator|+
name|firstStripeIndex
operator|+
literal|","
operator|+
name|lastStripeIndex
operator|+
literal|")"
argument_list|)
expr_stmt|;
return|return
operator|new
name|OrcRawRecordMerger
operator|.
name|KeyInterval
argument_list|(
literal|null
argument_list|,
literal|null
argument_list|)
return|;
block|}
name|RecordIdentifier
index|[]
name|keyIndex
init|=
name|OrcRecordUpdater
operator|.
name|parseKeyIndex
argument_list|(
name|reader
argument_list|)
decl_stmt|;
if|if
condition|(
name|keyIndex
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Could not find keyIndex ("
operator|+
name|firstStripeIndex
operator|+
literal|","
operator|+
name|lastStripeIndex
operator|+
literal|","
operator|+
name|stripes
operator|.
name|size
argument_list|()
operator|+
literal|")"
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|keyIndex
operator|!=
literal|null
operator|&&
name|keyIndex
operator|.
name|length
operator|!=
name|stripes
operator|.
name|size
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"keyIndex length doesn't match ("
operator|+
name|firstStripeIndex
operator|+
literal|","
operator|+
name|lastStripeIndex
operator|+
literal|","
operator|+
name|stripes
operator|.
name|size
argument_list|()
operator|+
literal|","
operator|+
name|keyIndex
operator|.
name|length
operator|+
literal|")"
argument_list|)
expr_stmt|;
return|return
operator|new
name|OrcRawRecordMerger
operator|.
name|KeyInterval
argument_list|(
literal|null
argument_list|,
literal|null
argument_list|)
return|;
block|}
comment|/**      * If {@link OrcConf.ROW_INDEX_STRIDE} is set to 0 all column stats on      * ORC file are disabled though objects for them exist but and have      * min/max set to MIN_LONG/MAX_LONG so we only use column stats if they      * are actually computed.  Streaming ingest used to set it 0 and Minor      * compaction so there are lots of legacy files with no (rather, bad)      * column stats*/
name|boolean
name|columnStatsPresent
init|=
name|reader
operator|.
name|getRowIndexStride
argument_list|()
operator|>
literal|0
decl_stmt|;
if|if
condition|(
operator|!
name|columnStatsPresent
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"findMinMaxKeys() No ORC column stats"
argument_list|)
expr_stmt|;
block|}
name|List
argument_list|<
name|StripeStatistics
argument_list|>
name|stats
init|=
name|reader
operator|.
name|getStripeStatistics
argument_list|()
decl_stmt|;
assert|assert
name|stripes
operator|.
name|size
argument_list|()
operator|==
name|stats
operator|.
name|size
argument_list|()
operator|:
literal|"str.s="
operator|+
name|stripes
operator|.
name|size
argument_list|()
operator|+
literal|" sta.s="
operator|+
name|stats
operator|.
name|size
argument_list|()
assert|;
name|RecordIdentifier
name|minKey
init|=
literal|null
decl_stmt|;
if|if
condition|(
name|firstStripeIndex
operator|>
literal|0
operator|&&
name|keyIndex
operator|!=
literal|null
condition|)
block|{
comment|//valid keys are strictly> than this key
name|minKey
operator|=
name|keyIndex
index|[
name|firstStripeIndex
operator|-
literal|1
index|]
expr_stmt|;
comment|//add 1 to make comparison>= to match the case of 0th stripe
name|minKey
operator|.
name|setRowId
argument_list|(
name|minKey
operator|.
name|getRowId
argument_list|()
operator|+
literal|1
argument_list|)
expr_stmt|;
block|}
else|else
block|{
if|if
condition|(
name|columnStatsPresent
condition|)
block|{
name|ColumnStatistics
index|[]
name|colStats
init|=
name|stats
operator|.
name|get
argument_list|(
name|firstStripeIndex
argument_list|)
operator|.
name|getColumnStatistics
argument_list|()
decl_stmt|;
comment|/*         Structure in data is like this:<op, owid, writerId, rowid, cwid,<f1, ... fn>>         The +1 is to account for the top level struct which has a         ColumnStatistics object in colsStats.  Top level struct is normally         dropped by the Reader (I guess because of orc.impl.SchemaEvolution)         */
name|IntegerColumnStatistics
name|origWriteId
init|=
operator|(
name|IntegerColumnStatistics
operator|)
name|colStats
index|[
name|OrcRecordUpdater
operator|.
name|ORIGINAL_WRITEID
operator|+
literal|1
index|]
decl_stmt|;
name|IntegerColumnStatistics
name|bucketProperty
init|=
operator|(
name|IntegerColumnStatistics
operator|)
name|colStats
index|[
name|OrcRecordUpdater
operator|.
name|BUCKET
operator|+
literal|1
index|]
decl_stmt|;
name|IntegerColumnStatistics
name|rowId
init|=
operator|(
name|IntegerColumnStatistics
operator|)
name|colStats
index|[
name|OrcRecordUpdater
operator|.
name|ROW_ID
operator|+
literal|1
index|]
decl_stmt|;
comment|//we may want to change bucketProperty from int to long in the
comment|// future(across the stack) this protects the following cast to int
assert|assert
name|bucketProperty
operator|.
name|getMinimum
argument_list|()
operator|<=
name|Integer
operator|.
name|MAX_VALUE
operator|:
literal|"was bucketProperty changed to a long ("
operator|+
name|bucketProperty
operator|.
name|getMinimum
argument_list|()
operator|+
literal|")?!:"
operator|+
name|orcSplit
assert|;
comment|//this a lower bound but not necessarily greatest lower bound
name|minKey
operator|=
operator|new
name|RecordIdentifier
argument_list|(
name|origWriteId
operator|.
name|getMinimum
argument_list|()
argument_list|,
operator|(
name|int
operator|)
name|bucketProperty
operator|.
name|getMinimum
argument_list|()
argument_list|,
name|rowId
operator|.
name|getMinimum
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
name|RecordIdentifier
name|maxKey
init|=
literal|null
decl_stmt|;
if|if
condition|(
name|keyIndex
operator|!=
literal|null
condition|)
block|{
name|maxKey
operator|=
name|keyIndex
index|[
name|lastStripeIndex
index|]
expr_stmt|;
block|}
else|else
block|{
if|if
condition|(
name|columnStatsPresent
condition|)
block|{
name|ColumnStatistics
index|[]
name|colStats
init|=
name|stats
operator|.
name|get
argument_list|(
name|lastStripeIndex
argument_list|)
operator|.
name|getColumnStatistics
argument_list|()
decl_stmt|;
name|IntegerColumnStatistics
name|origWriteId
init|=
operator|(
name|IntegerColumnStatistics
operator|)
name|colStats
index|[
name|OrcRecordUpdater
operator|.
name|ORIGINAL_WRITEID
operator|+
literal|1
index|]
decl_stmt|;
name|IntegerColumnStatistics
name|bucketProperty
init|=
operator|(
name|IntegerColumnStatistics
operator|)
name|colStats
index|[
name|OrcRecordUpdater
operator|.
name|BUCKET
operator|+
literal|1
index|]
decl_stmt|;
name|IntegerColumnStatistics
name|rowId
init|=
operator|(
name|IntegerColumnStatistics
operator|)
name|colStats
index|[
name|OrcRecordUpdater
operator|.
name|ROW_ID
operator|+
literal|1
index|]
decl_stmt|;
assert|assert
name|bucketProperty
operator|.
name|getMaximum
argument_list|()
operator|<=
name|Integer
operator|.
name|MAX_VALUE
operator|:
literal|"was bucketProperty changed to a long ("
operator|+
name|bucketProperty
operator|.
name|getMaximum
argument_list|()
operator|+
literal|")?!:"
operator|+
name|orcSplit
assert|;
comment|// this is an upper bound but not necessarily the least upper bound
name|maxKey
operator|=
operator|new
name|RecordIdentifier
argument_list|(
name|origWriteId
operator|.
name|getMaximum
argument_list|()
argument_list|,
operator|(
name|int
operator|)
name|bucketProperty
operator|.
name|getMaximum
argument_list|()
argument_list|,
name|rowId
operator|.
name|getMaximum
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
name|OrcRawRecordMerger
operator|.
name|KeyInterval
name|keyInterval
init|=
operator|new
name|OrcRawRecordMerger
operator|.
name|KeyInterval
argument_list|(
name|minKey
argument_list|,
name|maxKey
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"findMinMaxKeys(): "
operator|+
name|keyInterval
operator|+
literal|" stripes("
operator|+
name|firstStripeIndex
operator|+
literal|","
operator|+
name|lastStripeIndex
operator|+
literal|")"
argument_list|)
expr_stmt|;
name|long
name|minBucketProp
init|=
name|Long
operator|.
name|MAX_VALUE
decl_stmt|,
name|maxBucketProp
init|=
name|Long
operator|.
name|MIN_VALUE
decl_stmt|;
name|long
name|minRowId
init|=
name|Long
operator|.
name|MAX_VALUE
decl_stmt|,
name|maxRowId
init|=
name|Long
operator|.
name|MIN_VALUE
decl_stmt|;
if|if
condition|(
name|columnStatsPresent
condition|)
block|{
comment|/**        * figure out min/max bucket, rowid for push down.  This is different from        * min/max ROW__ID because ROW__ID comparison uses dictionary order on two        * tuples (a,b,c), but PPD can only do        * (a between (x,y) and b between(x1,y1) and c between(x2,y2))        * Consider:        * (0,536936448,0), (0,536936448,2), (10000001,536936448,0)        * 1st is min ROW_ID, 3r is max ROW_ID        * and Delete events (0,536936448,2),....,(10000001,536936448,1000000)        * So PPD based on min/max ROW_ID would have 0<= rowId<=0 which will        * miss this delete event.  But we still want PPD to filter out data if        * possible.        *        * So use stripe stats to find proper min/max for bucketProp and rowId        * writeId is the same in both cases        */
for|for
control|(
name|int
name|i
init|=
name|firstStripeIndex
init|;
name|i
operator|<=
name|lastStripeIndex
condition|;
name|i
operator|++
control|)
block|{
name|ColumnStatistics
index|[]
name|colStats
init|=
name|stats
operator|.
name|get
argument_list|(
name|firstStripeIndex
argument_list|)
operator|.
name|getColumnStatistics
argument_list|()
decl_stmt|;
name|IntegerColumnStatistics
name|bucketProperty
init|=
operator|(
name|IntegerColumnStatistics
operator|)
name|colStats
index|[
name|OrcRecordUpdater
operator|.
name|BUCKET
operator|+
literal|1
index|]
decl_stmt|;
name|IntegerColumnStatistics
name|rowId
init|=
operator|(
name|IntegerColumnStatistics
operator|)
name|colStats
index|[
name|OrcRecordUpdater
operator|.
name|ROW_ID
operator|+
literal|1
index|]
decl_stmt|;
if|if
condition|(
name|bucketProperty
operator|.
name|getMinimum
argument_list|()
operator|<
name|minBucketProp
condition|)
block|{
name|minBucketProp
operator|=
name|bucketProperty
operator|.
name|getMinimum
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
name|bucketProperty
operator|.
name|getMaximum
argument_list|()
operator|>
name|maxBucketProp
condition|)
block|{
name|maxBucketProp
operator|=
name|bucketProperty
operator|.
name|getMaximum
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
name|rowId
operator|.
name|getMinimum
argument_list|()
operator|<
name|minRowId
condition|)
block|{
name|minRowId
operator|=
name|rowId
operator|.
name|getMinimum
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
name|rowId
operator|.
name|getMaximum
argument_list|()
operator|>
name|maxRowId
condition|)
block|{
name|maxRowId
operator|=
name|rowId
operator|.
name|getMaximum
argument_list|()
expr_stmt|;
block|}
block|}
block|}
if|if
condition|(
name|minBucketProp
operator|==
name|Long
operator|.
name|MAX_VALUE
condition|)
name|minBucketProp
operator|=
name|Long
operator|.
name|MIN_VALUE
expr_stmt|;
if|if
condition|(
name|maxBucketProp
operator|==
name|Long
operator|.
name|MIN_VALUE
condition|)
name|maxBucketProp
operator|=
name|Long
operator|.
name|MAX_VALUE
expr_stmt|;
if|if
condition|(
name|minRowId
operator|==
name|Long
operator|.
name|MAX_VALUE
condition|)
name|minRowId
operator|=
name|Long
operator|.
name|MIN_VALUE
expr_stmt|;
if|if
condition|(
name|maxRowId
operator|==
name|Long
operator|.
name|MIN_VALUE
condition|)
name|maxRowId
operator|=
name|Long
operator|.
name|MAX_VALUE
expr_stmt|;
name|setSARG
argument_list|(
name|keyInterval
argument_list|,
name|deleteEventReaderOptions
argument_list|,
name|minBucketProp
argument_list|,
name|maxBucketProp
argument_list|,
name|minRowId
argument_list|,
name|maxRowId
argument_list|)
expr_stmt|;
return|return
name|keyInterval
return|;
block|}
specifier|private
name|OrcRawRecordMerger
operator|.
name|KeyInterval
name|findOriginalMinMaxKeys
parameter_list|(
name|OrcSplit
name|orcSplit
parameter_list|,
name|Reader
name|reader
parameter_list|,
name|Reader
operator|.
name|Options
name|deleteEventReaderOptions
parameter_list|)
block|{
comment|// This method returns the minimum and maximum synthetic row ids that are present in this split
comment|// because min and max keys are both inclusive when filtering out the delete delta records.
if|if
condition|(
name|syntheticProps
operator|==
literal|null
condition|)
block|{
comment|// syntheticProps containing the synthetic rowid offset is computed if there are delete delta files.
comment|// If there aren't any delete delta files, then we don't need this anyway.
return|return
operator|new
name|OrcRawRecordMerger
operator|.
name|KeyInterval
argument_list|(
literal|null
argument_list|,
literal|null
argument_list|)
return|;
block|}
name|long
name|splitStart
init|=
name|orcSplit
operator|.
name|getStart
argument_list|()
decl_stmt|;
name|long
name|splitEnd
init|=
name|orcSplit
operator|.
name|getStart
argument_list|()
operator|+
name|orcSplit
operator|.
name|getLength
argument_list|()
decl_stmt|;
name|long
name|minRowId
init|=
name|syntheticProps
operator|.
name|getRowIdOffset
argument_list|()
decl_stmt|;
name|long
name|maxRowId
init|=
name|syntheticProps
operator|.
name|getRowIdOffset
argument_list|()
decl_stmt|;
for|for
control|(
name|StripeInformation
name|stripe
range|:
name|reader
operator|.
name|getStripes
argument_list|()
control|)
block|{
if|if
condition|(
name|splitStart
operator|>
name|stripe
operator|.
name|getOffset
argument_list|()
condition|)
block|{
comment|// This stripe starts before the current split starts. This stripe is not included in this split.
name|minRowId
operator|+=
name|stripe
operator|.
name|getNumberOfRows
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
name|splitEnd
operator|>
name|stripe
operator|.
name|getOffset
argument_list|()
condition|)
block|{
comment|// This stripe starts before the current split ends.
name|maxRowId
operator|+=
name|stripe
operator|.
name|getNumberOfRows
argument_list|()
expr_stmt|;
block|}
else|else
block|{
comment|// The split ends before (or exactly where) this stripe starts.
comment|// Remaining stripes are not included in this split.
break|break;
block|}
block|}
name|RecordIdentifier
name|minKey
init|=
operator|new
name|RecordIdentifier
argument_list|(
name|syntheticProps
operator|.
name|getSyntheticWriteId
argument_list|()
argument_list|,
name|syntheticProps
operator|.
name|getBucketProperty
argument_list|()
argument_list|,
name|minRowId
argument_list|)
decl_stmt|;
name|RecordIdentifier
name|maxKey
init|=
operator|new
name|RecordIdentifier
argument_list|(
name|syntheticProps
operator|.
name|getSyntheticWriteId
argument_list|()
argument_list|,
name|syntheticProps
operator|.
name|getBucketProperty
argument_list|()
argument_list|,
name|maxRowId
operator|>
literal|0
condition|?
name|maxRowId
operator|-
literal|1
else|:
literal|0
argument_list|)
decl_stmt|;
name|OrcRawRecordMerger
operator|.
name|KeyInterval
name|keyIntervalTmp
init|=
operator|new
name|OrcRawRecordMerger
operator|.
name|KeyInterval
argument_list|(
name|minKey
argument_list|,
name|maxKey
argument_list|)
decl_stmt|;
if|if
condition|(
name|minRowId
operator|>=
name|maxRowId
condition|)
block|{
comment|/**        * The split lies entirely within a single stripe. In this case, the reader for this split will not read any data.        * See {@link org.apache.orc.impl.RecordReaderImpl#RecordReaderImpl        * We can return the min max key interval as is (it will not read any of the delete delta records into mem)        */
name|LOG
operator|.
name|info
argument_list|(
literal|"findOriginalMinMaxKeys(): This split starts and ends in the same stripe."
argument_list|)
expr_stmt|;
block|}
name|LOG
operator|.
name|info
argument_list|(
literal|"findOriginalMinMaxKeys(): "
operator|+
name|keyIntervalTmp
argument_list|)
expr_stmt|;
comment|// Using min/max ROW__ID from original will work for ppd to the delete deltas because the writeid is the same in
comment|// the min and the max ROW__ID
name|setSARG
argument_list|(
name|keyIntervalTmp
argument_list|,
name|deleteEventReaderOptions
argument_list|,
name|minKey
operator|.
name|getBucketProperty
argument_list|()
argument_list|,
name|maxKey
operator|.
name|getBucketProperty
argument_list|()
argument_list|,
name|minKey
operator|.
name|getRowId
argument_list|()
argument_list|,
name|maxKey
operator|.
name|getRowId
argument_list|()
argument_list|)
expr_stmt|;
return|return
name|keyIntervalTmp
return|;
block|}
comment|/**    * See {@link #next(NullWritable, VectorizedRowBatch)} first and    * {@link OrcRawRecordMerger.OriginalReaderPair}.    * When reading a split of an "original" file and we need to decorate data with ROW__ID.    * This requires treating multiple files that are part of the same bucket (tranche for unbucketed    * tables) as a single logical file to number rowids consistently.    */
specifier|static
name|OrcSplit
operator|.
name|OffsetAndBucketProperty
name|computeOffsetAndBucket
parameter_list|(
name|FileStatus
name|file
parameter_list|,
name|Path
name|rootDir
parameter_list|,
name|boolean
name|isOriginal
parameter_list|,
name|boolean
name|hasDeletes
parameter_list|,
name|Configuration
name|conf
parameter_list|)
throws|throws
name|IOException
block|{
name|VectorizedRowBatchCtx
name|vrbCtx
init|=
name|Utilities
operator|.
name|getVectorizedRowBatchCtx
argument_list|(
name|conf
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|needSyntheticRowIds
argument_list|(
name|isOriginal
argument_list|,
name|hasDeletes
argument_list|,
name|areRowIdsProjected
argument_list|(
name|vrbCtx
argument_list|)
argument_list|)
condition|)
block|{
if|if
condition|(
name|isOriginal
condition|)
block|{
comment|/**          * Even if we don't need to project ROW_IDs, we still need to check the write ID that          * created the file to see if it's committed.  See more in          * {@link #next(NullWritable, VectorizedRowBatch)}.  (In practice getAcidState() should          * filter out base/delta files but this makes fewer dependencies)          */
name|OrcRawRecordMerger
operator|.
name|TransactionMetaData
name|syntheticTxnInfo
init|=
name|OrcRawRecordMerger
operator|.
name|TransactionMetaData
operator|.
name|findWriteIDForSynthetcRowIDs
argument_list|(
name|file
operator|.
name|getPath
argument_list|()
argument_list|,
name|rootDir
argument_list|,
name|conf
argument_list|)
decl_stmt|;
return|return
operator|new
name|OrcSplit
operator|.
name|OffsetAndBucketProperty
argument_list|(
operator|-
literal|1
argument_list|,
operator|-
literal|1
argument_list|,
name|syntheticTxnInfo
operator|.
name|syntheticWriteId
argument_list|)
return|;
block|}
return|return
literal|null
return|;
block|}
name|String
name|txnString
init|=
name|conf
operator|.
name|get
argument_list|(
name|ValidWriteIdList
operator|.
name|VALID_WRITEIDS_KEY
argument_list|)
decl_stmt|;
name|ValidWriteIdList
name|validWriteIdList
init|=
operator|(
name|txnString
operator|==
literal|null
operator|)
condition|?
operator|new
name|ValidReaderWriteIdList
argument_list|()
else|:
operator|new
name|ValidReaderWriteIdList
argument_list|(
name|txnString
argument_list|)
decl_stmt|;
name|long
name|rowIdOffset
init|=
literal|0
decl_stmt|;
name|OrcRawRecordMerger
operator|.
name|TransactionMetaData
name|syntheticTxnInfo
init|=
name|OrcRawRecordMerger
operator|.
name|TransactionMetaData
operator|.
name|findWriteIDForSynthetcRowIDs
argument_list|(
name|file
operator|.
name|getPath
argument_list|()
argument_list|,
name|rootDir
argument_list|,
name|conf
argument_list|)
decl_stmt|;
name|int
name|bucketId
init|=
name|AcidUtils
operator|.
name|parseBucketId
argument_list|(
name|file
operator|.
name|getPath
argument_list|()
argument_list|)
decl_stmt|;
name|int
name|bucketProperty
init|=
name|BucketCodec
operator|.
name|V1
operator|.
name|encode
argument_list|(
operator|new
name|AcidOutputFormat
operator|.
name|Options
argument_list|(
name|conf
argument_list|)
comment|//statementId is from directory name (or 0 if there is none)
operator|.
name|statementId
argument_list|(
name|syntheticTxnInfo
operator|.
name|statementId
argument_list|)
operator|.
name|bucket
argument_list|(
name|bucketId
argument_list|)
argument_list|)
decl_stmt|;
name|AcidUtils
operator|.
name|Directory
name|directoryState
init|=
name|AcidUtils
operator|.
name|getAcidState
argument_list|(
literal|null
argument_list|,
name|syntheticTxnInfo
operator|.
name|folder
argument_list|,
name|conf
argument_list|,
name|validWriteIdList
argument_list|,
literal|false
argument_list|,
literal|true
argument_list|)
decl_stmt|;
for|for
control|(
name|HadoopShims
operator|.
name|HdfsFileStatusWithId
name|f
range|:
name|directoryState
operator|.
name|getOriginalFiles
argument_list|()
control|)
block|{
name|int
name|bucketIdFromPath
init|=
name|AcidUtils
operator|.
name|parseBucketId
argument_list|(
name|f
operator|.
name|getFileStatus
argument_list|()
operator|.
name|getPath
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|bucketIdFromPath
operator|!=
name|bucketId
condition|)
block|{
continue|continue;
comment|//HIVE-16952
block|}
if|if
condition|(
name|f
operator|.
name|getFileStatus
argument_list|()
operator|.
name|getPath
argument_list|()
operator|.
name|equals
argument_list|(
name|file
operator|.
name|getPath
argument_list|()
argument_list|)
condition|)
block|{
comment|//'f' is the file whence this split is
break|break;
block|}
name|Reader
name|reader
init|=
name|OrcFile
operator|.
name|createReader
argument_list|(
name|f
operator|.
name|getFileStatus
argument_list|()
operator|.
name|getPath
argument_list|()
argument_list|,
name|OrcFile
operator|.
name|readerOptions
argument_list|(
name|conf
argument_list|)
argument_list|)
decl_stmt|;
name|rowIdOffset
operator|+=
name|reader
operator|.
name|getNumberOfRows
argument_list|()
expr_stmt|;
block|}
return|return
operator|new
name|OrcSplit
operator|.
name|OffsetAndBucketProperty
argument_list|(
name|rowIdOffset
argument_list|,
name|bucketProperty
argument_list|,
name|syntheticTxnInfo
operator|.
name|syntheticWriteId
argument_list|)
return|;
block|}
comment|/**    * {@link VectorizedOrcAcidRowBatchReader} is always used for vectorized reads of acid tables.    * In some cases this cannot be used from LLAP IO elevator because    * {@link RecordReader#getRowNumber()} is not (currently) available there but is required to    * generate ROW__IDs for "original" files    * @param hasDeletes - if there are any deletes that apply to this split    * todo: HIVE-17944    */
specifier|static
name|boolean
name|canUseLlapForAcid
parameter_list|(
name|OrcSplit
name|split
parameter_list|,
name|boolean
name|hasDeletes
parameter_list|,
name|Configuration
name|conf
parameter_list|)
block|{
if|if
condition|(
operator|!
name|split
operator|.
name|isOriginal
argument_list|()
condition|)
block|{
return|return
literal|true
return|;
block|}
name|VectorizedRowBatchCtx
name|rbCtx
init|=
name|Utilities
operator|.
name|getVectorizedRowBatchCtx
argument_list|(
name|conf
argument_list|)
decl_stmt|;
if|if
condition|(
name|rbCtx
operator|==
literal|null
condition|)
block|{
throw|throw
operator|new
name|IllegalStateException
argument_list|(
literal|"Could not create VectorizedRowBatchCtx for "
operator|+
name|split
operator|.
name|getPath
argument_list|()
argument_list|)
throw|;
block|}
return|return
operator|!
name|needSyntheticRowIds
argument_list|(
name|split
operator|.
name|isOriginal
argument_list|()
argument_list|,
name|hasDeletes
argument_list|,
name|areRowIdsProjected
argument_list|(
name|rbCtx
argument_list|)
argument_list|)
return|;
block|}
comment|/**    * Does this reader need to decorate rows with ROW__IDs (for "original" reads).    * Even if ROW__ID is not projected you still need to decorate the rows with them to see if    * any of the delete events apply.    */
specifier|private
specifier|static
name|boolean
name|needSyntheticRowIds
parameter_list|(
name|boolean
name|isOriginal
parameter_list|,
name|boolean
name|hasDeletes
parameter_list|,
name|boolean
name|rowIdProjected
parameter_list|)
block|{
return|return
name|isOriginal
operator|&&
operator|(
name|hasDeletes
operator|||
name|rowIdProjected
operator|)
return|;
block|}
specifier|private
specifier|static
name|boolean
name|areRowIdsProjected
parameter_list|(
name|VectorizedRowBatchCtx
name|rbCtx
parameter_list|)
block|{
if|if
condition|(
name|rbCtx
operator|.
name|getVirtualColumnCount
argument_list|()
operator|==
literal|0
condition|)
block|{
return|return
literal|false
return|;
block|}
for|for
control|(
name|VirtualColumn
name|vc
range|:
name|rbCtx
operator|.
name|getNeededVirtualColumns
argument_list|()
control|)
block|{
if|if
condition|(
name|vc
operator|==
name|VirtualColumn
operator|.
name|ROWID
condition|)
block|{
comment|//The query needs ROW__ID: maybe explicitly asked, maybe it's part of
comment|// Update/Delete statement.
comment|//Either way, we need to decorate "original" rows with row__id
return|return
literal|true
return|;
block|}
block|}
return|return
literal|false
return|;
block|}
specifier|static
name|Path
index|[]
name|getDeleteDeltaDirsFromSplit
parameter_list|(
name|OrcSplit
name|orcSplit
parameter_list|)
throws|throws
name|IOException
block|{
name|Path
name|path
init|=
name|orcSplit
operator|.
name|getPath
argument_list|()
decl_stmt|;
name|Path
name|root
decl_stmt|;
if|if
condition|(
name|orcSplit
operator|.
name|hasBase
argument_list|()
condition|)
block|{
if|if
condition|(
name|orcSplit
operator|.
name|isOriginal
argument_list|()
condition|)
block|{
name|root
operator|=
name|orcSplit
operator|.
name|getRootDir
argument_list|()
expr_stmt|;
block|}
else|else
block|{
name|root
operator|=
name|path
operator|.
name|getParent
argument_list|()
operator|.
name|getParent
argument_list|()
expr_stmt|;
comment|//todo: why not just use getRootDir()?
assert|assert
name|root
operator|.
name|equals
argument_list|(
name|orcSplit
operator|.
name|getRootDir
argument_list|()
argument_list|)
operator|:
literal|"root mismatch: baseDir="
operator|+
name|orcSplit
operator|.
name|getRootDir
argument_list|()
operator|+
literal|" path.p.p="
operator|+
name|root
assert|;
block|}
block|}
else|else
block|{
throw|throw
operator|new
name|IllegalStateException
argument_list|(
literal|"Split w/o base w/Acid 2.0??: "
operator|+
name|path
argument_list|)
throw|;
block|}
return|return
name|AcidUtils
operator|.
name|deserializeDeleteDeltas
argument_list|(
name|root
argument_list|,
name|orcSplit
operator|.
name|getDeltas
argument_list|()
argument_list|)
return|;
block|}
comment|/**    * There are 2 types of schema from the {@link #baseReader} that this handles.  In the case    * the data was written to a transactional table from the start, every row is decorated with    * transaction related info and looks like&lt;op, owid, writerId, rowid, cwid,&lt;f1, ... fn&gt;&gt;.    *    * The other case is when data was written to non-transactional table and thus only has the user    * data:&lt;f1, ... fn&gt;.  Then this table was then converted to a transactional table but the data    * files are not changed until major compaction.  These are the "original" files.    *    * In this case we may need to decorate the outgoing data with transactional column values at    * read time.  (It's done somewhat out of band via VectorizedRowBatchCtx - ask Teddy Choi).    * The "owid, writerId, rowid" columns represent {@link RecordIdentifier}.  They are assigned    * each time the table is read in a way that needs to project {@link VirtualColumn#ROWID}.    * Major compaction will attach these values to each row permanently.    * It's critical that these generated column values are assigned exactly the same way by each    * read of the same row and by the Compactor.    * See {@link org.apache.hadoop.hive.ql.txn.compactor.CompactorMR} and    * {@link OrcRawRecordMerger.OriginalReaderPairToCompact} for the Compactor read path.    * (Longer term should make compactor use this class)    *    * This only decorates original rows with metadata if something above is requesting these values    * or if there are Delete events to apply.    *    * @return false where there is no more data, i.e. {@code value} is empty    */
annotation|@
name|Override
specifier|public
name|boolean
name|next
parameter_list|(
name|NullWritable
name|key
parameter_list|,
name|VectorizedRowBatch
name|value
parameter_list|)
throws|throws
name|IOException
block|{
try|try
block|{
comment|// Check and update partition cols if necessary. Ideally, this should be done
comment|// in CreateValue as the partition is constant per split. But since Hive uses
comment|// CombineHiveRecordReader and
comment|// as this does not call CreateValue for each new RecordReader it creates, this check is
comment|// required in next()
if|if
condition|(
name|addPartitionCols
condition|)
block|{
if|if
condition|(
name|partitionValues
operator|!=
literal|null
condition|)
block|{
name|rbCtx
operator|.
name|addPartitionColsToBatch
argument_list|(
name|value
argument_list|,
name|partitionValues
argument_list|)
expr_stmt|;
block|}
name|addPartitionCols
operator|=
literal|false
expr_stmt|;
block|}
if|if
condition|(
operator|!
name|baseReader
operator|.
name|next
argument_list|(
literal|null
argument_list|,
name|vectorizedRowBatchBase
argument_list|)
condition|)
block|{
return|return
literal|false
return|;
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"error iterating"
argument_list|,
name|e
argument_list|)
throw|;
block|}
if|if
condition|(
operator|!
name|includeAcidColumns
condition|)
block|{
comment|//if here, we don't need to filter anything wrt acid metadata columns
comment|//in fact, they are not even read from file/llap
name|value
operator|.
name|size
operator|=
name|vectorizedRowBatchBase
operator|.
name|size
expr_stmt|;
name|value
operator|.
name|selected
operator|=
name|vectorizedRowBatchBase
operator|.
name|selected
expr_stmt|;
name|value
operator|.
name|selectedInUse
operator|=
name|vectorizedRowBatchBase
operator|.
name|selectedInUse
expr_stmt|;
name|copyFromBase
argument_list|(
name|value
argument_list|)
expr_stmt|;
name|progress
operator|=
name|baseReader
operator|.
name|getProgress
argument_list|()
expr_stmt|;
return|return
literal|true
return|;
block|}
comment|// Once we have read the VectorizedRowBatchBase from the file, there are two kinds of cases
comment|// for which we might have to discard rows from the batch:
comment|// Case 1- when the row is created by a transaction that is not valid, or
comment|// Case 2- when the row has been deleted.
comment|// We will go through the batch to discover rows which match any of the cases and specifically
comment|// remove them from the selected vector. Of course, selectedInUse should also be set.
name|BitSet
name|selectedBitSet
init|=
operator|new
name|BitSet
argument_list|(
name|vectorizedRowBatchBase
operator|.
name|size
argument_list|)
decl_stmt|;
if|if
condition|(
name|vectorizedRowBatchBase
operator|.
name|selectedInUse
condition|)
block|{
comment|// When selectedInUse is true, start with every bit set to false and selectively set
comment|// certain bits to true based on the selected[] vector.
name|selectedBitSet
operator|.
name|set
argument_list|(
literal|0
argument_list|,
name|vectorizedRowBatchBase
operator|.
name|size
argument_list|,
literal|false
argument_list|)
expr_stmt|;
for|for
control|(
name|int
name|j
init|=
literal|0
init|;
name|j
operator|<
name|vectorizedRowBatchBase
operator|.
name|size
condition|;
operator|++
name|j
control|)
block|{
name|int
name|i
init|=
name|vectorizedRowBatchBase
operator|.
name|selected
index|[
name|j
index|]
decl_stmt|;
name|selectedBitSet
operator|.
name|set
argument_list|(
name|i
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
comment|// When selectedInUse is set to false, everything in the batch is selected.
name|selectedBitSet
operator|.
name|set
argument_list|(
literal|0
argument_list|,
name|vectorizedRowBatchBase
operator|.
name|size
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
name|ColumnVector
index|[]
name|innerRecordIdColumnVector
init|=
name|vectorizedRowBatchBase
operator|.
name|cols
decl_stmt|;
if|if
condition|(
name|isOriginal
condition|)
block|{
comment|// Handle synthetic row IDs for the original files.
name|innerRecordIdColumnVector
operator|=
name|handleOriginalFile
argument_list|(
name|selectedBitSet
argument_list|,
name|innerRecordIdColumnVector
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// Case 1- find rows which belong to write Ids that are not valid.
name|findRecordsWithInvalidWriteIds
argument_list|(
name|vectorizedRowBatchBase
argument_list|,
name|selectedBitSet
argument_list|)
expr_stmt|;
block|}
comment|// Case 2- find rows which have been deleted.
name|this
operator|.
name|deleteEventRegistry
operator|.
name|findDeletedRecords
argument_list|(
name|innerRecordIdColumnVector
argument_list|,
name|vectorizedRowBatchBase
operator|.
name|size
argument_list|,
name|selectedBitSet
argument_list|)
expr_stmt|;
if|if
condition|(
name|selectedBitSet
operator|.
name|cardinality
argument_list|()
operator|==
name|vectorizedRowBatchBase
operator|.
name|size
condition|)
block|{
comment|// None of the cases above matched and everything is selected. Hence, we will use the
comment|// same values for the selected and selectedInUse.
name|value
operator|.
name|size
operator|=
name|vectorizedRowBatchBase
operator|.
name|size
expr_stmt|;
name|value
operator|.
name|selected
operator|=
name|vectorizedRowBatchBase
operator|.
name|selected
expr_stmt|;
name|value
operator|.
name|selectedInUse
operator|=
name|vectorizedRowBatchBase
operator|.
name|selectedInUse
expr_stmt|;
block|}
else|else
block|{
name|value
operator|.
name|size
operator|=
name|selectedBitSet
operator|.
name|cardinality
argument_list|()
expr_stmt|;
name|value
operator|.
name|selectedInUse
operator|=
literal|true
expr_stmt|;
name|value
operator|.
name|selected
operator|=
operator|new
name|int
index|[
name|selectedBitSet
operator|.
name|cardinality
argument_list|()
index|]
expr_stmt|;
comment|// This loop fills up the selected[] vector with all the index positions that are selected.
for|for
control|(
name|int
name|setBitIndex
init|=
name|selectedBitSet
operator|.
name|nextSetBit
argument_list|(
literal|0
argument_list|)
init|,
name|selectedItr
init|=
literal|0
init|;
name|setBitIndex
operator|>=
literal|0
condition|;
name|setBitIndex
operator|=
name|selectedBitSet
operator|.
name|nextSetBit
argument_list|(
name|setBitIndex
operator|+
literal|1
argument_list|)
operator|,
operator|++
name|selectedItr
control|)
block|{
name|value
operator|.
name|selected
index|[
name|selectedItr
index|]
operator|=
name|setBitIndex
expr_stmt|;
block|}
block|}
if|if
condition|(
name|isOriginal
condition|)
block|{
comment|/* Just copy the payload.  {@link recordIdColumnVector} has already been populated */
name|System
operator|.
name|arraycopy
argument_list|(
name|vectorizedRowBatchBase
operator|.
name|cols
argument_list|,
literal|0
argument_list|,
name|value
operator|.
name|cols
argument_list|,
literal|0
argument_list|,
name|value
operator|.
name|getDataColumnCount
argument_list|()
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|copyFromBase
argument_list|(
name|value
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|rowIdProjected
condition|)
block|{
name|int
name|ix
init|=
name|rbCtx
operator|.
name|findVirtualColumnNum
argument_list|(
name|VirtualColumn
operator|.
name|ROWID
argument_list|)
decl_stmt|;
name|value
operator|.
name|cols
index|[
name|ix
index|]
operator|=
name|recordIdColumnVector
expr_stmt|;
block|}
name|progress
operator|=
name|baseReader
operator|.
name|getProgress
argument_list|()
expr_stmt|;
return|return
literal|true
return|;
block|}
comment|//get the 'data' cols and set in value as individual ColumnVector, then get
comment|//ColumnVectors for acid meta cols to create a single ColumnVector
comment|//representing RecordIdentifier and (optionally) set it in 'value'
specifier|private
name|void
name|copyFromBase
parameter_list|(
name|VectorizedRowBatch
name|value
parameter_list|)
block|{
assert|assert
operator|!
name|isOriginal
assert|;
if|if
condition|(
name|isFlatPayload
condition|)
block|{
name|int
name|payloadCol
init|=
name|includeAcidColumns
condition|?
name|OrcRecordUpdater
operator|.
name|ROW
else|:
literal|0
decl_stmt|;
comment|// Ignore the struct column and just copy all the following data columns.
name|System
operator|.
name|arraycopy
argument_list|(
name|vectorizedRowBatchBase
operator|.
name|cols
argument_list|,
name|payloadCol
operator|+
literal|1
argument_list|,
name|value
operator|.
name|cols
argument_list|,
literal|0
argument_list|,
name|vectorizedRowBatchBase
operator|.
name|cols
operator|.
name|length
operator|-
name|payloadCol
operator|-
literal|1
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|StructColumnVector
name|payloadStruct
init|=
operator|(
name|StructColumnVector
operator|)
name|vectorizedRowBatchBase
operator|.
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|ROW
index|]
decl_stmt|;
comment|// Transfer columnVector objects from base batch to outgoing batch.
name|System
operator|.
name|arraycopy
argument_list|(
name|payloadStruct
operator|.
name|fields
argument_list|,
literal|0
argument_list|,
name|value
operator|.
name|cols
argument_list|,
literal|0
argument_list|,
name|value
operator|.
name|getDataColumnCount
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|rowIdProjected
condition|)
block|{
name|recordIdColumnVector
operator|.
name|fields
index|[
literal|0
index|]
operator|=
name|vectorizedRowBatchBase
operator|.
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|ORIGINAL_WRITEID
index|]
expr_stmt|;
name|recordIdColumnVector
operator|.
name|fields
index|[
literal|1
index|]
operator|=
name|vectorizedRowBatchBase
operator|.
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|BUCKET
index|]
expr_stmt|;
name|recordIdColumnVector
operator|.
name|fields
index|[
literal|2
index|]
operator|=
name|vectorizedRowBatchBase
operator|.
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|ROW_ID
index|]
expr_stmt|;
block|}
block|}
specifier|private
name|ColumnVector
index|[]
name|handleOriginalFile
parameter_list|(
name|BitSet
name|selectedBitSet
parameter_list|,
name|ColumnVector
index|[]
name|innerRecordIdColumnVector
parameter_list|)
throws|throws
name|IOException
block|{
comment|/*      * If there are deletes and reading original file, we must produce synthetic ROW_IDs in order      * to see if any deletes apply      */
name|boolean
name|needSyntheticRowId
init|=
name|needSyntheticRowIds
argument_list|(
literal|true
argument_list|,
operator|!
name|deleteEventRegistry
operator|.
name|isEmpty
argument_list|()
argument_list|,
name|rowIdProjected
argument_list|)
decl_stmt|;
if|if
condition|(
name|needSyntheticRowId
condition|)
block|{
assert|assert
name|syntheticProps
operator|!=
literal|null
operator|:
literal|""
operator|+
name|syntheticProps
assert|;
assert|assert
name|syntheticProps
operator|.
name|getRowIdOffset
argument_list|()
operator|>=
literal|0
operator|:
literal|""
operator|+
name|syntheticProps
assert|;
assert|assert
name|syntheticProps
operator|.
name|getBucketProperty
argument_list|()
operator|>=
literal|0
operator|:
literal|""
operator|+
name|syntheticProps
assert|;
if|if
condition|(
name|innerReader
operator|==
literal|null
condition|)
block|{
throw|throw
operator|new
name|IllegalStateException
argument_list|(
name|getClass
argument_list|()
operator|.
name|getName
argument_list|()
operator|+
literal|" requires "
operator|+
name|org
operator|.
name|apache
operator|.
name|orc
operator|.
name|RecordReader
operator|.
name|class
operator|+
literal|" to handle original files that require ROW__IDs: "
operator|+
name|rootPath
argument_list|)
throw|;
block|}
comment|/**        * {@link RecordIdentifier#getWriteId()}        */
name|recordIdColumnVector
operator|.
name|fields
index|[
literal|0
index|]
operator|.
name|noNulls
operator|=
literal|true
expr_stmt|;
name|recordIdColumnVector
operator|.
name|fields
index|[
literal|0
index|]
operator|.
name|isRepeating
operator|=
literal|true
expr_stmt|;
operator|(
operator|(
name|LongColumnVector
operator|)
name|recordIdColumnVector
operator|.
name|fields
index|[
literal|0
index|]
operator|)
operator|.
name|vector
index|[
literal|0
index|]
operator|=
name|syntheticProps
operator|.
name|getSyntheticWriteId
argument_list|()
expr_stmt|;
comment|/**        * This is {@link RecordIdentifier#getBucketProperty()}        * Also see {@link BucketCodec}        */
name|recordIdColumnVector
operator|.
name|fields
index|[
literal|1
index|]
operator|.
name|noNulls
operator|=
literal|true
expr_stmt|;
name|recordIdColumnVector
operator|.
name|fields
index|[
literal|1
index|]
operator|.
name|isRepeating
operator|=
literal|true
expr_stmt|;
operator|(
operator|(
name|LongColumnVector
operator|)
name|recordIdColumnVector
operator|.
name|fields
index|[
literal|1
index|]
operator|)
operator|.
name|vector
index|[
literal|0
index|]
operator|=
name|syntheticProps
operator|.
name|getBucketProperty
argument_list|()
expr_stmt|;
comment|/**        * {@link RecordIdentifier#getRowId()}        */
name|recordIdColumnVector
operator|.
name|fields
index|[
literal|2
index|]
operator|.
name|noNulls
operator|=
literal|true
expr_stmt|;
name|recordIdColumnVector
operator|.
name|fields
index|[
literal|2
index|]
operator|.
name|isRepeating
operator|=
literal|false
expr_stmt|;
name|long
index|[]
name|rowIdVector
init|=
operator|(
operator|(
name|LongColumnVector
operator|)
name|recordIdColumnVector
operator|.
name|fields
index|[
literal|2
index|]
operator|)
operator|.
name|vector
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|vectorizedRowBatchBase
operator|.
name|size
condition|;
name|i
operator|++
control|)
block|{
comment|//baseReader.getRowNumber() seems to point at the start of the batch todo: validate
name|rowIdVector
index|[
name|i
index|]
operator|=
name|syntheticProps
operator|.
name|getRowIdOffset
argument_list|()
operator|+
name|innerReader
operator|.
name|getRowNumber
argument_list|()
operator|+
name|i
expr_stmt|;
block|}
comment|//Now populate a structure to use to apply delete events
name|innerRecordIdColumnVector
operator|=
operator|new
name|ColumnVector
index|[
name|OrcRecordUpdater
operator|.
name|FIELDS
index|]
expr_stmt|;
name|innerRecordIdColumnVector
index|[
name|OrcRecordUpdater
operator|.
name|ORIGINAL_WRITEID
index|]
operator|=
name|recordIdColumnVector
operator|.
name|fields
index|[
literal|0
index|]
expr_stmt|;
name|innerRecordIdColumnVector
index|[
name|OrcRecordUpdater
operator|.
name|BUCKET
index|]
operator|=
name|recordIdColumnVector
operator|.
name|fields
index|[
literal|1
index|]
expr_stmt|;
name|innerRecordIdColumnVector
index|[
name|OrcRecordUpdater
operator|.
name|ROW_ID
index|]
operator|=
name|recordIdColumnVector
operator|.
name|fields
index|[
literal|2
index|]
expr_stmt|;
comment|//these are insert events so (original txn == current) txn for all rows
name|innerRecordIdColumnVector
index|[
name|OrcRecordUpdater
operator|.
name|CURRENT_WRITEID
index|]
operator|=
name|recordIdColumnVector
operator|.
name|fields
index|[
literal|0
index|]
expr_stmt|;
block|}
if|if
condition|(
name|syntheticProps
operator|.
name|getSyntheticWriteId
argument_list|()
operator|>
literal|0
condition|)
block|{
comment|//"originals" (written before table was converted to acid) is considered written by
comment|// writeid:0 which is always committed so there is no need to check wrt invalid write Ids
comment|//But originals written by Load Data for example can be in base_x or delta_x_x so we must
comment|//check if 'x' is committed or not evn if ROW_ID is not needed in the Operator pipeline.
if|if
condition|(
name|needSyntheticRowId
condition|)
block|{
name|findRecordsWithInvalidWriteIds
argument_list|(
name|innerRecordIdColumnVector
argument_list|,
name|vectorizedRowBatchBase
operator|.
name|size
argument_list|,
name|selectedBitSet
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|/*since ROW_IDs are not needed we didn't create the ColumnVectors to hold them but we         * still have to check if the data being read is committed as far as current         * reader (transactions) is concerned.  Since here we are reading 'original' schema file,         * all rows in it have been created by the same txn, namely 'syntheticProps.syntheticWriteId'         */
if|if
condition|(
operator|!
name|validWriteIdList
operator|.
name|isWriteIdValid
argument_list|(
name|syntheticProps
operator|.
name|getSyntheticWriteId
argument_list|()
argument_list|)
condition|)
block|{
name|selectedBitSet
operator|.
name|clear
argument_list|(
literal|0
argument_list|,
name|vectorizedRowBatchBase
operator|.
name|size
argument_list|)
expr_stmt|;
block|}
block|}
block|}
return|return
name|innerRecordIdColumnVector
return|;
block|}
specifier|private
name|void
name|findRecordsWithInvalidWriteIds
parameter_list|(
name|VectorizedRowBatch
name|batch
parameter_list|,
name|BitSet
name|selectedBitSet
parameter_list|)
block|{
name|findRecordsWithInvalidWriteIds
argument_list|(
name|batch
operator|.
name|cols
argument_list|,
name|batch
operator|.
name|size
argument_list|,
name|selectedBitSet
argument_list|)
expr_stmt|;
block|}
specifier|private
name|void
name|findRecordsWithInvalidWriteIds
parameter_list|(
name|ColumnVector
index|[]
name|cols
parameter_list|,
name|int
name|size
parameter_list|,
name|BitSet
name|selectedBitSet
parameter_list|)
block|{
if|if
condition|(
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|CURRENT_WRITEID
index|]
operator|.
name|isRepeating
condition|)
block|{
comment|// When we have repeating values, we can unset the whole bitset at once
comment|// if the repeating value is not a valid write id.
name|long
name|currentWriteIdForBatch
init|=
operator|(
operator|(
name|LongColumnVector
operator|)
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|CURRENT_WRITEID
index|]
operator|)
operator|.
name|vector
index|[
literal|0
index|]
decl_stmt|;
if|if
condition|(
operator|!
name|validWriteIdList
operator|.
name|isWriteIdValid
argument_list|(
name|currentWriteIdForBatch
argument_list|)
condition|)
block|{
name|selectedBitSet
operator|.
name|clear
argument_list|(
literal|0
argument_list|,
name|size
argument_list|)
expr_stmt|;
block|}
return|return;
block|}
name|long
index|[]
name|currentWriteIdVector
init|=
operator|(
operator|(
name|LongColumnVector
operator|)
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|CURRENT_WRITEID
index|]
operator|)
operator|.
name|vector
decl_stmt|;
comment|// Loop through the bits that are set to true and mark those rows as false, if their
comment|// current write ids are not valid.
for|for
control|(
name|int
name|setBitIndex
init|=
name|selectedBitSet
operator|.
name|nextSetBit
argument_list|(
literal|0
argument_list|)
init|;
name|setBitIndex
operator|>=
literal|0
condition|;
name|setBitIndex
operator|=
name|selectedBitSet
operator|.
name|nextSetBit
argument_list|(
name|setBitIndex
operator|+
literal|1
argument_list|)
control|)
block|{
if|if
condition|(
operator|!
name|validWriteIdList
operator|.
name|isWriteIdValid
argument_list|(
name|currentWriteIdVector
index|[
name|setBitIndex
index|]
argument_list|)
condition|)
block|{
name|selectedBitSet
operator|.
name|clear
argument_list|(
name|setBitIndex
argument_list|)
expr_stmt|;
block|}
block|}
block|}
annotation|@
name|Override
specifier|public
name|NullWritable
name|createKey
parameter_list|()
block|{
return|return
name|NullWritable
operator|.
name|get
argument_list|()
return|;
block|}
annotation|@
name|Override
specifier|public
name|VectorizedRowBatch
name|createValue
parameter_list|()
block|{
return|return
name|rbCtx
operator|.
name|createVectorizedRowBatch
argument_list|()
return|;
block|}
annotation|@
name|Override
specifier|public
name|long
name|getPos
parameter_list|()
throws|throws
name|IOException
block|{
return|return
name|offset
operator|+
call|(
name|long
call|)
argument_list|(
name|progress
operator|*
name|length
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|close
parameter_list|()
throws|throws
name|IOException
block|{
try|try
block|{
name|this
operator|.
name|baseReader
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
finally|finally
block|{
name|this
operator|.
name|deleteEventRegistry
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
annotation|@
name|Override
specifier|public
name|float
name|getProgress
parameter_list|()
throws|throws
name|IOException
block|{
return|return
name|progress
return|;
block|}
annotation|@
name|VisibleForTesting
name|DeleteEventRegistry
name|getDeleteEventRegistry
parameter_list|()
block|{
return|return
name|deleteEventRegistry
return|;
block|}
comment|/**    * An interface that can determine which rows have been deleted    * from a given vectorized row batch. Implementations of this interface    * will read the delete delta files and will create their own internal    * data structures to maintain record ids of the records that got deleted.    */
specifier|protected
interface|interface
name|DeleteEventRegistry
block|{
comment|/**      * Modifies the passed bitset to indicate which of the rows in the batch      * have been deleted. Assumes that the batch.size is equal to bitset size.      * @param cols      * @param size      * @param selectedBitSet      * @throws IOException      */
specifier|public
name|void
name|findDeletedRecords
parameter_list|(
name|ColumnVector
index|[]
name|cols
parameter_list|,
name|int
name|size
parameter_list|,
name|BitSet
name|selectedBitSet
parameter_list|)
throws|throws
name|IOException
function_decl|;
comment|/**      * The close() method can be called externally to signal the implementing classes      * to free up resources.      * @throws IOException      */
specifier|public
name|void
name|close
parameter_list|()
throws|throws
name|IOException
function_decl|;
comment|/**      * @return {@code true} if no delete events were found      */
name|boolean
name|isEmpty
parameter_list|()
function_decl|;
block|}
comment|/**    * An implementation for DeleteEventRegistry that opens the delete delta files all    * at once, and then uses the sort-merge algorithm to maintain a sorted list of    * delete events. This internally uses the OrcRawRecordMerger and maintains a constant    * amount of memory usage, given the number of delete delta files. Therefore, this    * implementation will be picked up when the memory pressure is high.    *    * Don't bother to use KeyInterval from split here because since this doesn't    * buffer delete events in memory.    */
specifier|static
class|class
name|SortMergedDeleteEventRegistry
implements|implements
name|DeleteEventRegistry
block|{
specifier|private
name|OrcRawRecordMerger
name|deleteRecords
decl_stmt|;
specifier|private
name|OrcRawRecordMerger
operator|.
name|ReaderKey
name|deleteRecordKey
decl_stmt|;
specifier|private
name|OrcStruct
name|deleteRecordValue
decl_stmt|;
specifier|private
name|Boolean
name|isDeleteRecordAvailable
init|=
literal|null
decl_stmt|;
specifier|private
name|ValidWriteIdList
name|validWriteIdList
decl_stmt|;
name|SortMergedDeleteEventRegistry
parameter_list|(
name|JobConf
name|conf
parameter_list|,
name|OrcSplit
name|orcSplit
parameter_list|,
name|Reader
operator|.
name|Options
name|readerOptions
parameter_list|)
throws|throws
name|IOException
block|{
specifier|final
name|Path
index|[]
name|deleteDeltas
init|=
name|getDeleteDeltaDirsFromSplit
argument_list|(
name|orcSplit
argument_list|)
decl_stmt|;
if|if
condition|(
name|deleteDeltas
operator|.
name|length
operator|>
literal|0
condition|)
block|{
name|int
name|bucket
init|=
name|AcidUtils
operator|.
name|parseBucketId
argument_list|(
name|orcSplit
operator|.
name|getPath
argument_list|()
argument_list|)
decl_stmt|;
name|String
name|txnString
init|=
name|conf
operator|.
name|get
argument_list|(
name|ValidWriteIdList
operator|.
name|VALID_WRITEIDS_KEY
argument_list|)
decl_stmt|;
name|this
operator|.
name|validWriteIdList
operator|=
operator|(
name|txnString
operator|==
literal|null
operator|)
condition|?
operator|new
name|ValidReaderWriteIdList
argument_list|()
else|:
operator|new
name|ValidReaderWriteIdList
argument_list|(
name|txnString
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Using SortMergedDeleteEventRegistry"
argument_list|)
expr_stmt|;
name|OrcRawRecordMerger
operator|.
name|Options
name|mergerOptions
init|=
operator|new
name|OrcRawRecordMerger
operator|.
name|Options
argument_list|()
operator|.
name|isDeleteReader
argument_list|(
literal|true
argument_list|)
decl_stmt|;
assert|assert
operator|!
name|orcSplit
operator|.
name|isOriginal
argument_list|()
operator|:
literal|"If this now supports Original splits, set up mergeOptions properly"
assert|;
name|this
operator|.
name|deleteRecords
operator|=
operator|new
name|OrcRawRecordMerger
argument_list|(
name|conf
argument_list|,
literal|true
argument_list|,
literal|null
argument_list|,
literal|false
argument_list|,
name|bucket
argument_list|,
name|validWriteIdList
argument_list|,
name|readerOptions
argument_list|,
name|deleteDeltas
argument_list|,
name|mergerOptions
argument_list|)
expr_stmt|;
name|this
operator|.
name|deleteRecordKey
operator|=
operator|new
name|OrcRawRecordMerger
operator|.
name|ReaderKey
argument_list|()
expr_stmt|;
name|this
operator|.
name|deleteRecordValue
operator|=
name|this
operator|.
name|deleteRecords
operator|.
name|createValue
argument_list|()
expr_stmt|;
comment|// Initialize the first value in the delete reader.
name|this
operator|.
name|isDeleteRecordAvailable
operator|=
name|this
operator|.
name|deleteRecords
operator|.
name|next
argument_list|(
name|deleteRecordKey
argument_list|,
name|deleteRecordValue
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|this
operator|.
name|isDeleteRecordAvailable
operator|=
literal|false
expr_stmt|;
name|this
operator|.
name|deleteRecordKey
operator|=
literal|null
expr_stmt|;
name|this
operator|.
name|deleteRecordValue
operator|=
literal|null
expr_stmt|;
name|this
operator|.
name|deleteRecords
operator|=
literal|null
expr_stmt|;
block|}
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|isEmpty
parameter_list|()
block|{
if|if
condition|(
name|isDeleteRecordAvailable
operator|==
literal|null
condition|)
block|{
throw|throw
operator|new
name|IllegalStateException
argument_list|(
literal|"Not yet initialized"
argument_list|)
throw|;
block|}
return|return
operator|!
name|isDeleteRecordAvailable
return|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|findDeletedRecords
parameter_list|(
name|ColumnVector
index|[]
name|cols
parameter_list|,
name|int
name|size
parameter_list|,
name|BitSet
name|selectedBitSet
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
operator|!
name|isDeleteRecordAvailable
condition|)
block|{
return|return;
block|}
name|long
index|[]
name|originalWriteId
init|=
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|ORIGINAL_WRITEID
index|]
operator|.
name|isRepeating
condition|?
literal|null
else|:
operator|(
operator|(
name|LongColumnVector
operator|)
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|ORIGINAL_WRITEID
index|]
operator|)
operator|.
name|vector
decl_stmt|;
name|long
index|[]
name|bucket
init|=
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|BUCKET
index|]
operator|.
name|isRepeating
condition|?
literal|null
else|:
operator|(
operator|(
name|LongColumnVector
operator|)
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|BUCKET
index|]
operator|)
operator|.
name|vector
decl_stmt|;
name|long
index|[]
name|rowId
init|=
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|ROW_ID
index|]
operator|.
name|isRepeating
condition|?
literal|null
else|:
operator|(
operator|(
name|LongColumnVector
operator|)
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|ROW_ID
index|]
operator|)
operator|.
name|vector
decl_stmt|;
comment|// The following repeatedX values will be set, if any of the columns are repeating.
name|long
name|repeatedOriginalWriteId
init|=
operator|(
name|originalWriteId
operator|!=
literal|null
operator|)
condition|?
operator|-
literal|1
else|:
operator|(
operator|(
name|LongColumnVector
operator|)
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|ORIGINAL_WRITEID
index|]
operator|)
operator|.
name|vector
index|[
literal|0
index|]
decl_stmt|;
name|long
name|repeatedBucket
init|=
operator|(
name|bucket
operator|!=
literal|null
operator|)
condition|?
operator|-
literal|1
else|:
operator|(
operator|(
name|LongColumnVector
operator|)
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|BUCKET
index|]
operator|)
operator|.
name|vector
index|[
literal|0
index|]
decl_stmt|;
name|long
name|repeatedRowId
init|=
operator|(
name|rowId
operator|!=
literal|null
operator|)
condition|?
operator|-
literal|1
else|:
operator|(
operator|(
name|LongColumnVector
operator|)
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|ROW_ID
index|]
operator|)
operator|.
name|vector
index|[
literal|0
index|]
decl_stmt|;
comment|// Get the first valid row in the batch still available.
name|int
name|firstValidIndex
init|=
name|selectedBitSet
operator|.
name|nextSetBit
argument_list|(
literal|0
argument_list|)
decl_stmt|;
if|if
condition|(
name|firstValidIndex
operator|==
operator|-
literal|1
condition|)
block|{
return|return;
comment|// Everything in the batch has already been filtered out.
block|}
name|RecordIdentifier
name|firstRecordIdInBatch
init|=
operator|new
name|RecordIdentifier
argument_list|(
name|originalWriteId
operator|!=
literal|null
condition|?
name|originalWriteId
index|[
name|firstValidIndex
index|]
else|:
name|repeatedOriginalWriteId
argument_list|,
name|bucket
operator|!=
literal|null
condition|?
operator|(
name|int
operator|)
name|bucket
index|[
name|firstValidIndex
index|]
else|:
operator|(
name|int
operator|)
name|repeatedBucket
argument_list|,
name|rowId
operator|!=
literal|null
condition|?
operator|(
name|int
operator|)
name|rowId
index|[
name|firstValidIndex
index|]
else|:
name|repeatedRowId
argument_list|)
decl_stmt|;
comment|// Get the last valid row in the batch still available.
name|int
name|lastValidIndex
init|=
name|selectedBitSet
operator|.
name|previousSetBit
argument_list|(
name|size
operator|-
literal|1
argument_list|)
decl_stmt|;
name|RecordIdentifier
name|lastRecordIdInBatch
init|=
operator|new
name|RecordIdentifier
argument_list|(
name|originalWriteId
operator|!=
literal|null
condition|?
name|originalWriteId
index|[
name|lastValidIndex
index|]
else|:
name|repeatedOriginalWriteId
argument_list|,
name|bucket
operator|!=
literal|null
condition|?
operator|(
name|int
operator|)
name|bucket
index|[
name|lastValidIndex
index|]
else|:
operator|(
name|int
operator|)
name|repeatedBucket
argument_list|,
name|rowId
operator|!=
literal|null
condition|?
operator|(
name|int
operator|)
name|rowId
index|[
name|lastValidIndex
index|]
else|:
name|repeatedRowId
argument_list|)
decl_stmt|;
comment|// We must iterate over all the delete records, until we find one record with
comment|// deleteRecord>= firstRecordInBatch or until we exhaust all the delete records.
while|while
condition|(
name|deleteRecordKey
operator|.
name|compareRow
argument_list|(
name|firstRecordIdInBatch
argument_list|)
operator|==
operator|-
literal|1
condition|)
block|{
name|isDeleteRecordAvailable
operator|=
name|deleteRecords
operator|.
name|next
argument_list|(
name|deleteRecordKey
argument_list|,
name|deleteRecordValue
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|isDeleteRecordAvailable
condition|)
return|return;
comment|// exhausted all delete records, return.
block|}
comment|// If we are here, then we have established that firstRecordInBatch<= deleteRecord.
comment|// Now continue marking records which have been deleted until we reach the end of the batch
comment|// or we exhaust all the delete records.
name|int
name|currIndex
init|=
name|firstValidIndex
decl_stmt|;
name|RecordIdentifier
name|currRecordIdInBatch
init|=
operator|new
name|RecordIdentifier
argument_list|()
decl_stmt|;
while|while
condition|(
name|isDeleteRecordAvailable
operator|&&
name|currIndex
operator|!=
operator|-
literal|1
operator|&&
name|currIndex
operator|<=
name|lastValidIndex
condition|)
block|{
name|currRecordIdInBatch
operator|.
name|setValues
argument_list|(
operator|(
name|originalWriteId
operator|!=
literal|null
operator|)
condition|?
name|originalWriteId
index|[
name|currIndex
index|]
else|:
name|repeatedOriginalWriteId
argument_list|,
operator|(
name|bucket
operator|!=
literal|null
operator|)
condition|?
operator|(
name|int
operator|)
name|bucket
index|[
name|currIndex
index|]
else|:
operator|(
name|int
operator|)
name|repeatedBucket
argument_list|,
operator|(
name|rowId
operator|!=
literal|null
operator|)
condition|?
name|rowId
index|[
name|currIndex
index|]
else|:
name|repeatedRowId
argument_list|)
expr_stmt|;
if|if
condition|(
name|deleteRecordKey
operator|.
name|compareRow
argument_list|(
name|currRecordIdInBatch
argument_list|)
operator|==
literal|0
condition|)
block|{
comment|// When deleteRecordId == currRecordIdInBatch, this record in the batch has been deleted.
name|selectedBitSet
operator|.
name|clear
argument_list|(
name|currIndex
argument_list|)
expr_stmt|;
name|currIndex
operator|=
name|selectedBitSet
operator|.
name|nextSetBit
argument_list|(
name|currIndex
operator|+
literal|1
argument_list|)
expr_stmt|;
comment|// Move to next valid index.
block|}
elseif|else
if|if
condition|(
name|deleteRecordKey
operator|.
name|compareRow
argument_list|(
name|currRecordIdInBatch
argument_list|)
operator|==
literal|1
condition|)
block|{
comment|// When deleteRecordId> currRecordIdInBatch, we have to move on to look at the
comment|// next record in the batch.
comment|// But before that, can we short-circuit and skip the entire batch itself
comment|// by checking if the deleteRecordId> lastRecordInBatch?
if|if
condition|(
name|deleteRecordKey
operator|.
name|compareRow
argument_list|(
name|lastRecordIdInBatch
argument_list|)
operator|==
literal|1
condition|)
block|{
return|return;
comment|// Yay! We short-circuited, skip everything remaining in the batch and return.
block|}
name|currIndex
operator|=
name|selectedBitSet
operator|.
name|nextSetBit
argument_list|(
name|currIndex
operator|+
literal|1
argument_list|)
expr_stmt|;
comment|// Move to next valid index.
block|}
else|else
block|{
comment|// We have deleteRecordId< currRecordIdInBatch, we must now move on to find
comment|// next the larger deleteRecordId that can possibly match anything in the batch.
name|isDeleteRecordAvailable
operator|=
name|deleteRecords
operator|.
name|next
argument_list|(
name|deleteRecordKey
argument_list|,
name|deleteRecordValue
argument_list|)
expr_stmt|;
block|}
block|}
block|}
annotation|@
name|Override
specifier|public
name|void
name|close
parameter_list|()
throws|throws
name|IOException
block|{
if|if
condition|(
name|this
operator|.
name|deleteRecords
operator|!=
literal|null
condition|)
block|{
name|this
operator|.
name|deleteRecords
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
block|}
comment|/**    * An implementation for DeleteEventRegistry that optimizes for performance by loading    * all the delete events into memory at once from all the delete delta files.    * It starts by reading all the delete events through a regular sort merge logic    * into 3 vectors- one for original Write id (owid), one for bucket property and one for    * row id.  See {@link BucketCodec} for more about bucket property.    * The owids are likely to be repeated very often, as a single transaction    * often deletes thousands of rows. Hence, the owid vector is compressed to only store the    * toIndex and fromIndex ranges in the larger row id vector. Now, querying whether a    * record id is deleted or not, is done by performing a binary search on the    * compressed owid range. If a match is found, then a binary search is then performed on    * the larger rowId vector between the given toIndex and fromIndex. Of course, there is rough    * heuristic that prevents creation of an instance of this class if the memory pressure is high.    * The SortMergedDeleteEventRegistry is then the fallback method for such scenarios.    */
specifier|static
class|class
name|ColumnizedDeleteEventRegistry
implements|implements
name|DeleteEventRegistry
block|{
comment|/**      * A simple wrapper class to hold the (owid, bucketProperty, rowId) pair.      */
specifier|static
class|class
name|DeleteRecordKey
implements|implements
name|Comparable
argument_list|<
name|DeleteRecordKey
argument_list|>
block|{
specifier|private
specifier|static
specifier|final
name|DeleteRecordKey
name|otherKey
init|=
operator|new
name|DeleteRecordKey
argument_list|()
decl_stmt|;
specifier|private
name|long
name|originalWriteId
decl_stmt|;
comment|/**        * see {@link BucketCodec}        */
specifier|private
name|int
name|bucketProperty
decl_stmt|;
specifier|private
name|long
name|rowId
decl_stmt|;
name|DeleteRecordKey
parameter_list|()
block|{
name|this
operator|.
name|originalWriteId
operator|=
operator|-
literal|1
expr_stmt|;
name|this
operator|.
name|rowId
operator|=
operator|-
literal|1
expr_stmt|;
block|}
specifier|public
name|void
name|set
parameter_list|(
name|long
name|owid
parameter_list|,
name|int
name|bucketProperty
parameter_list|,
name|long
name|rowId
parameter_list|)
block|{
name|this
operator|.
name|originalWriteId
operator|=
name|owid
expr_stmt|;
name|this
operator|.
name|bucketProperty
operator|=
name|bucketProperty
expr_stmt|;
name|this
operator|.
name|rowId
operator|=
name|rowId
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|int
name|compareTo
parameter_list|(
name|DeleteRecordKey
name|other
parameter_list|)
block|{
if|if
condition|(
name|other
operator|==
literal|null
condition|)
block|{
return|return
operator|-
literal|1
return|;
block|}
if|if
condition|(
name|originalWriteId
operator|!=
name|other
operator|.
name|originalWriteId
condition|)
block|{
return|return
name|originalWriteId
operator|<
name|other
operator|.
name|originalWriteId
condition|?
operator|-
literal|1
else|:
literal|1
return|;
block|}
if|if
condition|(
name|bucketProperty
operator|!=
name|other
operator|.
name|bucketProperty
condition|)
block|{
return|return
name|bucketProperty
operator|<
name|other
operator|.
name|bucketProperty
condition|?
operator|-
literal|1
else|:
literal|1
return|;
block|}
if|if
condition|(
name|rowId
operator|!=
name|other
operator|.
name|rowId
condition|)
block|{
return|return
name|rowId
operator|<
name|other
operator|.
name|rowId
condition|?
operator|-
literal|1
else|:
literal|1
return|;
block|}
return|return
literal|0
return|;
block|}
specifier|private
name|int
name|compareTo
parameter_list|(
name|RecordIdentifier
name|other
parameter_list|)
block|{
if|if
condition|(
name|other
operator|==
literal|null
condition|)
block|{
return|return
operator|-
literal|1
return|;
block|}
name|otherKey
operator|.
name|set
argument_list|(
name|other
operator|.
name|getWriteId
argument_list|()
argument_list|,
name|other
operator|.
name|getBucketProperty
argument_list|()
argument_list|,
name|other
operator|.
name|getRowId
argument_list|()
argument_list|)
expr_stmt|;
return|return
name|compareTo
argument_list|(
name|otherKey
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|public
name|String
name|toString
parameter_list|()
block|{
return|return
literal|"DeleteRecordKey("
operator|+
name|originalWriteId
operator|+
literal|","
operator|+
name|RecordIdentifier
operator|.
name|bucketToString
argument_list|(
name|bucketProperty
argument_list|)
operator|+
literal|","
operator|+
name|rowId
operator|+
literal|")"
return|;
block|}
block|}
comment|/**      * This class actually reads the delete delta files in vectorized row batches.      * For every call to next(), it returns the next smallest record id in the file if available.      * Internally, the next() buffers a row batch and maintains an index pointer, reading the      * next batch when the previous batch is exhausted.      *      * For unbucketed tables this will currently return all delete events.  Once we trust that      * the N in bucketN for "base" spit is reliable, all delete events not matching N can be skipped.      */
specifier|static
class|class
name|DeleteReaderValue
block|{
specifier|private
name|VectorizedRowBatch
name|batch
decl_stmt|;
specifier|private
specifier|final
name|RecordReader
name|recordReader
decl_stmt|;
specifier|private
name|int
name|indexPtrInBatch
decl_stmt|;
specifier|private
specifier|final
name|int
name|bucketForSplit
decl_stmt|;
comment|// The bucket value should be same for all the records.
specifier|private
specifier|final
name|ValidWriteIdList
name|validWriteIdList
decl_stmt|;
specifier|private
name|boolean
name|isBucketPropertyRepeating
decl_stmt|;
specifier|private
specifier|final
name|boolean
name|isBucketedTable
decl_stmt|;
specifier|private
specifier|final
name|Reader
name|reader
decl_stmt|;
specifier|private
specifier|final
name|Path
name|deleteDeltaFile
decl_stmt|;
specifier|private
specifier|final
name|OrcRawRecordMerger
operator|.
name|KeyInterval
name|keyInterval
decl_stmt|;
specifier|private
specifier|final
name|OrcSplit
name|orcSplit
decl_stmt|;
comment|/**        * total number in the file        */
specifier|private
specifier|final
name|long
name|numEvents
decl_stmt|;
comment|/**        * number of events lifted from disk        * some may be skipped due to PPD        */
specifier|private
name|long
name|numEventsFromDisk
init|=
literal|0
decl_stmt|;
comment|/**        * number of events actually loaded in memory        */
specifier|private
name|long
name|numEventsLoaded
init|=
literal|0
decl_stmt|;
name|DeleteReaderValue
parameter_list|(
name|Reader
name|deleteDeltaReader
parameter_list|,
name|Path
name|deleteDeltaFile
parameter_list|,
name|Reader
operator|.
name|Options
name|readerOptions
parameter_list|,
name|int
name|bucket
parameter_list|,
name|ValidWriteIdList
name|validWriteIdList
parameter_list|,
name|boolean
name|isBucketedTable
parameter_list|,
specifier|final
name|JobConf
name|conf
parameter_list|,
name|OrcRawRecordMerger
operator|.
name|KeyInterval
name|keyInterval
parameter_list|,
name|OrcSplit
name|orcSplit
parameter_list|)
throws|throws
name|IOException
block|{
name|this
operator|.
name|reader
operator|=
name|deleteDeltaReader
expr_stmt|;
name|this
operator|.
name|deleteDeltaFile
operator|=
name|deleteDeltaFile
expr_stmt|;
name|this
operator|.
name|recordReader
operator|=
name|deleteDeltaReader
operator|.
name|rowsOptions
argument_list|(
name|readerOptions
argument_list|,
name|conf
argument_list|)
expr_stmt|;
name|this
operator|.
name|bucketForSplit
operator|=
name|bucket
expr_stmt|;
specifier|final
name|boolean
name|useDecimal64ColumnVector
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|conf
argument_list|,
name|ConfVars
operator|.
name|HIVE_VECTORIZED_INPUT_FORMAT_SUPPORTS_ENABLED
argument_list|)
operator|.
name|equalsIgnoreCase
argument_list|(
literal|"decimal_64"
argument_list|)
decl_stmt|;
if|if
condition|(
name|useDecimal64ColumnVector
condition|)
block|{
name|this
operator|.
name|batch
operator|=
name|deleteDeltaReader
operator|.
name|getSchema
argument_list|()
operator|.
name|createRowBatchV2
argument_list|()
expr_stmt|;
block|}
else|else
block|{
name|this
operator|.
name|batch
operator|=
name|deleteDeltaReader
operator|.
name|getSchema
argument_list|()
operator|.
name|createRowBatch
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
operator|!
name|recordReader
operator|.
name|nextBatch
argument_list|(
name|batch
argument_list|)
condition|)
block|{
comment|// Read the first batch.
name|this
operator|.
name|batch
operator|=
literal|null
expr_stmt|;
comment|// Oh! the first batch itself was null. Close the reader.
block|}
name|this
operator|.
name|indexPtrInBatch
operator|=
literal|0
expr_stmt|;
name|this
operator|.
name|validWriteIdList
operator|=
name|validWriteIdList
expr_stmt|;
name|this
operator|.
name|isBucketedTable
operator|=
name|isBucketedTable
expr_stmt|;
if|if
condition|(
name|batch
operator|!=
literal|null
condition|)
block|{
name|checkBucketId
argument_list|()
expr_stmt|;
comment|//check 1st batch
block|}
name|this
operator|.
name|keyInterval
operator|=
name|keyInterval
expr_stmt|;
name|this
operator|.
name|orcSplit
operator|=
name|orcSplit
expr_stmt|;
name|this
operator|.
name|numEvents
operator|=
name|deleteDeltaReader
operator|.
name|getNumberOfRows
argument_list|()
expr_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Num events stats({},x,x)"
argument_list|,
name|numEvents
argument_list|)
expr_stmt|;
block|}
specifier|public
name|boolean
name|next
parameter_list|(
name|DeleteRecordKey
name|deleteRecordKey
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|batch
operator|==
literal|null
condition|)
block|{
return|return
literal|false
return|;
block|}
name|boolean
name|isValidNext
init|=
literal|false
decl_stmt|;
while|while
condition|(
operator|!
name|isValidNext
condition|)
block|{
if|if
condition|(
name|indexPtrInBatch
operator|>=
name|batch
operator|.
name|size
condition|)
block|{
comment|// We have exhausted our current batch, read the next batch.
if|if
condition|(
name|recordReader
operator|.
name|nextBatch
argument_list|(
name|batch
argument_list|)
condition|)
block|{
name|checkBucketId
argument_list|()
expr_stmt|;
name|indexPtrInBatch
operator|=
literal|0
expr_stmt|;
comment|// After reading the batch, reset the pointer to beginning.
block|}
else|else
block|{
return|return
literal|false
return|;
comment|// no more batches to read, exhausted the reader.
block|}
block|}
name|long
name|currentWriteId
init|=
name|setCurrentDeleteKey
argument_list|(
name|deleteRecordKey
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|isBucketPropertyRepeating
condition|)
block|{
name|checkBucketId
argument_list|(
name|deleteRecordKey
operator|.
name|bucketProperty
argument_list|)
expr_stmt|;
block|}
operator|++
name|indexPtrInBatch
expr_stmt|;
name|numEventsFromDisk
operator|++
expr_stmt|;
if|if
condition|(
operator|!
name|isDeleteEventInRange
argument_list|(
name|keyInterval
argument_list|,
name|deleteRecordKey
argument_list|)
condition|)
block|{
continue|continue;
block|}
if|if
condition|(
name|validWriteIdList
operator|.
name|isWriteIdValid
argument_list|(
name|currentWriteId
argument_list|)
condition|)
block|{
name|isValidNext
operator|=
literal|true
expr_stmt|;
block|}
block|}
name|numEventsLoaded
operator|++
expr_stmt|;
return|return
literal|true
return|;
block|}
specifier|static
name|boolean
name|isDeleteEventInRange
parameter_list|(
name|OrcRawRecordMerger
operator|.
name|KeyInterval
name|keyInterval
parameter_list|,
name|DeleteRecordKey
name|deleteRecordKey
parameter_list|)
block|{
if|if
condition|(
name|keyInterval
operator|.
name|getMinKey
argument_list|()
operator|!=
literal|null
operator|&&
name|deleteRecordKey
operator|.
name|compareTo
argument_list|(
name|keyInterval
operator|.
name|getMinKey
argument_list|()
argument_list|)
operator|<
literal|0
condition|)
block|{
comment|//current deleteEvent is< than minKey
return|return
literal|false
return|;
block|}
if|if
condition|(
name|keyInterval
operator|.
name|getMaxKey
argument_list|()
operator|!=
literal|null
operator|&&
name|deleteRecordKey
operator|.
name|compareTo
argument_list|(
name|keyInterval
operator|.
name|getMaxKey
argument_list|()
argument_list|)
operator|>
literal|0
condition|)
block|{
comment|//current deleteEvent is> than maxKey
return|return
literal|false
return|;
block|}
return|return
literal|true
return|;
block|}
specifier|public
name|void
name|close
parameter_list|()
throws|throws
name|IOException
block|{
name|this
operator|.
name|recordReader
operator|.
name|close
argument_list|()
expr_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Num events stats({},{},{})"
argument_list|,
name|numEvents
argument_list|,
name|numEventsFromDisk
argument_list|,
name|numEventsLoaded
argument_list|)
expr_stmt|;
block|}
specifier|private
name|long
name|setCurrentDeleteKey
parameter_list|(
name|DeleteRecordKey
name|deleteRecordKey
parameter_list|)
block|{
name|int
name|originalWriteIdIndex
init|=
name|batch
operator|.
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|ORIGINAL_WRITEID
index|]
operator|.
name|isRepeating
condition|?
literal|0
else|:
name|indexPtrInBatch
decl_stmt|;
name|long
name|originalWriteId
init|=
operator|(
operator|(
name|LongColumnVector
operator|)
name|batch
operator|.
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|ORIGINAL_WRITEID
index|]
operator|)
operator|.
name|vector
index|[
name|originalWriteIdIndex
index|]
decl_stmt|;
name|int
name|bucketPropertyIndex
init|=
name|batch
operator|.
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|BUCKET
index|]
operator|.
name|isRepeating
condition|?
literal|0
else|:
name|indexPtrInBatch
decl_stmt|;
name|int
name|bucketProperty
init|=
call|(
name|int
call|)
argument_list|(
operator|(
name|LongColumnVector
operator|)
name|batch
operator|.
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|BUCKET
index|]
argument_list|)
operator|.
name|vector
index|[
name|bucketPropertyIndex
index|]
decl_stmt|;
name|long
name|rowId
init|=
operator|(
operator|(
name|LongColumnVector
operator|)
name|batch
operator|.
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|ROW_ID
index|]
operator|)
operator|.
name|vector
index|[
name|indexPtrInBatch
index|]
decl_stmt|;
name|int
name|currentWriteIdIndex
init|=
name|batch
operator|.
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|CURRENT_WRITEID
index|]
operator|.
name|isRepeating
condition|?
literal|0
else|:
name|indexPtrInBatch
decl_stmt|;
name|long
name|currentWriteId
init|=
operator|(
operator|(
name|LongColumnVector
operator|)
name|batch
operator|.
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|CURRENT_WRITEID
index|]
operator|)
operator|.
name|vector
index|[
name|currentWriteIdIndex
index|]
decl_stmt|;
name|deleteRecordKey
operator|.
name|set
argument_list|(
name|originalWriteId
argument_list|,
name|bucketProperty
argument_list|,
name|rowId
argument_list|)
expr_stmt|;
return|return
name|currentWriteId
return|;
block|}
specifier|private
name|void
name|checkBucketId
parameter_list|()
throws|throws
name|IOException
block|{
name|isBucketPropertyRepeating
operator|=
name|batch
operator|.
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|BUCKET
index|]
operator|.
name|isRepeating
expr_stmt|;
if|if
condition|(
name|isBucketPropertyRepeating
condition|)
block|{
name|int
name|bucketPropertyFromRecord
init|=
call|(
name|int
call|)
argument_list|(
operator|(
name|LongColumnVector
operator|)
name|batch
operator|.
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|BUCKET
index|]
argument_list|)
operator|.
name|vector
index|[
literal|0
index|]
decl_stmt|;
name|checkBucketId
argument_list|(
name|bucketPropertyFromRecord
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**        * Whenever we are reading a batch, we must ensure that all the records in the batch        * have the same bucket id as the bucket id of the split. If not, throw exception.        */
specifier|private
name|void
name|checkBucketId
parameter_list|(
name|int
name|bucketPropertyFromRecord
parameter_list|)
throws|throws
name|IOException
block|{
name|int
name|bucketIdFromRecord
init|=
name|BucketCodec
operator|.
name|determineVersion
argument_list|(
name|bucketPropertyFromRecord
argument_list|)
operator|.
name|decodeWriterId
argument_list|(
name|bucketPropertyFromRecord
argument_list|)
decl_stmt|;
if|if
condition|(
name|bucketIdFromRecord
operator|!=
name|bucketForSplit
condition|)
block|{
name|DeleteRecordKey
name|dummy
init|=
operator|new
name|DeleteRecordKey
argument_list|()
decl_stmt|;
name|setCurrentDeleteKey
argument_list|(
name|dummy
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Corrupted records with different bucket ids "
operator|+
literal|"from the containing bucket file found! Expected bucket id "
operator|+
name|bucketForSplit
operator|+
literal|", however found "
operator|+
name|dummy
operator|+
literal|".  ("
operator|+
name|orcSplit
operator|+
literal|","
operator|+
name|deleteDeltaFile
operator|+
literal|")"
argument_list|)
throw|;
block|}
block|}
annotation|@
name|Override
specifier|public
name|String
name|toString
parameter_list|()
block|{
return|return
literal|"{reader="
operator|+
name|reader
operator|+
literal|", isBucketPropertyRepeating="
operator|+
name|isBucketPropertyRepeating
operator|+
literal|", bucketForSplit="
operator|+
name|bucketForSplit
operator|+
literal|", isBucketedTable="
operator|+
name|isBucketedTable
operator|+
literal|"}"
return|;
block|}
block|}
comment|/**      * A CompressedOwid class stores a compressed representation of the original      * write ids (owids) read from the delete delta files. Since the record ids      * are sorted by (owid, rowId) and owids are highly likely to be repetitive, it is      * efficient to compress them as a CompressedOwid that stores the fromIndex and      * the toIndex. These fromIndex and toIndex reference the larger vector formed by      * concatenating the correspondingly ordered rowIds.      */
specifier|private
specifier|final
class|class
name|CompressedOwid
implements|implements
name|Comparable
argument_list|<
name|CompressedOwid
argument_list|>
block|{
specifier|final
name|long
name|originalWriteId
decl_stmt|;
specifier|final
name|int
name|bucketProperty
decl_stmt|;
specifier|final
name|int
name|fromIndex
decl_stmt|;
comment|// inclusive
name|int
name|toIndex
decl_stmt|;
comment|// exclusive
name|CompressedOwid
parameter_list|(
name|long
name|owid
parameter_list|,
name|int
name|bucketProperty
parameter_list|,
name|int
name|fromIndex
parameter_list|,
name|int
name|toIndex
parameter_list|)
block|{
name|this
operator|.
name|originalWriteId
operator|=
name|owid
expr_stmt|;
name|this
operator|.
name|bucketProperty
operator|=
name|bucketProperty
expr_stmt|;
name|this
operator|.
name|fromIndex
operator|=
name|fromIndex
expr_stmt|;
name|this
operator|.
name|toIndex
operator|=
name|toIndex
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|int
name|compareTo
parameter_list|(
name|CompressedOwid
name|other
parameter_list|)
block|{
comment|// When comparing the CompressedOwid, the one with the lesser value is smaller.
if|if
condition|(
name|originalWriteId
operator|!=
name|other
operator|.
name|originalWriteId
condition|)
block|{
return|return
name|originalWriteId
operator|<
name|other
operator|.
name|originalWriteId
condition|?
operator|-
literal|1
else|:
literal|1
return|;
block|}
if|if
condition|(
name|bucketProperty
operator|!=
name|other
operator|.
name|bucketProperty
condition|)
block|{
return|return
name|bucketProperty
operator|<
name|other
operator|.
name|bucketProperty
condition|?
operator|-
literal|1
else|:
literal|1
return|;
block|}
return|return
literal|0
return|;
block|}
block|}
comment|/**      * Food for thought:      * this is a bit problematic - in order to load ColumnizedDeleteEventRegistry we still open      * all delete deltas at once - possibly causing OOM same as for {@link SortMergedDeleteEventRegistry}      * which uses {@link OrcRawRecordMerger}.  Why not load all delete_delta sequentially.  Each      * dd is sorted by {@link RecordIdentifier} so we could create a BTree like structure where the      * 1st level is an array of originalWriteId where each entry points at an array      * of bucketIds where each entry points at an array of rowIds.  We could probably use ArrayList      * to manage insertion as the structure is built (LinkedList?).  This should reduce memory      * footprint (as far as OrcReader to a single reader) - probably bad for LLAP IO      * Or much simpler, make compaction of delete deltas very aggressive so that      * we never have move than a few delete files to read.      */
specifier|private
name|TreeMap
argument_list|<
name|DeleteRecordKey
argument_list|,
name|DeleteReaderValue
argument_list|>
name|sortMerger
decl_stmt|;
specifier|private
name|long
name|rowIds
index|[]
decl_stmt|;
specifier|private
name|CompressedOwid
name|compressedOwids
index|[]
decl_stmt|;
specifier|private
name|ValidWriteIdList
name|validWriteIdList
decl_stmt|;
specifier|private
name|Boolean
name|isEmpty
decl_stmt|;
specifier|private
specifier|final
name|int
name|maxEventsInMemory
decl_stmt|;
specifier|private
specifier|final
name|OrcSplit
name|orcSplit
decl_stmt|;
specifier|private
specifier|final
name|boolean
name|testMode
decl_stmt|;
name|ColumnizedDeleteEventRegistry
parameter_list|(
name|JobConf
name|conf
parameter_list|,
name|OrcSplit
name|orcSplit
parameter_list|,
name|Reader
operator|.
name|Options
name|readerOptions
parameter_list|,
name|OrcRawRecordMerger
operator|.
name|KeyInterval
name|keyInterval
parameter_list|)
throws|throws
name|IOException
throws|,
name|DeleteEventsOverflowMemoryException
block|{
name|this
operator|.
name|testMode
operator|=
name|conf
operator|.
name|getBoolean
argument_list|(
name|ConfVars
operator|.
name|HIVE_IN_TEST
operator|.
name|varname
argument_list|,
literal|false
argument_list|)
expr_stmt|;
name|int
name|bucket
init|=
name|AcidUtils
operator|.
name|parseBucketId
argument_list|(
name|orcSplit
operator|.
name|getPath
argument_list|()
argument_list|)
decl_stmt|;
name|String
name|txnString
init|=
name|conf
operator|.
name|get
argument_list|(
name|ValidWriteIdList
operator|.
name|VALID_WRITEIDS_KEY
argument_list|)
decl_stmt|;
name|this
operator|.
name|validWriteIdList
operator|=
operator|(
name|txnString
operator|==
literal|null
operator|)
condition|?
operator|new
name|ValidReaderWriteIdList
argument_list|()
else|:
operator|new
name|ValidReaderWriteIdList
argument_list|(
name|txnString
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Using ColumnizedDeleteEventRegistry"
argument_list|)
expr_stmt|;
name|this
operator|.
name|sortMerger
operator|=
operator|new
name|TreeMap
argument_list|<>
argument_list|()
expr_stmt|;
name|this
operator|.
name|rowIds
operator|=
literal|null
expr_stmt|;
name|this
operator|.
name|compressedOwids
operator|=
literal|null
expr_stmt|;
name|maxEventsInMemory
operator|=
name|HiveConf
operator|.
name|getIntVar
argument_list|(
name|conf
argument_list|,
name|ConfVars
operator|.
name|HIVE_TRANSACTIONAL_NUM_EVENTS_IN_MEMORY
argument_list|)
expr_stmt|;
specifier|final
name|boolean
name|isBucketedTable
init|=
name|conf
operator|.
name|getInt
argument_list|(
name|hive_metastoreConstants
operator|.
name|BUCKET_COUNT
argument_list|,
literal|0
argument_list|)
operator|>
literal|0
decl_stmt|;
name|this
operator|.
name|orcSplit
operator|=
name|orcSplit
expr_stmt|;
try|try
block|{
specifier|final
name|Path
index|[]
name|deleteDeltaDirs
init|=
name|getDeleteDeltaDirsFromSplit
argument_list|(
name|orcSplit
argument_list|)
decl_stmt|;
if|if
condition|(
name|deleteDeltaDirs
operator|.
name|length
operator|>
literal|0
condition|)
block|{
name|int
name|totalDeleteEventCount
init|=
literal|0
decl_stmt|;
for|for
control|(
name|Path
name|deleteDeltaDir
range|:
name|deleteDeltaDirs
control|)
block|{
name|FileSystem
name|fs
init|=
name|deleteDeltaDir
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|Path
index|[]
name|deleteDeltaFiles
init|=
name|OrcRawRecordMerger
operator|.
name|getDeltaFiles
argument_list|(
name|deleteDeltaDir
argument_list|,
name|bucket
argument_list|,
operator|new
name|OrcRawRecordMerger
operator|.
name|Options
argument_list|()
operator|.
name|isCompacting
argument_list|(
literal|false
argument_list|)
argument_list|)
decl_stmt|;
for|for
control|(
name|Path
name|deleteDeltaFile
range|:
name|deleteDeltaFiles
control|)
block|{
comment|// NOTE: Calling last flush length below is more for future-proofing when we have
comment|// streaming deletes. But currently we don't support streaming deletes, and this can
comment|// be removed if this becomes a performance issue.
name|long
name|length
init|=
name|OrcAcidUtils
operator|.
name|getLastFlushLength
argument_list|(
name|fs
argument_list|,
name|deleteDeltaFile
argument_list|)
decl_stmt|;
comment|// NOTE: A check for existence of deleteDeltaFile is required because we may not have
comment|// deletes for the bucket being taken into consideration for this split processing.
if|if
condition|(
name|length
operator|!=
operator|-
literal|1
operator|&&
name|fs
operator|.
name|exists
argument_list|(
name|deleteDeltaFile
argument_list|)
condition|)
block|{
comment|/**                  * todo: we have OrcSplit.orcTail so we should be able to get stats from there                  */
name|Reader
name|deleteDeltaReader
init|=
name|OrcFile
operator|.
name|createReader
argument_list|(
name|deleteDeltaFile
argument_list|,
name|OrcFile
operator|.
name|readerOptions
argument_list|(
name|conf
argument_list|)
operator|.
name|maxLength
argument_list|(
name|length
argument_list|)
argument_list|)
decl_stmt|;
if|if
condition|(
name|deleteDeltaReader
operator|.
name|getNumberOfRows
argument_list|()
operator|<=
literal|0
condition|)
block|{
continue|continue;
comment|// just a safe check to ensure that we are not reading empty delete files.
block|}
name|totalDeleteEventCount
operator|+=
name|deleteDeltaReader
operator|.
name|getNumberOfRows
argument_list|()
expr_stmt|;
name|DeleteReaderValue
name|deleteReaderValue
init|=
operator|new
name|DeleteReaderValue
argument_list|(
name|deleteDeltaReader
argument_list|,
name|deleteDeltaFile
argument_list|,
name|readerOptions
argument_list|,
name|bucket
argument_list|,
name|validWriteIdList
argument_list|,
name|isBucketedTable
argument_list|,
name|conf
argument_list|,
name|keyInterval
argument_list|,
name|orcSplit
argument_list|)
decl_stmt|;
name|DeleteRecordKey
name|deleteRecordKey
init|=
operator|new
name|DeleteRecordKey
argument_list|()
decl_stmt|;
if|if
condition|(
name|deleteReaderValue
operator|.
name|next
argument_list|(
name|deleteRecordKey
argument_list|)
condition|)
block|{
name|sortMerger
operator|.
name|put
argument_list|(
name|deleteRecordKey
argument_list|,
name|deleteReaderValue
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|deleteReaderValue
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
block|}
block|}
name|readAllDeleteEventsFromDeleteDeltas
argument_list|()
expr_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Number of delete events(limit, actual)=({},{})"
argument_list|,
name|totalDeleteEventCount
argument_list|,
name|size
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|isEmpty
operator|=
name|compressedOwids
operator|==
literal|null
operator|||
name|rowIds
operator|==
literal|null
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
decl||
name|DeleteEventsOverflowMemoryException
name|e
parameter_list|)
block|{
name|close
argument_list|()
expr_stmt|;
comment|// close any open readers, if there was some exception during initialization.
throw|throw
name|e
throw|;
comment|// rethrow the exception so that the caller can handle.
block|}
block|}
specifier|private
name|void
name|checkSize
parameter_list|(
name|int
name|index
parameter_list|)
throws|throws
name|DeleteEventsOverflowMemoryException
block|{
if|if
condition|(
name|index
operator|>
name|maxEventsInMemory
condition|)
block|{
comment|//check to prevent OOM errors
name|LOG
operator|.
name|info
argument_list|(
literal|"Total number of delete events exceeds the maximum number of "
operator|+
literal|"delete events that can be loaded into memory for "
operator|+
name|orcSplit
operator|+
literal|". The max limit is currently set at "
operator|+
name|maxEventsInMemory
operator|+
literal|" and can be changed by setting the Hive config variable "
operator|+
name|ConfVars
operator|.
name|HIVE_TRANSACTIONAL_NUM_EVENTS_IN_MEMORY
operator|.
name|varname
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|DeleteEventsOverflowMemoryException
argument_list|()
throw|;
block|}
if|if
condition|(
name|index
operator|<
name|rowIds
operator|.
name|length
condition|)
block|{
return|return;
block|}
name|int
name|newLength
init|=
name|rowIds
operator|.
name|length
operator|+
literal|1000000
decl_stmt|;
if|if
condition|(
name|rowIds
operator|.
name|length
operator|<=
literal|1000000
condition|)
block|{
comment|//double small arrays; increase by 1M large arrays
name|newLength
operator|=
name|rowIds
operator|.
name|length
operator|*
literal|2
expr_stmt|;
block|}
name|rowIds
operator|=
name|Arrays
operator|.
name|copyOf
argument_list|(
name|rowIds
argument_list|,
name|newLength
argument_list|)
expr_stmt|;
block|}
comment|/**      * This is not done quite right.  The intent of {@link CompressedOwid} is a hedge against      * "delete from T" that generates a huge number of delete events possibly even 2G - max array      * size.  (assuming no one txn inserts> 2G rows (in a bucket)).  As implemented, the algorithm      * first loads all data into one array owid[] and rowIds[] which defeats the purpose.      * In practice we should be filtering delete evens by min/max ROW_ID from the split.  The later      * is also not yet implemented: HIVE-16812.      */
specifier|private
name|void
name|readAllDeleteEventsFromDeleteDeltas
parameter_list|()
throws|throws
name|IOException
throws|,
name|DeleteEventsOverflowMemoryException
block|{
if|if
condition|(
name|sortMerger
operator|==
literal|null
operator|||
name|sortMerger
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
return|return;
comment|// trivial case, nothing to read.
block|}
comment|// Initialize the rowId array when we have some delete events.
name|rowIds
operator|=
operator|new
name|long
index|[
name|testMode
condition|?
literal|1
else|:
literal|10000
index|]
expr_stmt|;
name|int
name|index
init|=
literal|0
decl_stmt|;
comment|// We compress the owids into CompressedOwid data structure that records
comment|// the fromIndex(inclusive) and toIndex(exclusive) for each unique owid.
name|List
argument_list|<
name|CompressedOwid
argument_list|>
name|compressedOwids
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|()
decl_stmt|;
name|CompressedOwid
name|lastCo
init|=
literal|null
decl_stmt|;
while|while
condition|(
operator|!
name|sortMerger
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
comment|// The sortMerger is a heap data structure that stores a pair of
comment|// (deleteRecordKey, deleteReaderValue) at each node and is ordered by deleteRecordKey.
comment|// The deleteReaderValue is the actual wrapper class that has the reference to the
comment|// underlying delta file that is being read, and its corresponding deleteRecordKey
comment|// is the smallest record id for that file. In each iteration of this loop, we extract(poll)
comment|// the minimum deleteRecordKey pair. Once we have processed that deleteRecordKey, we
comment|// advance the pointer for the corresponding deleteReaderValue. If the underlying file
comment|// itself has no more records, then we remove that pair from the heap, or else we
comment|// add the updated pair back to the heap.
name|Entry
argument_list|<
name|DeleteRecordKey
argument_list|,
name|DeleteReaderValue
argument_list|>
name|entry
init|=
name|sortMerger
operator|.
name|pollFirstEntry
argument_list|()
decl_stmt|;
name|DeleteRecordKey
name|deleteRecordKey
init|=
name|entry
operator|.
name|getKey
argument_list|()
decl_stmt|;
name|DeleteReaderValue
name|deleteReaderValue
init|=
name|entry
operator|.
name|getValue
argument_list|()
decl_stmt|;
name|long
name|owid
init|=
name|deleteRecordKey
operator|.
name|originalWriteId
decl_stmt|;
name|int
name|bp
init|=
name|deleteRecordKey
operator|.
name|bucketProperty
decl_stmt|;
name|checkSize
argument_list|(
name|index
argument_list|)
expr_stmt|;
name|rowIds
index|[
name|index
index|]
operator|=
name|deleteRecordKey
operator|.
name|rowId
expr_stmt|;
if|if
condition|(
name|lastCo
operator|==
literal|null
operator|||
name|lastCo
operator|.
name|originalWriteId
operator|!=
name|owid
operator|||
name|lastCo
operator|.
name|bucketProperty
operator|!=
name|bp
condition|)
block|{
if|if
condition|(
name|lastCo
operator|!=
literal|null
condition|)
block|{
name|lastCo
operator|.
name|toIndex
operator|=
name|index
expr_stmt|;
comment|// Finalize the previous record.
block|}
name|lastCo
operator|=
operator|new
name|CompressedOwid
argument_list|(
name|owid
argument_list|,
name|bp
argument_list|,
name|index
argument_list|,
operator|-
literal|1
argument_list|)
expr_stmt|;
name|compressedOwids
operator|.
name|add
argument_list|(
name|lastCo
argument_list|)
expr_stmt|;
block|}
operator|++
name|index
expr_stmt|;
if|if
condition|(
name|deleteReaderValue
operator|.
name|next
argument_list|(
name|deleteRecordKey
argument_list|)
condition|)
block|{
name|sortMerger
operator|.
name|put
argument_list|(
name|deleteRecordKey
argument_list|,
name|deleteReaderValue
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|deleteReaderValue
operator|.
name|close
argument_list|()
expr_stmt|;
comment|// Exhausted reading all records, close the reader.
block|}
block|}
if|if
condition|(
name|lastCo
operator|!=
literal|null
condition|)
block|{
name|lastCo
operator|.
name|toIndex
operator|=
name|index
expr_stmt|;
comment|// Finalize the last record.
name|lastCo
operator|=
literal|null
expr_stmt|;
block|}
if|if
condition|(
name|rowIds
operator|.
name|length
operator|>
name|index
condition|)
block|{
name|rowIds
operator|=
name|Arrays
operator|.
name|copyOf
argument_list|(
name|rowIds
argument_list|,
name|index
argument_list|)
expr_stmt|;
block|}
name|this
operator|.
name|compressedOwids
operator|=
name|compressedOwids
operator|.
name|toArray
argument_list|(
operator|new
name|CompressedOwid
index|[
name|compressedOwids
operator|.
name|size
argument_list|()
index|]
argument_list|)
expr_stmt|;
block|}
specifier|private
name|boolean
name|isDeleted
parameter_list|(
name|long
name|owid
parameter_list|,
name|int
name|bucketProperty
parameter_list|,
name|long
name|rowId
parameter_list|)
block|{
if|if
condition|(
name|compressedOwids
operator|==
literal|null
operator|||
name|rowIds
operator|==
literal|null
condition|)
block|{
return|return
literal|false
return|;
block|}
comment|// To find if a given (owid, rowId) pair is deleted or not, we perform
comment|// two binary searches at most. The first binary search is on the
comment|// compressed owids. If a match is found, only then we do the next
comment|// binary search in the larger rowId vector between the given toIndex& fromIndex.
comment|// Check if owid is outside the range of all owids present.
if|if
condition|(
name|owid
operator|<
name|compressedOwids
index|[
literal|0
index|]
operator|.
name|originalWriteId
operator|||
name|owid
operator|>
name|compressedOwids
index|[
name|compressedOwids
operator|.
name|length
operator|-
literal|1
index|]
operator|.
name|originalWriteId
condition|)
block|{
return|return
literal|false
return|;
block|}
comment|// Create a dummy key for searching the owid/bucket in the compressed owid ranges.
name|CompressedOwid
name|key
init|=
operator|new
name|CompressedOwid
argument_list|(
name|owid
argument_list|,
name|bucketProperty
argument_list|,
operator|-
literal|1
argument_list|,
operator|-
literal|1
argument_list|)
decl_stmt|;
name|int
name|pos
init|=
name|Arrays
operator|.
name|binarySearch
argument_list|(
name|compressedOwids
argument_list|,
name|key
argument_list|)
decl_stmt|;
if|if
condition|(
name|pos
operator|>=
literal|0
condition|)
block|{
comment|// Owid with the given value found! Searching now for rowId...
name|key
operator|=
name|compressedOwids
index|[
name|pos
index|]
expr_stmt|;
comment|// Retrieve the actual CompressedOwid that matched.
comment|// Check if rowId is outside the range of all rowIds present for this owid.
if|if
condition|(
name|rowId
argument_list|<
name|rowIds
index|[
name|key
operator|.
name|fromIndex
index|]
operator|||
name|rowId
argument_list|>
name|rowIds
index|[
name|key
operator|.
name|toIndex
operator|-
literal|1
index|]
condition|)
block|{
return|return
literal|false
return|;
block|}
if|if
condition|(
name|Arrays
operator|.
name|binarySearch
argument_list|(
name|rowIds
argument_list|,
name|key
operator|.
name|fromIndex
argument_list|,
name|key
operator|.
name|toIndex
argument_list|,
name|rowId
argument_list|)
operator|>=
literal|0
condition|)
block|{
return|return
literal|true
return|;
comment|// rowId also found!
block|}
block|}
return|return
literal|false
return|;
block|}
comment|/**      * @return how many delete events are actually loaded      */
name|int
name|size
parameter_list|()
block|{
return|return
name|rowIds
operator|==
literal|null
condition|?
literal|0
else|:
name|rowIds
operator|.
name|length
return|;
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|isEmpty
parameter_list|()
block|{
if|if
condition|(
name|isEmpty
operator|==
literal|null
condition|)
block|{
throw|throw
operator|new
name|IllegalStateException
argument_list|(
literal|"Not yet initialized"
argument_list|)
throw|;
block|}
return|return
name|isEmpty
return|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|findDeletedRecords
parameter_list|(
name|ColumnVector
index|[]
name|cols
parameter_list|,
name|int
name|size
parameter_list|,
name|BitSet
name|selectedBitSet
parameter_list|)
block|{
if|if
condition|(
name|rowIds
operator|==
literal|null
operator|||
name|compressedOwids
operator|==
literal|null
condition|)
block|{
return|return;
block|}
comment|// Iterate through the batch and for each (owid, rowid) in the batch
comment|// check if it is deleted or not.
name|long
index|[]
name|originalWriteIdVector
init|=
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|ORIGINAL_WRITEID
index|]
operator|.
name|isRepeating
condition|?
literal|null
else|:
operator|(
operator|(
name|LongColumnVector
operator|)
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|ORIGINAL_WRITEID
index|]
operator|)
operator|.
name|vector
decl_stmt|;
name|long
name|repeatedOriginalWriteId
init|=
operator|(
name|originalWriteIdVector
operator|!=
literal|null
operator|)
condition|?
operator|-
literal|1
else|:
operator|(
operator|(
name|LongColumnVector
operator|)
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|ORIGINAL_WRITEID
index|]
operator|)
operator|.
name|vector
index|[
literal|0
index|]
decl_stmt|;
name|long
index|[]
name|bucketProperties
init|=
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|BUCKET
index|]
operator|.
name|isRepeating
condition|?
literal|null
else|:
operator|(
operator|(
name|LongColumnVector
operator|)
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|BUCKET
index|]
operator|)
operator|.
name|vector
decl_stmt|;
name|int
name|repeatedBucketProperty
init|=
operator|(
name|bucketProperties
operator|!=
literal|null
operator|)
condition|?
operator|-
literal|1
else|:
call|(
name|int
call|)
argument_list|(
operator|(
name|LongColumnVector
operator|)
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|BUCKET
index|]
argument_list|)
operator|.
name|vector
index|[
literal|0
index|]
decl_stmt|;
name|long
index|[]
name|rowIdVector
init|=
operator|(
operator|(
name|LongColumnVector
operator|)
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|ROW_ID
index|]
operator|)
operator|.
name|vector
decl_stmt|;
for|for
control|(
name|int
name|setBitIndex
init|=
name|selectedBitSet
operator|.
name|nextSetBit
argument_list|(
literal|0
argument_list|)
init|;
name|setBitIndex
operator|>=
literal|0
condition|;
name|setBitIndex
operator|=
name|selectedBitSet
operator|.
name|nextSetBit
argument_list|(
name|setBitIndex
operator|+
literal|1
argument_list|)
control|)
block|{
name|long
name|owid
init|=
name|originalWriteIdVector
operator|!=
literal|null
condition|?
name|originalWriteIdVector
index|[
name|setBitIndex
index|]
else|:
name|repeatedOriginalWriteId
decl_stmt|;
name|int
name|bucketProperty
init|=
name|bucketProperties
operator|!=
literal|null
condition|?
operator|(
name|int
operator|)
name|bucketProperties
index|[
name|setBitIndex
index|]
else|:
name|repeatedBucketProperty
decl_stmt|;
name|long
name|rowId
init|=
name|rowIdVector
index|[
name|setBitIndex
index|]
decl_stmt|;
if|if
condition|(
name|isDeleted
argument_list|(
name|owid
argument_list|,
name|bucketProperty
argument_list|,
name|rowId
argument_list|)
condition|)
block|{
name|selectedBitSet
operator|.
name|clear
argument_list|(
name|setBitIndex
argument_list|)
expr_stmt|;
block|}
block|}
block|}
annotation|@
name|Override
specifier|public
name|void
name|close
parameter_list|()
throws|throws
name|IOException
block|{
comment|// ColumnizedDeleteEventRegistry reads all the delete events into memory during initialization
comment|// and it closes the delete event readers after it. If an exception gets thrown during
comment|// initialization, we may have to close any readers that are still left open.
while|while
condition|(
operator|!
name|sortMerger
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|Entry
argument_list|<
name|DeleteRecordKey
argument_list|,
name|DeleteReaderValue
argument_list|>
name|entry
init|=
name|sortMerger
operator|.
name|pollFirstEntry
argument_list|()
decl_stmt|;
name|entry
operator|.
name|getValue
argument_list|()
operator|.
name|close
argument_list|()
expr_stmt|;
comment|// close the reader for this entry
block|}
block|}
block|}
specifier|static
class|class
name|DeleteEventsOverflowMemoryException
extends|extends
name|Exception
block|{
specifier|private
specifier|static
specifier|final
name|long
name|serialVersionUID
init|=
literal|1L
decl_stmt|;
block|}
annotation|@
name|VisibleForTesting
name|OrcRawRecordMerger
operator|.
name|KeyInterval
name|getKeyInterval
parameter_list|()
block|{
return|return
name|keyInterval
return|;
block|}
annotation|@
name|VisibleForTesting
name|SearchArgument
name|getDeleteEventSarg
parameter_list|()
block|{
return|return
name|deleteEventSarg
return|;
block|}
block|}
end_class

end_unit

