begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/*  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|orc
package|;
end_package

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Arrays
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|BitSet
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
operator|.
name|Entry
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|TreeMap
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configuration
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|ValidReaderWriteIdList
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|ValidWriteIdList
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
operator|.
name|ConfVars
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|hive_metastoreConstants
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|Utilities
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|vector
operator|.
name|ColumnVector
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|vector
operator|.
name|LongColumnVector
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|vector
operator|.
name|StructColumnVector
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|vector
operator|.
name|VectorizedRowBatch
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|vector
operator|.
name|VectorizedRowBatchCtx
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|AcidOutputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|AcidUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|BucketCodec
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|RecordIdentifier
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|VirtualColumn
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|shims
operator|.
name|HadoopShims
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|NullWritable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|JobConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|Reporter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|orc
operator|.
name|impl
operator|.
name|AcidStats
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|orc
operator|.
name|impl
operator|.
name|OrcAcidUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|Logger
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|LoggerFactory
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|annotations
operator|.
name|VisibleForTesting
import|;
end_import

begin_comment
comment|/**  * A fast vectorized batch reader class for ACID. Insert events are read directly  * from the base files/insert_only deltas in vectorized row batches. The deleted  * rows can then be easily indicated via the 'selected' field of the vectorized row batch.  * Refer HIVE-14233 for more details.  */
end_comment

begin_class
specifier|public
class|class
name|VectorizedOrcAcidRowBatchReader
implements|implements
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|RecordReader
argument_list|<
name|NullWritable
argument_list|,
name|VectorizedRowBatch
argument_list|>
block|{
specifier|private
specifier|static
specifier|final
name|Logger
name|LOG
init|=
name|LoggerFactory
operator|.
name|getLogger
argument_list|(
name|VectorizedOrcAcidRowBatchReader
operator|.
name|class
argument_list|)
decl_stmt|;
specifier|private
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|RecordReader
argument_list|<
name|NullWritable
argument_list|,
name|VectorizedRowBatch
argument_list|>
name|baseReader
decl_stmt|;
specifier|private
specifier|final
name|VectorizedRowBatchCtx
name|rbCtx
decl_stmt|;
specifier|private
name|VectorizedRowBatch
name|vectorizedRowBatchBase
decl_stmt|;
specifier|private
name|long
name|offset
decl_stmt|;
specifier|private
name|long
name|length
decl_stmt|;
specifier|protected
name|float
name|progress
init|=
literal|0.0f
decl_stmt|;
specifier|protected
name|Object
index|[]
name|partitionValues
decl_stmt|;
specifier|private
name|boolean
name|addPartitionCols
init|=
literal|true
decl_stmt|;
specifier|private
specifier|final
name|boolean
name|isFlatPayload
decl_stmt|;
specifier|private
specifier|final
name|ValidWriteIdList
name|validWriteIdList
decl_stmt|;
specifier|private
specifier|final
name|DeleteEventRegistry
name|deleteEventRegistry
decl_stmt|;
comment|/**    * {@link RecordIdentifier}/{@link VirtualColumn#ROWID} information    */
specifier|private
specifier|final
name|StructColumnVector
name|recordIdColumnVector
decl_stmt|;
specifier|private
specifier|final
name|Reader
operator|.
name|Options
name|readerOptions
decl_stmt|;
specifier|private
specifier|final
name|boolean
name|isOriginal
decl_stmt|;
comment|/**    * something further in the data pipeline wants {@link VirtualColumn#ROWID}    */
specifier|private
specifier|final
name|boolean
name|rowIdProjected
decl_stmt|;
comment|/**    * partition/table root    */
specifier|private
specifier|final
name|Path
name|rootPath
decl_stmt|;
comment|/**    * for reading "original" files    */
specifier|private
specifier|final
name|OffsetAndBucketProperty
name|syntheticProps
decl_stmt|;
comment|/**    * To have access to {@link RecordReader#getRowNumber()} in the underlying file    */
specifier|private
name|RecordReader
name|innerReader
decl_stmt|;
name|VectorizedOrcAcidRowBatchReader
parameter_list|(
name|OrcSplit
name|inputSplit
parameter_list|,
name|JobConf
name|conf
parameter_list|,
name|Reporter
name|reporter
parameter_list|)
throws|throws
name|IOException
block|{
name|this
argument_list|(
name|inputSplit
argument_list|,
name|conf
argument_list|,
name|reporter
argument_list|,
literal|null
argument_list|)
expr_stmt|;
block|}
annotation|@
name|VisibleForTesting
name|VectorizedOrcAcidRowBatchReader
parameter_list|(
name|OrcSplit
name|inputSplit
parameter_list|,
name|JobConf
name|conf
parameter_list|,
name|Reporter
name|reporter
parameter_list|,
name|VectorizedRowBatchCtx
name|rbCtx
parameter_list|)
throws|throws
name|IOException
block|{
name|this
argument_list|(
name|conf
argument_list|,
name|inputSplit
argument_list|,
name|reporter
argument_list|,
name|rbCtx
operator|==
literal|null
condition|?
name|Utilities
operator|.
name|getVectorizedRowBatchCtx
argument_list|(
name|conf
argument_list|)
else|:
name|rbCtx
argument_list|,
literal|false
argument_list|)
expr_stmt|;
specifier|final
name|Reader
name|reader
init|=
name|OrcInputFormat
operator|.
name|createOrcReaderForSplit
argument_list|(
name|conf
argument_list|,
operator|(
name|OrcSplit
operator|)
name|inputSplit
argument_list|)
decl_stmt|;
comment|// Careful with the range here now, we do not want to read the whole base file like deltas.
name|innerReader
operator|=
name|reader
operator|.
name|rowsOptions
argument_list|(
name|readerOptions
operator|.
name|range
argument_list|(
name|offset
argument_list|,
name|length
argument_list|)
argument_list|)
expr_stmt|;
name|baseReader
operator|=
operator|new
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|RecordReader
argument_list|<
name|NullWritable
argument_list|,
name|VectorizedRowBatch
argument_list|>
argument_list|()
block|{
annotation|@
name|Override
specifier|public
name|boolean
name|next
parameter_list|(
name|NullWritable
name|key
parameter_list|,
name|VectorizedRowBatch
name|value
parameter_list|)
throws|throws
name|IOException
block|{
return|return
name|innerReader
operator|.
name|nextBatch
argument_list|(
name|value
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|public
name|NullWritable
name|createKey
parameter_list|()
block|{
return|return
name|NullWritable
operator|.
name|get
argument_list|()
return|;
block|}
annotation|@
name|Override
specifier|public
name|VectorizedRowBatch
name|createValue
parameter_list|()
block|{
return|return
name|rbCtx
operator|.
name|createVectorizedRowBatch
argument_list|()
return|;
block|}
annotation|@
name|Override
specifier|public
name|long
name|getPos
parameter_list|()
throws|throws
name|IOException
block|{
return|return
literal|0
return|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|close
parameter_list|()
throws|throws
name|IOException
block|{
name|innerReader
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|float
name|getProgress
parameter_list|()
throws|throws
name|IOException
block|{
return|return
name|innerReader
operator|.
name|getProgress
argument_list|()
return|;
block|}
block|}
expr_stmt|;
name|this
operator|.
name|vectorizedRowBatchBase
operator|=
operator|(
operator|(
name|RecordReaderImpl
operator|)
name|innerReader
operator|)
operator|.
name|createRowBatch
argument_list|()
expr_stmt|;
block|}
comment|/**    * LLAP IO c'tor    */
specifier|public
name|VectorizedOrcAcidRowBatchReader
parameter_list|(
name|OrcSplit
name|inputSplit
parameter_list|,
name|JobConf
name|conf
parameter_list|,
name|Reporter
name|reporter
parameter_list|,
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|RecordReader
argument_list|<
name|NullWritable
argument_list|,
name|VectorizedRowBatch
argument_list|>
name|baseReader
parameter_list|,
name|VectorizedRowBatchCtx
name|rbCtx
parameter_list|,
name|boolean
name|isFlatPayload
parameter_list|)
throws|throws
name|IOException
block|{
name|this
argument_list|(
name|conf
argument_list|,
name|inputSplit
argument_list|,
name|reporter
argument_list|,
name|rbCtx
argument_list|,
name|isFlatPayload
argument_list|)
expr_stmt|;
name|this
operator|.
name|baseReader
operator|=
name|baseReader
expr_stmt|;
name|this
operator|.
name|innerReader
operator|=
literal|null
expr_stmt|;
name|this
operator|.
name|vectorizedRowBatchBase
operator|=
name|baseReader
operator|.
name|createValue
argument_list|()
expr_stmt|;
block|}
specifier|private
name|VectorizedOrcAcidRowBatchReader
parameter_list|(
name|JobConf
name|conf
parameter_list|,
name|OrcSplit
name|orcSplit
parameter_list|,
name|Reporter
name|reporter
parameter_list|,
name|VectorizedRowBatchCtx
name|rowBatchCtx
parameter_list|,
name|boolean
name|isFlatPayload
parameter_list|)
throws|throws
name|IOException
block|{
name|this
operator|.
name|isFlatPayload
operator|=
name|isFlatPayload
expr_stmt|;
name|this
operator|.
name|rbCtx
operator|=
name|rowBatchCtx
expr_stmt|;
specifier|final
name|boolean
name|isAcidRead
init|=
name|AcidUtils
operator|.
name|isFullAcidScan
argument_list|(
name|conf
argument_list|)
decl_stmt|;
specifier|final
name|AcidUtils
operator|.
name|AcidOperationalProperties
name|acidOperationalProperties
init|=
name|AcidUtils
operator|.
name|getAcidOperationalProperties
argument_list|(
name|conf
argument_list|)
decl_stmt|;
comment|// This type of VectorizedOrcAcidRowBatchReader can only be created when split-update is
comment|// enabled for an ACID case and the file format is ORC.
name|boolean
name|isReadNotAllowed
init|=
operator|!
name|isAcidRead
operator|||
operator|!
name|acidOperationalProperties
operator|.
name|isSplitUpdate
argument_list|()
decl_stmt|;
if|if
condition|(
name|isReadNotAllowed
condition|)
block|{
name|OrcInputFormat
operator|.
name|raiseAcidTablesMustBeReadWithAcidReaderException
argument_list|(
name|conf
argument_list|)
expr_stmt|;
block|}
name|reporter
operator|.
name|setStatus
argument_list|(
name|orcSplit
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
name|readerOptions
operator|=
name|OrcRawRecordMerger
operator|.
name|createEventOptions
argument_list|(
name|OrcInputFormat
operator|.
name|createOptionsForReader
argument_list|(
name|conf
argument_list|)
argument_list|)
expr_stmt|;
name|this
operator|.
name|offset
operator|=
name|orcSplit
operator|.
name|getStart
argument_list|()
expr_stmt|;
name|this
operator|.
name|length
operator|=
name|orcSplit
operator|.
name|getLength
argument_list|()
expr_stmt|;
name|int
name|partitionColumnCount
init|=
operator|(
name|rbCtx
operator|!=
literal|null
operator|)
condition|?
name|rbCtx
operator|.
name|getPartitionColumnCount
argument_list|()
else|:
literal|0
decl_stmt|;
if|if
condition|(
name|partitionColumnCount
operator|>
literal|0
condition|)
block|{
name|partitionValues
operator|=
operator|new
name|Object
index|[
name|partitionColumnCount
index|]
expr_stmt|;
name|VectorizedRowBatchCtx
operator|.
name|getPartitionValues
argument_list|(
name|rbCtx
argument_list|,
name|conf
argument_list|,
name|orcSplit
argument_list|,
name|partitionValues
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|partitionValues
operator|=
literal|null
expr_stmt|;
block|}
name|String
name|txnString
init|=
name|conf
operator|.
name|get
argument_list|(
name|ValidWriteIdList
operator|.
name|VALID_WRITEIDS_KEY
argument_list|)
decl_stmt|;
name|this
operator|.
name|validWriteIdList
operator|=
operator|(
name|txnString
operator|==
literal|null
operator|)
condition|?
operator|new
name|ValidReaderWriteIdList
argument_list|()
else|:
operator|new
name|ValidReaderWriteIdList
argument_list|(
name|txnString
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"VectorizedOrcAcidRowBatchReader:: Read ValidWriteIdList: "
operator|+
name|this
operator|.
name|validWriteIdList
operator|.
name|toString
argument_list|()
operator|+
literal|" isFullAcidTable: "
operator|+
name|AcidUtils
operator|.
name|isFullAcidScan
argument_list|(
name|conf
argument_list|)
argument_list|)
expr_stmt|;
comment|// Clone readerOptions for deleteEvents.
name|Reader
operator|.
name|Options
name|deleteEventReaderOptions
init|=
name|readerOptions
operator|.
name|clone
argument_list|()
decl_stmt|;
comment|// Set the range on the deleteEventReaderOptions to 0 to INTEGER_MAX because
comment|// we always want to read all the delete delta files.
name|deleteEventReaderOptions
operator|.
name|range
argument_list|(
literal|0
argument_list|,
name|Long
operator|.
name|MAX_VALUE
argument_list|)
expr_stmt|;
comment|//  Disable SARGs for deleteEventReaders, as SARGs have no meaning.
name|deleteEventReaderOptions
operator|.
name|searchArgument
argument_list|(
literal|null
argument_list|,
literal|null
argument_list|)
expr_stmt|;
name|DeleteEventRegistry
name|der
decl_stmt|;
try|try
block|{
comment|// See if we can load all the delete events from all the delete deltas in memory...
name|der
operator|=
operator|new
name|ColumnizedDeleteEventRegistry
argument_list|(
name|conf
argument_list|,
name|orcSplit
argument_list|,
name|deleteEventReaderOptions
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|DeleteEventsOverflowMemoryException
name|e
parameter_list|)
block|{
comment|// If not, then create a set of hanging readers that do sort-merge to find the next smallest
comment|// delete event on-demand. Caps the memory consumption to (some_const * no. of readers).
name|der
operator|=
operator|new
name|SortMergedDeleteEventRegistry
argument_list|(
name|conf
argument_list|,
name|orcSplit
argument_list|,
name|deleteEventReaderOptions
argument_list|)
expr_stmt|;
block|}
name|this
operator|.
name|deleteEventRegistry
operator|=
name|der
expr_stmt|;
name|isOriginal
operator|=
name|orcSplit
operator|.
name|isOriginal
argument_list|()
expr_stmt|;
if|if
condition|(
name|isOriginal
condition|)
block|{
name|recordIdColumnVector
operator|=
operator|new
name|StructColumnVector
argument_list|(
name|VectorizedRowBatch
operator|.
name|DEFAULT_SIZE
argument_list|,
operator|new
name|LongColumnVector
argument_list|()
argument_list|,
operator|new
name|LongColumnVector
argument_list|()
argument_list|,
operator|new
name|LongColumnVector
argument_list|()
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// Will swap in the Vectors from underlying row batch.
name|recordIdColumnVector
operator|=
operator|new
name|StructColumnVector
argument_list|(
name|VectorizedRowBatch
operator|.
name|DEFAULT_SIZE
argument_list|,
literal|null
argument_list|,
literal|null
argument_list|,
literal|null
argument_list|)
expr_stmt|;
block|}
name|rowIdProjected
operator|=
name|areRowIdsProjected
argument_list|(
name|rbCtx
argument_list|)
expr_stmt|;
name|rootPath
operator|=
name|orcSplit
operator|.
name|getRootDir
argument_list|()
expr_stmt|;
comment|//why even compute syntheticProps if !isOriginal???
name|syntheticProps
operator|=
name|computeOffsetAndBucket
argument_list|(
name|orcSplit
argument_list|,
name|conf
argument_list|,
name|validWriteIdList
argument_list|)
expr_stmt|;
block|}
comment|/**    * Used for generating synthetic ROW__IDs for reading "original" files    */
specifier|private
specifier|static
specifier|final
class|class
name|OffsetAndBucketProperty
block|{
specifier|private
specifier|final
name|long
name|rowIdOffset
decl_stmt|;
specifier|private
specifier|final
name|int
name|bucketProperty
decl_stmt|;
specifier|private
specifier|final
name|long
name|syntheticTxnId
decl_stmt|;
specifier|private
name|OffsetAndBucketProperty
parameter_list|(
name|long
name|rowIdOffset
parameter_list|,
name|int
name|bucketProperty
parameter_list|,
name|long
name|syntheticTxnId
parameter_list|)
block|{
name|this
operator|.
name|rowIdOffset
operator|=
name|rowIdOffset
expr_stmt|;
name|this
operator|.
name|bucketProperty
operator|=
name|bucketProperty
expr_stmt|;
name|this
operator|.
name|syntheticTxnId
operator|=
name|syntheticTxnId
expr_stmt|;
block|}
block|}
comment|/**    * See {@link #next(NullWritable, VectorizedRowBatch)} first and    * {@link OrcRawRecordMerger.OriginalReaderPair}.    * When reading a split of an "original" file and we need to decorate data with ROW__ID.    * This requires treating multiple files that are part of the same bucket (tranche for unbucketed    * tables) as a single logical file to number rowids consistently.    *    * todo: This logic is executed per split of every "original" file.  The computed result is the    * same for every split form the same file so this could be optimized by moving it to    * before/during split computation and passing the info in the split.  (HIVE-17917)    */
specifier|private
name|OffsetAndBucketProperty
name|computeOffsetAndBucket
parameter_list|(
name|OrcSplit
name|split
parameter_list|,
name|JobConf
name|conf
parameter_list|,
name|ValidWriteIdList
name|validWriteIdList
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
operator|!
name|needSyntheticRowIds
argument_list|(
name|split
operator|.
name|isOriginal
argument_list|()
argument_list|,
operator|!
name|deleteEventRegistry
operator|.
name|isEmpty
argument_list|()
argument_list|,
name|rowIdProjected
argument_list|)
condition|)
block|{
if|if
condition|(
name|split
operator|.
name|isOriginal
argument_list|()
condition|)
block|{
comment|/**          * Even if we don't need to project ROW_IDs, we still need to check the transaction ID that          * created the file to see if it's committed.  See more in          * {@link #next(NullWritable, VectorizedRowBatch)}.  (In practice getAcidState() should          * filter out base/delta files but this makes fewer dependencies)          */
name|OrcRawRecordMerger
operator|.
name|TransactionMetaData
name|syntheticTxnInfo
init|=
name|OrcRawRecordMerger
operator|.
name|TransactionMetaData
operator|.
name|findWriteIDForSynthetcRowIDs
argument_list|(
name|split
operator|.
name|getPath
argument_list|()
argument_list|,
name|split
operator|.
name|getRootDir
argument_list|()
argument_list|,
name|conf
argument_list|)
decl_stmt|;
return|return
operator|new
name|OffsetAndBucketProperty
argument_list|(
operator|-
literal|1
argument_list|,
operator|-
literal|1
argument_list|,
name|syntheticTxnInfo
operator|.
name|syntheticWriteId
argument_list|)
return|;
block|}
return|return
literal|null
return|;
block|}
name|long
name|rowIdOffset
init|=
literal|0
decl_stmt|;
name|OrcRawRecordMerger
operator|.
name|TransactionMetaData
name|syntheticTxnInfo
init|=
name|OrcRawRecordMerger
operator|.
name|TransactionMetaData
operator|.
name|findWriteIDForSynthetcRowIDs
argument_list|(
name|split
operator|.
name|getPath
argument_list|()
argument_list|,
name|split
operator|.
name|getRootDir
argument_list|()
argument_list|,
name|conf
argument_list|)
decl_stmt|;
name|int
name|bucketId
init|=
name|AcidUtils
operator|.
name|parseBaseOrDeltaBucketFilename
argument_list|(
name|split
operator|.
name|getPath
argument_list|()
argument_list|,
name|conf
argument_list|)
operator|.
name|getBucketId
argument_list|()
decl_stmt|;
name|int
name|bucketProperty
init|=
name|BucketCodec
operator|.
name|V1
operator|.
name|encode
argument_list|(
operator|new
name|AcidOutputFormat
operator|.
name|Options
argument_list|(
name|conf
argument_list|)
comment|//statementId is from directory name (or 0 if there is none)
operator|.
name|statementId
argument_list|(
name|syntheticTxnInfo
operator|.
name|statementId
argument_list|)
operator|.
name|bucket
argument_list|(
name|bucketId
argument_list|)
argument_list|)
decl_stmt|;
name|AcidUtils
operator|.
name|Directory
name|directoryState
init|=
name|AcidUtils
operator|.
name|getAcidState
argument_list|(
name|syntheticTxnInfo
operator|.
name|folder
argument_list|,
name|conf
argument_list|,
name|validWriteIdList
argument_list|,
literal|false
argument_list|,
literal|true
argument_list|)
decl_stmt|;
for|for
control|(
name|HadoopShims
operator|.
name|HdfsFileStatusWithId
name|f
range|:
name|directoryState
operator|.
name|getOriginalFiles
argument_list|()
control|)
block|{
name|AcidOutputFormat
operator|.
name|Options
name|bucketOptions
init|=
name|AcidUtils
operator|.
name|parseBaseOrDeltaBucketFilename
argument_list|(
name|f
operator|.
name|getFileStatus
argument_list|()
operator|.
name|getPath
argument_list|()
argument_list|,
name|conf
argument_list|)
decl_stmt|;
if|if
condition|(
name|bucketOptions
operator|.
name|getBucketId
argument_list|()
operator|!=
name|bucketId
condition|)
block|{
continue|continue;
comment|//HIVE-16952
block|}
if|if
condition|(
name|f
operator|.
name|getFileStatus
argument_list|()
operator|.
name|getPath
argument_list|()
operator|.
name|equals
argument_list|(
name|split
operator|.
name|getPath
argument_list|()
argument_list|)
condition|)
block|{
comment|//'f' is the file whence this split is
break|break;
block|}
name|Reader
name|reader
init|=
name|OrcFile
operator|.
name|createReader
argument_list|(
name|f
operator|.
name|getFileStatus
argument_list|()
operator|.
name|getPath
argument_list|()
argument_list|,
name|OrcFile
operator|.
name|readerOptions
argument_list|(
name|conf
argument_list|)
argument_list|)
decl_stmt|;
name|rowIdOffset
operator|+=
name|reader
operator|.
name|getNumberOfRows
argument_list|()
expr_stmt|;
block|}
return|return
operator|new
name|OffsetAndBucketProperty
argument_list|(
name|rowIdOffset
argument_list|,
name|bucketProperty
argument_list|,
name|syntheticTxnInfo
operator|.
name|syntheticWriteId
argument_list|)
return|;
block|}
comment|/**    * {@link VectorizedOrcAcidRowBatchReader} is always used for vectorized reads of acid tables.    * In some cases this cannot be used from LLAP IO elevator because    * {@link RecordReader#getRowNumber()} is not (currently) available there but is required to    * generate ROW__IDs for "original" files    * @param hasDeletes - if there are any deletes that apply to this split    * todo: HIVE-17944    */
specifier|static
name|boolean
name|canUseLlapForAcid
parameter_list|(
name|OrcSplit
name|split
parameter_list|,
name|boolean
name|hasDeletes
parameter_list|,
name|Configuration
name|conf
parameter_list|)
block|{
if|if
condition|(
operator|!
name|split
operator|.
name|isOriginal
argument_list|()
condition|)
block|{
return|return
literal|true
return|;
block|}
name|VectorizedRowBatchCtx
name|rbCtx
init|=
name|Utilities
operator|.
name|getVectorizedRowBatchCtx
argument_list|(
name|conf
argument_list|)
decl_stmt|;
if|if
condition|(
name|rbCtx
operator|==
literal|null
condition|)
block|{
throw|throw
operator|new
name|IllegalStateException
argument_list|(
literal|"Could not create VectorizedRowBatchCtx for "
operator|+
name|split
operator|.
name|getPath
argument_list|()
argument_list|)
throw|;
block|}
return|return
operator|!
name|needSyntheticRowIds
argument_list|(
name|split
operator|.
name|isOriginal
argument_list|()
argument_list|,
name|hasDeletes
argument_list|,
name|areRowIdsProjected
argument_list|(
name|rbCtx
argument_list|)
argument_list|)
return|;
block|}
comment|/**    * Does this reader need to decorate rows with ROW__IDs (for "original" reads).    * Even if ROW__ID is not projected you still need to decorate the rows with them to see if    * any of the delete events apply.    */
specifier|private
specifier|static
name|boolean
name|needSyntheticRowIds
parameter_list|(
name|boolean
name|isOriginal
parameter_list|,
name|boolean
name|hasDeletes
parameter_list|,
name|boolean
name|rowIdProjected
parameter_list|)
block|{
return|return
name|isOriginal
operator|&&
operator|(
name|hasDeletes
operator|||
name|rowIdProjected
operator|)
return|;
block|}
specifier|private
specifier|static
name|boolean
name|areRowIdsProjected
parameter_list|(
name|VectorizedRowBatchCtx
name|rbCtx
parameter_list|)
block|{
if|if
condition|(
name|rbCtx
operator|.
name|getVirtualColumnCount
argument_list|()
operator|==
literal|0
condition|)
block|{
return|return
literal|false
return|;
block|}
for|for
control|(
name|VirtualColumn
name|vc
range|:
name|rbCtx
operator|.
name|getNeededVirtualColumns
argument_list|()
control|)
block|{
if|if
condition|(
name|vc
operator|==
name|VirtualColumn
operator|.
name|ROWID
condition|)
block|{
comment|//The query needs ROW__ID: maybe explicitly asked, maybe it's part of
comment|// Update/Delete statement.
comment|//Either way, we need to decorate "original" rows with row__id
return|return
literal|true
return|;
block|}
block|}
return|return
literal|false
return|;
block|}
specifier|static
name|Path
index|[]
name|getDeleteDeltaDirsFromSplit
parameter_list|(
name|OrcSplit
name|orcSplit
parameter_list|)
throws|throws
name|IOException
block|{
name|Path
name|path
init|=
name|orcSplit
operator|.
name|getPath
argument_list|()
decl_stmt|;
name|Path
name|root
decl_stmt|;
if|if
condition|(
name|orcSplit
operator|.
name|hasBase
argument_list|()
condition|)
block|{
if|if
condition|(
name|orcSplit
operator|.
name|isOriginal
argument_list|()
condition|)
block|{
name|root
operator|=
name|orcSplit
operator|.
name|getRootDir
argument_list|()
expr_stmt|;
block|}
else|else
block|{
name|root
operator|=
name|path
operator|.
name|getParent
argument_list|()
operator|.
name|getParent
argument_list|()
expr_stmt|;
comment|//todo: why not just use getRootDir()?
assert|assert
name|root
operator|.
name|equals
argument_list|(
name|orcSplit
operator|.
name|getRootDir
argument_list|()
argument_list|)
operator|:
literal|"root mismatch: baseDir="
operator|+
name|orcSplit
operator|.
name|getRootDir
argument_list|()
operator|+
literal|" path.p.p="
operator|+
name|root
assert|;
block|}
block|}
else|else
block|{
throw|throw
operator|new
name|IllegalStateException
argument_list|(
literal|"Split w/o base w/Acid 2.0??: "
operator|+
name|path
argument_list|)
throw|;
block|}
return|return
name|AcidUtils
operator|.
name|deserializeDeleteDeltas
argument_list|(
name|root
argument_list|,
name|orcSplit
operator|.
name|getDeltas
argument_list|()
argument_list|)
return|;
block|}
comment|/**    * There are 2 types of schema from the {@link #baseReader} that this handles.  In the case    * the data was written to a transactional table from the start, every row is decorated with    * transaction related info and looks like<op, otid, writerId, rowid, ctid,<f1, ... fn>>.    *    * The other case is when data was written to non-transactional table and thus only has the user    * data:<f1, ... fn>.  Then this table was then converted to a transactional table but the data    * files are not changed until major compaction.  These are the "original" files.    *    * In this case we may need to decorate the outgoing data with transactional column values at    * read time.  (It's done somewhat out of band via VectorizedRowBatchCtx - ask Teddy Choi).    * The "otid, writerId, rowid" columns represent {@link RecordIdentifier}.  They are assigned    * each time the table is read in a way that needs to project {@link VirtualColumn#ROWID}.    * Major compaction will attach these values to each row permanently.    * It's critical that these generated column values are assigned exactly the same way by each    * read of the same row and by the Compactor.    * See {@link org.apache.hadoop.hive.ql.txn.compactor.CompactorMR} and    * {@link OrcRawRecordMerger.OriginalReaderPairToCompact} for the Compactor read path.    * (Longer term should make compactor use this class)    *    * This only decorates original rows with metadata if something above is requesting these values    * or if there are Delete events to apply.    *    * @return false where there is no more data, i.e. {@code value} is empty    */
annotation|@
name|Override
specifier|public
name|boolean
name|next
parameter_list|(
name|NullWritable
name|key
parameter_list|,
name|VectorizedRowBatch
name|value
parameter_list|)
throws|throws
name|IOException
block|{
try|try
block|{
comment|// Check and update partition cols if necessary. Ideally, this should be done
comment|// in CreateValue as the partition is constant per split. But since Hive uses
comment|// CombineHiveRecordReader and
comment|// as this does not call CreateValue for each new RecordReader it creates, this check is
comment|// required in next()
if|if
condition|(
name|addPartitionCols
condition|)
block|{
if|if
condition|(
name|partitionValues
operator|!=
literal|null
condition|)
block|{
name|rbCtx
operator|.
name|addPartitionColsToBatch
argument_list|(
name|value
argument_list|,
name|partitionValues
argument_list|)
expr_stmt|;
block|}
name|addPartitionCols
operator|=
literal|false
expr_stmt|;
block|}
if|if
condition|(
operator|!
name|baseReader
operator|.
name|next
argument_list|(
literal|null
argument_list|,
name|vectorizedRowBatchBase
argument_list|)
condition|)
block|{
return|return
literal|false
return|;
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"error iterating"
argument_list|,
name|e
argument_list|)
throw|;
block|}
comment|// Once we have read the VectorizedRowBatchBase from the file, there are two kinds of cases
comment|// for which we might have to discard rows from the batch:
comment|// Case 1- when the row is created by a transaction that is not valid, or
comment|// Case 2- when the row has been deleted.
comment|// We will go through the batch to discover rows which match any of the cases and specifically
comment|// remove them from the selected vector. Of course, selectedInUse should also be set.
name|BitSet
name|selectedBitSet
init|=
operator|new
name|BitSet
argument_list|(
name|vectorizedRowBatchBase
operator|.
name|size
argument_list|)
decl_stmt|;
if|if
condition|(
name|vectorizedRowBatchBase
operator|.
name|selectedInUse
condition|)
block|{
comment|// When selectedInUse is true, start with every bit set to false and selectively set
comment|// certain bits to true based on the selected[] vector.
name|selectedBitSet
operator|.
name|set
argument_list|(
literal|0
argument_list|,
name|vectorizedRowBatchBase
operator|.
name|size
argument_list|,
literal|false
argument_list|)
expr_stmt|;
for|for
control|(
name|int
name|j
init|=
literal|0
init|;
name|j
operator|<
name|vectorizedRowBatchBase
operator|.
name|size
condition|;
operator|++
name|j
control|)
block|{
name|int
name|i
init|=
name|vectorizedRowBatchBase
operator|.
name|selected
index|[
name|j
index|]
decl_stmt|;
name|selectedBitSet
operator|.
name|set
argument_list|(
name|i
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
comment|// When selectedInUse is set to false, everything in the batch is selected.
name|selectedBitSet
operator|.
name|set
argument_list|(
literal|0
argument_list|,
name|vectorizedRowBatchBase
operator|.
name|size
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
name|ColumnVector
index|[]
name|innerRecordIdColumnVector
init|=
name|vectorizedRowBatchBase
operator|.
name|cols
decl_stmt|;
if|if
condition|(
name|isOriginal
condition|)
block|{
comment|// Handle synthetic row IDs for the original files.
name|innerRecordIdColumnVector
operator|=
name|handleOriginalFile
argument_list|(
name|selectedBitSet
argument_list|,
name|innerRecordIdColumnVector
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// Case 1- find rows which belong to transactions that are not valid.
name|findRecordsWithInvalidTransactionIds
argument_list|(
name|vectorizedRowBatchBase
argument_list|,
name|selectedBitSet
argument_list|)
expr_stmt|;
block|}
comment|// Case 2- find rows which have been deleted.
name|this
operator|.
name|deleteEventRegistry
operator|.
name|findDeletedRecords
argument_list|(
name|innerRecordIdColumnVector
argument_list|,
name|vectorizedRowBatchBase
operator|.
name|size
argument_list|,
name|selectedBitSet
argument_list|)
expr_stmt|;
if|if
condition|(
name|selectedBitSet
operator|.
name|cardinality
argument_list|()
operator|==
name|vectorizedRowBatchBase
operator|.
name|size
condition|)
block|{
comment|// None of the cases above matched and everything is selected. Hence, we will use the
comment|// same values for the selected and selectedInUse.
name|value
operator|.
name|size
operator|=
name|vectorizedRowBatchBase
operator|.
name|size
expr_stmt|;
name|value
operator|.
name|selected
operator|=
name|vectorizedRowBatchBase
operator|.
name|selected
expr_stmt|;
name|value
operator|.
name|selectedInUse
operator|=
name|vectorizedRowBatchBase
operator|.
name|selectedInUse
expr_stmt|;
block|}
else|else
block|{
name|value
operator|.
name|size
operator|=
name|selectedBitSet
operator|.
name|cardinality
argument_list|()
expr_stmt|;
name|value
operator|.
name|selectedInUse
operator|=
literal|true
expr_stmt|;
name|value
operator|.
name|selected
operator|=
operator|new
name|int
index|[
name|selectedBitSet
operator|.
name|cardinality
argument_list|()
index|]
expr_stmt|;
comment|// This loop fills up the selected[] vector with all the index positions that are selected.
for|for
control|(
name|int
name|setBitIndex
init|=
name|selectedBitSet
operator|.
name|nextSetBit
argument_list|(
literal|0
argument_list|)
init|,
name|selectedItr
init|=
literal|0
init|;
name|setBitIndex
operator|>=
literal|0
condition|;
name|setBitIndex
operator|=
name|selectedBitSet
operator|.
name|nextSetBit
argument_list|(
name|setBitIndex
operator|+
literal|1
argument_list|)
operator|,
operator|++
name|selectedItr
control|)
block|{
name|value
operator|.
name|selected
index|[
name|selectedItr
index|]
operator|=
name|setBitIndex
expr_stmt|;
block|}
block|}
if|if
condition|(
name|isOriginal
condition|)
block|{
comment|/* Just copy the payload.  {@link recordIdColumnVector} has already been populated */
name|System
operator|.
name|arraycopy
argument_list|(
name|vectorizedRowBatchBase
operator|.
name|cols
argument_list|,
literal|0
argument_list|,
name|value
operator|.
name|cols
argument_list|,
literal|0
argument_list|,
name|value
operator|.
name|getDataColumnCount
argument_list|()
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|int
name|payloadCol
init|=
name|OrcRecordUpdater
operator|.
name|ROW
decl_stmt|;
if|if
condition|(
name|isFlatPayload
condition|)
block|{
comment|// Ignore the struct column and just copy all the following data columns.
name|System
operator|.
name|arraycopy
argument_list|(
name|vectorizedRowBatchBase
operator|.
name|cols
argument_list|,
name|payloadCol
operator|+
literal|1
argument_list|,
name|value
operator|.
name|cols
argument_list|,
literal|0
argument_list|,
name|vectorizedRowBatchBase
operator|.
name|cols
operator|.
name|length
operator|-
name|payloadCol
operator|-
literal|1
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|StructColumnVector
name|payloadStruct
init|=
operator|(
name|StructColumnVector
operator|)
name|vectorizedRowBatchBase
operator|.
name|cols
index|[
name|payloadCol
index|]
decl_stmt|;
comment|// Transfer columnVector objects from base batch to outgoing batch.
name|System
operator|.
name|arraycopy
argument_list|(
name|payloadStruct
operator|.
name|fields
argument_list|,
literal|0
argument_list|,
name|value
operator|.
name|cols
argument_list|,
literal|0
argument_list|,
name|value
operator|.
name|getDataColumnCount
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|rowIdProjected
condition|)
block|{
name|recordIdColumnVector
operator|.
name|fields
index|[
literal|0
index|]
operator|=
name|vectorizedRowBatchBase
operator|.
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|ORIGINAL_WRITEID
index|]
expr_stmt|;
name|recordIdColumnVector
operator|.
name|fields
index|[
literal|1
index|]
operator|=
name|vectorizedRowBatchBase
operator|.
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|BUCKET
index|]
expr_stmt|;
name|recordIdColumnVector
operator|.
name|fields
index|[
literal|2
index|]
operator|=
name|vectorizedRowBatchBase
operator|.
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|ROW_ID
index|]
expr_stmt|;
block|}
block|}
if|if
condition|(
name|rowIdProjected
condition|)
block|{
name|rbCtx
operator|.
name|setRecordIdColumnVector
argument_list|(
name|recordIdColumnVector
argument_list|)
expr_stmt|;
block|}
name|progress
operator|=
name|baseReader
operator|.
name|getProgress
argument_list|()
expr_stmt|;
return|return
literal|true
return|;
block|}
specifier|private
name|ColumnVector
index|[]
name|handleOriginalFile
parameter_list|(
name|BitSet
name|selectedBitSet
parameter_list|,
name|ColumnVector
index|[]
name|innerRecordIdColumnVector
parameter_list|)
throws|throws
name|IOException
block|{
comment|/*      * If there are deletes and reading original file, we must produce synthetic ROW_IDs in order      * to see if any deletes apply      */
name|boolean
name|needSyntheticRowId
init|=
name|needSyntheticRowIds
argument_list|(
literal|true
argument_list|,
operator|!
name|deleteEventRegistry
operator|.
name|isEmpty
argument_list|()
argument_list|,
name|rowIdProjected
argument_list|)
decl_stmt|;
if|if
condition|(
name|needSyntheticRowId
condition|)
block|{
assert|assert
name|syntheticProps
operator|!=
literal|null
operator|&&
name|syntheticProps
operator|.
name|rowIdOffset
operator|>=
literal|0
operator|:
literal|""
operator|+
name|syntheticProps
assert|;
assert|assert
name|syntheticProps
operator|!=
literal|null
operator|&&
name|syntheticProps
operator|.
name|bucketProperty
operator|>=
literal|0
operator|:
literal|""
operator|+
name|syntheticProps
assert|;
if|if
condition|(
name|innerReader
operator|==
literal|null
condition|)
block|{
throw|throw
operator|new
name|IllegalStateException
argument_list|(
name|getClass
argument_list|()
operator|.
name|getName
argument_list|()
operator|+
literal|" requires "
operator|+
name|org
operator|.
name|apache
operator|.
name|orc
operator|.
name|RecordReader
operator|.
name|class
operator|+
literal|" to handle original files that require ROW__IDs: "
operator|+
name|rootPath
argument_list|)
throw|;
block|}
comment|/**        * {@link RecordIdentifier#getWriteId()}        */
name|recordIdColumnVector
operator|.
name|fields
index|[
literal|0
index|]
operator|.
name|noNulls
operator|=
literal|true
expr_stmt|;
name|recordIdColumnVector
operator|.
name|fields
index|[
literal|0
index|]
operator|.
name|isRepeating
operator|=
literal|true
expr_stmt|;
operator|(
operator|(
name|LongColumnVector
operator|)
name|recordIdColumnVector
operator|.
name|fields
index|[
literal|0
index|]
operator|)
operator|.
name|vector
index|[
literal|0
index|]
operator|=
name|syntheticProps
operator|.
name|syntheticTxnId
expr_stmt|;
comment|/**        * This is {@link RecordIdentifier#getBucketProperty()}        * Also see {@link BucketCodec}        */
name|recordIdColumnVector
operator|.
name|fields
index|[
literal|1
index|]
operator|.
name|noNulls
operator|=
literal|true
expr_stmt|;
name|recordIdColumnVector
operator|.
name|fields
index|[
literal|1
index|]
operator|.
name|isRepeating
operator|=
literal|true
expr_stmt|;
operator|(
operator|(
name|LongColumnVector
operator|)
name|recordIdColumnVector
operator|.
name|fields
index|[
literal|1
index|]
operator|)
operator|.
name|vector
index|[
literal|0
index|]
operator|=
name|syntheticProps
operator|.
name|bucketProperty
expr_stmt|;
comment|/**        * {@link RecordIdentifier#getRowId()}        */
name|recordIdColumnVector
operator|.
name|fields
index|[
literal|2
index|]
operator|.
name|noNulls
operator|=
literal|true
expr_stmt|;
name|recordIdColumnVector
operator|.
name|fields
index|[
literal|2
index|]
operator|.
name|isRepeating
operator|=
literal|false
expr_stmt|;
name|long
index|[]
name|rowIdVector
init|=
operator|(
operator|(
name|LongColumnVector
operator|)
name|recordIdColumnVector
operator|.
name|fields
index|[
literal|2
index|]
operator|)
operator|.
name|vector
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|vectorizedRowBatchBase
operator|.
name|size
condition|;
name|i
operator|++
control|)
block|{
comment|//baseReader.getRowNumber() seems to point at the start of the batch todo: validate
name|rowIdVector
index|[
name|i
index|]
operator|=
name|syntheticProps
operator|.
name|rowIdOffset
operator|+
name|innerReader
operator|.
name|getRowNumber
argument_list|()
operator|+
name|i
expr_stmt|;
block|}
comment|//Now populate a structure to use to apply delete events
name|innerRecordIdColumnVector
operator|=
operator|new
name|ColumnVector
index|[
name|OrcRecordUpdater
operator|.
name|FIELDS
index|]
expr_stmt|;
name|innerRecordIdColumnVector
index|[
name|OrcRecordUpdater
operator|.
name|ORIGINAL_WRITEID
index|]
operator|=
name|recordIdColumnVector
operator|.
name|fields
index|[
literal|0
index|]
expr_stmt|;
name|innerRecordIdColumnVector
index|[
name|OrcRecordUpdater
operator|.
name|BUCKET
index|]
operator|=
name|recordIdColumnVector
operator|.
name|fields
index|[
literal|1
index|]
expr_stmt|;
name|innerRecordIdColumnVector
index|[
name|OrcRecordUpdater
operator|.
name|ROW_ID
index|]
operator|=
name|recordIdColumnVector
operator|.
name|fields
index|[
literal|2
index|]
expr_stmt|;
comment|//these are insert events so (original txn == current) txn for all rows
name|innerRecordIdColumnVector
index|[
name|OrcRecordUpdater
operator|.
name|CURRENT_WRITEID
index|]
operator|=
name|recordIdColumnVector
operator|.
name|fields
index|[
literal|0
index|]
expr_stmt|;
block|}
if|if
condition|(
name|syntheticProps
operator|.
name|syntheticTxnId
operator|>
literal|0
condition|)
block|{
comment|//"originals" (written before table was converted to acid) is considered written by
comment|// txnid:0 which is always committed so there is no need to check wrt invalid transactions
comment|//But originals written by Load Data for example can be in base_x or delta_x_x so we must
comment|//check if 'x' is committed or not evn if ROW_ID is not needed in the Operator pipeline.
if|if
condition|(
name|needSyntheticRowId
condition|)
block|{
name|findRecordsWithInvalidTransactionIds
argument_list|(
name|innerRecordIdColumnVector
argument_list|,
name|vectorizedRowBatchBase
operator|.
name|size
argument_list|,
name|selectedBitSet
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|/*since ROW_IDs are not needed we didn't create the ColumnVectors to hold them but we         * still have to check if the data being read is committed as far as current         * reader (transactions) is concerned.  Since here we are reading 'original' schema file,         * all rows in it have been created by the same txn, namely 'syntheticProps.syntheticTxnId'         */
if|if
condition|(
operator|!
name|validWriteIdList
operator|.
name|isWriteIdValid
argument_list|(
name|syntheticProps
operator|.
name|syntheticTxnId
argument_list|)
condition|)
block|{
name|selectedBitSet
operator|.
name|clear
argument_list|(
literal|0
argument_list|,
name|vectorizedRowBatchBase
operator|.
name|size
argument_list|)
expr_stmt|;
block|}
block|}
block|}
return|return
name|innerRecordIdColumnVector
return|;
block|}
specifier|private
name|void
name|findRecordsWithInvalidTransactionIds
parameter_list|(
name|VectorizedRowBatch
name|batch
parameter_list|,
name|BitSet
name|selectedBitSet
parameter_list|)
block|{
name|findRecordsWithInvalidTransactionIds
argument_list|(
name|batch
operator|.
name|cols
argument_list|,
name|batch
operator|.
name|size
argument_list|,
name|selectedBitSet
argument_list|)
expr_stmt|;
block|}
specifier|private
name|void
name|findRecordsWithInvalidTransactionIds
parameter_list|(
name|ColumnVector
index|[]
name|cols
parameter_list|,
name|int
name|size
parameter_list|,
name|BitSet
name|selectedBitSet
parameter_list|)
block|{
if|if
condition|(
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|CURRENT_WRITEID
index|]
operator|.
name|isRepeating
condition|)
block|{
comment|// When we have repeating values, we can unset the whole bitset at once
comment|// if the repeating value is not a valid transaction.
name|long
name|currentTransactionIdForBatch
init|=
operator|(
operator|(
name|LongColumnVector
operator|)
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|CURRENT_WRITEID
index|]
operator|)
operator|.
name|vector
index|[
literal|0
index|]
decl_stmt|;
if|if
condition|(
operator|!
name|validWriteIdList
operator|.
name|isWriteIdValid
argument_list|(
name|currentTransactionIdForBatch
argument_list|)
condition|)
block|{
name|selectedBitSet
operator|.
name|clear
argument_list|(
literal|0
argument_list|,
name|size
argument_list|)
expr_stmt|;
block|}
return|return;
block|}
name|long
index|[]
name|currentTransactionVector
init|=
operator|(
operator|(
name|LongColumnVector
operator|)
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|CURRENT_WRITEID
index|]
operator|)
operator|.
name|vector
decl_stmt|;
comment|// Loop through the bits that are set to true and mark those rows as false, if their
comment|// current transactions are not valid.
for|for
control|(
name|int
name|setBitIndex
init|=
name|selectedBitSet
operator|.
name|nextSetBit
argument_list|(
literal|0
argument_list|)
init|;
name|setBitIndex
operator|>=
literal|0
condition|;
name|setBitIndex
operator|=
name|selectedBitSet
operator|.
name|nextSetBit
argument_list|(
name|setBitIndex
operator|+
literal|1
argument_list|)
control|)
block|{
if|if
condition|(
operator|!
name|validWriteIdList
operator|.
name|isWriteIdValid
argument_list|(
name|currentTransactionVector
index|[
name|setBitIndex
index|]
argument_list|)
condition|)
block|{
name|selectedBitSet
operator|.
name|clear
argument_list|(
name|setBitIndex
argument_list|)
expr_stmt|;
block|}
block|}
block|}
annotation|@
name|Override
specifier|public
name|NullWritable
name|createKey
parameter_list|()
block|{
return|return
name|NullWritable
operator|.
name|get
argument_list|()
return|;
block|}
annotation|@
name|Override
specifier|public
name|VectorizedRowBatch
name|createValue
parameter_list|()
block|{
return|return
name|rbCtx
operator|.
name|createVectorizedRowBatch
argument_list|()
return|;
block|}
annotation|@
name|Override
specifier|public
name|long
name|getPos
parameter_list|()
throws|throws
name|IOException
block|{
return|return
name|offset
operator|+
call|(
name|long
call|)
argument_list|(
name|progress
operator|*
name|length
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|close
parameter_list|()
throws|throws
name|IOException
block|{
try|try
block|{
name|this
operator|.
name|baseReader
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
finally|finally
block|{
name|this
operator|.
name|deleteEventRegistry
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
annotation|@
name|Override
specifier|public
name|float
name|getProgress
parameter_list|()
throws|throws
name|IOException
block|{
return|return
name|progress
return|;
block|}
annotation|@
name|VisibleForTesting
name|DeleteEventRegistry
name|getDeleteEventRegistry
parameter_list|()
block|{
return|return
name|deleteEventRegistry
return|;
block|}
comment|/**    * An interface that can determine which rows have been deleted    * from a given vectorized row batch. Implementations of this interface    * will read the delete delta files and will create their own internal    * data structures to maintain record ids of the records that got deleted.    */
specifier|protected
specifier|static
interface|interface
name|DeleteEventRegistry
block|{
comment|/**      * Modifies the passed bitset to indicate which of the rows in the batch      * have been deleted. Assumes that the batch.size is equal to bitset size.      * @param cols      * @param size      * @param selectedBitSet      * @throws IOException      */
specifier|public
name|void
name|findDeletedRecords
parameter_list|(
name|ColumnVector
index|[]
name|cols
parameter_list|,
name|int
name|size
parameter_list|,
name|BitSet
name|selectedBitSet
parameter_list|)
throws|throws
name|IOException
function_decl|;
comment|/**      * The close() method can be called externally to signal the implementing classes      * to free up resources.      * @throws IOException      */
specifier|public
name|void
name|close
parameter_list|()
throws|throws
name|IOException
function_decl|;
comment|/**      * @return {@code true} if no delete events were found      */
name|boolean
name|isEmpty
parameter_list|()
function_decl|;
block|}
comment|/**    * An implementation for DeleteEventRegistry that opens the delete delta files all    * at once, and then uses the sort-merge algorithm to maintain a sorted list of    * delete events. This internally uses the OrcRawRecordMerger and maintains a constant    * amount of memory usage, given the number of delete delta files. Therefore, this    * implementation will be picked up when the memory pressure is high.    */
specifier|static
class|class
name|SortMergedDeleteEventRegistry
implements|implements
name|DeleteEventRegistry
block|{
specifier|private
name|OrcRawRecordMerger
name|deleteRecords
decl_stmt|;
specifier|private
name|OrcRawRecordMerger
operator|.
name|ReaderKey
name|deleteRecordKey
decl_stmt|;
specifier|private
name|OrcStruct
name|deleteRecordValue
decl_stmt|;
specifier|private
name|Boolean
name|isDeleteRecordAvailable
init|=
literal|null
decl_stmt|;
specifier|private
name|ValidWriteIdList
name|validWriteIdList
decl_stmt|;
name|SortMergedDeleteEventRegistry
parameter_list|(
name|JobConf
name|conf
parameter_list|,
name|OrcSplit
name|orcSplit
parameter_list|,
name|Reader
operator|.
name|Options
name|readerOptions
parameter_list|)
throws|throws
name|IOException
block|{
specifier|final
name|Path
index|[]
name|deleteDeltas
init|=
name|getDeleteDeltaDirsFromSplit
argument_list|(
name|orcSplit
argument_list|)
decl_stmt|;
if|if
condition|(
name|deleteDeltas
operator|.
name|length
operator|>
literal|0
condition|)
block|{
name|int
name|bucket
init|=
name|AcidUtils
operator|.
name|parseBaseOrDeltaBucketFilename
argument_list|(
name|orcSplit
operator|.
name|getPath
argument_list|()
argument_list|,
name|conf
argument_list|)
operator|.
name|getBucketId
argument_list|()
decl_stmt|;
name|String
name|txnString
init|=
name|conf
operator|.
name|get
argument_list|(
name|ValidWriteIdList
operator|.
name|VALID_WRITEIDS_KEY
argument_list|)
decl_stmt|;
name|this
operator|.
name|validWriteIdList
operator|=
operator|(
name|txnString
operator|==
literal|null
operator|)
condition|?
operator|new
name|ValidReaderWriteIdList
argument_list|()
else|:
operator|new
name|ValidReaderWriteIdList
argument_list|(
name|txnString
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"SortMergedDeleteEventRegistry:: Read ValidWriteIdList: "
operator|+
name|this
operator|.
name|validWriteIdList
operator|.
name|toString
argument_list|()
operator|+
literal|" isFullAcidTable: "
operator|+
name|AcidUtils
operator|.
name|isFullAcidScan
argument_list|(
name|conf
argument_list|)
argument_list|)
expr_stmt|;
name|OrcRawRecordMerger
operator|.
name|Options
name|mergerOptions
init|=
operator|new
name|OrcRawRecordMerger
operator|.
name|Options
argument_list|()
operator|.
name|isDeleteReader
argument_list|(
literal|true
argument_list|)
decl_stmt|;
assert|assert
operator|!
name|orcSplit
operator|.
name|isOriginal
argument_list|()
operator|:
literal|"If this now supports Original splits, set up mergeOptions properly"
assert|;
name|this
operator|.
name|deleteRecords
operator|=
operator|new
name|OrcRawRecordMerger
argument_list|(
name|conf
argument_list|,
literal|true
argument_list|,
literal|null
argument_list|,
literal|false
argument_list|,
name|bucket
argument_list|,
name|validWriteIdList
argument_list|,
name|readerOptions
argument_list|,
name|deleteDeltas
argument_list|,
name|mergerOptions
argument_list|)
expr_stmt|;
name|this
operator|.
name|deleteRecordKey
operator|=
operator|new
name|OrcRawRecordMerger
operator|.
name|ReaderKey
argument_list|()
expr_stmt|;
name|this
operator|.
name|deleteRecordValue
operator|=
name|this
operator|.
name|deleteRecords
operator|.
name|createValue
argument_list|()
expr_stmt|;
comment|// Initialize the first value in the delete reader.
name|this
operator|.
name|isDeleteRecordAvailable
operator|=
name|this
operator|.
name|deleteRecords
operator|.
name|next
argument_list|(
name|deleteRecordKey
argument_list|,
name|deleteRecordValue
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|this
operator|.
name|isDeleteRecordAvailable
operator|=
literal|false
expr_stmt|;
name|this
operator|.
name|deleteRecordKey
operator|=
literal|null
expr_stmt|;
name|this
operator|.
name|deleteRecordValue
operator|=
literal|null
expr_stmt|;
name|this
operator|.
name|deleteRecords
operator|=
literal|null
expr_stmt|;
block|}
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|isEmpty
parameter_list|()
block|{
if|if
condition|(
name|isDeleteRecordAvailable
operator|==
literal|null
condition|)
block|{
throw|throw
operator|new
name|IllegalStateException
argument_list|(
literal|"Not yet initialized"
argument_list|)
throw|;
block|}
return|return
operator|!
name|isDeleteRecordAvailable
return|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|findDeletedRecords
parameter_list|(
name|ColumnVector
index|[]
name|cols
parameter_list|,
name|int
name|size
parameter_list|,
name|BitSet
name|selectedBitSet
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
operator|!
name|isDeleteRecordAvailable
condition|)
block|{
return|return;
block|}
name|long
index|[]
name|originalTransaction
init|=
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|ORIGINAL_WRITEID
index|]
operator|.
name|isRepeating
condition|?
literal|null
else|:
operator|(
operator|(
name|LongColumnVector
operator|)
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|ORIGINAL_WRITEID
index|]
operator|)
operator|.
name|vector
decl_stmt|;
name|long
index|[]
name|bucket
init|=
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|BUCKET
index|]
operator|.
name|isRepeating
condition|?
literal|null
else|:
operator|(
operator|(
name|LongColumnVector
operator|)
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|BUCKET
index|]
operator|)
operator|.
name|vector
decl_stmt|;
name|long
index|[]
name|rowId
init|=
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|ROW_ID
index|]
operator|.
name|isRepeating
condition|?
literal|null
else|:
operator|(
operator|(
name|LongColumnVector
operator|)
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|ROW_ID
index|]
operator|)
operator|.
name|vector
decl_stmt|;
comment|// The following repeatedX values will be set, if any of the columns are repeating.
name|long
name|repeatedOriginalTransaction
init|=
operator|(
name|originalTransaction
operator|!=
literal|null
operator|)
condition|?
operator|-
literal|1
else|:
operator|(
operator|(
name|LongColumnVector
operator|)
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|ORIGINAL_WRITEID
index|]
operator|)
operator|.
name|vector
index|[
literal|0
index|]
decl_stmt|;
name|long
name|repeatedBucket
init|=
operator|(
name|bucket
operator|!=
literal|null
operator|)
condition|?
operator|-
literal|1
else|:
operator|(
operator|(
name|LongColumnVector
operator|)
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|BUCKET
index|]
operator|)
operator|.
name|vector
index|[
literal|0
index|]
decl_stmt|;
name|long
name|repeatedRowId
init|=
operator|(
name|rowId
operator|!=
literal|null
operator|)
condition|?
operator|-
literal|1
else|:
operator|(
operator|(
name|LongColumnVector
operator|)
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|ROW_ID
index|]
operator|)
operator|.
name|vector
index|[
literal|0
index|]
decl_stmt|;
comment|// Get the first valid row in the batch still available.
name|int
name|firstValidIndex
init|=
name|selectedBitSet
operator|.
name|nextSetBit
argument_list|(
literal|0
argument_list|)
decl_stmt|;
if|if
condition|(
name|firstValidIndex
operator|==
operator|-
literal|1
condition|)
block|{
return|return;
comment|// Everything in the batch has already been filtered out.
block|}
name|RecordIdentifier
name|firstRecordIdInBatch
init|=
operator|new
name|RecordIdentifier
argument_list|(
name|originalTransaction
operator|!=
literal|null
condition|?
name|originalTransaction
index|[
name|firstValidIndex
index|]
else|:
name|repeatedOriginalTransaction
argument_list|,
name|bucket
operator|!=
literal|null
condition|?
operator|(
name|int
operator|)
name|bucket
index|[
name|firstValidIndex
index|]
else|:
operator|(
name|int
operator|)
name|repeatedBucket
argument_list|,
name|rowId
operator|!=
literal|null
condition|?
operator|(
name|int
operator|)
name|rowId
index|[
name|firstValidIndex
index|]
else|:
name|repeatedRowId
argument_list|)
decl_stmt|;
comment|// Get the last valid row in the batch still available.
name|int
name|lastValidIndex
init|=
name|selectedBitSet
operator|.
name|previousSetBit
argument_list|(
name|size
operator|-
literal|1
argument_list|)
decl_stmt|;
name|RecordIdentifier
name|lastRecordIdInBatch
init|=
operator|new
name|RecordIdentifier
argument_list|(
name|originalTransaction
operator|!=
literal|null
condition|?
name|originalTransaction
index|[
name|lastValidIndex
index|]
else|:
name|repeatedOriginalTransaction
argument_list|,
name|bucket
operator|!=
literal|null
condition|?
operator|(
name|int
operator|)
name|bucket
index|[
name|lastValidIndex
index|]
else|:
operator|(
name|int
operator|)
name|repeatedBucket
argument_list|,
name|rowId
operator|!=
literal|null
condition|?
operator|(
name|int
operator|)
name|rowId
index|[
name|lastValidIndex
index|]
else|:
name|repeatedRowId
argument_list|)
decl_stmt|;
comment|// We must iterate over all the delete records, until we find one record with
comment|// deleteRecord>= firstRecordInBatch or until we exhaust all the delete records.
while|while
condition|(
name|deleteRecordKey
operator|.
name|compareRow
argument_list|(
name|firstRecordIdInBatch
argument_list|)
operator|==
operator|-
literal|1
condition|)
block|{
name|isDeleteRecordAvailable
operator|=
name|deleteRecords
operator|.
name|next
argument_list|(
name|deleteRecordKey
argument_list|,
name|deleteRecordValue
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|isDeleteRecordAvailable
condition|)
return|return;
comment|// exhausted all delete records, return.
block|}
comment|// If we are here, then we have established that firstRecordInBatch<= deleteRecord.
comment|// Now continue marking records which have been deleted until we reach the end of the batch
comment|// or we exhaust all the delete records.
name|int
name|currIndex
init|=
name|firstValidIndex
decl_stmt|;
name|RecordIdentifier
name|currRecordIdInBatch
init|=
operator|new
name|RecordIdentifier
argument_list|()
decl_stmt|;
while|while
condition|(
name|isDeleteRecordAvailable
operator|&&
name|currIndex
operator|!=
operator|-
literal|1
operator|&&
name|currIndex
operator|<=
name|lastValidIndex
condition|)
block|{
name|currRecordIdInBatch
operator|.
name|setValues
argument_list|(
operator|(
name|originalTransaction
operator|!=
literal|null
operator|)
condition|?
name|originalTransaction
index|[
name|currIndex
index|]
else|:
name|repeatedOriginalTransaction
argument_list|,
operator|(
name|bucket
operator|!=
literal|null
operator|)
condition|?
operator|(
name|int
operator|)
name|bucket
index|[
name|currIndex
index|]
else|:
operator|(
name|int
operator|)
name|repeatedBucket
argument_list|,
operator|(
name|rowId
operator|!=
literal|null
operator|)
condition|?
name|rowId
index|[
name|currIndex
index|]
else|:
name|repeatedRowId
argument_list|)
expr_stmt|;
if|if
condition|(
name|deleteRecordKey
operator|.
name|compareRow
argument_list|(
name|currRecordIdInBatch
argument_list|)
operator|==
literal|0
condition|)
block|{
comment|// When deleteRecordId == currRecordIdInBatch, this record in the batch has been deleted.
name|selectedBitSet
operator|.
name|clear
argument_list|(
name|currIndex
argument_list|)
expr_stmt|;
name|currIndex
operator|=
name|selectedBitSet
operator|.
name|nextSetBit
argument_list|(
name|currIndex
operator|+
literal|1
argument_list|)
expr_stmt|;
comment|// Move to next valid index.
block|}
elseif|else
if|if
condition|(
name|deleteRecordKey
operator|.
name|compareRow
argument_list|(
name|currRecordIdInBatch
argument_list|)
operator|==
literal|1
condition|)
block|{
comment|// When deleteRecordId> currRecordIdInBatch, we have to move on to look at the
comment|// next record in the batch.
comment|// But before that, can we short-circuit and skip the entire batch itself
comment|// by checking if the deleteRecordId> lastRecordInBatch?
if|if
condition|(
name|deleteRecordKey
operator|.
name|compareRow
argument_list|(
name|lastRecordIdInBatch
argument_list|)
operator|==
literal|1
condition|)
block|{
return|return;
comment|// Yay! We short-circuited, skip everything remaining in the batch and return.
block|}
name|currIndex
operator|=
name|selectedBitSet
operator|.
name|nextSetBit
argument_list|(
name|currIndex
operator|+
literal|1
argument_list|)
expr_stmt|;
comment|// Move to next valid index.
block|}
else|else
block|{
comment|// We have deleteRecordId< currRecordIdInBatch, we must now move on to find
comment|// next the larger deleteRecordId that can possibly match anything in the batch.
name|isDeleteRecordAvailable
operator|=
name|deleteRecords
operator|.
name|next
argument_list|(
name|deleteRecordKey
argument_list|,
name|deleteRecordValue
argument_list|)
expr_stmt|;
block|}
block|}
block|}
annotation|@
name|Override
specifier|public
name|void
name|close
parameter_list|()
throws|throws
name|IOException
block|{
if|if
condition|(
name|this
operator|.
name|deleteRecords
operator|!=
literal|null
condition|)
block|{
name|this
operator|.
name|deleteRecords
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
block|}
comment|/**    * An implementation for DeleteEventRegistry that optimizes for performance by loading    * all the delete events into memory at once from all the delete delta files.    * It starts by reading all the delete events through a regular sort merge logic    * into 3 vectors- one for original transaction id (otid), one for bucket property and one for    * row id.  See {@link BucketCodec} for more about bucket property.    * The otids are likely to be repeated very often, as a single transaction    * often deletes thousands of rows. Hence, the otid vector is compressed to only store the    * toIndex and fromIndex ranges in the larger row id vector. Now, querying whether a    * record id is deleted or not, is done by performing a binary search on the    * compressed otid range. If a match is found, then a binary search is then performed on    * the larger rowId vector between the given toIndex and fromIndex. Of course, there is rough    * heuristic that prevents creation of an instance of this class if the memory pressure is high.    * The SortMergedDeleteEventRegistry is then the fallback method for such scenarios.    */
specifier|static
class|class
name|ColumnizedDeleteEventRegistry
implements|implements
name|DeleteEventRegistry
block|{
comment|/**      * A simple wrapper class to hold the (otid, bucketProperty, rowId) pair.      */
specifier|static
class|class
name|DeleteRecordKey
implements|implements
name|Comparable
argument_list|<
name|DeleteRecordKey
argument_list|>
block|{
specifier|private
name|long
name|originalTransactionId
decl_stmt|;
comment|/**        * see {@link BucketCodec}        */
specifier|private
name|int
name|bucketProperty
decl_stmt|;
specifier|private
name|long
name|rowId
decl_stmt|;
name|DeleteRecordKey
parameter_list|()
block|{
name|this
operator|.
name|originalTransactionId
operator|=
operator|-
literal|1
expr_stmt|;
name|this
operator|.
name|rowId
operator|=
operator|-
literal|1
expr_stmt|;
block|}
specifier|public
name|void
name|set
parameter_list|(
name|long
name|otid
parameter_list|,
name|int
name|bucketProperty
parameter_list|,
name|long
name|rowId
parameter_list|)
block|{
name|this
operator|.
name|originalTransactionId
operator|=
name|otid
expr_stmt|;
name|this
operator|.
name|bucketProperty
operator|=
name|bucketProperty
expr_stmt|;
name|this
operator|.
name|rowId
operator|=
name|rowId
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|int
name|compareTo
parameter_list|(
name|DeleteRecordKey
name|other
parameter_list|)
block|{
if|if
condition|(
name|other
operator|==
literal|null
condition|)
block|{
return|return
operator|-
literal|1
return|;
block|}
if|if
condition|(
name|originalTransactionId
operator|!=
name|other
operator|.
name|originalTransactionId
condition|)
block|{
return|return
name|originalTransactionId
operator|<
name|other
operator|.
name|originalTransactionId
condition|?
operator|-
literal|1
else|:
literal|1
return|;
block|}
if|if
condition|(
name|bucketProperty
operator|!=
name|other
operator|.
name|bucketProperty
condition|)
block|{
return|return
name|bucketProperty
operator|<
name|other
operator|.
name|bucketProperty
condition|?
operator|-
literal|1
else|:
literal|1
return|;
block|}
if|if
condition|(
name|rowId
operator|!=
name|other
operator|.
name|rowId
condition|)
block|{
return|return
name|rowId
operator|<
name|other
operator|.
name|rowId
condition|?
operator|-
literal|1
else|:
literal|1
return|;
block|}
return|return
literal|0
return|;
block|}
annotation|@
name|Override
specifier|public
name|String
name|toString
parameter_list|()
block|{
return|return
literal|"otid: "
operator|+
name|originalTransactionId
operator|+
literal|" bucketP:"
operator|+
name|bucketProperty
operator|+
literal|" rowid: "
operator|+
name|rowId
return|;
block|}
block|}
comment|/**      * This class actually reads the delete delta files in vectorized row batches.      * For every call to next(), it returns the next smallest record id in the file if available.      * Internally, the next() buffers a row batch and maintains an index pointer, reading the      * next batch when the previous batch is exhausted.      *      * For unbucketed tables this will currently return all delete events.  Once we trust that      * the N in bucketN for "base" spit is reliable, all delete events not matching N can be skipped.      */
specifier|static
class|class
name|DeleteReaderValue
block|{
specifier|private
name|VectorizedRowBatch
name|batch
decl_stmt|;
specifier|private
specifier|final
name|RecordReader
name|recordReader
decl_stmt|;
specifier|private
name|int
name|indexPtrInBatch
decl_stmt|;
specifier|private
specifier|final
name|int
name|bucketForSplit
decl_stmt|;
comment|// The bucket value should be same for all the records.
specifier|private
specifier|final
name|ValidWriteIdList
name|validWriteIdList
decl_stmt|;
specifier|private
name|boolean
name|isBucketPropertyRepeating
decl_stmt|;
specifier|private
specifier|final
name|boolean
name|isBucketedTable
decl_stmt|;
name|DeleteReaderValue
parameter_list|(
name|Reader
name|deleteDeltaReader
parameter_list|,
name|Reader
operator|.
name|Options
name|readerOptions
parameter_list|,
name|int
name|bucket
parameter_list|,
name|ValidWriteIdList
name|validWriteIdList
parameter_list|,
name|boolean
name|isBucketedTable
parameter_list|)
throws|throws
name|IOException
block|{
name|this
operator|.
name|recordReader
operator|=
name|deleteDeltaReader
operator|.
name|rowsOptions
argument_list|(
name|readerOptions
argument_list|)
expr_stmt|;
name|this
operator|.
name|bucketForSplit
operator|=
name|bucket
expr_stmt|;
name|this
operator|.
name|batch
operator|=
name|deleteDeltaReader
operator|.
name|getSchema
argument_list|()
operator|.
name|createRowBatch
argument_list|()
expr_stmt|;
if|if
condition|(
operator|!
name|recordReader
operator|.
name|nextBatch
argument_list|(
name|batch
argument_list|)
condition|)
block|{
comment|// Read the first batch.
name|this
operator|.
name|batch
operator|=
literal|null
expr_stmt|;
comment|// Oh! the first batch itself was null. Close the reader.
block|}
name|this
operator|.
name|indexPtrInBatch
operator|=
literal|0
expr_stmt|;
name|this
operator|.
name|validWriteIdList
operator|=
name|validWriteIdList
expr_stmt|;
name|this
operator|.
name|isBucketedTable
operator|=
name|isBucketedTable
expr_stmt|;
name|checkBucketId
argument_list|()
expr_stmt|;
comment|//check 1st batch
block|}
specifier|public
name|boolean
name|next
parameter_list|(
name|DeleteRecordKey
name|deleteRecordKey
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|batch
operator|==
literal|null
condition|)
block|{
return|return
literal|false
return|;
block|}
name|boolean
name|isValidNext
init|=
literal|false
decl_stmt|;
while|while
condition|(
operator|!
name|isValidNext
condition|)
block|{
if|if
condition|(
name|indexPtrInBatch
operator|>=
name|batch
operator|.
name|size
condition|)
block|{
comment|// We have exhausted our current batch, read the next batch.
if|if
condition|(
name|recordReader
operator|.
name|nextBatch
argument_list|(
name|batch
argument_list|)
condition|)
block|{
name|checkBucketId
argument_list|()
expr_stmt|;
name|indexPtrInBatch
operator|=
literal|0
expr_stmt|;
comment|// After reading the batch, reset the pointer to beginning.
block|}
else|else
block|{
return|return
literal|false
return|;
comment|// no more batches to read, exhausted the reader.
block|}
block|}
name|long
name|currentTransaction
init|=
name|setCurrentDeleteKey
argument_list|(
name|deleteRecordKey
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|isBucketPropertyRepeating
condition|)
block|{
name|checkBucketId
argument_list|(
name|deleteRecordKey
operator|.
name|bucketProperty
argument_list|)
expr_stmt|;
block|}
operator|++
name|indexPtrInBatch
expr_stmt|;
if|if
condition|(
name|validWriteIdList
operator|.
name|isWriteIdValid
argument_list|(
name|currentTransaction
argument_list|)
condition|)
block|{
name|isValidNext
operator|=
literal|true
expr_stmt|;
block|}
block|}
return|return
literal|true
return|;
block|}
specifier|public
name|void
name|close
parameter_list|()
throws|throws
name|IOException
block|{
name|this
operator|.
name|recordReader
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
specifier|private
name|long
name|setCurrentDeleteKey
parameter_list|(
name|DeleteRecordKey
name|deleteRecordKey
parameter_list|)
block|{
name|int
name|originalTransactionIndex
init|=
name|batch
operator|.
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|ORIGINAL_WRITEID
index|]
operator|.
name|isRepeating
condition|?
literal|0
else|:
name|indexPtrInBatch
decl_stmt|;
name|long
name|originalTransaction
init|=
operator|(
operator|(
name|LongColumnVector
operator|)
name|batch
operator|.
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|ORIGINAL_WRITEID
index|]
operator|)
operator|.
name|vector
index|[
name|originalTransactionIndex
index|]
decl_stmt|;
name|int
name|bucketPropertyIndex
init|=
name|batch
operator|.
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|BUCKET
index|]
operator|.
name|isRepeating
condition|?
literal|0
else|:
name|indexPtrInBatch
decl_stmt|;
name|int
name|bucketProperty
init|=
call|(
name|int
call|)
argument_list|(
operator|(
name|LongColumnVector
operator|)
name|batch
operator|.
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|BUCKET
index|]
argument_list|)
operator|.
name|vector
index|[
name|bucketPropertyIndex
index|]
decl_stmt|;
name|long
name|rowId
init|=
operator|(
operator|(
name|LongColumnVector
operator|)
name|batch
operator|.
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|ROW_ID
index|]
operator|)
operator|.
name|vector
index|[
name|indexPtrInBatch
index|]
decl_stmt|;
name|int
name|currentTransactionIndex
init|=
name|batch
operator|.
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|CURRENT_WRITEID
index|]
operator|.
name|isRepeating
condition|?
literal|0
else|:
name|indexPtrInBatch
decl_stmt|;
name|long
name|currentTransaction
init|=
operator|(
operator|(
name|LongColumnVector
operator|)
name|batch
operator|.
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|CURRENT_WRITEID
index|]
operator|)
operator|.
name|vector
index|[
name|currentTransactionIndex
index|]
decl_stmt|;
name|deleteRecordKey
operator|.
name|set
argument_list|(
name|originalTransaction
argument_list|,
name|bucketProperty
argument_list|,
name|rowId
argument_list|)
expr_stmt|;
return|return
name|currentTransaction
return|;
block|}
specifier|private
name|void
name|checkBucketId
parameter_list|()
throws|throws
name|IOException
block|{
name|isBucketPropertyRepeating
operator|=
name|batch
operator|.
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|BUCKET
index|]
operator|.
name|isRepeating
expr_stmt|;
if|if
condition|(
name|isBucketPropertyRepeating
condition|)
block|{
name|int
name|bucketPropertyFromRecord
init|=
call|(
name|int
call|)
argument_list|(
operator|(
name|LongColumnVector
operator|)
name|batch
operator|.
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|BUCKET
index|]
argument_list|)
operator|.
name|vector
index|[
literal|0
index|]
decl_stmt|;
name|checkBucketId
argument_list|(
name|bucketPropertyFromRecord
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**        * Whenever we are reading a batch, we must ensure that all the records in the batch        * have the same bucket id as the bucket id of the split. If not, throw exception.        * NOTE: this assertion might not hold, once virtual bucketing is in place. However,        * it should be simple to fix that case. Just replace check for bucket equality with        * a check for valid bucket mapping. Until virtual bucketing is added, it means        * either the split computation got messed up or we found some corrupted records.        */
specifier|private
name|void
name|checkBucketId
parameter_list|(
name|int
name|bucketPropertyFromRecord
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
operator|!
name|isBucketedTable
condition|)
block|{
comment|/**            * in this case a file inside a delete_delta_x_y/bucketN may contain any value for            * bucketId in {@link RecordIdentifier#getBucketProperty()}            */
return|return;
block|}
name|int
name|bucketIdFromRecord
init|=
name|BucketCodec
operator|.
name|determineVersion
argument_list|(
name|bucketPropertyFromRecord
argument_list|)
operator|.
name|decodeWriterId
argument_list|(
name|bucketPropertyFromRecord
argument_list|)
decl_stmt|;
if|if
condition|(
name|bucketIdFromRecord
operator|!=
name|bucketForSplit
condition|)
block|{
name|DeleteRecordKey
name|dummy
init|=
operator|new
name|DeleteRecordKey
argument_list|()
decl_stmt|;
name|long
name|curTxnId
init|=
name|setCurrentDeleteKey
argument_list|(
name|dummy
argument_list|)
decl_stmt|;
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Corrupted records with different bucket ids "
operator|+
literal|"from the containing bucket file found! Expected bucket id "
operator|+
name|bucketForSplit
operator|+
literal|", however found the bucket id "
operator|+
name|bucketIdFromRecord
operator|+
literal|" from "
operator|+
name|dummy
operator|+
literal|" curTxnId: "
operator|+
name|curTxnId
argument_list|)
throw|;
block|}
block|}
block|}
comment|/**      * A CompressedOtid class stores a compressed representation of the original      * transaction ids (otids) read from the delete delta files. Since the record ids      * are sorted by (otid, rowId) and otids are highly likely to be repetitive, it is      * efficient to compress them as a CompressedOtid that stores the fromIndex and      * the toIndex. These fromIndex and toIndex reference the larger vector formed by      * concatenating the correspondingly ordered rowIds.      */
specifier|private
specifier|final
class|class
name|CompressedOtid
implements|implements
name|Comparable
argument_list|<
name|CompressedOtid
argument_list|>
block|{
specifier|final
name|long
name|originalTransactionId
decl_stmt|;
specifier|final
name|int
name|bucketProperty
decl_stmt|;
specifier|final
name|int
name|fromIndex
decl_stmt|;
comment|// inclusive
specifier|final
name|int
name|toIndex
decl_stmt|;
comment|// exclusive
name|CompressedOtid
parameter_list|(
name|long
name|otid
parameter_list|,
name|int
name|bucketProperty
parameter_list|,
name|int
name|fromIndex
parameter_list|,
name|int
name|toIndex
parameter_list|)
block|{
name|this
operator|.
name|originalTransactionId
operator|=
name|otid
expr_stmt|;
name|this
operator|.
name|bucketProperty
operator|=
name|bucketProperty
expr_stmt|;
name|this
operator|.
name|fromIndex
operator|=
name|fromIndex
expr_stmt|;
name|this
operator|.
name|toIndex
operator|=
name|toIndex
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|int
name|compareTo
parameter_list|(
name|CompressedOtid
name|other
parameter_list|)
block|{
comment|// When comparing the CompressedOtid, the one with the lesser value is smaller.
if|if
condition|(
name|originalTransactionId
operator|!=
name|other
operator|.
name|originalTransactionId
condition|)
block|{
return|return
name|originalTransactionId
operator|<
name|other
operator|.
name|originalTransactionId
condition|?
operator|-
literal|1
else|:
literal|1
return|;
block|}
if|if
condition|(
name|bucketProperty
operator|!=
name|other
operator|.
name|bucketProperty
condition|)
block|{
return|return
name|bucketProperty
operator|<
name|other
operator|.
name|bucketProperty
condition|?
operator|-
literal|1
else|:
literal|1
return|;
block|}
return|return
literal|0
return|;
block|}
block|}
comment|/**      * Food for thought:      * this is a bit problematic - in order to load ColumnizedDeleteEventRegistry we still open      * all delete deltas at once - possibly causing OOM same as for {@link SortMergedDeleteEventRegistry}      * which uses {@link OrcRawRecordMerger}.  Why not load all delete_delta sequentially.  Each      * dd is sorted by {@link RecordIdentifier} so we could create a BTree like structure where the      * 1st level is an array of originalTransactionId where each entry points at an array      * of bucketIds where each entry points at an array of rowIds.  We could probably use ArrayList      * to manage insertion as the structure is built (LinkedList?).  This should reduce memory      * footprint (as far as OrcReader to a single reader) - probably bad for LLAP IO      */
specifier|private
name|TreeMap
argument_list|<
name|DeleteRecordKey
argument_list|,
name|DeleteReaderValue
argument_list|>
name|sortMerger
decl_stmt|;
specifier|private
name|long
name|rowIds
index|[]
decl_stmt|;
specifier|private
name|CompressedOtid
name|compressedOtids
index|[]
decl_stmt|;
specifier|private
name|ValidWriteIdList
name|validWriteIdList
decl_stmt|;
specifier|private
name|Boolean
name|isEmpty
init|=
literal|null
decl_stmt|;
name|ColumnizedDeleteEventRegistry
parameter_list|(
name|JobConf
name|conf
parameter_list|,
name|OrcSplit
name|orcSplit
parameter_list|,
name|Reader
operator|.
name|Options
name|readerOptions
parameter_list|)
throws|throws
name|IOException
throws|,
name|DeleteEventsOverflowMemoryException
block|{
name|int
name|bucket
init|=
name|AcidUtils
operator|.
name|parseBaseOrDeltaBucketFilename
argument_list|(
name|orcSplit
operator|.
name|getPath
argument_list|()
argument_list|,
name|conf
argument_list|)
operator|.
name|getBucketId
argument_list|()
decl_stmt|;
name|String
name|txnString
init|=
name|conf
operator|.
name|get
argument_list|(
name|ValidWriteIdList
operator|.
name|VALID_WRITEIDS_KEY
argument_list|)
decl_stmt|;
name|this
operator|.
name|validWriteIdList
operator|=
operator|(
name|txnString
operator|==
literal|null
operator|)
condition|?
operator|new
name|ValidReaderWriteIdList
argument_list|()
else|:
operator|new
name|ValidReaderWriteIdList
argument_list|(
name|txnString
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"ColumnizedDeleteEventRegistry:: Read ValidWriteIdList: "
operator|+
name|this
operator|.
name|validWriteIdList
operator|.
name|toString
argument_list|()
operator|+
literal|" isFullAcidTable: "
operator|+
name|AcidUtils
operator|.
name|isFullAcidScan
argument_list|(
name|conf
argument_list|)
argument_list|)
expr_stmt|;
name|this
operator|.
name|sortMerger
operator|=
operator|new
name|TreeMap
argument_list|<
name|DeleteRecordKey
argument_list|,
name|DeleteReaderValue
argument_list|>
argument_list|()
expr_stmt|;
name|this
operator|.
name|rowIds
operator|=
literal|null
expr_stmt|;
name|this
operator|.
name|compressedOtids
operator|=
literal|null
expr_stmt|;
name|int
name|maxEventsInMemory
init|=
name|HiveConf
operator|.
name|getIntVar
argument_list|(
name|conf
argument_list|,
name|ConfVars
operator|.
name|HIVE_TRANSACTIONAL_NUM_EVENTS_IN_MEMORY
argument_list|)
decl_stmt|;
specifier|final
name|boolean
name|isBucketedTable
init|=
name|conf
operator|.
name|getInt
argument_list|(
name|hive_metastoreConstants
operator|.
name|BUCKET_COUNT
argument_list|,
literal|0
argument_list|)
operator|>
literal|0
decl_stmt|;
try|try
block|{
specifier|final
name|Path
index|[]
name|deleteDeltaDirs
init|=
name|getDeleteDeltaDirsFromSplit
argument_list|(
name|orcSplit
argument_list|)
decl_stmt|;
if|if
condition|(
name|deleteDeltaDirs
operator|.
name|length
operator|>
literal|0
condition|)
block|{
name|int
name|totalDeleteEventCount
init|=
literal|0
decl_stmt|;
for|for
control|(
name|Path
name|deleteDeltaDir
range|:
name|deleteDeltaDirs
control|)
block|{
name|FileSystem
name|fs
init|=
name|deleteDeltaDir
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
for|for
control|(
name|Path
name|deleteDeltaFile
range|:
name|OrcRawRecordMerger
operator|.
name|getDeltaFiles
argument_list|(
name|deleteDeltaDir
argument_list|,
name|bucket
argument_list|,
name|conf
argument_list|,
operator|new
name|OrcRawRecordMerger
operator|.
name|Options
argument_list|()
operator|.
name|isCompacting
argument_list|(
literal|false
argument_list|)
argument_list|,
name|isBucketedTable
argument_list|)
control|)
block|{
comment|// NOTE: Calling last flush length below is more for future-proofing when we have
comment|// streaming deletes. But currently we don't support streaming deletes, and this can
comment|// be removed if this becomes a performance issue.
name|long
name|length
init|=
name|OrcAcidUtils
operator|.
name|getLastFlushLength
argument_list|(
name|fs
argument_list|,
name|deleteDeltaFile
argument_list|)
decl_stmt|;
comment|// NOTE: A check for existence of deleteDeltaFile is required because we may not have
comment|// deletes for the bucket being taken into consideration for this split processing.
if|if
condition|(
name|length
operator|!=
operator|-
literal|1
operator|&&
name|fs
operator|.
name|exists
argument_list|(
name|deleteDeltaFile
argument_list|)
condition|)
block|{
name|Reader
name|deleteDeltaReader
init|=
name|OrcFile
operator|.
name|createReader
argument_list|(
name|deleteDeltaFile
argument_list|,
name|OrcFile
operator|.
name|readerOptions
argument_list|(
name|conf
argument_list|)
operator|.
name|maxLength
argument_list|(
name|length
argument_list|)
argument_list|)
decl_stmt|;
name|AcidStats
name|acidStats
init|=
name|OrcAcidUtils
operator|.
name|parseAcidStats
argument_list|(
name|deleteDeltaReader
argument_list|)
decl_stmt|;
if|if
condition|(
name|acidStats
operator|.
name|deletes
operator|==
literal|0
condition|)
block|{
continue|continue;
comment|// just a safe check to ensure that we are not reading empty delete files.
block|}
name|totalDeleteEventCount
operator|+=
name|acidStats
operator|.
name|deletes
expr_stmt|;
if|if
condition|(
name|totalDeleteEventCount
operator|>
name|maxEventsInMemory
condition|)
block|{
comment|// ColumnizedDeleteEventRegistry loads all the delete events from all the delete deltas
comment|// into memory. To prevent out-of-memory errors, this check is a rough heuristic that
comment|// prevents creation of an object of this class if the total number of delete events
comment|// exceed this value. By default, it has been set to 10 million delete events per bucket.
name|LOG
operator|.
name|info
argument_list|(
literal|"Total number of delete events exceeds the maximum number of delete events "
operator|+
literal|"that can be loaded into memory for the delete deltas in the directory at : "
operator|+
name|deleteDeltaDirs
operator|.
name|toString
argument_list|()
operator|+
literal|". The max limit is currently set at "
operator|+
name|maxEventsInMemory
operator|+
literal|" and can be changed by setting the Hive config variable "
operator|+
name|ConfVars
operator|.
name|HIVE_TRANSACTIONAL_NUM_EVENTS_IN_MEMORY
operator|.
name|varname
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|DeleteEventsOverflowMemoryException
argument_list|()
throw|;
block|}
name|DeleteReaderValue
name|deleteReaderValue
init|=
operator|new
name|DeleteReaderValue
argument_list|(
name|deleteDeltaReader
argument_list|,
name|readerOptions
argument_list|,
name|bucket
argument_list|,
name|validWriteIdList
argument_list|,
name|isBucketedTable
argument_list|)
decl_stmt|;
name|DeleteRecordKey
name|deleteRecordKey
init|=
operator|new
name|DeleteRecordKey
argument_list|()
decl_stmt|;
if|if
condition|(
name|deleteReaderValue
operator|.
name|next
argument_list|(
name|deleteRecordKey
argument_list|)
condition|)
block|{
name|sortMerger
operator|.
name|put
argument_list|(
name|deleteRecordKey
argument_list|,
name|deleteReaderValue
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|deleteReaderValue
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
block|}
block|}
if|if
condition|(
name|totalDeleteEventCount
operator|>
literal|0
condition|)
block|{
comment|// Initialize the rowId array when we have some delete events.
name|rowIds
operator|=
operator|new
name|long
index|[
name|totalDeleteEventCount
index|]
expr_stmt|;
name|readAllDeleteEventsFromDeleteDeltas
argument_list|()
expr_stmt|;
block|}
block|}
name|isEmpty
operator|=
name|compressedOtids
operator|==
literal|null
operator|||
name|rowIds
operator|==
literal|null
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
decl||
name|DeleteEventsOverflowMemoryException
name|e
parameter_list|)
block|{
name|close
argument_list|()
expr_stmt|;
comment|// close any open readers, if there was some exception during initialization.
throw|throw
name|e
throw|;
comment|// rethrow the exception so that the caller can handle.
block|}
block|}
comment|/**      * This is not done quite right.  The intent of {@link CompressedOtid} is a hedge against      * "delete from T" that generates a huge number of delete events possibly even 2G - max array      * size.  (assuming no one txn inserts> 2G rows (in a bucket)).  As implemented, the algorithm      * first loads all data into one array otid[] and rowIds[] which defeats the purpose.      * In practice we should be filtering delete evens by min/max ROW_ID from the split.  The later      * is also not yet implemented: HIVE-16812.      */
specifier|private
name|void
name|readAllDeleteEventsFromDeleteDeltas
parameter_list|()
throws|throws
name|IOException
block|{
if|if
condition|(
name|sortMerger
operator|==
literal|null
operator|||
name|sortMerger
operator|.
name|isEmpty
argument_list|()
condition|)
return|return;
comment|// trivial case, nothing to read.
name|int
name|distinctOtids
init|=
literal|0
decl_stmt|;
name|long
name|lastSeenOtid
init|=
operator|-
literal|1
decl_stmt|;
name|int
name|lastSeenBucketProperty
init|=
operator|-
literal|1
decl_stmt|;
name|long
name|otids
index|[]
init|=
operator|new
name|long
index|[
name|rowIds
operator|.
name|length
index|]
decl_stmt|;
name|int
index|[]
name|bucketProperties
init|=
operator|new
name|int
index|[
name|rowIds
operator|.
name|length
index|]
decl_stmt|;
name|int
name|index
init|=
literal|0
decl_stmt|;
while|while
condition|(
operator|!
name|sortMerger
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
comment|// The sortMerger is a heap data structure that stores a pair of
comment|// (deleteRecordKey, deleteReaderValue) at each node and is ordered by deleteRecordKey.
comment|// The deleteReaderValue is the actual wrapper class that has the reference to the
comment|// underlying delta file that is being read, and its corresponding deleteRecordKey
comment|// is the smallest record id for that file. In each iteration of this loop, we extract(poll)
comment|// the minimum deleteRecordKey pair. Once we have processed that deleteRecordKey, we
comment|// advance the pointer for the corresponding deleteReaderValue. If the underlying file
comment|// itself has no more records, then we remove that pair from the heap, or else we
comment|// add the updated pair back to the heap.
name|Entry
argument_list|<
name|DeleteRecordKey
argument_list|,
name|DeleteReaderValue
argument_list|>
name|entry
init|=
name|sortMerger
operator|.
name|pollFirstEntry
argument_list|()
decl_stmt|;
name|DeleteRecordKey
name|deleteRecordKey
init|=
name|entry
operator|.
name|getKey
argument_list|()
decl_stmt|;
name|DeleteReaderValue
name|deleteReaderValue
init|=
name|entry
operator|.
name|getValue
argument_list|()
decl_stmt|;
name|otids
index|[
name|index
index|]
operator|=
name|deleteRecordKey
operator|.
name|originalTransactionId
expr_stmt|;
name|bucketProperties
index|[
name|index
index|]
operator|=
name|deleteRecordKey
operator|.
name|bucketProperty
expr_stmt|;
name|rowIds
index|[
name|index
index|]
operator|=
name|deleteRecordKey
operator|.
name|rowId
expr_stmt|;
operator|++
name|index
expr_stmt|;
if|if
condition|(
name|lastSeenOtid
operator|!=
name|deleteRecordKey
operator|.
name|originalTransactionId
operator|||
name|lastSeenBucketProperty
operator|!=
name|deleteRecordKey
operator|.
name|bucketProperty
condition|)
block|{
operator|++
name|distinctOtids
expr_stmt|;
name|lastSeenOtid
operator|=
name|deleteRecordKey
operator|.
name|originalTransactionId
expr_stmt|;
name|lastSeenBucketProperty
operator|=
name|deleteRecordKey
operator|.
name|bucketProperty
expr_stmt|;
block|}
if|if
condition|(
name|deleteReaderValue
operator|.
name|next
argument_list|(
name|deleteRecordKey
argument_list|)
condition|)
block|{
name|sortMerger
operator|.
name|put
argument_list|(
name|deleteRecordKey
argument_list|,
name|deleteReaderValue
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|deleteReaderValue
operator|.
name|close
argument_list|()
expr_stmt|;
comment|// Exhausted reading all records, close the reader.
block|}
block|}
comment|// Once we have processed all the delete events and seen all the distinct otids,
comment|// we compress the otids into CompressedOtid data structure that records
comment|// the fromIndex(inclusive) and toIndex(exclusive) for each unique otid.
name|this
operator|.
name|compressedOtids
operator|=
operator|new
name|CompressedOtid
index|[
name|distinctOtids
index|]
expr_stmt|;
name|lastSeenOtid
operator|=
name|otids
index|[
literal|0
index|]
expr_stmt|;
name|lastSeenBucketProperty
operator|=
name|bucketProperties
index|[
literal|0
index|]
expr_stmt|;
name|int
name|fromIndex
init|=
literal|0
decl_stmt|,
name|pos
init|=
literal|0
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|1
init|;
name|i
operator|<
name|otids
operator|.
name|length
condition|;
operator|++
name|i
control|)
block|{
if|if
condition|(
name|otids
index|[
name|i
index|]
operator|!=
name|lastSeenOtid
operator|||
name|lastSeenBucketProperty
operator|!=
name|bucketProperties
index|[
name|i
index|]
condition|)
block|{
name|compressedOtids
index|[
name|pos
index|]
operator|=
operator|new
name|CompressedOtid
argument_list|(
name|lastSeenOtid
argument_list|,
name|lastSeenBucketProperty
argument_list|,
name|fromIndex
argument_list|,
name|i
argument_list|)
expr_stmt|;
name|lastSeenOtid
operator|=
name|otids
index|[
name|i
index|]
expr_stmt|;
name|lastSeenBucketProperty
operator|=
name|bucketProperties
index|[
name|i
index|]
expr_stmt|;
name|fromIndex
operator|=
name|i
expr_stmt|;
operator|++
name|pos
expr_stmt|;
block|}
block|}
comment|// account for the last distinct otid
name|compressedOtids
index|[
name|pos
index|]
operator|=
operator|new
name|CompressedOtid
argument_list|(
name|lastSeenOtid
argument_list|,
name|lastSeenBucketProperty
argument_list|,
name|fromIndex
argument_list|,
name|otids
operator|.
name|length
argument_list|)
expr_stmt|;
block|}
specifier|private
name|boolean
name|isDeleted
parameter_list|(
name|long
name|otid
parameter_list|,
name|int
name|bucketProperty
parameter_list|,
name|long
name|rowId
parameter_list|)
block|{
if|if
condition|(
name|compressedOtids
operator|==
literal|null
operator|||
name|rowIds
operator|==
literal|null
condition|)
block|{
return|return
literal|false
return|;
block|}
comment|// To find if a given (otid, rowId) pair is deleted or not, we perform
comment|// two binary searches at most. The first binary search is on the
comment|// compressed otids. If a match is found, only then we do the next
comment|// binary search in the larger rowId vector between the given toIndex& fromIndex.
comment|// Check if otid is outside the range of all otids present.
if|if
condition|(
name|otid
operator|<
name|compressedOtids
index|[
literal|0
index|]
operator|.
name|originalTransactionId
operator|||
name|otid
operator|>
name|compressedOtids
index|[
name|compressedOtids
operator|.
name|length
operator|-
literal|1
index|]
operator|.
name|originalTransactionId
condition|)
block|{
return|return
literal|false
return|;
block|}
comment|// Create a dummy key for searching the otid/bucket in the compressed otid ranges.
name|CompressedOtid
name|key
init|=
operator|new
name|CompressedOtid
argument_list|(
name|otid
argument_list|,
name|bucketProperty
argument_list|,
operator|-
literal|1
argument_list|,
operator|-
literal|1
argument_list|)
decl_stmt|;
name|int
name|pos
init|=
name|Arrays
operator|.
name|binarySearch
argument_list|(
name|compressedOtids
argument_list|,
name|key
argument_list|)
decl_stmt|;
if|if
condition|(
name|pos
operator|>=
literal|0
condition|)
block|{
comment|// Otid with the given value found! Searching now for rowId...
name|key
operator|=
name|compressedOtids
index|[
name|pos
index|]
expr_stmt|;
comment|// Retrieve the actual CompressedOtid that matched.
comment|// Check if rowId is outside the range of all rowIds present for this otid.
if|if
condition|(
name|rowId
argument_list|<
name|rowIds
index|[
name|key
operator|.
name|fromIndex
index|]
operator|||
name|rowId
argument_list|>
name|rowIds
index|[
name|key
operator|.
name|toIndex
operator|-
literal|1
index|]
condition|)
block|{
return|return
literal|false
return|;
block|}
if|if
condition|(
name|Arrays
operator|.
name|binarySearch
argument_list|(
name|rowIds
argument_list|,
name|key
operator|.
name|fromIndex
argument_list|,
name|key
operator|.
name|toIndex
argument_list|,
name|rowId
argument_list|)
operator|>=
literal|0
condition|)
block|{
return|return
literal|true
return|;
comment|// rowId also found!
block|}
block|}
return|return
literal|false
return|;
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|isEmpty
parameter_list|()
block|{
if|if
condition|(
name|isEmpty
operator|==
literal|null
condition|)
block|{
throw|throw
operator|new
name|IllegalStateException
argument_list|(
literal|"Not yet initialized"
argument_list|)
throw|;
block|}
return|return
name|isEmpty
return|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|findDeletedRecords
parameter_list|(
name|ColumnVector
index|[]
name|cols
parameter_list|,
name|int
name|size
parameter_list|,
name|BitSet
name|selectedBitSet
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|rowIds
operator|==
literal|null
operator|||
name|compressedOtids
operator|==
literal|null
condition|)
block|{
return|return;
block|}
comment|// Iterate through the batch and for each (otid, rowid) in the batch
comment|// check if it is deleted or not.
name|long
index|[]
name|originalTransactionVector
init|=
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|ORIGINAL_WRITEID
index|]
operator|.
name|isRepeating
condition|?
literal|null
else|:
operator|(
operator|(
name|LongColumnVector
operator|)
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|ORIGINAL_WRITEID
index|]
operator|)
operator|.
name|vector
decl_stmt|;
name|long
name|repeatedOriginalTransaction
init|=
operator|(
name|originalTransactionVector
operator|!=
literal|null
operator|)
condition|?
operator|-
literal|1
else|:
operator|(
operator|(
name|LongColumnVector
operator|)
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|ORIGINAL_WRITEID
index|]
operator|)
operator|.
name|vector
index|[
literal|0
index|]
decl_stmt|;
name|long
index|[]
name|bucketProperties
init|=
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|BUCKET
index|]
operator|.
name|isRepeating
condition|?
literal|null
else|:
operator|(
operator|(
name|LongColumnVector
operator|)
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|BUCKET
index|]
operator|)
operator|.
name|vector
decl_stmt|;
name|int
name|repeatedBucketProperty
init|=
operator|(
name|bucketProperties
operator|!=
literal|null
operator|)
condition|?
operator|-
literal|1
else|:
call|(
name|int
call|)
argument_list|(
operator|(
name|LongColumnVector
operator|)
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|BUCKET
index|]
argument_list|)
operator|.
name|vector
index|[
literal|0
index|]
decl_stmt|;
name|long
index|[]
name|rowIdVector
init|=
operator|(
operator|(
name|LongColumnVector
operator|)
name|cols
index|[
name|OrcRecordUpdater
operator|.
name|ROW_ID
index|]
operator|)
operator|.
name|vector
decl_stmt|;
for|for
control|(
name|int
name|setBitIndex
init|=
name|selectedBitSet
operator|.
name|nextSetBit
argument_list|(
literal|0
argument_list|)
init|;
name|setBitIndex
operator|>=
literal|0
condition|;
name|setBitIndex
operator|=
name|selectedBitSet
operator|.
name|nextSetBit
argument_list|(
name|setBitIndex
operator|+
literal|1
argument_list|)
control|)
block|{
name|long
name|otid
init|=
name|originalTransactionVector
operator|!=
literal|null
condition|?
name|originalTransactionVector
index|[
name|setBitIndex
index|]
else|:
name|repeatedOriginalTransaction
decl_stmt|;
name|int
name|bucketProperty
init|=
name|bucketProperties
operator|!=
literal|null
condition|?
operator|(
name|int
operator|)
name|bucketProperties
index|[
name|setBitIndex
index|]
else|:
name|repeatedBucketProperty
decl_stmt|;
name|long
name|rowId
init|=
name|rowIdVector
index|[
name|setBitIndex
index|]
decl_stmt|;
if|if
condition|(
name|isDeleted
argument_list|(
name|otid
argument_list|,
name|bucketProperty
argument_list|,
name|rowId
argument_list|)
condition|)
block|{
name|selectedBitSet
operator|.
name|clear
argument_list|(
name|setBitIndex
argument_list|)
expr_stmt|;
block|}
block|}
block|}
annotation|@
name|Override
specifier|public
name|void
name|close
parameter_list|()
throws|throws
name|IOException
block|{
comment|// ColumnizedDeleteEventRegistry reads all the delete events into memory during initialization
comment|// and it closes the delete event readers after it. If an exception gets thrown during
comment|// initialization, we may have to close any readers that are still left open.
while|while
condition|(
operator|!
name|sortMerger
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|Entry
argument_list|<
name|DeleteRecordKey
argument_list|,
name|DeleteReaderValue
argument_list|>
name|entry
init|=
name|sortMerger
operator|.
name|pollFirstEntry
argument_list|()
decl_stmt|;
name|entry
operator|.
name|getValue
argument_list|()
operator|.
name|close
argument_list|()
expr_stmt|;
comment|// close the reader for this entry
block|}
block|}
block|}
specifier|static
class|class
name|DeleteEventsOverflowMemoryException
extends|extends
name|Exception
block|{
specifier|private
specifier|static
specifier|final
name|long
name|serialVersionUID
init|=
literal|1L
decl_stmt|;
block|}
block|}
end_class

end_unit

