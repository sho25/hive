begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/*  *  Licensed to the Apache Software Foundation (ASF) under one  *  or more contributor license agreements.  See the NOTICE file  *  distributed with this work for additional information  *  regarding copyright ownership.  The ASF licenses this file  *  to you under the Apache License, Version 2.0 (the  *  "License"); you may not use this file except in compliance  *  with the License.  You may obtain a copy of the License at  *  *      http://www.apache.org/licenses/LICENSE-2.0  *  *  Unless required by applicable law or agreed to in writing, software  *  distributed under the License is distributed on an "AS IS" BASIS,  *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  *  See the License for the specific language governing permissions and  *  limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|parse
operator|.
name|spark
package|;
end_package

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|Serializable
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashSet
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|LinkedHashMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Set
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Stack
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|atomic
operator|.
name|AtomicInteger
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|Context
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|ConditionalTask
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|DummyStoreOperator
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|FileSinkOperator
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|FilterOperator
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|JoinOperator
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|MapJoinOperator
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|Operator
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|OperatorUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|ReduceSinkOperator
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|SMBMapJoinOperator
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|TableScanOperator
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|Task
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|UnionOperator
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|spark
operator|.
name|SparkTask
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|spark
operator|.
name|SparkUtilities
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|hooks
operator|.
name|ReadEntity
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|hooks
operator|.
name|WriteEntity
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|lib
operator|.
name|CompositeProcessor
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|lib
operator|.
name|DefaultGraphWalker
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|lib
operator|.
name|DefaultRuleDispatcher
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|lib
operator|.
name|Dispatcher
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|lib
operator|.
name|ForwardWalker
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|lib
operator|.
name|GraphWalker
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|lib
operator|.
name|Node
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|lib
operator|.
name|NodeProcessor
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|lib
operator|.
name|NodeProcessorCtx
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|lib
operator|.
name|PreOrderWalker
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|lib
operator|.
name|Rule
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|lib
operator|.
name|RuleRegExp
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|lib
operator|.
name|TypeRule
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|log
operator|.
name|PerfLogger
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|optimizer
operator|.
name|ConstantPropagate
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|optimizer
operator|.
name|ConstantPropagateProcCtx
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|optimizer
operator|.
name|DynamicPartitionPruningOptimization
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|optimizer
operator|.
name|SparkRemoveDynamicPruning
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|optimizer
operator|.
name|metainfo
operator|.
name|annotation
operator|.
name|AnnotateWithOpTraits
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|optimizer
operator|.
name|physical
operator|.
name|AnnotateRunTimeStatsOptimizer
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|optimizer
operator|.
name|physical
operator|.
name|MetadataOnlyOptimizer
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|optimizer
operator|.
name|physical
operator|.
name|NullScanOptimizer
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|optimizer
operator|.
name|physical
operator|.
name|PhysicalContext
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|optimizer
operator|.
name|physical
operator|.
name|SparkCrossProductCheck
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|optimizer
operator|.
name|physical
operator|.
name|SparkDynamicPartitionPruningResolver
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|optimizer
operator|.
name|physical
operator|.
name|SparkMapJoinResolver
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|optimizer
operator|.
name|physical
operator|.
name|StageIDsRearranger
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|optimizer
operator|.
name|physical
operator|.
name|Vectorizer
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|optimizer
operator|.
name|spark
operator|.
name|CombineEquivalentWorkResolver
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|optimizer
operator|.
name|spark
operator|.
name|SetSparkReducerParallelism
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|optimizer
operator|.
name|spark
operator|.
name|SparkJoinHintOptimizer
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|optimizer
operator|.
name|spark
operator|.
name|SparkJoinOptimizer
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|optimizer
operator|.
name|spark
operator|.
name|SparkPartitionPruningSinkDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|optimizer
operator|.
name|spark
operator|.
name|SparkReduceSinkMapJoinProc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|optimizer
operator|.
name|spark
operator|.
name|SparkSkewJoinResolver
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|optimizer
operator|.
name|spark
operator|.
name|SplitSparkWorkResolver
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|optimizer
operator|.
name|stats
operator|.
name|annotation
operator|.
name|AnnotateWithStatistics
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|parse
operator|.
name|GlobalLimitCtx
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|parse
operator|.
name|ParseContext
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|parse
operator|.
name|SemanticException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|parse
operator|.
name|TaskCompiler
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|BaseWork
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|MapWork
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|MoveWork
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|OperatorDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|SparkWork
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|session
operator|.
name|SessionState
import|;
end_import

begin_comment
comment|/**  * SparkCompiler translates the operator plan into SparkTasks.  *  * Cloned from TezCompiler.  */
end_comment

begin_class
specifier|public
class|class
name|SparkCompiler
extends|extends
name|TaskCompiler
block|{
specifier|private
specifier|static
specifier|final
name|String
name|CLASS_NAME
init|=
name|SparkCompiler
operator|.
name|class
operator|.
name|getName
argument_list|()
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|PerfLogger
name|PERF_LOGGER
init|=
name|SessionState
operator|.
name|getPerfLogger
argument_list|()
decl_stmt|;
specifier|public
name|SparkCompiler
parameter_list|()
block|{   }
annotation|@
name|Override
specifier|protected
name|void
name|optimizeOperatorPlan
parameter_list|(
name|ParseContext
name|pCtx
parameter_list|,
name|Set
argument_list|<
name|ReadEntity
argument_list|>
name|inputs
parameter_list|,
name|Set
argument_list|<
name|WriteEntity
argument_list|>
name|outputs
parameter_list|)
throws|throws
name|SemanticException
block|{
name|PERF_LOGGER
operator|.
name|PerfLogBegin
argument_list|(
name|CLASS_NAME
argument_list|,
name|PerfLogger
operator|.
name|SPARK_OPTIMIZE_OPERATOR_TREE
argument_list|)
expr_stmt|;
name|OptimizeSparkProcContext
name|procCtx
init|=
operator|new
name|OptimizeSparkProcContext
argument_list|(
name|conf
argument_list|,
name|pCtx
argument_list|,
name|inputs
argument_list|,
name|outputs
argument_list|)
decl_stmt|;
comment|// Run Spark Dynamic Partition Pruning
name|runDynamicPartitionPruning
argument_list|(
name|procCtx
argument_list|)
expr_stmt|;
comment|// Annotation OP tree with statistics
name|runStatsAnnotation
argument_list|(
name|procCtx
argument_list|)
expr_stmt|;
comment|// Run Dynamic Partitioning sort Optimization.
name|runDynPartitionSortOptimizations
argument_list|(
name|procCtx
argument_list|)
expr_stmt|;
comment|// Set reducer parallelism
name|runSetReducerParallelism
argument_list|(
name|procCtx
argument_list|)
expr_stmt|;
comment|// Run Join releated optimizations
name|runJoinOptimizations
argument_list|(
name|procCtx
argument_list|)
expr_stmt|;
if|if
condition|(
name|conf
operator|.
name|isSparkDPPAny
argument_list|()
condition|)
block|{
comment|// Remove DPP based on expected size of the output data
name|runRemoveDynamicPruning
argument_list|(
name|procCtx
argument_list|)
expr_stmt|;
comment|// Remove cyclic dependencies for DPP
name|runCycleAnalysisForPartitionPruning
argument_list|(
name|procCtx
argument_list|)
expr_stmt|;
comment|// Remove nested DPPs
name|SparkUtilities
operator|.
name|removeNestedDPP
argument_list|(
name|procCtx
argument_list|)
expr_stmt|;
block|}
comment|// Re-run constant propagation so we fold any new constants introduced by the operator optimizers
comment|// Specifically necessary for DPP because we might have created lots of "and true and true" conditions
if|if
condition|(
name|procCtx
operator|.
name|getConf
argument_list|()
operator|.
name|getBoolVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEOPTCONSTANTPROPAGATION
argument_list|)
condition|)
block|{
operator|new
name|ConstantPropagate
argument_list|(
name|ConstantPropagateProcCtx
operator|.
name|ConstantPropagateOption
operator|.
name|SHORTCUT
argument_list|)
operator|.
name|transform
argument_list|(
name|pCtx
argument_list|)
expr_stmt|;
block|}
comment|// ATTENTION : DO NOT, I REPEAT, DO NOT WRITE ANYTHING AFTER updateBucketingVersionForUpgrade()
comment|// ANYTHING WHICH NEEDS TO BE ADDED MUST BE ADDED ABOVE
comment|// This call updates the bucketing version of final ReduceSinkOp based on
comment|// the bucketing version of FileSinkOp. This operation must happen at the
comment|// end to ensure there is no further rewrite of plan which may end up
comment|// removing/updating the ReduceSinkOp as was the case with SortedDynPartitionOptimizer
comment|// Update bucketing version of ReduceSinkOp if needed
comment|// Note: This has been copied here from TezCompiler, change seems needed for bucketing to work
comment|// properly moving forward.
name|updateBucketingVersionForUpgrade
argument_list|(
name|procCtx
argument_list|)
expr_stmt|;
name|PERF_LOGGER
operator|.
name|PerfLogEnd
argument_list|(
name|CLASS_NAME
argument_list|,
name|PerfLogger
operator|.
name|SPARK_OPTIMIZE_OPERATOR_TREE
argument_list|)
expr_stmt|;
block|}
specifier|private
name|void
name|runRemoveDynamicPruning
parameter_list|(
name|OptimizeSparkProcContext
name|procCtx
parameter_list|)
throws|throws
name|SemanticException
block|{
name|ParseContext
name|pCtx
init|=
name|procCtx
operator|.
name|getParseContext
argument_list|()
decl_stmt|;
name|Map
argument_list|<
name|Rule
argument_list|,
name|NodeProcessor
argument_list|>
name|opRules
init|=
operator|new
name|LinkedHashMap
argument_list|<
name|Rule
argument_list|,
name|NodeProcessor
argument_list|>
argument_list|()
decl_stmt|;
name|opRules
operator|.
name|put
argument_list|(
operator|new
name|RuleRegExp
argument_list|(
literal|"Disabling Dynamic Partition Pruning"
argument_list|,
name|SparkPartitionPruningSinkOperator
operator|.
name|getOperatorName
argument_list|()
operator|+
literal|"%"
argument_list|)
argument_list|,
operator|new
name|SparkRemoveDynamicPruning
argument_list|()
argument_list|)
expr_stmt|;
comment|// The dispatcher fires the processor corresponding to the closest matching
comment|// rule and passes the context along
name|Dispatcher
name|disp
init|=
operator|new
name|DefaultRuleDispatcher
argument_list|(
literal|null
argument_list|,
name|opRules
argument_list|,
name|procCtx
argument_list|)
decl_stmt|;
name|GraphWalker
name|ogw
init|=
operator|new
name|DefaultGraphWalker
argument_list|(
name|disp
argument_list|)
decl_stmt|;
comment|// Create a list of topop nodes
name|ArrayList
argument_list|<
name|Node
argument_list|>
name|topNodes
init|=
operator|new
name|ArrayList
argument_list|<
name|Node
argument_list|>
argument_list|()
decl_stmt|;
name|topNodes
operator|.
name|addAll
argument_list|(
name|pCtx
operator|.
name|getTopOps
argument_list|()
operator|.
name|values
argument_list|()
argument_list|)
expr_stmt|;
name|ogw
operator|.
name|startWalking
argument_list|(
name|topNodes
argument_list|,
literal|null
argument_list|)
expr_stmt|;
block|}
specifier|private
name|void
name|runCycleAnalysisForPartitionPruning
parameter_list|(
name|OptimizeSparkProcContext
name|procCtx
parameter_list|)
block|{
name|boolean
name|cycleFree
init|=
literal|false
decl_stmt|;
while|while
condition|(
operator|!
name|cycleFree
condition|)
block|{
name|cycleFree
operator|=
literal|true
expr_stmt|;
name|Set
argument_list|<
name|Set
argument_list|<
name|Operator
argument_list|<
name|?
argument_list|>
argument_list|>
argument_list|>
name|components
init|=
name|getComponents
argument_list|(
name|procCtx
argument_list|)
decl_stmt|;
for|for
control|(
name|Set
argument_list|<
name|Operator
argument_list|<
name|?
argument_list|>
argument_list|>
name|component
range|:
name|components
control|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Component: "
argument_list|)
expr_stmt|;
for|for
control|(
name|Operator
argument_list|<
name|?
argument_list|>
name|co
range|:
name|component
control|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Operator: "
operator|+
name|co
operator|.
name|getName
argument_list|()
operator|+
literal|", "
operator|+
name|co
operator|.
name|getIdentifier
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
name|component
operator|.
name|size
argument_list|()
operator|!=
literal|1
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Found cycle in operator plan..."
argument_list|)
expr_stmt|;
name|cycleFree
operator|=
literal|false
expr_stmt|;
name|removeDPPOperator
argument_list|(
name|component
argument_list|,
name|procCtx
argument_list|)
expr_stmt|;
break|break;
block|}
block|}
name|LOG
operator|.
name|info
argument_list|(
literal|"Cycle free: "
operator|+
name|cycleFree
argument_list|)
expr_stmt|;
block|}
block|}
specifier|private
name|void
name|removeDPPOperator
parameter_list|(
name|Set
argument_list|<
name|Operator
argument_list|<
name|?
argument_list|>
argument_list|>
name|component
parameter_list|,
name|OptimizeSparkProcContext
name|context
parameter_list|)
block|{
name|SparkPartitionPruningSinkOperator
name|toRemove
init|=
literal|null
decl_stmt|;
for|for
control|(
name|Operator
argument_list|<
name|?
argument_list|>
name|o
range|:
name|component
control|)
block|{
if|if
condition|(
name|o
operator|instanceof
name|SparkPartitionPruningSinkOperator
condition|)
block|{
comment|// we want to remove the DPP with bigger data size
if|if
condition|(
name|toRemove
operator|==
literal|null
operator|||
name|o
operator|.
name|getConf
argument_list|()
operator|.
name|getStatistics
argument_list|()
operator|.
name|getDataSize
argument_list|()
operator|>
name|toRemove
operator|.
name|getConf
argument_list|()
operator|.
name|getStatistics
argument_list|()
operator|.
name|getDataSize
argument_list|()
condition|)
block|{
name|toRemove
operator|=
operator|(
name|SparkPartitionPruningSinkOperator
operator|)
name|o
expr_stmt|;
block|}
block|}
block|}
if|if
condition|(
name|toRemove
operator|==
literal|null
condition|)
block|{
return|return;
block|}
name|OperatorUtils
operator|.
name|removeBranch
argument_list|(
name|toRemove
argument_list|)
expr_stmt|;
comment|// at this point we've found the fork in the op pipeline that has the pruning as a child plan.
name|LOG
operator|.
name|info
argument_list|(
literal|"Disabling dynamic pruning for: "
operator|+
name|toRemove
operator|.
name|getConf
argument_list|()
operator|.
name|getTableScanNames
argument_list|()
operator|+
literal|". Needed to break cyclic dependency"
argument_list|)
expr_stmt|;
block|}
comment|// Tarjan's algo
specifier|private
name|Set
argument_list|<
name|Set
argument_list|<
name|Operator
argument_list|<
name|?
argument_list|>
argument_list|>
argument_list|>
name|getComponents
parameter_list|(
name|OptimizeSparkProcContext
name|procCtx
parameter_list|)
block|{
name|AtomicInteger
name|index
init|=
operator|new
name|AtomicInteger
argument_list|()
decl_stmt|;
name|Map
argument_list|<
name|Operator
argument_list|<
name|?
argument_list|>
argument_list|,
name|Integer
argument_list|>
name|indexes
init|=
operator|new
name|HashMap
argument_list|<
name|Operator
argument_list|<
name|?
argument_list|>
argument_list|,
name|Integer
argument_list|>
argument_list|()
decl_stmt|;
name|Map
argument_list|<
name|Operator
argument_list|<
name|?
argument_list|>
argument_list|,
name|Integer
argument_list|>
name|lowLinks
init|=
operator|new
name|HashMap
argument_list|<
name|Operator
argument_list|<
name|?
argument_list|>
argument_list|,
name|Integer
argument_list|>
argument_list|()
decl_stmt|;
name|Stack
argument_list|<
name|Operator
argument_list|<
name|?
argument_list|>
argument_list|>
name|nodes
init|=
operator|new
name|Stack
argument_list|<
name|Operator
argument_list|<
name|?
argument_list|>
argument_list|>
argument_list|()
decl_stmt|;
name|Set
argument_list|<
name|Set
argument_list|<
name|Operator
argument_list|<
name|?
argument_list|>
argument_list|>
argument_list|>
name|components
init|=
operator|new
name|HashSet
argument_list|<
name|Set
argument_list|<
name|Operator
argument_list|<
name|?
argument_list|>
argument_list|>
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|Operator
argument_list|<
name|?
argument_list|>
name|o
range|:
name|procCtx
operator|.
name|getParseContext
argument_list|()
operator|.
name|getTopOps
argument_list|()
operator|.
name|values
argument_list|()
control|)
block|{
if|if
condition|(
operator|!
name|indexes
operator|.
name|containsKey
argument_list|(
name|o
argument_list|)
condition|)
block|{
name|connect
argument_list|(
name|o
argument_list|,
name|index
argument_list|,
name|nodes
argument_list|,
name|indexes
argument_list|,
name|lowLinks
argument_list|,
name|components
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|components
return|;
block|}
specifier|private
name|void
name|connect
parameter_list|(
name|Operator
argument_list|<
name|?
argument_list|>
name|o
parameter_list|,
name|AtomicInteger
name|index
parameter_list|,
name|Stack
argument_list|<
name|Operator
argument_list|<
name|?
argument_list|>
argument_list|>
name|nodes
parameter_list|,
name|Map
argument_list|<
name|Operator
argument_list|<
name|?
argument_list|>
argument_list|,
name|Integer
argument_list|>
name|indexes
parameter_list|,
name|Map
argument_list|<
name|Operator
argument_list|<
name|?
argument_list|>
argument_list|,
name|Integer
argument_list|>
name|lowLinks
parameter_list|,
name|Set
argument_list|<
name|Set
argument_list|<
name|Operator
argument_list|<
name|?
argument_list|>
argument_list|>
argument_list|>
name|components
parameter_list|)
block|{
name|indexes
operator|.
name|put
argument_list|(
name|o
argument_list|,
name|index
operator|.
name|get
argument_list|()
argument_list|)
expr_stmt|;
name|lowLinks
operator|.
name|put
argument_list|(
name|o
argument_list|,
name|index
operator|.
name|get
argument_list|()
argument_list|)
expr_stmt|;
name|index
operator|.
name|incrementAndGet
argument_list|()
expr_stmt|;
name|nodes
operator|.
name|push
argument_list|(
name|o
argument_list|)
expr_stmt|;
name|List
argument_list|<
name|Operator
argument_list|<
name|?
argument_list|>
argument_list|>
name|children
decl_stmt|;
if|if
condition|(
name|o
operator|instanceof
name|SparkPartitionPruningSinkOperator
condition|)
block|{
name|children
operator|=
operator|new
name|ArrayList
argument_list|<>
argument_list|()
expr_stmt|;
name|children
operator|.
name|addAll
argument_list|(
name|o
operator|.
name|getChildOperators
argument_list|()
argument_list|)
expr_stmt|;
name|SparkPartitionPruningSinkDesc
name|dppDesc
init|=
operator|(
operator|(
name|SparkPartitionPruningSinkOperator
operator|)
name|o
operator|)
operator|.
name|getConf
argument_list|()
decl_stmt|;
for|for
control|(
name|SparkPartitionPruningSinkDesc
operator|.
name|DPPTargetInfo
name|targetInfo
range|:
name|dppDesc
operator|.
name|getTargetInfos
argument_list|()
control|)
block|{
name|TableScanOperator
name|ts
init|=
name|targetInfo
operator|.
name|tableScan
decl_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Adding special edge: "
operator|+
name|o
operator|.
name|getName
argument_list|()
operator|+
literal|" --> "
operator|+
name|ts
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
name|children
operator|.
name|add
argument_list|(
name|ts
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
name|children
operator|=
name|o
operator|.
name|getChildOperators
argument_list|()
expr_stmt|;
block|}
for|for
control|(
name|Operator
argument_list|<
name|?
argument_list|>
name|child
range|:
name|children
control|)
block|{
if|if
condition|(
operator|!
name|indexes
operator|.
name|containsKey
argument_list|(
name|child
argument_list|)
condition|)
block|{
name|connect
argument_list|(
name|child
argument_list|,
name|index
argument_list|,
name|nodes
argument_list|,
name|indexes
argument_list|,
name|lowLinks
argument_list|,
name|components
argument_list|)
expr_stmt|;
name|lowLinks
operator|.
name|put
argument_list|(
name|o
argument_list|,
name|Math
operator|.
name|min
argument_list|(
name|lowLinks
operator|.
name|get
argument_list|(
name|o
argument_list|)
argument_list|,
name|lowLinks
operator|.
name|get
argument_list|(
name|child
argument_list|)
argument_list|)
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|nodes
operator|.
name|contains
argument_list|(
name|child
argument_list|)
condition|)
block|{
name|lowLinks
operator|.
name|put
argument_list|(
name|o
argument_list|,
name|Math
operator|.
name|min
argument_list|(
name|lowLinks
operator|.
name|get
argument_list|(
name|o
argument_list|)
argument_list|,
name|indexes
operator|.
name|get
argument_list|(
name|child
argument_list|)
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
name|lowLinks
operator|.
name|get
argument_list|(
name|o
argument_list|)
operator|.
name|equals
argument_list|(
name|indexes
operator|.
name|get
argument_list|(
name|o
argument_list|)
argument_list|)
condition|)
block|{
name|Set
argument_list|<
name|Operator
argument_list|<
name|?
argument_list|>
argument_list|>
name|component
init|=
operator|new
name|HashSet
argument_list|<
name|Operator
argument_list|<
name|?
argument_list|>
argument_list|>
argument_list|()
decl_stmt|;
name|components
operator|.
name|add
argument_list|(
name|component
argument_list|)
expr_stmt|;
name|Operator
argument_list|<
name|?
argument_list|>
name|current
decl_stmt|;
do|do
block|{
name|current
operator|=
name|nodes
operator|.
name|pop
argument_list|()
expr_stmt|;
name|component
operator|.
name|add
argument_list|(
name|current
argument_list|)
expr_stmt|;
block|}
do|while
condition|(
name|current
operator|!=
name|o
condition|)
do|;
block|}
block|}
specifier|private
name|void
name|runStatsAnnotation
parameter_list|(
name|OptimizeSparkProcContext
name|procCtx
parameter_list|)
throws|throws
name|SemanticException
block|{
operator|new
name|AnnotateWithStatistics
argument_list|()
operator|.
name|transform
argument_list|(
name|procCtx
operator|.
name|getParseContext
argument_list|()
argument_list|)
expr_stmt|;
operator|new
name|AnnotateWithOpTraits
argument_list|()
operator|.
name|transform
argument_list|(
name|procCtx
operator|.
name|getParseContext
argument_list|()
argument_list|)
expr_stmt|;
block|}
specifier|private
name|void
name|runDynamicPartitionPruning
parameter_list|(
name|OptimizeSparkProcContext
name|procCtx
parameter_list|)
throws|throws
name|SemanticException
block|{
if|if
condition|(
operator|!
name|conf
operator|.
name|isSparkDPPAny
argument_list|()
condition|)
block|{
return|return;
block|}
name|ParseContext
name|parseContext
init|=
name|procCtx
operator|.
name|getParseContext
argument_list|()
decl_stmt|;
name|Map
argument_list|<
name|Rule
argument_list|,
name|NodeProcessor
argument_list|>
name|opRules
init|=
operator|new
name|LinkedHashMap
argument_list|<
name|Rule
argument_list|,
name|NodeProcessor
argument_list|>
argument_list|()
decl_stmt|;
name|opRules
operator|.
name|put
argument_list|(
operator|new
name|RuleRegExp
argument_list|(
operator|new
name|String
argument_list|(
literal|"Dynamic Partition Pruning"
argument_list|)
argument_list|,
name|FilterOperator
operator|.
name|getOperatorName
argument_list|()
operator|+
literal|"%"
argument_list|)
argument_list|,
operator|new
name|DynamicPartitionPruningOptimization
argument_list|()
argument_list|)
expr_stmt|;
comment|// The dispatcher fires the processor corresponding to the closest matching
comment|// rule and passes the context along
name|Dispatcher
name|disp
init|=
operator|new
name|DefaultRuleDispatcher
argument_list|(
literal|null
argument_list|,
name|opRules
argument_list|,
name|procCtx
argument_list|)
decl_stmt|;
name|GraphWalker
name|ogw
init|=
operator|new
name|ForwardWalker
argument_list|(
name|disp
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|Node
argument_list|>
name|topNodes
init|=
operator|new
name|ArrayList
argument_list|<
name|Node
argument_list|>
argument_list|()
decl_stmt|;
name|topNodes
operator|.
name|addAll
argument_list|(
name|parseContext
operator|.
name|getTopOps
argument_list|()
operator|.
name|values
argument_list|()
argument_list|)
expr_stmt|;
name|ogw
operator|.
name|startWalking
argument_list|(
name|topNodes
argument_list|,
literal|null
argument_list|)
expr_stmt|;
block|}
specifier|private
name|void
name|runSetReducerParallelism
parameter_list|(
name|OptimizeSparkProcContext
name|procCtx
parameter_list|)
throws|throws
name|SemanticException
block|{
name|ParseContext
name|pCtx
init|=
name|procCtx
operator|.
name|getParseContext
argument_list|()
decl_stmt|;
name|Map
argument_list|<
name|Rule
argument_list|,
name|NodeProcessor
argument_list|>
name|opRules
init|=
operator|new
name|LinkedHashMap
argument_list|<
name|Rule
argument_list|,
name|NodeProcessor
argument_list|>
argument_list|()
decl_stmt|;
name|opRules
operator|.
name|put
argument_list|(
operator|new
name|RuleRegExp
argument_list|(
literal|"Set parallelism - ReduceSink"
argument_list|,
name|ReduceSinkOperator
operator|.
name|getOperatorName
argument_list|()
operator|+
literal|"%"
argument_list|)
argument_list|,
operator|new
name|SetSparkReducerParallelism
argument_list|(
name|pCtx
operator|.
name|getConf
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
comment|// The dispatcher fires the processor corresponding to the closest matching
comment|// rule and passes the context along
name|Dispatcher
name|disp
init|=
operator|new
name|DefaultRuleDispatcher
argument_list|(
literal|null
argument_list|,
name|opRules
argument_list|,
name|procCtx
argument_list|)
decl_stmt|;
name|GraphWalker
name|ogw
init|=
operator|new
name|PreOrderWalker
argument_list|(
name|disp
argument_list|)
decl_stmt|;
comment|// Create a list of topop nodes
name|ArrayList
argument_list|<
name|Node
argument_list|>
name|topNodes
init|=
operator|new
name|ArrayList
argument_list|<
name|Node
argument_list|>
argument_list|()
decl_stmt|;
name|topNodes
operator|.
name|addAll
argument_list|(
name|pCtx
operator|.
name|getTopOps
argument_list|()
operator|.
name|values
argument_list|()
argument_list|)
expr_stmt|;
name|ogw
operator|.
name|startWalking
argument_list|(
name|topNodes
argument_list|,
literal|null
argument_list|)
expr_stmt|;
block|}
specifier|private
name|void
name|runJoinOptimizations
parameter_list|(
name|OptimizeSparkProcContext
name|procCtx
parameter_list|)
throws|throws
name|SemanticException
block|{
name|ParseContext
name|pCtx
init|=
name|procCtx
operator|.
name|getParseContext
argument_list|()
decl_stmt|;
name|Map
argument_list|<
name|Rule
argument_list|,
name|NodeProcessor
argument_list|>
name|opRules
init|=
operator|new
name|LinkedHashMap
argument_list|<
name|Rule
argument_list|,
name|NodeProcessor
argument_list|>
argument_list|()
decl_stmt|;
name|opRules
operator|.
name|put
argument_list|(
operator|new
name|TypeRule
argument_list|(
name|JoinOperator
operator|.
name|class
argument_list|)
argument_list|,
operator|new
name|SparkJoinOptimizer
argument_list|(
name|pCtx
argument_list|)
argument_list|)
expr_stmt|;
name|opRules
operator|.
name|put
argument_list|(
operator|new
name|TypeRule
argument_list|(
name|MapJoinOperator
operator|.
name|class
argument_list|)
argument_list|,
operator|new
name|SparkJoinHintOptimizer
argument_list|(
name|pCtx
argument_list|)
argument_list|)
expr_stmt|;
comment|// The dispatcher fires the processor corresponding to the closest matching
comment|// rule and passes the context along
name|Dispatcher
name|disp
init|=
operator|new
name|DefaultRuleDispatcher
argument_list|(
literal|null
argument_list|,
name|opRules
argument_list|,
name|procCtx
argument_list|)
decl_stmt|;
name|GraphWalker
name|ogw
init|=
operator|new
name|DefaultGraphWalker
argument_list|(
name|disp
argument_list|)
decl_stmt|;
comment|// Create a list of topop nodes
name|ArrayList
argument_list|<
name|Node
argument_list|>
name|topNodes
init|=
operator|new
name|ArrayList
argument_list|<
name|Node
argument_list|>
argument_list|()
decl_stmt|;
name|topNodes
operator|.
name|addAll
argument_list|(
name|pCtx
operator|.
name|getTopOps
argument_list|()
operator|.
name|values
argument_list|()
argument_list|)
expr_stmt|;
name|ogw
operator|.
name|startWalking
argument_list|(
name|topNodes
argument_list|,
literal|null
argument_list|)
expr_stmt|;
block|}
specifier|private
name|void
name|runDynPartitionSortOptimizations
parameter_list|(
name|OptimizeSparkProcContext
name|procCtx
parameter_list|)
throws|throws
name|SemanticException
block|{
comment|// run Sorted dynamic partition optimization
name|HiveConf
name|hConf
init|=
name|procCtx
operator|.
name|getConf
argument_list|()
decl_stmt|;
name|ParseContext
name|parseContext
init|=
name|procCtx
operator|.
name|getParseContext
argument_list|()
decl_stmt|;
name|runDynPartitionSortOptimizations
argument_list|(
name|parseContext
argument_list|,
name|hConf
argument_list|)
expr_stmt|;
block|}
comment|/**    * TODO: need to turn on rules that's commented out and add more if necessary.    */
annotation|@
name|Override
specifier|protected
name|void
name|generateTaskTree
parameter_list|(
name|List
argument_list|<
name|Task
argument_list|<
name|?
extends|extends
name|Serializable
argument_list|>
argument_list|>
name|rootTasks
parameter_list|,
name|ParseContext
name|pCtx
parameter_list|,
name|List
argument_list|<
name|Task
argument_list|<
name|MoveWork
argument_list|>
argument_list|>
name|mvTask
parameter_list|,
name|Set
argument_list|<
name|ReadEntity
argument_list|>
name|inputs
parameter_list|,
name|Set
argument_list|<
name|WriteEntity
argument_list|>
name|outputs
parameter_list|)
throws|throws
name|SemanticException
block|{
name|PERF_LOGGER
operator|.
name|PerfLogBegin
argument_list|(
name|CLASS_NAME
argument_list|,
name|PerfLogger
operator|.
name|SPARK_GENERATE_TASK_TREE
argument_list|)
expr_stmt|;
name|GenSparkUtils
name|utils
init|=
name|GenSparkUtils
operator|.
name|getUtils
argument_list|()
decl_stmt|;
name|utils
operator|.
name|resetSequenceNumber
argument_list|()
expr_stmt|;
name|ParseContext
name|tempParseContext
init|=
name|getParseContext
argument_list|(
name|pCtx
argument_list|,
name|rootTasks
argument_list|)
decl_stmt|;
name|GenSparkProcContext
name|procCtx
init|=
operator|new
name|GenSparkProcContext
argument_list|(
name|conf
argument_list|,
name|tempParseContext
argument_list|,
name|mvTask
argument_list|,
name|rootTasks
argument_list|,
name|inputs
argument_list|,
name|outputs
argument_list|,
name|pCtx
operator|.
name|getTopOps
argument_list|()
argument_list|)
decl_stmt|;
comment|// -------------------------------- First Pass ---------------------------------- //
comment|// Identify SparkPartitionPruningSinkOperators, and break OP tree if necessary
name|Map
argument_list|<
name|Rule
argument_list|,
name|NodeProcessor
argument_list|>
name|opRules
init|=
operator|new
name|LinkedHashMap
argument_list|<
name|Rule
argument_list|,
name|NodeProcessor
argument_list|>
argument_list|()
decl_stmt|;
name|opRules
operator|.
name|put
argument_list|(
operator|new
name|RuleRegExp
argument_list|(
literal|"Clone OP tree for PartitionPruningSink"
argument_list|,
name|SparkPartitionPruningSinkOperator
operator|.
name|getOperatorName
argument_list|()
operator|+
literal|"%"
argument_list|)
argument_list|,
operator|new
name|SplitOpTreeForDPP
argument_list|()
argument_list|)
expr_stmt|;
name|Dispatcher
name|disp
init|=
operator|new
name|DefaultRuleDispatcher
argument_list|(
literal|null
argument_list|,
name|opRules
argument_list|,
name|procCtx
argument_list|)
decl_stmt|;
name|GraphWalker
name|ogw
init|=
operator|new
name|GenSparkWorkWalker
argument_list|(
name|disp
argument_list|,
name|procCtx
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|Node
argument_list|>
name|topNodes
init|=
operator|new
name|ArrayList
argument_list|<
name|Node
argument_list|>
argument_list|()
decl_stmt|;
name|topNodes
operator|.
name|addAll
argument_list|(
name|pCtx
operator|.
name|getTopOps
argument_list|()
operator|.
name|values
argument_list|()
argument_list|)
expr_stmt|;
name|ogw
operator|.
name|startWalking
argument_list|(
name|topNodes
argument_list|,
literal|null
argument_list|)
expr_stmt|;
comment|// -------------------------------- Second Pass ---------------------------------- //
comment|// Process operator tree in two steps: first we process the extra op trees generated
comment|// in the first pass. Then we process the main op tree, and the result task will depend
comment|// on the task generated in the first pass.
name|topNodes
operator|.
name|clear
argument_list|()
expr_stmt|;
name|topNodes
operator|.
name|addAll
argument_list|(
name|procCtx
operator|.
name|topOps
operator|.
name|values
argument_list|()
argument_list|)
expr_stmt|;
name|generateTaskTreeHelper
argument_list|(
name|procCtx
argument_list|,
name|topNodes
argument_list|)
expr_stmt|;
comment|// If this set is not empty, it means we need to generate a separate task for collecting
comment|// the partitions used.
if|if
condition|(
operator|!
name|procCtx
operator|.
name|clonedPruningTableScanSet
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|SparkTask
name|pruningTask
init|=
name|SparkUtilities
operator|.
name|createSparkTask
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|SparkTask
name|mainTask
init|=
name|procCtx
operator|.
name|currentTask
decl_stmt|;
name|pruningTask
operator|.
name|addDependentTask
argument_list|(
name|procCtx
operator|.
name|currentTask
argument_list|)
expr_stmt|;
name|procCtx
operator|.
name|rootTasks
operator|.
name|remove
argument_list|(
name|procCtx
operator|.
name|currentTask
argument_list|)
expr_stmt|;
name|procCtx
operator|.
name|rootTasks
operator|.
name|add
argument_list|(
name|pruningTask
argument_list|)
expr_stmt|;
name|procCtx
operator|.
name|currentTask
operator|=
name|pruningTask
expr_stmt|;
name|topNodes
operator|.
name|clear
argument_list|()
expr_stmt|;
name|topNodes
operator|.
name|addAll
argument_list|(
name|procCtx
operator|.
name|clonedPruningTableScanSet
argument_list|)
expr_stmt|;
name|generateTaskTreeHelper
argument_list|(
name|procCtx
argument_list|,
name|topNodes
argument_list|)
expr_stmt|;
name|procCtx
operator|.
name|currentTask
operator|=
name|mainTask
expr_stmt|;
block|}
comment|// -------------------------------- Post Pass ---------------------------------- //
comment|// we need to clone some operator plans and remove union operators still
for|for
control|(
name|BaseWork
name|w
range|:
name|procCtx
operator|.
name|workWithUnionOperators
control|)
block|{
name|GenSparkUtils
operator|.
name|getUtils
argument_list|()
operator|.
name|removeUnionOperators
argument_list|(
name|procCtx
argument_list|,
name|w
argument_list|)
expr_stmt|;
block|}
comment|// we need to fill MapWork with 'local' work and bucket information for SMB Join.
name|GenSparkUtils
operator|.
name|getUtils
argument_list|()
operator|.
name|annotateMapWork
argument_list|(
name|procCtx
argument_list|)
expr_stmt|;
comment|// finally make sure the file sink operators are set up right
for|for
control|(
name|FileSinkOperator
name|fileSink
range|:
name|procCtx
operator|.
name|fileSinkSet
control|)
block|{
name|GenSparkUtils
operator|.
name|getUtils
argument_list|()
operator|.
name|processFileSink
argument_list|(
name|procCtx
argument_list|,
name|fileSink
argument_list|)
expr_stmt|;
block|}
comment|// Process partition pruning sinks
for|for
control|(
name|Operator
argument_list|<
name|?
argument_list|>
name|prunerSink
range|:
name|procCtx
operator|.
name|pruningSinkSet
control|)
block|{
name|utils
operator|.
name|processPartitionPruningSink
argument_list|(
name|procCtx
argument_list|,
operator|(
name|SparkPartitionPruningSinkOperator
operator|)
name|prunerSink
argument_list|)
expr_stmt|;
block|}
name|PERF_LOGGER
operator|.
name|PerfLogEnd
argument_list|(
name|CLASS_NAME
argument_list|,
name|PerfLogger
operator|.
name|SPARK_GENERATE_TASK_TREE
argument_list|)
expr_stmt|;
block|}
specifier|private
name|void
name|generateTaskTreeHelper
parameter_list|(
name|GenSparkProcContext
name|procCtx
parameter_list|,
name|List
argument_list|<
name|Node
argument_list|>
name|topNodes
parameter_list|)
throws|throws
name|SemanticException
block|{
comment|// create a walker which walks the tree in a DFS manner while maintaining
comment|// the operator stack. The dispatcher generates the plan from the operator tree
name|Map
argument_list|<
name|Rule
argument_list|,
name|NodeProcessor
argument_list|>
name|opRules
init|=
operator|new
name|LinkedHashMap
argument_list|<
name|Rule
argument_list|,
name|NodeProcessor
argument_list|>
argument_list|()
decl_stmt|;
name|GenSparkWork
name|genSparkWork
init|=
operator|new
name|GenSparkWork
argument_list|(
name|GenSparkUtils
operator|.
name|getUtils
argument_list|()
argument_list|)
decl_stmt|;
name|opRules
operator|.
name|put
argument_list|(
operator|new
name|RuleRegExp
argument_list|(
literal|"Split Work - ReduceSink"
argument_list|,
name|ReduceSinkOperator
operator|.
name|getOperatorName
argument_list|()
operator|+
literal|"%"
argument_list|)
argument_list|,
name|genSparkWork
argument_list|)
expr_stmt|;
name|opRules
operator|.
name|put
argument_list|(
operator|new
name|RuleRegExp
argument_list|(
literal|"Split Work - SparkPartitionPruningSink"
argument_list|,
name|SparkPartitionPruningSinkOperator
operator|.
name|getOperatorName
argument_list|()
operator|+
literal|"%"
argument_list|)
argument_list|,
name|genSparkWork
argument_list|)
expr_stmt|;
name|opRules
operator|.
name|put
argument_list|(
operator|new
name|TypeRule
argument_list|(
name|MapJoinOperator
operator|.
name|class
argument_list|)
argument_list|,
operator|new
name|SparkReduceSinkMapJoinProc
argument_list|()
argument_list|)
expr_stmt|;
name|opRules
operator|.
name|put
argument_list|(
operator|new
name|RuleRegExp
argument_list|(
literal|"Split Work + Move/Merge - FileSink"
argument_list|,
name|FileSinkOperator
operator|.
name|getOperatorName
argument_list|()
operator|+
literal|"%"
argument_list|)
argument_list|,
operator|new
name|CompositeProcessor
argument_list|(
operator|new
name|SparkFileSinkProcessor
argument_list|()
argument_list|,
name|genSparkWork
argument_list|)
argument_list|)
expr_stmt|;
name|opRules
operator|.
name|put
argument_list|(
operator|new
name|RuleRegExp
argument_list|(
literal|"Handle Analyze Command"
argument_list|,
name|TableScanOperator
operator|.
name|getOperatorName
argument_list|()
operator|+
literal|"%"
argument_list|)
argument_list|,
operator|new
name|SparkProcessAnalyzeTable
argument_list|(
name|GenSparkUtils
operator|.
name|getUtils
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
name|opRules
operator|.
name|put
argument_list|(
operator|new
name|RuleRegExp
argument_list|(
literal|"Remember union"
argument_list|,
name|UnionOperator
operator|.
name|getOperatorName
argument_list|()
operator|+
literal|"%"
argument_list|)
argument_list|,
operator|new
name|NodeProcessor
argument_list|()
block|{
annotation|@
name|Override
specifier|public
name|Object
name|process
parameter_list|(
name|Node
name|n
parameter_list|,
name|Stack
argument_list|<
name|Node
argument_list|>
name|s
parameter_list|,
name|NodeProcessorCtx
name|procCtx
parameter_list|,
name|Object
modifier|...
name|os
parameter_list|)
throws|throws
name|SemanticException
block|{
name|GenSparkProcContext
name|context
init|=
operator|(
name|GenSparkProcContext
operator|)
name|procCtx
decl_stmt|;
name|UnionOperator
name|union
init|=
operator|(
name|UnionOperator
operator|)
name|n
decl_stmt|;
comment|// simply need to remember that we've seen a union.
name|context
operator|.
name|currentUnionOperators
operator|.
name|add
argument_list|(
name|union
argument_list|)
expr_stmt|;
return|return
literal|null
return|;
block|}
block|}
argument_list|)
expr_stmt|;
comment|/**      *  SMB join case:   (Big)   (Small)  (Small)      *                     TS       TS       TS      *                      \       |       /      *                       \      DS     DS      *                         \   |    /      *                         SMBJoinOP      *      * Some of the other processors are expecting only one traversal beyond SMBJoinOp.      * We need to traverse from the big-table path only, and stop traversing on the      * small-table path once we reach SMBJoinOp.      * Also add some SMB join information to the context, so we can properly annotate      * the MapWork later on.      */
name|opRules
operator|.
name|put
argument_list|(
operator|new
name|TypeRule
argument_list|(
name|SMBMapJoinOperator
operator|.
name|class
argument_list|)
argument_list|,
operator|new
name|NodeProcessor
argument_list|()
block|{
annotation|@
name|Override
specifier|public
name|Object
name|process
parameter_list|(
name|Node
name|currNode
parameter_list|,
name|Stack
argument_list|<
name|Node
argument_list|>
name|stack
parameter_list|,
name|NodeProcessorCtx
name|procCtx
parameter_list|,
name|Object
modifier|...
name|os
parameter_list|)
throws|throws
name|SemanticException
block|{
name|GenSparkProcContext
name|context
init|=
operator|(
name|GenSparkProcContext
operator|)
name|procCtx
decl_stmt|;
name|SMBMapJoinOperator
name|currSmbNode
init|=
operator|(
name|SMBMapJoinOperator
operator|)
name|currNode
decl_stmt|;
name|SparkSMBMapJoinInfo
name|smbMapJoinCtx
init|=
name|context
operator|.
name|smbMapJoinCtxMap
operator|.
name|get
argument_list|(
name|currSmbNode
argument_list|)
decl_stmt|;
if|if
condition|(
name|smbMapJoinCtx
operator|==
literal|null
condition|)
block|{
name|smbMapJoinCtx
operator|=
operator|new
name|SparkSMBMapJoinInfo
argument_list|()
expr_stmt|;
name|context
operator|.
name|smbMapJoinCtxMap
operator|.
name|put
argument_list|(
name|currSmbNode
argument_list|,
name|smbMapJoinCtx
argument_list|)
expr_stmt|;
block|}
for|for
control|(
name|Node
name|stackNode
range|:
name|stack
control|)
block|{
if|if
condition|(
name|stackNode
operator|instanceof
name|DummyStoreOperator
condition|)
block|{
comment|//If coming from small-table side, do some book-keeping, and skip traversal.
name|smbMapJoinCtx
operator|.
name|smallTableRootOps
operator|.
name|add
argument_list|(
name|context
operator|.
name|currentRootOperator
argument_list|)
expr_stmt|;
return|return
literal|true
return|;
block|}
block|}
comment|//If coming from big-table side, do some book-keeping, and continue traversal
name|smbMapJoinCtx
operator|.
name|bigTableRootOp
operator|=
name|context
operator|.
name|currentRootOperator
expr_stmt|;
return|return
literal|false
return|;
block|}
block|}
argument_list|)
expr_stmt|;
comment|// The dispatcher fires the processor corresponding to the closest matching
comment|// rule and passes the context along
name|Dispatcher
name|disp
init|=
operator|new
name|DefaultRuleDispatcher
argument_list|(
literal|null
argument_list|,
name|opRules
argument_list|,
name|procCtx
argument_list|)
decl_stmt|;
name|GraphWalker
name|ogw
init|=
operator|new
name|GenSparkWorkWalker
argument_list|(
name|disp
argument_list|,
name|procCtx
argument_list|)
decl_stmt|;
name|ogw
operator|.
name|startWalking
argument_list|(
name|topNodes
argument_list|,
literal|null
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
specifier|protected
name|void
name|setInputFormat
parameter_list|(
name|Task
argument_list|<
name|?
extends|extends
name|Serializable
argument_list|>
name|task
parameter_list|)
block|{
if|if
condition|(
name|task
operator|instanceof
name|SparkTask
condition|)
block|{
name|SparkWork
name|work
init|=
operator|(
operator|(
name|SparkTask
operator|)
name|task
operator|)
operator|.
name|getWork
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|BaseWork
argument_list|>
name|all
init|=
name|work
operator|.
name|getAllWork
argument_list|()
decl_stmt|;
for|for
control|(
name|BaseWork
name|w
range|:
name|all
control|)
block|{
if|if
condition|(
name|w
operator|instanceof
name|MapWork
condition|)
block|{
name|MapWork
name|mapWork
init|=
operator|(
name|MapWork
operator|)
name|w
decl_stmt|;
name|HashMap
argument_list|<
name|String
argument_list|,
name|Operator
argument_list|<
name|?
extends|extends
name|OperatorDesc
argument_list|>
argument_list|>
name|opMap
init|=
name|mapWork
operator|.
name|getAliasToWork
argument_list|()
decl_stmt|;
if|if
condition|(
operator|!
name|opMap
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
for|for
control|(
name|Operator
argument_list|<
name|?
extends|extends
name|OperatorDesc
argument_list|>
name|op
range|:
name|opMap
operator|.
name|values
argument_list|()
control|)
block|{
name|setInputFormat
argument_list|(
name|mapWork
argument_list|,
name|op
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
block|}
elseif|else
if|if
condition|(
name|task
operator|instanceof
name|ConditionalTask
condition|)
block|{
name|List
argument_list|<
name|Task
argument_list|<
name|?
extends|extends
name|Serializable
argument_list|>
argument_list|>
name|listTasks
init|=
operator|(
operator|(
name|ConditionalTask
operator|)
name|task
operator|)
operator|.
name|getListTasks
argument_list|()
decl_stmt|;
for|for
control|(
name|Task
argument_list|<
name|?
extends|extends
name|Serializable
argument_list|>
name|tsk
range|:
name|listTasks
control|)
block|{
name|setInputFormat
argument_list|(
name|tsk
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
name|task
operator|.
name|getChildTasks
argument_list|()
operator|!=
literal|null
condition|)
block|{
for|for
control|(
name|Task
argument_list|<
name|?
extends|extends
name|Serializable
argument_list|>
name|childTask
range|:
name|task
operator|.
name|getChildTasks
argument_list|()
control|)
block|{
name|setInputFormat
argument_list|(
name|childTask
argument_list|)
expr_stmt|;
block|}
block|}
block|}
specifier|private
name|void
name|setInputFormat
parameter_list|(
name|MapWork
name|work
parameter_list|,
name|Operator
argument_list|<
name|?
extends|extends
name|OperatorDesc
argument_list|>
name|op
parameter_list|)
block|{
if|if
condition|(
name|op
operator|.
name|isUseBucketizedHiveInputFormat
argument_list|()
condition|)
block|{
name|work
operator|.
name|setUseBucketizedHiveInputFormat
argument_list|(
literal|true
argument_list|)
expr_stmt|;
return|return;
block|}
if|if
condition|(
name|op
operator|.
name|getChildOperators
argument_list|()
operator|!=
literal|null
condition|)
block|{
for|for
control|(
name|Operator
argument_list|<
name|?
extends|extends
name|OperatorDesc
argument_list|>
name|childOp
range|:
name|op
operator|.
name|getChildOperators
argument_list|()
control|)
block|{
name|setInputFormat
argument_list|(
name|work
argument_list|,
name|childOp
argument_list|)
expr_stmt|;
block|}
block|}
block|}
annotation|@
name|Override
specifier|protected
name|void
name|decideExecMode
parameter_list|(
name|List
argument_list|<
name|Task
argument_list|<
name|?
extends|extends
name|Serializable
argument_list|>
argument_list|>
name|rootTasks
parameter_list|,
name|Context
name|ctx
parameter_list|,
name|GlobalLimitCtx
name|globalLimitCtx
parameter_list|)
throws|throws
name|SemanticException
block|{
comment|// currently all Spark work is on the cluster
return|return;
block|}
annotation|@
name|Override
specifier|protected
name|void
name|optimizeTaskPlan
parameter_list|(
name|List
argument_list|<
name|Task
argument_list|<
name|?
extends|extends
name|Serializable
argument_list|>
argument_list|>
name|rootTasks
parameter_list|,
name|ParseContext
name|pCtx
parameter_list|,
name|Context
name|ctx
parameter_list|)
throws|throws
name|SemanticException
block|{
name|PERF_LOGGER
operator|.
name|PerfLogBegin
argument_list|(
name|CLASS_NAME
argument_list|,
name|PerfLogger
operator|.
name|SPARK_OPTIMIZE_TASK_TREE
argument_list|)
expr_stmt|;
name|PhysicalContext
name|physicalCtx
init|=
operator|new
name|PhysicalContext
argument_list|(
name|conf
argument_list|,
name|pCtx
argument_list|,
name|pCtx
operator|.
name|getContext
argument_list|()
argument_list|,
name|rootTasks
argument_list|,
name|pCtx
operator|.
name|getFetchTask
argument_list|()
argument_list|)
decl_stmt|;
name|physicalCtx
operator|=
operator|new
name|SplitSparkWorkResolver
argument_list|()
operator|.
name|resolve
argument_list|(
name|physicalCtx
argument_list|)
expr_stmt|;
if|if
condition|(
name|conf
operator|.
name|getBoolVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVESKEWJOIN
argument_list|)
condition|)
block|{
operator|(
operator|new
name|SparkSkewJoinResolver
argument_list|()
operator|)
operator|.
name|resolve
argument_list|(
name|physicalCtx
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Skipping runtime skew join optimization"
argument_list|)
expr_stmt|;
block|}
name|physicalCtx
operator|=
operator|new
name|SparkMapJoinResolver
argument_list|()
operator|.
name|resolve
argument_list|(
name|physicalCtx
argument_list|)
expr_stmt|;
if|if
condition|(
name|conf
operator|.
name|isSparkDPPAny
argument_list|()
condition|)
block|{
name|physicalCtx
operator|=
operator|new
name|SparkDynamicPartitionPruningResolver
argument_list|()
operator|.
name|resolve
argument_list|(
name|physicalCtx
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|conf
operator|.
name|getBoolVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVENULLSCANOPTIMIZE
argument_list|)
condition|)
block|{
name|physicalCtx
operator|=
operator|new
name|NullScanOptimizer
argument_list|()
operator|.
name|resolve
argument_list|(
name|physicalCtx
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Skipping null scan query optimization"
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|conf
operator|.
name|getBoolVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEMETADATAONLYQUERIES
argument_list|)
condition|)
block|{
name|physicalCtx
operator|=
operator|new
name|MetadataOnlyOptimizer
argument_list|()
operator|.
name|resolve
argument_list|(
name|physicalCtx
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Skipping metadata only query optimization"
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|conf
operator|.
name|getBoolVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_CHECK_CROSS_PRODUCT
argument_list|)
condition|)
block|{
name|physicalCtx
operator|=
operator|new
name|SparkCrossProductCheck
argument_list|()
operator|.
name|resolve
argument_list|(
name|physicalCtx
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Skipping cross product analysis"
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|conf
operator|.
name|getBoolVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_VECTORIZATION_ENABLED
argument_list|)
condition|)
block|{
operator|(
operator|new
name|Vectorizer
argument_list|()
operator|)
operator|.
name|resolve
argument_list|(
name|physicalCtx
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Skipping vectorization"
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
operator|!
literal|"none"
operator|.
name|equalsIgnoreCase
argument_list|(
name|conf
operator|.
name|getVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVESTAGEIDREARRANGE
argument_list|)
argument_list|)
condition|)
block|{
operator|(
operator|new
name|StageIDsRearranger
argument_list|()
operator|)
operator|.
name|resolve
argument_list|(
name|physicalCtx
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Skipping stage id rearranger"
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|conf
operator|.
name|getBoolVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_COMBINE_EQUIVALENT_WORK_OPTIMIZATION
argument_list|)
condition|)
block|{
operator|new
name|CombineEquivalentWorkResolver
argument_list|()
operator|.
name|resolve
argument_list|(
name|physicalCtx
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Skipping combine equivalent work optimization"
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|physicalCtx
operator|.
name|getContext
argument_list|()
operator|.
name|getExplainAnalyze
argument_list|()
operator|!=
literal|null
condition|)
block|{
operator|new
name|AnnotateRunTimeStatsOptimizer
argument_list|()
operator|.
name|resolve
argument_list|(
name|physicalCtx
argument_list|)
expr_stmt|;
block|}
name|PERF_LOGGER
operator|.
name|PerfLogEnd
argument_list|(
name|CLASS_NAME
argument_list|,
name|PerfLogger
operator|.
name|SPARK_OPTIMIZE_TASK_TREE
argument_list|)
expr_stmt|;
return|return;
block|}
specifier|private
name|void
name|updateBucketingVersionForUpgrade
parameter_list|(
name|OptimizeSparkProcContext
name|procCtx
parameter_list|)
block|{
comment|// Fetch all the FileSinkOperators.
name|Set
argument_list|<
name|FileSinkOperator
argument_list|>
name|fsOpsAll
init|=
operator|new
name|HashSet
argument_list|<>
argument_list|()
decl_stmt|;
for|for
control|(
name|TableScanOperator
name|ts
range|:
name|procCtx
operator|.
name|getParseContext
argument_list|()
operator|.
name|getTopOps
argument_list|()
operator|.
name|values
argument_list|()
control|)
block|{
name|Set
argument_list|<
name|FileSinkOperator
argument_list|>
name|fsOps
init|=
name|OperatorUtils
operator|.
name|findOperators
argument_list|(
name|ts
argument_list|,
name|FileSinkOperator
operator|.
name|class
argument_list|)
decl_stmt|;
name|fsOpsAll
operator|.
name|addAll
argument_list|(
name|fsOps
argument_list|)
expr_stmt|;
block|}
for|for
control|(
name|FileSinkOperator
name|fsOp
range|:
name|fsOpsAll
control|)
block|{
if|if
condition|(
operator|!
name|fsOp
operator|.
name|getConf
argument_list|()
operator|.
name|getTableInfo
argument_list|()
operator|.
name|isSetBucketingVersion
argument_list|()
condition|)
block|{
continue|continue;
block|}
comment|// Look for direct parent ReduceSinkOp
comment|// If there are more than 1 parent, bail out.
name|Operator
argument_list|<
name|?
argument_list|>
name|parent
init|=
name|fsOp
decl_stmt|;
name|List
argument_list|<
name|Operator
argument_list|<
name|?
argument_list|>
argument_list|>
name|parentOps
init|=
name|parent
operator|.
name|getParentOperators
argument_list|()
decl_stmt|;
while|while
condition|(
name|parentOps
operator|!=
literal|null
operator|&&
name|parentOps
operator|.
name|size
argument_list|()
operator|==
literal|1
condition|)
block|{
name|parent
operator|=
name|parentOps
operator|.
name|get
argument_list|(
literal|0
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
operator|(
name|parent
operator|instanceof
name|ReduceSinkOperator
operator|)
condition|)
block|{
name|parentOps
operator|=
name|parent
operator|.
name|getParentOperators
argument_list|()
expr_stmt|;
continue|continue;
block|}
comment|// Found the target RSOp
name|parent
operator|.
name|setBucketingVersion
argument_list|(
name|fsOp
operator|.
name|getConf
argument_list|()
operator|.
name|getTableInfo
argument_list|()
operator|.
name|getBucketingVersion
argument_list|()
argument_list|)
expr_stmt|;
break|break;
block|}
block|}
block|}
block|}
end_class

end_unit

