begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/**  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|orc
package|;
end_package

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|nio
operator|.
name|ByteBuffer
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Arrays
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Iterator
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
operator|.
name|Entry
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|NavigableMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|TreeMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|Callable
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|CompletionService
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|ExecutorCompletionService
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|ExecutorService
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|Executors
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|Future
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|atomic
operator|.
name|AtomicInteger
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|codec
operator|.
name|binary
operator|.
name|Hex
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|Log
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|LogFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configuration
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|BlockLocation
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileStatus
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|ValidReadTxnList
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|ValidTxnList
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
operator|.
name|ConfVars
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|hive_metastoreConstants
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|Utilities
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|vector
operator|.
name|VectorizedInputFormatInterface
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|AcidInputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|AcidOutputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|AcidUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|AcidUtils
operator|.
name|Directory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|CombineHiveInputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|InputFormatChecker
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|LlapWrappableInputFormatInterface
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|RecordIdentifier
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|StatsProvidingRecordReader
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|orc
operator|.
name|OrcFile
operator|.
name|WriterVersion
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|sarg
operator|.
name|ConvertAstToSearchArg
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|sarg
operator|.
name|PredicateLeaf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|sarg
operator|.
name|SearchArgument
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|sarg
operator|.
name|SearchArgument
operator|.
name|TruthValue
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|Hive
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|HiveException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|sarg
operator|.
name|SearchArgumentFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|ColumnProjectionUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|SerDeStats
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|objectinspector
operator|.
name|ObjectInspector
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|objectinspector
operator|.
name|StructObjectInspector
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|shims
operator|.
name|HadoopShims
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|shims
operator|.
name|HadoopShims
operator|.
name|HdfsFileStatusWithId
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|shims
operator|.
name|ShimLoader
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|LongWritable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|NullWritable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|FileSplit
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|InputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|InputSplit
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|JobConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|Reporter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|StringUtils
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|annotations
operator|.
name|VisibleForTesting
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|cache
operator|.
name|Cache
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|cache
operator|.
name|CacheBuilder
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|collect
operator|.
name|Lists
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|util
operator|.
name|concurrent
operator|.
name|ThreadFactoryBuilder
import|;
end_import

begin_comment
comment|/**  * A MapReduce/Hive input format for ORC files.  *<p>  * This class implements both the classic InputFormat, which stores the rows  * directly, and AcidInputFormat, which stores a series of events with the  * following schema:  *<pre>  *   class AcidEvent&lt;ROW&gt; {  *     enum ACTION {INSERT, UPDATE, DELETE}  *     ACTION operation;  *     long originalTransaction;  *     int bucket;  *     long rowId;  *     long currentTransaction;  *     ROW row;  *   }  *</pre>  * Each AcidEvent object corresponds to an update event. The  * originalTransaction, bucket, and rowId are the unique identifier for the row.  * The operation and currentTransaction are the operation and the transaction  * that added this event. Insert and update events include the entire row, while  * delete events have null for row.  */
end_comment

begin_class
specifier|public
class|class
name|OrcInputFormat
implements|implements
name|InputFormat
argument_list|<
name|NullWritable
argument_list|,
name|OrcStruct
argument_list|>
implements|,
name|InputFormatChecker
implements|,
name|VectorizedInputFormatInterface
implements|,
name|LlapWrappableInputFormatInterface
implements|,
name|AcidInputFormat
argument_list|<
name|NullWritable
argument_list|,
name|OrcStruct
argument_list|>
implements|,
name|CombineHiveInputFormat
operator|.
name|AvoidSplitCombination
block|{
specifier|static
enum|enum
name|SplitStrategyKind
block|{
name|HYBRID
block|,
name|BI
block|,
name|ETL
block|}
specifier|private
specifier|static
specifier|final
name|Log
name|LOG
init|=
name|LogFactory
operator|.
name|getLog
argument_list|(
name|OrcInputFormat
operator|.
name|class
argument_list|)
decl_stmt|;
specifier|private
specifier|static
name|boolean
name|isDebugEnabled
init|=
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
decl_stmt|;
specifier|static
specifier|final
name|HadoopShims
name|SHIMS
init|=
name|ShimLoader
operator|.
name|getHadoopShims
argument_list|()
decl_stmt|;
specifier|static
specifier|final
name|String
name|MIN_SPLIT_SIZE
init|=
name|SHIMS
operator|.
name|getHadoopConfNames
argument_list|()
operator|.
name|get
argument_list|(
literal|"MAPREDMINSPLITSIZE"
argument_list|)
decl_stmt|;
specifier|static
specifier|final
name|String
name|MAX_SPLIT_SIZE
init|=
name|SHIMS
operator|.
name|getHadoopConfNames
argument_list|()
operator|.
name|get
argument_list|(
literal|"MAPREDMAXSPLITSIZE"
argument_list|)
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|long
name|DEFAULT_MIN_SPLIT_SIZE
init|=
literal|16
operator|*
literal|1024
operator|*
literal|1024
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|long
name|DEFAULT_MAX_SPLIT_SIZE
init|=
literal|256
operator|*
literal|1024
operator|*
literal|1024
decl_stmt|;
comment|/**    * When picking the hosts for a split that crosses block boundaries,    * drop any host that has fewer than MIN_INCLUDED_LOCATION of the    * number of bytes available on the host with the most.    * If host1 has 10MB of the split, host2 has 20MB, and host3 has 18MB the    * split will contain host2 (100% of host2) and host3 (90% of host2). Host1    * with 50% will be dropped.    */
specifier|private
specifier|static
specifier|final
name|double
name|MIN_INCLUDED_LOCATION
init|=
literal|0.80
decl_stmt|;
annotation|@
name|Override
specifier|public
name|boolean
name|shouldSkipCombine
parameter_list|(
name|Path
name|path
parameter_list|,
name|Configuration
name|conf
parameter_list|)
throws|throws
name|IOException
block|{
return|return
operator|(
name|conf
operator|.
name|get
argument_list|(
name|AcidUtils
operator|.
name|CONF_ACID_KEY
argument_list|)
operator|!=
literal|null
operator|)
operator|||
name|AcidUtils
operator|.
name|isAcid
argument_list|(
name|path
argument_list|,
name|conf
argument_list|)
return|;
block|}
specifier|private
specifier|static
class|class
name|OrcRecordReader
implements|implements
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|RecordReader
argument_list|<
name|NullWritable
argument_list|,
name|OrcStruct
argument_list|>
implements|,
name|StatsProvidingRecordReader
block|{
specifier|private
specifier|final
name|RecordReader
name|reader
decl_stmt|;
specifier|private
specifier|final
name|long
name|offset
decl_stmt|;
specifier|private
specifier|final
name|long
name|length
decl_stmt|;
specifier|private
specifier|final
name|int
name|numColumns
decl_stmt|;
specifier|private
name|float
name|progress
init|=
literal|0.0f
decl_stmt|;
specifier|private
specifier|final
name|Reader
name|file
decl_stmt|;
specifier|private
specifier|final
name|SerDeStats
name|stats
decl_stmt|;
name|OrcRecordReader
parameter_list|(
name|Reader
name|file
parameter_list|,
name|Configuration
name|conf
parameter_list|,
name|FileSplit
name|split
parameter_list|)
throws|throws
name|IOException
block|{
name|List
argument_list|<
name|OrcProto
operator|.
name|Type
argument_list|>
name|types
init|=
name|file
operator|.
name|getTypes
argument_list|()
decl_stmt|;
name|this
operator|.
name|file
operator|=
name|file
expr_stmt|;
name|numColumns
operator|=
operator|(
name|types
operator|.
name|size
argument_list|()
operator|==
literal|0
operator|)
condition|?
literal|0
else|:
name|types
operator|.
name|get
argument_list|(
literal|0
argument_list|)
operator|.
name|getSubtypesCount
argument_list|()
expr_stmt|;
name|this
operator|.
name|offset
operator|=
name|split
operator|.
name|getStart
argument_list|()
expr_stmt|;
name|this
operator|.
name|length
operator|=
name|split
operator|.
name|getLength
argument_list|()
expr_stmt|;
name|this
operator|.
name|reader
operator|=
name|createReaderFromFile
argument_list|(
name|file
argument_list|,
name|conf
argument_list|,
name|offset
argument_list|,
name|length
argument_list|)
expr_stmt|;
name|this
operator|.
name|stats
operator|=
operator|new
name|SerDeStats
argument_list|()
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|next
parameter_list|(
name|NullWritable
name|key
parameter_list|,
name|OrcStruct
name|value
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|reader
operator|.
name|hasNext
argument_list|()
condition|)
block|{
name|reader
operator|.
name|next
argument_list|(
name|value
argument_list|)
expr_stmt|;
name|progress
operator|=
name|reader
operator|.
name|getProgress
argument_list|()
expr_stmt|;
return|return
literal|true
return|;
block|}
else|else
block|{
return|return
literal|false
return|;
block|}
block|}
annotation|@
name|Override
specifier|public
name|NullWritable
name|createKey
parameter_list|()
block|{
return|return
name|NullWritable
operator|.
name|get
argument_list|()
return|;
block|}
annotation|@
name|Override
specifier|public
name|OrcStruct
name|createValue
parameter_list|()
block|{
return|return
operator|new
name|OrcStruct
argument_list|(
name|numColumns
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|public
name|long
name|getPos
parameter_list|()
throws|throws
name|IOException
block|{
return|return
name|offset
operator|+
call|(
name|long
call|)
argument_list|(
name|progress
operator|*
name|length
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|close
parameter_list|()
throws|throws
name|IOException
block|{
name|reader
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|float
name|getProgress
parameter_list|()
throws|throws
name|IOException
block|{
return|return
name|progress
return|;
block|}
annotation|@
name|Override
specifier|public
name|SerDeStats
name|getStats
parameter_list|()
block|{
name|stats
operator|.
name|setRawDataSize
argument_list|(
name|file
operator|.
name|getRawDataSize
argument_list|()
argument_list|)
expr_stmt|;
name|stats
operator|.
name|setRowCount
argument_list|(
name|file
operator|.
name|getNumberOfRows
argument_list|()
argument_list|)
expr_stmt|;
return|return
name|stats
return|;
block|}
block|}
comment|/**    * Get the root column for the row. In ACID format files, it is offset by    * the extra metadata columns.    * @param isOriginal is the file in the original format?    * @return the column number for the root of row.    */
specifier|private
specifier|static
name|int
name|getRootColumn
parameter_list|(
name|boolean
name|isOriginal
parameter_list|)
block|{
return|return
name|isOriginal
condition|?
literal|0
else|:
operator|(
name|OrcRecordUpdater
operator|.
name|ROW
operator|+
literal|1
operator|)
return|;
block|}
specifier|public
specifier|static
name|RecordReader
name|createReaderFromFile
parameter_list|(
name|Reader
name|file
parameter_list|,
name|Configuration
name|conf
parameter_list|,
name|long
name|offset
parameter_list|,
name|long
name|length
parameter_list|)
throws|throws
name|IOException
block|{
name|Reader
operator|.
name|Options
name|options
init|=
operator|new
name|Reader
operator|.
name|Options
argument_list|()
operator|.
name|range
argument_list|(
name|offset
argument_list|,
name|length
argument_list|)
decl_stmt|;
name|boolean
name|isOriginal
init|=
name|isOriginal
argument_list|(
name|file
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|OrcProto
operator|.
name|Type
argument_list|>
name|types
init|=
name|file
operator|.
name|getTypes
argument_list|()
decl_stmt|;
name|options
operator|.
name|include
argument_list|(
name|genIncludedColumns
argument_list|(
name|types
argument_list|,
name|conf
argument_list|,
name|isOriginal
argument_list|)
argument_list|)
expr_stmt|;
name|setSearchArgument
argument_list|(
name|options
argument_list|,
name|types
argument_list|,
name|conf
argument_list|,
name|isOriginal
argument_list|)
expr_stmt|;
return|return
name|file
operator|.
name|rowsOptions
argument_list|(
name|options
argument_list|)
return|;
block|}
specifier|public
specifier|static
name|boolean
name|isOriginal
parameter_list|(
name|Reader
name|file
parameter_list|)
block|{
return|return
operator|!
name|file
operator|.
name|hasMetadataValue
argument_list|(
name|OrcRecordUpdater
operator|.
name|ACID_KEY_INDEX_NAME
argument_list|)
return|;
block|}
comment|/**    * Recurse down into a type subtree turning on all of the sub-columns.    * @param types the types of the file    * @param result the global view of columns that should be included    * @param typeId the root of tree to enable    * @param rootColumn the top column    */
specifier|private
specifier|static
name|void
name|includeColumnRecursive
parameter_list|(
name|List
argument_list|<
name|OrcProto
operator|.
name|Type
argument_list|>
name|types
parameter_list|,
name|boolean
index|[]
name|result
parameter_list|,
name|int
name|typeId
parameter_list|,
name|int
name|rootColumn
parameter_list|)
block|{
name|result
index|[
name|typeId
operator|-
name|rootColumn
index|]
operator|=
literal|true
expr_stmt|;
name|OrcProto
operator|.
name|Type
name|type
init|=
name|types
operator|.
name|get
argument_list|(
name|typeId
argument_list|)
decl_stmt|;
name|int
name|children
init|=
name|type
operator|.
name|getSubtypesCount
argument_list|()
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|children
condition|;
operator|++
name|i
control|)
block|{
name|includeColumnRecursive
argument_list|(
name|types
argument_list|,
name|result
argument_list|,
name|type
operator|.
name|getSubtypes
argument_list|(
name|i
argument_list|)
argument_list|,
name|rootColumn
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * Modifies the SARG, replacing column names with column indexes in target table schema. This    * basically does the same thing as all the shennannigans with included columns, except for the    * last step where ORC gets direct subtypes of root column and uses the ordered match to map    * table columns to file columns. The numbers put into predicate leaf should allow to go into    * said subtypes directly by index to get the proper index in the file.    * This won't work with schema evolution, although it's probably much easier to reason about    * if schema evolution was to be supported, because this is a clear boundary between table    * schema columns and all things ORC. None of the ORC stuff is used here and none of the    * table schema stuff is used after that - ORC doesn't need a bunch of extra crap to apply    * the SARG thus modified.    */
specifier|public
specifier|static
name|void
name|translateSargToTableColIndexes
parameter_list|(
name|SearchArgument
name|sarg
parameter_list|,
name|Configuration
name|conf
parameter_list|,
name|int
name|rootColumn
parameter_list|)
block|{
name|String
name|nameStr
init|=
name|getNeededColumnNamesString
argument_list|(
name|conf
argument_list|)
decl_stmt|,
name|idStr
init|=
name|getSargColumnIDsString
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|String
index|[]
name|knownNames
init|=
name|nameStr
operator|.
name|split
argument_list|(
literal|","
argument_list|)
decl_stmt|;
name|String
index|[]
name|idStrs
init|=
operator|(
name|idStr
operator|==
literal|null
operator|)
condition|?
literal|null
else|:
name|idStr
operator|.
name|split
argument_list|(
literal|","
argument_list|)
decl_stmt|;
assert|assert
name|idStrs
operator|==
literal|null
operator|||
name|knownNames
operator|.
name|length
operator|==
name|idStrs
operator|.
name|length
assert|;
name|HashMap
argument_list|<
name|String
argument_list|,
name|Integer
argument_list|>
name|nameIdMap
init|=
operator|new
name|HashMap
argument_list|<>
argument_list|()
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|knownNames
operator|.
name|length
condition|;
operator|++
name|i
control|)
block|{
name|nameIdMap
operator|.
name|put
argument_list|(
name|knownNames
index|[
name|i
index|]
argument_list|,
name|idStrs
operator|!=
literal|null
condition|?
name|Integer
operator|.
name|parseInt
argument_list|(
name|idStrs
index|[
name|i
index|]
argument_list|)
else|:
name|i
argument_list|)
expr_stmt|;
block|}
name|List
argument_list|<
name|PredicateLeaf
argument_list|>
name|leaves
init|=
name|sarg
operator|.
name|getLeaves
argument_list|()
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|leaves
operator|.
name|size
argument_list|()
condition|;
operator|++
name|i
control|)
block|{
name|PredicateLeaf
name|pl
init|=
name|leaves
operator|.
name|get
argument_list|(
name|i
argument_list|)
decl_stmt|;
name|Integer
name|colId
init|=
name|nameIdMap
operator|.
name|get
argument_list|(
name|pl
operator|.
name|getColumnName
argument_list|()
argument_list|)
decl_stmt|;
name|String
name|newColName
init|=
name|RecordReaderImpl
operator|.
name|encodeTranslatedSargColumn
argument_list|(
name|rootColumn
argument_list|,
name|colId
argument_list|)
decl_stmt|;
name|SearchArgumentFactory
operator|.
name|setPredicateLeafColumn
argument_list|(
name|pl
argument_list|,
name|newColName
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"SARG translated into "
operator|+
name|sarg
argument_list|)
expr_stmt|;
block|}
block|}
specifier|public
specifier|static
name|boolean
index|[]
name|genIncludedColumns
parameter_list|(
name|List
argument_list|<
name|OrcProto
operator|.
name|Type
argument_list|>
name|types
parameter_list|,
name|List
argument_list|<
name|Integer
argument_list|>
name|included
parameter_list|,
name|boolean
name|isOriginal
parameter_list|)
block|{
name|int
name|rootColumn
init|=
name|getRootColumn
argument_list|(
name|isOriginal
argument_list|)
decl_stmt|;
name|int
name|numColumns
init|=
name|types
operator|.
name|size
argument_list|()
operator|-
name|rootColumn
decl_stmt|;
name|boolean
index|[]
name|result
init|=
operator|new
name|boolean
index|[
name|numColumns
index|]
decl_stmt|;
name|result
index|[
literal|0
index|]
operator|=
literal|true
expr_stmt|;
name|OrcProto
operator|.
name|Type
name|root
init|=
name|types
operator|.
name|get
argument_list|(
name|rootColumn
argument_list|)
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|root
operator|.
name|getSubtypesCount
argument_list|()
condition|;
operator|++
name|i
control|)
block|{
if|if
condition|(
name|included
operator|.
name|contains
argument_list|(
name|i
argument_list|)
condition|)
block|{
name|includeColumnRecursive
argument_list|(
name|types
argument_list|,
name|result
argument_list|,
name|root
operator|.
name|getSubtypes
argument_list|(
name|i
argument_list|)
argument_list|,
name|rootColumn
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|result
return|;
block|}
comment|/**    * Take the configuration and figure out which columns we need to include.    * @param types the types for the file    * @param conf the configuration    * @param isOriginal is the file in the original format?    */
specifier|public
specifier|static
name|boolean
index|[]
name|genIncludedColumns
parameter_list|(
name|List
argument_list|<
name|OrcProto
operator|.
name|Type
argument_list|>
name|types
parameter_list|,
name|Configuration
name|conf
parameter_list|,
name|boolean
name|isOriginal
parameter_list|)
block|{
if|if
condition|(
operator|!
name|ColumnProjectionUtils
operator|.
name|isReadAllColumns
argument_list|(
name|conf
argument_list|)
condition|)
block|{
name|List
argument_list|<
name|Integer
argument_list|>
name|included
init|=
name|ColumnProjectionUtils
operator|.
name|getReadColumnIDs
argument_list|(
name|conf
argument_list|)
decl_stmt|;
return|return
name|genIncludedColumns
argument_list|(
name|types
argument_list|,
name|included
argument_list|,
name|isOriginal
argument_list|)
return|;
block|}
else|else
block|{
return|return
literal|null
return|;
block|}
block|}
specifier|public
specifier|static
name|String
index|[]
name|getSargColumnNames
parameter_list|(
name|String
index|[]
name|originalColumnNames
parameter_list|,
name|List
argument_list|<
name|OrcProto
operator|.
name|Type
argument_list|>
name|types
parameter_list|,
name|boolean
index|[]
name|includedColumns
parameter_list|,
name|boolean
name|isOriginal
parameter_list|)
block|{
name|int
name|rootColumn
init|=
name|getRootColumn
argument_list|(
name|isOriginal
argument_list|)
decl_stmt|;
name|String
index|[]
name|columnNames
init|=
operator|new
name|String
index|[
name|types
operator|.
name|size
argument_list|()
operator|-
name|rootColumn
index|]
decl_stmt|;
name|int
name|i
init|=
literal|0
decl_stmt|;
comment|// The way this works is as such. originalColumnNames is the equivalent on getNeededColumns
comment|// from TSOP. They are assumed to be in the same order as the columns in ORC file, AND they are
comment|// assumed to be equivalent to the columns in includedColumns (because it was generated from
comment|// the same column list at some point in the past), minus the subtype columns. Therefore, when
comment|// we go thru all the top level ORC file columns that are included, in order, they match
comment|// originalColumnNames. This way, we do not depend on names stored inside ORC for SARG leaf
comment|// column name resolution (see mapSargColumns method).
for|for
control|(
name|int
name|columnId
range|:
name|types
operator|.
name|get
argument_list|(
name|rootColumn
argument_list|)
operator|.
name|getSubtypesList
argument_list|()
control|)
block|{
if|if
condition|(
name|includedColumns
operator|==
literal|null
operator|||
name|includedColumns
index|[
name|columnId
operator|-
name|rootColumn
index|]
condition|)
block|{
comment|// this is guaranteed to be positive because types only have children
comment|// ids greater than their own id.
name|columnNames
index|[
name|columnId
operator|-
name|rootColumn
index|]
operator|=
name|originalColumnNames
index|[
name|i
operator|++
index|]
expr_stmt|;
block|}
block|}
return|return
name|columnNames
return|;
block|}
specifier|static
name|void
name|setSearchArgument
parameter_list|(
name|Reader
operator|.
name|Options
name|options
parameter_list|,
name|List
argument_list|<
name|OrcProto
operator|.
name|Type
argument_list|>
name|types
parameter_list|,
name|Configuration
name|conf
parameter_list|,
name|boolean
name|isOriginal
parameter_list|)
block|{
name|String
name|neededColumnNames
init|=
name|getNeededColumnNamesString
argument_list|(
name|conf
argument_list|)
decl_stmt|;
if|if
condition|(
name|neededColumnNames
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"No ORC pushdown predicate - no column names"
argument_list|)
expr_stmt|;
name|options
operator|.
name|searchArgument
argument_list|(
literal|null
argument_list|,
literal|null
argument_list|)
expr_stmt|;
return|return;
block|}
name|SearchArgument
name|sarg
init|=
name|ConvertAstToSearchArg
operator|.
name|createFromConf
argument_list|(
name|conf
argument_list|)
decl_stmt|;
if|if
condition|(
name|sarg
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"No ORC pushdown predicate"
argument_list|)
expr_stmt|;
name|options
operator|.
name|searchArgument
argument_list|(
literal|null
argument_list|,
literal|null
argument_list|)
expr_stmt|;
return|return;
block|}
name|LOG
operator|.
name|info
argument_list|(
literal|"ORC pushdown predicate: "
operator|+
name|sarg
argument_list|)
expr_stmt|;
name|options
operator|.
name|searchArgument
argument_list|(
name|sarg
argument_list|,
name|getSargColumnNames
argument_list|(
name|neededColumnNames
operator|.
name|split
argument_list|(
literal|","
argument_list|)
argument_list|,
name|types
argument_list|,
name|options
operator|.
name|getInclude
argument_list|()
argument_list|,
name|isOriginal
argument_list|)
argument_list|)
expr_stmt|;
block|}
specifier|static
name|boolean
name|canCreateSargFromConf
parameter_list|(
name|Configuration
name|conf
parameter_list|)
block|{
if|if
condition|(
name|getNeededColumnNamesString
argument_list|(
name|conf
argument_list|)
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"No ORC pushdown predicate - no column names"
argument_list|)
expr_stmt|;
return|return
literal|false
return|;
block|}
if|if
condition|(
operator|!
name|ConvertAstToSearchArg
operator|.
name|canCreateFromConf
argument_list|(
name|conf
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"No ORC pushdown predicate"
argument_list|)
expr_stmt|;
return|return
literal|false
return|;
block|}
return|return
literal|true
return|;
block|}
specifier|private
specifier|static
name|String
index|[]
name|extractNeededColNames
parameter_list|(
name|List
argument_list|<
name|OrcProto
operator|.
name|Type
argument_list|>
name|types
parameter_list|,
name|Configuration
name|conf
parameter_list|,
name|boolean
index|[]
name|include
parameter_list|,
name|boolean
name|isOriginal
parameter_list|)
block|{
return|return
name|extractNeededColNames
argument_list|(
name|types
argument_list|,
name|getNeededColumnNamesString
argument_list|(
name|conf
argument_list|)
argument_list|,
name|include
argument_list|,
name|isOriginal
argument_list|)
return|;
block|}
specifier|private
specifier|static
name|String
index|[]
name|extractNeededColNames
parameter_list|(
name|List
argument_list|<
name|OrcProto
operator|.
name|Type
argument_list|>
name|types
parameter_list|,
name|String
name|columnNamesString
parameter_list|,
name|boolean
index|[]
name|include
parameter_list|,
name|boolean
name|isOriginal
parameter_list|)
block|{
return|return
name|getSargColumnNames
argument_list|(
name|columnNamesString
operator|.
name|split
argument_list|(
literal|","
argument_list|)
argument_list|,
name|types
argument_list|,
name|include
argument_list|,
name|isOriginal
argument_list|)
return|;
block|}
specifier|private
specifier|static
name|String
name|getNeededColumnNamesString
parameter_list|(
name|Configuration
name|conf
parameter_list|)
block|{
return|return
name|conf
operator|.
name|get
argument_list|(
name|ColumnProjectionUtils
operator|.
name|READ_COLUMN_NAMES_CONF_STR
argument_list|)
return|;
block|}
specifier|private
specifier|static
name|String
name|getSargColumnIDsString
parameter_list|(
name|Configuration
name|conf
parameter_list|)
block|{
return|return
name|conf
operator|.
name|getBoolean
argument_list|(
name|ColumnProjectionUtils
operator|.
name|READ_ALL_COLUMNS
argument_list|,
literal|true
argument_list|)
condition|?
literal|null
else|:
name|conf
operator|.
name|get
argument_list|(
name|ColumnProjectionUtils
operator|.
name|READ_COLUMN_IDS_CONF_STR
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|validateInput
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|HiveConf
name|conf
parameter_list|,
name|List
argument_list|<
name|FileStatus
argument_list|>
name|files
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|Utilities
operator|.
name|isVectorMode
argument_list|(
name|conf
argument_list|)
condition|)
block|{
return|return
operator|new
name|VectorizedOrcInputFormat
argument_list|()
operator|.
name|validateInput
argument_list|(
name|fs
argument_list|,
name|conf
argument_list|,
name|files
argument_list|)
return|;
block|}
if|if
condition|(
name|files
operator|.
name|size
argument_list|()
operator|<=
literal|0
condition|)
block|{
return|return
literal|false
return|;
block|}
for|for
control|(
name|FileStatus
name|file
range|:
name|files
control|)
block|{
try|try
block|{
name|OrcFile
operator|.
name|createReader
argument_list|(
name|file
operator|.
name|getPath
argument_list|()
argument_list|,
name|OrcFile
operator|.
name|readerOptions
argument_list|(
name|conf
argument_list|)
operator|.
name|filesystem
argument_list|(
name|fs
argument_list|)
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
return|return
literal|false
return|;
block|}
block|}
return|return
literal|true
return|;
block|}
comment|/**    * Get the list of input {@link Path}s for the map-reduce job.    *    * @param conf The configuration of the job    * @return the list of input {@link Path}s for the map-reduce job.    */
specifier|static
name|Path
index|[]
name|getInputPaths
parameter_list|(
name|Configuration
name|conf
parameter_list|)
throws|throws
name|IOException
block|{
name|String
name|dirs
init|=
name|conf
operator|.
name|get
argument_list|(
literal|"mapred.input.dir"
argument_list|)
decl_stmt|;
if|if
condition|(
name|dirs
operator|==
literal|null
condition|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Configuration mapred.input.dir is not defined."
argument_list|)
throw|;
block|}
name|String
index|[]
name|list
init|=
name|StringUtils
operator|.
name|split
argument_list|(
name|dirs
argument_list|)
decl_stmt|;
name|Path
index|[]
name|result
init|=
operator|new
name|Path
index|[
name|list
operator|.
name|length
index|]
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|list
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|result
index|[
name|i
index|]
operator|=
operator|new
name|Path
argument_list|(
name|StringUtils
operator|.
name|unEscapeString
argument_list|(
name|list
index|[
name|i
index|]
argument_list|)
argument_list|)
expr_stmt|;
block|}
return|return
name|result
return|;
block|}
comment|/**    * The global information about the split generation that we pass around to    * the different worker threads.    */
specifier|static
class|class
name|Context
block|{
specifier|private
specifier|final
name|Configuration
name|conf
decl_stmt|;
comment|// We store all caches in variables to change the main one based on config.
comment|// This is not thread safe between different split generations (and wasn't anyway).
specifier|private
name|FooterCache
name|footerCache
decl_stmt|;
specifier|private
specifier|static
name|LocalCache
name|localCache
decl_stmt|;
specifier|private
specifier|static
name|MetastoreCache
name|metaCache
decl_stmt|;
specifier|private
specifier|static
name|ExecutorService
name|threadPool
init|=
literal|null
decl_stmt|;
specifier|private
specifier|final
name|int
name|numBuckets
decl_stmt|;
specifier|private
specifier|final
name|long
name|maxSize
decl_stmt|;
specifier|private
specifier|final
name|long
name|minSize
decl_stmt|;
specifier|private
specifier|final
name|int
name|minSplits
decl_stmt|;
specifier|private
specifier|final
name|boolean
name|footerInSplits
decl_stmt|;
specifier|private
specifier|final
name|boolean
name|cacheStripeDetails
decl_stmt|;
specifier|private
specifier|final
name|AtomicInteger
name|cacheHitCounter
init|=
operator|new
name|AtomicInteger
argument_list|(
literal|0
argument_list|)
decl_stmt|;
specifier|private
specifier|final
name|AtomicInteger
name|numFilesCounter
init|=
operator|new
name|AtomicInteger
argument_list|(
literal|0
argument_list|)
decl_stmt|;
specifier|private
name|ValidTxnList
name|transactionList
decl_stmt|;
specifier|private
name|SplitStrategyKind
name|splitStrategyKind
decl_stmt|;
name|Context
parameter_list|(
name|Configuration
name|conf
parameter_list|)
block|{
name|this
argument_list|(
name|conf
argument_list|,
literal|1
argument_list|)
expr_stmt|;
block|}
name|Context
parameter_list|(
name|Configuration
name|conf
parameter_list|,
specifier|final
name|int
name|minSplits
parameter_list|)
block|{
name|this
operator|.
name|conf
operator|=
name|conf
expr_stmt|;
name|minSize
operator|=
name|conf
operator|.
name|getLong
argument_list|(
name|MIN_SPLIT_SIZE
argument_list|,
name|DEFAULT_MIN_SPLIT_SIZE
argument_list|)
expr_stmt|;
name|maxSize
operator|=
name|conf
operator|.
name|getLong
argument_list|(
name|MAX_SPLIT_SIZE
argument_list|,
name|DEFAULT_MAX_SPLIT_SIZE
argument_list|)
expr_stmt|;
name|String
name|ss
init|=
name|conf
operator|.
name|get
argument_list|(
name|ConfVars
operator|.
name|HIVE_ORC_SPLIT_STRATEGY
operator|.
name|varname
argument_list|)
decl_stmt|;
if|if
condition|(
name|ss
operator|==
literal|null
operator|||
name|ss
operator|.
name|equals
argument_list|(
name|SplitStrategyKind
operator|.
name|HYBRID
operator|.
name|name
argument_list|()
argument_list|)
condition|)
block|{
name|splitStrategyKind
operator|=
name|SplitStrategyKind
operator|.
name|HYBRID
expr_stmt|;
block|}
else|else
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Enforcing "
operator|+
name|ss
operator|+
literal|" ORC split strategy"
argument_list|)
expr_stmt|;
name|splitStrategyKind
operator|=
name|SplitStrategyKind
operator|.
name|valueOf
argument_list|(
name|ss
argument_list|)
expr_stmt|;
block|}
name|footerInSplits
operator|=
name|HiveConf
operator|.
name|getBoolVar
argument_list|(
name|conf
argument_list|,
name|ConfVars
operator|.
name|HIVE_ORC_INCLUDE_FILE_FOOTER_IN_SPLITS
argument_list|)
expr_stmt|;
name|numBuckets
operator|=
name|Math
operator|.
name|max
argument_list|(
name|conf
operator|.
name|getInt
argument_list|(
name|hive_metastoreConstants
operator|.
name|BUCKET_COUNT
argument_list|,
literal|0
argument_list|)
argument_list|,
literal|0
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Number of buckets specified by conf file is "
operator|+
name|numBuckets
argument_list|)
expr_stmt|;
name|int
name|cacheStripeDetailsSize
init|=
name|HiveConf
operator|.
name|getIntVar
argument_list|(
name|conf
argument_list|,
name|ConfVars
operator|.
name|HIVE_ORC_CACHE_STRIPE_DETAILS_SIZE
argument_list|)
decl_stmt|;
name|int
name|numThreads
init|=
name|HiveConf
operator|.
name|getIntVar
argument_list|(
name|conf
argument_list|,
name|ConfVars
operator|.
name|HIVE_ORC_COMPUTE_SPLITS_NUM_THREADS
argument_list|)
decl_stmt|;
name|cacheStripeDetails
operator|=
operator|(
name|cacheStripeDetailsSize
operator|>
literal|0
operator|)
expr_stmt|;
name|this
operator|.
name|minSplits
operator|=
name|Math
operator|.
name|min
argument_list|(
name|cacheStripeDetailsSize
argument_list|,
name|minSplits
argument_list|)
expr_stmt|;
synchronized|synchronized
init|(
name|Context
operator|.
name|class
init|)
block|{
if|if
condition|(
name|threadPool
operator|==
literal|null
condition|)
block|{
name|threadPool
operator|=
name|Executors
operator|.
name|newFixedThreadPool
argument_list|(
name|numThreads
argument_list|,
operator|new
name|ThreadFactoryBuilder
argument_list|()
operator|.
name|setDaemon
argument_list|(
literal|true
argument_list|)
operator|.
name|setNameFormat
argument_list|(
literal|"ORC_GET_SPLITS #%d"
argument_list|)
operator|.
name|build
argument_list|()
argument_list|)
expr_stmt|;
block|}
comment|// TODO: local cache is created once, so the configs for future queries will not be honored.
if|if
condition|(
name|cacheStripeDetails
condition|)
block|{
comment|// Note that there's no FS check here; we implicitly only use metastore cache for
comment|// HDFS, because only HDFS would return fileIds for us. If fileId is extended using
comment|// size/mod time/etc. for other FSes, we might need to check FSes explicitly because
comment|// using such an aggregate fileId cache is not bulletproof and should be disable-able.
name|boolean
name|useMetastoreCache
init|=
name|HiveConf
operator|.
name|getBoolVar
argument_list|(
name|conf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_ORC_MS_FOOTER_CACHE_ENABLED
argument_list|)
decl_stmt|;
if|if
condition|(
name|localCache
operator|==
literal|null
condition|)
block|{
name|localCache
operator|=
operator|new
name|LocalCache
argument_list|(
name|numThreads
argument_list|,
name|cacheStripeDetailsSize
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|useMetastoreCache
condition|)
block|{
if|if
condition|(
name|metaCache
operator|==
literal|null
condition|)
block|{
name|metaCache
operator|=
operator|new
name|MetastoreCache
argument_list|(
name|localCache
argument_list|)
expr_stmt|;
block|}
assert|assert
name|conf
operator|instanceof
name|HiveConf
assert|;
name|metaCache
operator|.
name|configure
argument_list|(
operator|(
name|HiveConf
operator|)
name|conf
argument_list|)
expr_stmt|;
block|}
comment|// Set footer cache for current split generation. See field comment - not thread safe.
name|footerCache
operator|=
name|useMetastoreCache
condition|?
name|metaCache
else|:
name|localCache
expr_stmt|;
block|}
block|}
name|String
name|value
init|=
name|conf
operator|.
name|get
argument_list|(
name|ValidTxnList
operator|.
name|VALID_TXNS_KEY
argument_list|,
name|Long
operator|.
name|MAX_VALUE
operator|+
literal|":"
argument_list|)
decl_stmt|;
name|transactionList
operator|=
operator|new
name|ValidReadTxnList
argument_list|(
name|value
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * The full ACID directory information needed for splits; no more calls to HDFS needed.    * We could just live with AcidUtils.Directory but...    * 1) That doesn't have base files for the base-directory case.    * 2) We save fs for convenience to avoid getting it twice.    */
annotation|@
name|VisibleForTesting
specifier|static
specifier|final
class|class
name|AcidDirInfo
block|{
specifier|public
name|AcidDirInfo
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|Path
name|splitPath
parameter_list|,
name|Directory
name|acidInfo
parameter_list|,
name|List
argument_list|<
name|HdfsFileStatusWithId
argument_list|>
name|baseOrOriginalFiles
parameter_list|)
block|{
name|this
operator|.
name|splitPath
operator|=
name|splitPath
expr_stmt|;
name|this
operator|.
name|acidInfo
operator|=
name|acidInfo
expr_stmt|;
name|this
operator|.
name|baseOrOriginalFiles
operator|=
name|baseOrOriginalFiles
expr_stmt|;
name|this
operator|.
name|fs
operator|=
name|fs
expr_stmt|;
block|}
specifier|final
name|FileSystem
name|fs
decl_stmt|;
specifier|final
name|Path
name|splitPath
decl_stmt|;
specifier|final
name|AcidUtils
operator|.
name|Directory
name|acidInfo
decl_stmt|;
specifier|final
name|List
argument_list|<
name|HdfsFileStatusWithId
argument_list|>
name|baseOrOriginalFiles
decl_stmt|;
block|}
annotation|@
name|VisibleForTesting
interface|interface
name|SplitStrategy
parameter_list|<
name|T
parameter_list|>
block|{
name|List
argument_list|<
name|T
argument_list|>
name|getSplits
parameter_list|()
throws|throws
name|IOException
function_decl|;
block|}
annotation|@
name|VisibleForTesting
specifier|static
specifier|final
class|class
name|SplitInfo
extends|extends
name|ACIDSplitStrategy
block|{
specifier|private
specifier|final
name|Context
name|context
decl_stmt|;
specifier|private
specifier|final
name|FileSystem
name|fs
decl_stmt|;
specifier|private
specifier|final
name|HdfsFileStatusWithId
name|fileWithId
decl_stmt|;
specifier|private
specifier|final
name|FileInfo
name|fileInfo
decl_stmt|;
specifier|private
specifier|final
name|boolean
name|isOriginal
decl_stmt|;
specifier|private
specifier|final
name|List
argument_list|<
name|DeltaMetaData
argument_list|>
name|deltas
decl_stmt|;
specifier|private
specifier|final
name|boolean
name|hasBase
decl_stmt|;
name|SplitInfo
parameter_list|(
name|Context
name|context
parameter_list|,
name|FileSystem
name|fs
parameter_list|,
name|HdfsFileStatusWithId
name|fileWithId
parameter_list|,
name|FileInfo
name|fileInfo
parameter_list|,
name|boolean
name|isOriginal
parameter_list|,
name|List
argument_list|<
name|DeltaMetaData
argument_list|>
name|deltas
parameter_list|,
name|boolean
name|hasBase
parameter_list|,
name|Path
name|dir
parameter_list|,
name|boolean
index|[]
name|covered
parameter_list|)
throws|throws
name|IOException
block|{
name|super
argument_list|(
name|dir
argument_list|,
name|context
operator|.
name|numBuckets
argument_list|,
name|deltas
argument_list|,
name|covered
argument_list|)
expr_stmt|;
name|this
operator|.
name|context
operator|=
name|context
expr_stmt|;
name|this
operator|.
name|fs
operator|=
name|fs
expr_stmt|;
name|this
operator|.
name|fileWithId
operator|=
name|fileWithId
expr_stmt|;
name|this
operator|.
name|fileInfo
operator|=
name|fileInfo
expr_stmt|;
name|this
operator|.
name|isOriginal
operator|=
name|isOriginal
expr_stmt|;
name|this
operator|.
name|deltas
operator|=
name|deltas
expr_stmt|;
name|this
operator|.
name|hasBase
operator|=
name|hasBase
expr_stmt|;
block|}
annotation|@
name|VisibleForTesting
specifier|public
name|SplitInfo
parameter_list|(
name|Context
name|context
parameter_list|,
name|FileSystem
name|fs
parameter_list|,
name|FileStatus
name|fileStatus
parameter_list|,
name|FileInfo
name|fileInfo
parameter_list|,
name|boolean
name|isOriginal
parameter_list|,
name|ArrayList
argument_list|<
name|DeltaMetaData
argument_list|>
name|deltas
parameter_list|,
name|boolean
name|hasBase
parameter_list|,
name|Path
name|dir
parameter_list|,
name|boolean
index|[]
name|covered
parameter_list|)
throws|throws
name|IOException
block|{
name|this
argument_list|(
name|context
argument_list|,
name|fs
argument_list|,
name|AcidUtils
operator|.
name|createOriginalObj
argument_list|(
literal|null
argument_list|,
name|fileStatus
argument_list|)
argument_list|,
name|fileInfo
argument_list|,
name|isOriginal
argument_list|,
name|deltas
argument_list|,
name|hasBase
argument_list|,
name|dir
argument_list|,
name|covered
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * ETL strategy is used when spending little more time in split generation is acceptable    * (split generation reads and caches file footers).    */
specifier|static
specifier|final
class|class
name|ETLSplitStrategy
implements|implements
name|SplitStrategy
argument_list|<
name|SplitInfo
argument_list|>
implements|,
name|Callable
argument_list|<
name|Void
argument_list|>
block|{
name|Context
name|context
decl_stmt|;
name|FileSystem
name|fs
decl_stmt|;
name|List
argument_list|<
name|HdfsFileStatusWithId
argument_list|>
name|files
decl_stmt|;
name|boolean
name|isOriginal
decl_stmt|;
name|List
argument_list|<
name|DeltaMetaData
argument_list|>
name|deltas
decl_stmt|;
name|Path
name|dir
decl_stmt|;
name|boolean
index|[]
name|covered
decl_stmt|;
specifier|private
name|List
argument_list|<
name|Future
argument_list|<
name|List
argument_list|<
name|OrcSplit
argument_list|>
argument_list|>
argument_list|>
name|splitFuturesRef
decl_stmt|;
specifier|public
name|ETLSplitStrategy
parameter_list|(
name|Context
name|context
parameter_list|,
name|FileSystem
name|fs
parameter_list|,
name|Path
name|dir
parameter_list|,
name|List
argument_list|<
name|HdfsFileStatusWithId
argument_list|>
name|children
parameter_list|,
name|boolean
name|isOriginal
parameter_list|,
name|List
argument_list|<
name|DeltaMetaData
argument_list|>
name|deltas
parameter_list|,
name|boolean
index|[]
name|covered
parameter_list|)
block|{
name|this
operator|.
name|context
operator|=
name|context
expr_stmt|;
name|this
operator|.
name|dir
operator|=
name|dir
expr_stmt|;
name|this
operator|.
name|fs
operator|=
name|fs
expr_stmt|;
name|this
operator|.
name|files
operator|=
name|children
expr_stmt|;
name|this
operator|.
name|isOriginal
operator|=
name|isOriginal
expr_stmt|;
name|this
operator|.
name|deltas
operator|=
name|deltas
expr_stmt|;
name|this
operator|.
name|covered
operator|=
name|covered
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|List
argument_list|<
name|SplitInfo
argument_list|>
name|getSplits
parameter_list|()
throws|throws
name|IOException
block|{
name|List
argument_list|<
name|SplitInfo
argument_list|>
name|result
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|(
name|files
operator|.
name|size
argument_list|()
argument_list|)
decl_stmt|;
comment|// TODO: Right now, we do the metastore call here, so there will be a metastore call per
comment|//       partition. If we had a sync point after getting file lists, we could make just one
comment|//       call; this might be too much sync for many partitions and also cause issues with the
comment|//       huge metastore call result that cannot be handled with in-API batching. To have an
comment|//       optimal number of metastore calls, we should wait for batch-size number of files (a
comment|//       few hundreds) to become available, then call metastore.
comment|// Force local cache if we have deltas.
name|FooterCache
name|cache
init|=
name|context
operator|.
name|cacheStripeDetails
condition|?
operator|(
name|deltas
operator|==
literal|null
condition|?
name|context
operator|.
name|footerCache
else|:
name|Context
operator|.
name|localCache
operator|)
else|:
literal|null
decl_stmt|;
if|if
condition|(
name|cache
operator|!=
literal|null
condition|)
block|{
name|FileInfo
index|[]
name|infos
init|=
name|cache
operator|.
name|getAndValidate
argument_list|(
name|files
argument_list|)
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|files
operator|.
name|size
argument_list|()
condition|;
operator|++
name|i
control|)
block|{
name|FileInfo
name|info
init|=
name|infos
index|[
name|i
index|]
decl_stmt|;
if|if
condition|(
name|info
operator|!=
literal|null
condition|)
block|{
comment|// Cached copy is valid
name|context
operator|.
name|cacheHitCounter
operator|.
name|incrementAndGet
argument_list|()
expr_stmt|;
block|}
name|HdfsFileStatusWithId
name|file
init|=
name|files
operator|.
name|get
argument_list|(
name|i
argument_list|)
decl_stmt|;
comment|// ignore files of 0 length
if|if
condition|(
name|file
operator|.
name|getFileStatus
argument_list|()
operator|.
name|getLen
argument_list|()
operator|>
literal|0
condition|)
block|{
name|result
operator|.
name|add
argument_list|(
operator|new
name|SplitInfo
argument_list|(
name|context
argument_list|,
name|fs
argument_list|,
name|file
argument_list|,
name|info
argument_list|,
name|isOriginal
argument_list|,
name|deltas
argument_list|,
literal|true
argument_list|,
name|dir
argument_list|,
name|covered
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
block|}
else|else
block|{
for|for
control|(
name|HdfsFileStatusWithId
name|file
range|:
name|files
control|)
block|{
comment|// ignore files of 0 length
if|if
condition|(
name|file
operator|.
name|getFileStatus
argument_list|()
operator|.
name|getLen
argument_list|()
operator|>
literal|0
condition|)
block|{
name|result
operator|.
name|add
argument_list|(
operator|new
name|SplitInfo
argument_list|(
name|context
argument_list|,
name|fs
argument_list|,
name|file
argument_list|,
literal|null
argument_list|,
name|isOriginal
argument_list|,
name|deltas
argument_list|,
literal|true
argument_list|,
name|dir
argument_list|,
name|covered
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
block|}
return|return
name|result
return|;
block|}
annotation|@
name|Override
specifier|public
name|String
name|toString
parameter_list|()
block|{
return|return
name|ETLSplitStrategy
operator|.
name|class
operator|.
name|getSimpleName
argument_list|()
operator|+
literal|" strategy for "
operator|+
name|dir
return|;
block|}
specifier|public
name|Future
argument_list|<
name|Void
argument_list|>
name|generateSplitWork
parameter_list|(
name|Context
name|context
parameter_list|,
name|List
argument_list|<
name|Future
argument_list|<
name|List
argument_list|<
name|OrcSplit
argument_list|>
argument_list|>
argument_list|>
name|splitFutures
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|context
operator|.
name|cacheStripeDetails
operator|&&
name|context
operator|.
name|footerCache
operator|.
name|isBlocking
argument_list|()
condition|)
block|{
name|this
operator|.
name|splitFuturesRef
operator|=
name|splitFutures
expr_stmt|;
return|return
name|Context
operator|.
name|threadPool
operator|.
name|submit
argument_list|(
name|this
argument_list|)
return|;
block|}
else|else
block|{
name|runGetSplitsSync
argument_list|(
name|splitFutures
argument_list|)
expr_stmt|;
return|return
literal|null
return|;
block|}
block|}
annotation|@
name|Override
specifier|public
name|Void
name|call
parameter_list|()
throws|throws
name|IOException
block|{
name|runGetSplitsSync
argument_list|(
name|splitFuturesRef
argument_list|)
expr_stmt|;
return|return
literal|null
return|;
block|}
specifier|private
name|void
name|runGetSplitsSync
parameter_list|(
name|List
argument_list|<
name|Future
argument_list|<
name|List
argument_list|<
name|OrcSplit
argument_list|>
argument_list|>
argument_list|>
name|splitFutures
parameter_list|)
throws|throws
name|IOException
block|{
name|List
argument_list|<
name|SplitInfo
argument_list|>
name|splits
init|=
name|getSplits
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|Future
argument_list|<
name|List
argument_list|<
name|OrcSplit
argument_list|>
argument_list|>
argument_list|>
name|localList
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|(
name|splits
operator|.
name|size
argument_list|()
argument_list|)
decl_stmt|;
for|for
control|(
name|SplitInfo
name|splitInfo
range|:
name|splits
control|)
block|{
name|localList
operator|.
name|add
argument_list|(
name|Context
operator|.
name|threadPool
operator|.
name|submit
argument_list|(
operator|new
name|SplitGenerator
argument_list|(
name|splitInfo
argument_list|)
argument_list|)
argument_list|)
expr_stmt|;
block|}
synchronized|synchronized
init|(
name|splitFutures
init|)
block|{
name|splitFutures
operator|.
name|addAll
argument_list|(
name|localList
argument_list|)
expr_stmt|;
block|}
block|}
block|}
comment|/**    * BI strategy is used when the requirement is to spend less time in split generation    * as opposed to query execution (split generation does not read or cache file footers).    */
specifier|static
specifier|final
class|class
name|BISplitStrategy
extends|extends
name|ACIDSplitStrategy
block|{
name|List
argument_list|<
name|HdfsFileStatusWithId
argument_list|>
name|fileStatuses
decl_stmt|;
name|boolean
name|isOriginal
decl_stmt|;
name|List
argument_list|<
name|DeltaMetaData
argument_list|>
name|deltas
decl_stmt|;
name|FileSystem
name|fs
decl_stmt|;
name|Context
name|context
decl_stmt|;
name|Path
name|dir
decl_stmt|;
specifier|public
name|BISplitStrategy
parameter_list|(
name|Context
name|context
parameter_list|,
name|FileSystem
name|fs
parameter_list|,
name|Path
name|dir
parameter_list|,
name|List
argument_list|<
name|HdfsFileStatusWithId
argument_list|>
name|fileStatuses
parameter_list|,
name|boolean
name|isOriginal
parameter_list|,
name|List
argument_list|<
name|DeltaMetaData
argument_list|>
name|deltas
parameter_list|,
name|boolean
index|[]
name|covered
parameter_list|)
block|{
name|super
argument_list|(
name|dir
argument_list|,
name|context
operator|.
name|numBuckets
argument_list|,
name|deltas
argument_list|,
name|covered
argument_list|)
expr_stmt|;
name|this
operator|.
name|context
operator|=
name|context
expr_stmt|;
name|this
operator|.
name|fileStatuses
operator|=
name|fileStatuses
expr_stmt|;
name|this
operator|.
name|isOriginal
operator|=
name|isOriginal
expr_stmt|;
name|this
operator|.
name|deltas
operator|=
name|deltas
expr_stmt|;
name|this
operator|.
name|fs
operator|=
name|fs
expr_stmt|;
name|this
operator|.
name|dir
operator|=
name|dir
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|List
argument_list|<
name|OrcSplit
argument_list|>
name|getSplits
parameter_list|()
throws|throws
name|IOException
block|{
name|List
argument_list|<
name|OrcSplit
argument_list|>
name|splits
init|=
name|Lists
operator|.
name|newArrayList
argument_list|()
decl_stmt|;
for|for
control|(
name|HdfsFileStatusWithId
name|file
range|:
name|fileStatuses
control|)
block|{
name|FileStatus
name|fileStatus
init|=
name|file
operator|.
name|getFileStatus
argument_list|()
decl_stmt|;
name|String
index|[]
name|hosts
init|=
name|SHIMS
operator|.
name|getLocationsWithOffset
argument_list|(
name|fs
argument_list|,
name|fileStatus
argument_list|)
operator|.
name|firstEntry
argument_list|()
operator|.
name|getValue
argument_list|()
operator|.
name|getHosts
argument_list|()
decl_stmt|;
name|OrcSplit
name|orcSplit
init|=
operator|new
name|OrcSplit
argument_list|(
name|fileStatus
operator|.
name|getPath
argument_list|()
argument_list|,
name|file
operator|.
name|getFileId
argument_list|()
argument_list|,
literal|0
argument_list|,
name|fileStatus
operator|.
name|getLen
argument_list|()
argument_list|,
name|hosts
argument_list|,
literal|null
argument_list|,
name|isOriginal
argument_list|,
literal|true
argument_list|,
name|deltas
argument_list|,
operator|-
literal|1
argument_list|)
decl_stmt|;
name|splits
operator|.
name|add
argument_list|(
name|orcSplit
argument_list|)
expr_stmt|;
block|}
comment|// add uncovered ACID delta splits
name|splits
operator|.
name|addAll
argument_list|(
name|super
operator|.
name|getSplits
argument_list|()
argument_list|)
expr_stmt|;
return|return
name|splits
return|;
block|}
annotation|@
name|Override
specifier|public
name|String
name|toString
parameter_list|()
block|{
return|return
name|BISplitStrategy
operator|.
name|class
operator|.
name|getSimpleName
argument_list|()
operator|+
literal|" strategy for "
operator|+
name|dir
return|;
block|}
block|}
comment|/**    * ACID split strategy is used when there is no base directory (when transactions are enabled).    */
specifier|static
class|class
name|ACIDSplitStrategy
implements|implements
name|SplitStrategy
argument_list|<
name|OrcSplit
argument_list|>
block|{
name|Path
name|dir
decl_stmt|;
name|List
argument_list|<
name|DeltaMetaData
argument_list|>
name|deltas
decl_stmt|;
name|boolean
index|[]
name|covered
decl_stmt|;
name|int
name|numBuckets
decl_stmt|;
specifier|public
name|ACIDSplitStrategy
parameter_list|(
name|Path
name|dir
parameter_list|,
name|int
name|numBuckets
parameter_list|,
name|List
argument_list|<
name|DeltaMetaData
argument_list|>
name|deltas
parameter_list|,
name|boolean
index|[]
name|covered
parameter_list|)
block|{
name|this
operator|.
name|dir
operator|=
name|dir
expr_stmt|;
name|this
operator|.
name|numBuckets
operator|=
name|numBuckets
expr_stmt|;
name|this
operator|.
name|deltas
operator|=
name|deltas
expr_stmt|;
name|this
operator|.
name|covered
operator|=
name|covered
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|List
argument_list|<
name|OrcSplit
argument_list|>
name|getSplits
parameter_list|()
throws|throws
name|IOException
block|{
comment|// Generate a split for any buckets that weren't covered.
comment|// This happens in the case where a bucket just has deltas and no
comment|// base.
name|List
argument_list|<
name|OrcSplit
argument_list|>
name|splits
init|=
name|Lists
operator|.
name|newArrayList
argument_list|()
decl_stmt|;
if|if
condition|(
operator|!
name|deltas
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
for|for
control|(
name|int
name|b
init|=
literal|0
init|;
name|b
operator|<
name|numBuckets
condition|;
operator|++
name|b
control|)
block|{
if|if
condition|(
operator|!
name|covered
index|[
name|b
index|]
condition|)
block|{
name|splits
operator|.
name|add
argument_list|(
operator|new
name|OrcSplit
argument_list|(
name|dir
argument_list|,
literal|null
argument_list|,
name|b
argument_list|,
literal|0
argument_list|,
operator|new
name|String
index|[
literal|0
index|]
argument_list|,
literal|null
argument_list|,
literal|false
argument_list|,
literal|false
argument_list|,
name|deltas
argument_list|,
operator|-
literal|1
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
block|}
return|return
name|splits
return|;
block|}
annotation|@
name|Override
specifier|public
name|String
name|toString
parameter_list|()
block|{
return|return
name|ACIDSplitStrategy
operator|.
name|class
operator|.
name|getSimpleName
argument_list|()
operator|+
literal|" strategy for "
operator|+
name|dir
return|;
block|}
block|}
comment|/**    * Given a directory, get the list of files and blocks in those files.    * To parallelize file generator use "mapreduce.input.fileinputformat.list-status.num-threads"    */
specifier|static
specifier|final
class|class
name|FileGenerator
implements|implements
name|Callable
argument_list|<
name|AcidDirInfo
argument_list|>
block|{
specifier|private
specifier|final
name|Context
name|context
decl_stmt|;
specifier|private
specifier|final
name|FileSystem
name|fs
decl_stmt|;
specifier|private
specifier|final
name|Path
name|dir
decl_stmt|;
specifier|private
specifier|final
name|boolean
name|useFileIds
decl_stmt|;
name|FileGenerator
parameter_list|(
name|Context
name|context
parameter_list|,
name|FileSystem
name|fs
parameter_list|,
name|Path
name|dir
parameter_list|,
name|boolean
name|useFileIds
parameter_list|)
block|{
name|this
operator|.
name|context
operator|=
name|context
expr_stmt|;
name|this
operator|.
name|fs
operator|=
name|fs
expr_stmt|;
name|this
operator|.
name|dir
operator|=
name|dir
expr_stmt|;
name|this
operator|.
name|useFileIds
operator|=
name|useFileIds
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|AcidDirInfo
name|call
parameter_list|()
throws|throws
name|IOException
block|{
name|AcidUtils
operator|.
name|Directory
name|dirInfo
init|=
name|AcidUtils
operator|.
name|getAcidState
argument_list|(
name|dir
argument_list|,
name|context
operator|.
name|conf
argument_list|,
name|context
operator|.
name|transactionList
argument_list|,
name|useFileIds
argument_list|)
decl_stmt|;
name|Path
name|base
init|=
name|dirInfo
operator|.
name|getBaseDirectory
argument_list|()
decl_stmt|;
comment|// find the base files (original or new style)
name|List
argument_list|<
name|HdfsFileStatusWithId
argument_list|>
name|children
init|=
operator|(
name|base
operator|==
literal|null
operator|)
condition|?
name|dirInfo
operator|.
name|getOriginalFiles
argument_list|()
else|:
name|findBaseFiles
argument_list|(
name|base
argument_list|,
name|useFileIds
argument_list|)
decl_stmt|;
return|return
operator|new
name|AcidDirInfo
argument_list|(
name|fs
argument_list|,
name|dir
argument_list|,
name|dirInfo
argument_list|,
name|children
argument_list|)
return|;
block|}
specifier|private
name|List
argument_list|<
name|HdfsFileStatusWithId
argument_list|>
name|findBaseFiles
parameter_list|(
name|Path
name|base
parameter_list|,
name|boolean
name|useFileIds
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|useFileIds
condition|)
block|{
try|try
block|{
return|return
name|SHIMS
operator|.
name|listLocatedHdfsStatus
argument_list|(
name|fs
argument_list|,
name|base
argument_list|,
name|AcidUtils
operator|.
name|hiddenFileFilter
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|Throwable
name|t
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Failed to get files with ID; using regular API"
argument_list|,
name|t
argument_list|)
expr_stmt|;
block|}
block|}
comment|// Fall back to regular API and create states without ID.
name|List
argument_list|<
name|FileStatus
argument_list|>
name|children
init|=
name|SHIMS
operator|.
name|listLocatedStatus
argument_list|(
name|fs
argument_list|,
name|base
argument_list|,
name|AcidUtils
operator|.
name|hiddenFileFilter
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|HdfsFileStatusWithId
argument_list|>
name|result
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|(
name|children
operator|.
name|size
argument_list|()
argument_list|)
decl_stmt|;
for|for
control|(
name|FileStatus
name|child
range|:
name|children
control|)
block|{
name|result
operator|.
name|add
argument_list|(
name|AcidUtils
operator|.
name|createOriginalObj
argument_list|(
literal|null
argument_list|,
name|child
argument_list|)
argument_list|)
expr_stmt|;
block|}
return|return
name|result
return|;
block|}
block|}
comment|/**    * Split the stripes of a given file into input splits.    * A thread is used for each file.    */
specifier|static
specifier|final
class|class
name|SplitGenerator
implements|implements
name|Callable
argument_list|<
name|List
argument_list|<
name|OrcSplit
argument_list|>
argument_list|>
block|{
specifier|private
specifier|final
name|Context
name|context
decl_stmt|;
specifier|private
specifier|final
name|FileSystem
name|fs
decl_stmt|;
specifier|private
specifier|final
name|HdfsFileStatusWithId
name|fileWithId
decl_stmt|;
specifier|private
specifier|final
name|FileStatus
name|file
decl_stmt|;
specifier|private
specifier|final
name|long
name|blockSize
decl_stmt|;
specifier|private
specifier|final
name|TreeMap
argument_list|<
name|Long
argument_list|,
name|BlockLocation
argument_list|>
name|locations
decl_stmt|;
specifier|private
specifier|final
name|FileInfo
name|fileInfo
decl_stmt|;
specifier|private
name|List
argument_list|<
name|StripeInformation
argument_list|>
name|stripes
decl_stmt|;
specifier|private
name|FileMetaInfo
name|fileMetaInfo
decl_stmt|;
specifier|private
name|List
argument_list|<
name|StripeStatistics
argument_list|>
name|stripeStats
decl_stmt|;
specifier|private
name|List
argument_list|<
name|OrcProto
operator|.
name|Type
argument_list|>
name|types
decl_stmt|;
specifier|private
name|boolean
index|[]
name|includedCols
decl_stmt|;
specifier|private
specifier|final
name|boolean
name|isOriginal
decl_stmt|;
specifier|private
specifier|final
name|List
argument_list|<
name|DeltaMetaData
argument_list|>
name|deltas
decl_stmt|;
specifier|private
specifier|final
name|boolean
name|hasBase
decl_stmt|;
specifier|private
name|OrcFile
operator|.
name|WriterVersion
name|writerVersion
decl_stmt|;
specifier|private
name|long
name|projColsUncompressedSize
decl_stmt|;
specifier|private
name|List
argument_list|<
name|OrcSplit
argument_list|>
name|deltaSplits
decl_stmt|;
specifier|public
name|SplitGenerator
parameter_list|(
name|SplitInfo
name|splitInfo
parameter_list|)
throws|throws
name|IOException
block|{
name|this
operator|.
name|context
operator|=
name|splitInfo
operator|.
name|context
expr_stmt|;
name|this
operator|.
name|fs
operator|=
name|splitInfo
operator|.
name|fs
expr_stmt|;
name|this
operator|.
name|fileWithId
operator|=
name|splitInfo
operator|.
name|fileWithId
expr_stmt|;
name|this
operator|.
name|file
operator|=
name|this
operator|.
name|fileWithId
operator|.
name|getFileStatus
argument_list|()
expr_stmt|;
name|this
operator|.
name|blockSize
operator|=
name|this
operator|.
name|file
operator|.
name|getBlockSize
argument_list|()
expr_stmt|;
name|this
operator|.
name|fileInfo
operator|=
name|splitInfo
operator|.
name|fileInfo
expr_stmt|;
comment|// TODO: potential DFS call
name|this
operator|.
name|locations
operator|=
name|SHIMS
operator|.
name|getLocationsWithOffset
argument_list|(
name|fs
argument_list|,
name|fileWithId
operator|.
name|getFileStatus
argument_list|()
argument_list|)
expr_stmt|;
name|this
operator|.
name|isOriginal
operator|=
name|splitInfo
operator|.
name|isOriginal
expr_stmt|;
name|this
operator|.
name|deltas
operator|=
name|splitInfo
operator|.
name|deltas
expr_stmt|;
name|this
operator|.
name|hasBase
operator|=
name|splitInfo
operator|.
name|hasBase
expr_stmt|;
name|this
operator|.
name|projColsUncompressedSize
operator|=
operator|-
literal|1
expr_stmt|;
name|this
operator|.
name|deltaSplits
operator|=
name|splitInfo
operator|.
name|getSplits
argument_list|()
expr_stmt|;
block|}
name|Path
name|getPath
parameter_list|()
block|{
return|return
name|fileWithId
operator|.
name|getFileStatus
argument_list|()
operator|.
name|getPath
argument_list|()
return|;
block|}
annotation|@
name|Override
specifier|public
name|String
name|toString
parameter_list|()
block|{
return|return
literal|"splitter("
operator|+
name|fileWithId
operator|.
name|getFileStatus
argument_list|()
operator|.
name|getPath
argument_list|()
operator|+
literal|")"
return|;
block|}
comment|/**      * Compute the number of bytes that overlap between the two ranges.      * @param offset1 start of range1      * @param length1 length of range1      * @param offset2 start of range2      * @param length2 length of range2      * @return the number of bytes in the overlap range      */
specifier|static
name|long
name|getOverlap
parameter_list|(
name|long
name|offset1
parameter_list|,
name|long
name|length1
parameter_list|,
name|long
name|offset2
parameter_list|,
name|long
name|length2
parameter_list|)
block|{
name|long
name|end1
init|=
name|offset1
operator|+
name|length1
decl_stmt|;
name|long
name|end2
init|=
name|offset2
operator|+
name|length2
decl_stmt|;
if|if
condition|(
name|end2
operator|<=
name|offset1
operator|||
name|end1
operator|<=
name|offset2
condition|)
block|{
return|return
literal|0
return|;
block|}
else|else
block|{
return|return
name|Math
operator|.
name|min
argument_list|(
name|end1
argument_list|,
name|end2
argument_list|)
operator|-
name|Math
operator|.
name|max
argument_list|(
name|offset1
argument_list|,
name|offset2
argument_list|)
return|;
block|}
block|}
comment|/**      * Create an input split over the given range of bytes. The location of the      * split is based on where the majority of the byte are coming from. ORC      * files are unlikely to have splits that cross between blocks because they      * are written with large block sizes.      * @param offset the start of the split      * @param length the length of the split      * @param fileMetaInfo file metadata from footer and postscript      * @throws IOException      */
name|OrcSplit
name|createSplit
parameter_list|(
name|long
name|offset
parameter_list|,
name|long
name|length
parameter_list|,
name|FileMetaInfo
name|fileMetaInfo
parameter_list|)
throws|throws
name|IOException
block|{
name|String
index|[]
name|hosts
decl_stmt|;
name|Map
operator|.
name|Entry
argument_list|<
name|Long
argument_list|,
name|BlockLocation
argument_list|>
name|startEntry
init|=
name|locations
operator|.
name|floorEntry
argument_list|(
name|offset
argument_list|)
decl_stmt|;
name|BlockLocation
name|start
init|=
name|startEntry
operator|.
name|getValue
argument_list|()
decl_stmt|;
if|if
condition|(
name|offset
operator|+
name|length
operator|<=
name|start
operator|.
name|getOffset
argument_list|()
operator|+
name|start
operator|.
name|getLength
argument_list|()
condition|)
block|{
comment|// handle the single block case
name|hosts
operator|=
name|start
operator|.
name|getHosts
argument_list|()
expr_stmt|;
block|}
else|else
block|{
name|Map
operator|.
name|Entry
argument_list|<
name|Long
argument_list|,
name|BlockLocation
argument_list|>
name|endEntry
init|=
name|locations
operator|.
name|floorEntry
argument_list|(
name|offset
operator|+
name|length
argument_list|)
decl_stmt|;
name|BlockLocation
name|end
init|=
name|endEntry
operator|.
name|getValue
argument_list|()
decl_stmt|;
comment|//get the submap
name|NavigableMap
argument_list|<
name|Long
argument_list|,
name|BlockLocation
argument_list|>
name|navigableMap
init|=
name|locations
operator|.
name|subMap
argument_list|(
name|startEntry
operator|.
name|getKey
argument_list|()
argument_list|,
literal|true
argument_list|,
name|endEntry
operator|.
name|getKey
argument_list|()
argument_list|,
literal|true
argument_list|)
decl_stmt|;
comment|// Calculate the number of bytes in the split that are local to each
comment|// host.
name|Map
argument_list|<
name|String
argument_list|,
name|LongWritable
argument_list|>
name|sizes
init|=
operator|new
name|HashMap
argument_list|<
name|String
argument_list|,
name|LongWritable
argument_list|>
argument_list|()
decl_stmt|;
name|long
name|maxSize
init|=
literal|0
decl_stmt|;
for|for
control|(
name|BlockLocation
name|block
range|:
name|navigableMap
operator|.
name|values
argument_list|()
control|)
block|{
name|long
name|overlap
init|=
name|getOverlap
argument_list|(
name|offset
argument_list|,
name|length
argument_list|,
name|block
operator|.
name|getOffset
argument_list|()
argument_list|,
name|block
operator|.
name|getLength
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|overlap
operator|>
literal|0
condition|)
block|{
for|for
control|(
name|String
name|host
range|:
name|block
operator|.
name|getHosts
argument_list|()
control|)
block|{
name|LongWritable
name|val
init|=
name|sizes
operator|.
name|get
argument_list|(
name|host
argument_list|)
decl_stmt|;
if|if
condition|(
name|val
operator|==
literal|null
condition|)
block|{
name|val
operator|=
operator|new
name|LongWritable
argument_list|()
expr_stmt|;
name|sizes
operator|.
name|put
argument_list|(
name|host
argument_list|,
name|val
argument_list|)
expr_stmt|;
block|}
name|val
operator|.
name|set
argument_list|(
name|val
operator|.
name|get
argument_list|()
operator|+
name|overlap
argument_list|)
expr_stmt|;
name|maxSize
operator|=
name|Math
operator|.
name|max
argument_list|(
name|maxSize
argument_list|,
name|val
operator|.
name|get
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"File "
operator|+
name|fileWithId
operator|.
name|getFileStatus
argument_list|()
operator|.
name|getPath
argument_list|()
operator|.
name|toString
argument_list|()
operator|+
literal|" should have had overlap on block starting at "
operator|+
name|block
operator|.
name|getOffset
argument_list|()
argument_list|)
throw|;
block|}
block|}
comment|// filter the list of locations to those that have at least 80% of the
comment|// max
name|long
name|threshold
init|=
call|(
name|long
call|)
argument_list|(
name|maxSize
operator|*
name|MIN_INCLUDED_LOCATION
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|hostList
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
comment|// build the locations in a predictable order to simplify testing
for|for
control|(
name|BlockLocation
name|block
range|:
name|navigableMap
operator|.
name|values
argument_list|()
control|)
block|{
for|for
control|(
name|String
name|host
range|:
name|block
operator|.
name|getHosts
argument_list|()
control|)
block|{
if|if
condition|(
name|sizes
operator|.
name|containsKey
argument_list|(
name|host
argument_list|)
condition|)
block|{
if|if
condition|(
name|sizes
operator|.
name|get
argument_list|(
name|host
argument_list|)
operator|.
name|get
argument_list|()
operator|>=
name|threshold
condition|)
block|{
name|hostList
operator|.
name|add
argument_list|(
name|host
argument_list|)
expr_stmt|;
block|}
name|sizes
operator|.
name|remove
argument_list|(
name|host
argument_list|)
expr_stmt|;
block|}
block|}
block|}
name|hosts
operator|=
operator|new
name|String
index|[
name|hostList
operator|.
name|size
argument_list|()
index|]
expr_stmt|;
name|hostList
operator|.
name|toArray
argument_list|(
name|hosts
argument_list|)
expr_stmt|;
block|}
comment|// scale the raw data size to split level based on ratio of split wrt to file length
specifier|final
name|long
name|fileLen
init|=
name|file
operator|.
name|getLen
argument_list|()
decl_stmt|;
specifier|final
name|double
name|splitRatio
init|=
operator|(
name|double
operator|)
name|length
operator|/
operator|(
name|double
operator|)
name|fileLen
decl_stmt|;
specifier|final
name|long
name|scaledProjSize
init|=
name|projColsUncompressedSize
operator|>
literal|0
condition|?
call|(
name|long
call|)
argument_list|(
name|splitRatio
operator|*
name|projColsUncompressedSize
argument_list|)
else|:
name|fileLen
decl_stmt|;
return|return
operator|new
name|OrcSplit
argument_list|(
name|file
operator|.
name|getPath
argument_list|()
argument_list|,
name|fileWithId
operator|.
name|getFileId
argument_list|()
argument_list|,
name|offset
argument_list|,
name|length
argument_list|,
name|hosts
argument_list|,
name|fileMetaInfo
argument_list|,
name|isOriginal
argument_list|,
name|hasBase
argument_list|,
name|deltas
argument_list|,
name|scaledProjSize
argument_list|)
return|;
block|}
comment|/**      * Divide the adjacent stripes in the file into input splits based on the      * block size and the configured minimum and maximum sizes.      */
annotation|@
name|Override
specifier|public
name|List
argument_list|<
name|OrcSplit
argument_list|>
name|call
parameter_list|()
throws|throws
name|IOException
block|{
name|populateAndCacheStripeDetails
argument_list|()
expr_stmt|;
name|List
argument_list|<
name|OrcSplit
argument_list|>
name|splits
init|=
name|Lists
operator|.
name|newArrayList
argument_list|()
decl_stmt|;
comment|// figure out which stripes we need to read
name|boolean
index|[]
name|includeStripe
init|=
literal|null
decl_stmt|;
comment|// we can't eliminate stripes if there are deltas because the
comment|// deltas may change the rows making them match the predicate.
if|if
condition|(
name|deltas
operator|.
name|isEmpty
argument_list|()
operator|&&
name|canCreateSargFromConf
argument_list|(
name|context
operator|.
name|conf
argument_list|)
condition|)
block|{
name|SearchArgument
name|sarg
init|=
name|ConvertAstToSearchArg
operator|.
name|createFromConf
argument_list|(
name|context
operator|.
name|conf
argument_list|)
decl_stmt|;
name|String
index|[]
name|sargColNames
init|=
name|extractNeededColNames
argument_list|(
name|types
argument_list|,
name|context
operator|.
name|conf
argument_list|,
name|includedCols
argument_list|,
name|isOriginal
argument_list|)
decl_stmt|;
name|includeStripe
operator|=
name|pickStripes
argument_list|(
name|sarg
argument_list|,
name|sargColNames
argument_list|,
name|writerVersion
argument_list|,
name|isOriginal
argument_list|,
name|stripeStats
argument_list|,
name|stripes
operator|.
name|size
argument_list|()
argument_list|,
name|file
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
block|}
comment|// if we didn't have predicate pushdown, read everything
if|if
condition|(
name|includeStripe
operator|==
literal|null
condition|)
block|{
name|includeStripe
operator|=
operator|new
name|boolean
index|[
name|stripes
operator|.
name|size
argument_list|()
index|]
expr_stmt|;
name|Arrays
operator|.
name|fill
argument_list|(
name|includeStripe
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
name|long
name|currentOffset
init|=
operator|-
literal|1
decl_stmt|;
name|long
name|currentLength
init|=
literal|0
decl_stmt|;
name|int
name|idx
init|=
operator|-
literal|1
decl_stmt|;
for|for
control|(
name|StripeInformation
name|stripe
range|:
name|stripes
control|)
block|{
name|idx
operator|++
expr_stmt|;
if|if
condition|(
operator|!
name|includeStripe
index|[
name|idx
index|]
condition|)
block|{
comment|// create split for the previous unfinished stripe
if|if
condition|(
name|currentOffset
operator|!=
operator|-
literal|1
condition|)
block|{
name|splits
operator|.
name|add
argument_list|(
name|createSplit
argument_list|(
name|currentOffset
argument_list|,
name|currentLength
argument_list|,
name|fileMetaInfo
argument_list|)
argument_list|)
expr_stmt|;
name|currentOffset
operator|=
operator|-
literal|1
expr_stmt|;
block|}
continue|continue;
block|}
comment|// if we are working on a stripe, over the min stripe size, and
comment|// crossed a block boundary, cut the input split here.
if|if
condition|(
name|currentOffset
operator|!=
operator|-
literal|1
operator|&&
name|currentLength
operator|>
name|context
operator|.
name|minSize
operator|&&
operator|(
name|currentOffset
operator|/
name|blockSize
operator|!=
name|stripe
operator|.
name|getOffset
argument_list|()
operator|/
name|blockSize
operator|)
condition|)
block|{
name|splits
operator|.
name|add
argument_list|(
name|createSplit
argument_list|(
name|currentOffset
argument_list|,
name|currentLength
argument_list|,
name|fileMetaInfo
argument_list|)
argument_list|)
expr_stmt|;
name|currentOffset
operator|=
operator|-
literal|1
expr_stmt|;
block|}
comment|// if we aren't building a split, start a new one.
if|if
condition|(
name|currentOffset
operator|==
operator|-
literal|1
condition|)
block|{
name|currentOffset
operator|=
name|stripe
operator|.
name|getOffset
argument_list|()
expr_stmt|;
name|currentLength
operator|=
name|stripe
operator|.
name|getLength
argument_list|()
expr_stmt|;
block|}
else|else
block|{
name|currentLength
operator|=
operator|(
name|stripe
operator|.
name|getOffset
argument_list|()
operator|+
name|stripe
operator|.
name|getLength
argument_list|()
operator|)
operator|-
name|currentOffset
expr_stmt|;
block|}
if|if
condition|(
name|currentLength
operator|>=
name|context
operator|.
name|maxSize
condition|)
block|{
name|splits
operator|.
name|add
argument_list|(
name|createSplit
argument_list|(
name|currentOffset
argument_list|,
name|currentLength
argument_list|,
name|fileMetaInfo
argument_list|)
argument_list|)
expr_stmt|;
name|currentOffset
operator|=
operator|-
literal|1
expr_stmt|;
block|}
block|}
if|if
condition|(
name|currentOffset
operator|!=
operator|-
literal|1
condition|)
block|{
name|splits
operator|.
name|add
argument_list|(
name|createSplit
argument_list|(
name|currentOffset
argument_list|,
name|currentLength
argument_list|,
name|fileMetaInfo
argument_list|)
argument_list|)
expr_stmt|;
block|}
comment|// add uncovered ACID delta splits
name|splits
operator|.
name|addAll
argument_list|(
name|deltaSplits
argument_list|)
expr_stmt|;
return|return
name|splits
return|;
block|}
specifier|private
name|void
name|populateAndCacheStripeDetails
parameter_list|()
throws|throws
name|IOException
block|{
comment|// Only create OrcReader if we are missing some information.
name|List
argument_list|<
name|OrcProto
operator|.
name|ColumnStatistics
argument_list|>
name|colStatsLocal
decl_stmt|;
name|List
argument_list|<
name|OrcProto
operator|.
name|Type
argument_list|>
name|typesLocal
decl_stmt|;
if|if
condition|(
name|fileInfo
operator|!=
literal|null
condition|)
block|{
name|stripes
operator|=
name|fileInfo
operator|.
name|stripeInfos
expr_stmt|;
name|stripeStats
operator|=
name|fileInfo
operator|.
name|stripeStats
expr_stmt|;
name|fileMetaInfo
operator|=
name|fileInfo
operator|.
name|fileMetaInfo
expr_stmt|;
name|typesLocal
operator|=
name|types
operator|=
name|fileInfo
operator|.
name|types
expr_stmt|;
name|colStatsLocal
operator|=
name|fileInfo
operator|.
name|fileStats
expr_stmt|;
name|writerVersion
operator|=
name|fileInfo
operator|.
name|writerVersion
expr_stmt|;
comment|// For multiple runs, in case sendSplitsInFooter changes
if|if
condition|(
name|fileMetaInfo
operator|==
literal|null
operator|&&
name|context
operator|.
name|footerInSplits
condition|)
block|{
name|Reader
name|orcReader
init|=
name|createOrcReader
argument_list|()
decl_stmt|;
name|fileInfo
operator|.
name|fileMetaInfo
operator|=
operator|(
operator|(
name|ReaderImpl
operator|)
name|orcReader
operator|)
operator|.
name|getFileMetaInfo
argument_list|()
expr_stmt|;
assert|assert
name|fileInfo
operator|.
name|stripeStats
operator|!=
literal|null
operator|&&
name|fileInfo
operator|.
name|types
operator|!=
literal|null
operator|&&
name|fileInfo
operator|.
name|writerVersion
operator|!=
literal|null
assert|;
comment|// We assume that if we needed to create a reader, we need to cache it to meta cache.
comment|// TODO: This will also needlessly overwrite it in local cache for now.
name|context
operator|.
name|footerCache
operator|.
name|put
argument_list|(
name|fileWithId
operator|.
name|getFileId
argument_list|()
argument_list|,
name|file
argument_list|,
name|fileInfo
operator|.
name|fileMetaInfo
argument_list|,
name|orcReader
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
name|Reader
name|orcReader
init|=
name|createOrcReader
argument_list|()
decl_stmt|;
name|stripes
operator|=
name|orcReader
operator|.
name|getStripes
argument_list|()
expr_stmt|;
name|typesLocal
operator|=
name|types
operator|=
name|orcReader
operator|.
name|getTypes
argument_list|()
expr_stmt|;
name|colStatsLocal
operator|=
name|orcReader
operator|.
name|getOrcProtoFileStatistics
argument_list|()
expr_stmt|;
name|writerVersion
operator|=
name|orcReader
operator|.
name|getWriterVersion
argument_list|()
expr_stmt|;
name|stripeStats
operator|=
name|orcReader
operator|.
name|getStripeStatistics
argument_list|()
expr_stmt|;
name|fileMetaInfo
operator|=
name|context
operator|.
name|footerInSplits
condition|?
operator|(
operator|(
name|ReaderImpl
operator|)
name|orcReader
operator|)
operator|.
name|getFileMetaInfo
argument_list|()
else|:
literal|null
expr_stmt|;
if|if
condition|(
name|context
operator|.
name|cacheStripeDetails
condition|)
block|{
name|Long
name|fileId
init|=
name|fileWithId
operator|.
name|getFileId
argument_list|()
decl_stmt|;
name|context
operator|.
name|footerCache
operator|.
name|put
argument_list|(
name|fileId
argument_list|,
name|file
argument_list|,
name|fileMetaInfo
argument_list|,
name|orcReader
argument_list|)
expr_stmt|;
block|}
block|}
name|includedCols
operator|=
name|genIncludedColumns
argument_list|(
name|types
argument_list|,
name|context
operator|.
name|conf
argument_list|,
name|isOriginal
argument_list|)
expr_stmt|;
name|projColsUncompressedSize
operator|=
name|computeProjectionSize
argument_list|(
name|typesLocal
argument_list|,
name|colStatsLocal
argument_list|,
name|includedCols
argument_list|,
name|isOriginal
argument_list|)
expr_stmt|;
block|}
specifier|private
name|Reader
name|createOrcReader
parameter_list|()
throws|throws
name|IOException
block|{
return|return
name|OrcFile
operator|.
name|createReader
argument_list|(
name|file
operator|.
name|getPath
argument_list|()
argument_list|,
name|OrcFile
operator|.
name|readerOptions
argument_list|(
name|context
operator|.
name|conf
argument_list|)
operator|.
name|filesystem
argument_list|(
name|fs
argument_list|)
argument_list|)
return|;
block|}
specifier|private
name|long
name|computeProjectionSize
parameter_list|(
name|List
argument_list|<
name|OrcProto
operator|.
name|Type
argument_list|>
name|types
parameter_list|,
name|List
argument_list|<
name|OrcProto
operator|.
name|ColumnStatistics
argument_list|>
name|stats
parameter_list|,
name|boolean
index|[]
name|includedCols
parameter_list|,
name|boolean
name|isOriginal
parameter_list|)
block|{
specifier|final
name|int
name|rootIdx
init|=
name|getRootColumn
argument_list|(
name|isOriginal
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|Integer
argument_list|>
name|internalColIds
init|=
name|Lists
operator|.
name|newArrayList
argument_list|()
decl_stmt|;
if|if
condition|(
name|includedCols
operator|!=
literal|null
condition|)
block|{
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|includedCols
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
if|if
condition|(
name|includedCols
index|[
name|i
index|]
condition|)
block|{
name|internalColIds
operator|.
name|add
argument_list|(
name|rootIdx
operator|+
name|i
argument_list|)
expr_stmt|;
block|}
block|}
block|}
return|return
name|ReaderImpl
operator|.
name|getRawDataSizeFromColIndices
argument_list|(
name|internalColIds
argument_list|,
name|types
argument_list|,
name|stats
argument_list|)
return|;
block|}
block|}
specifier|static
name|List
argument_list|<
name|OrcSplit
argument_list|>
name|generateSplitsInfo
parameter_list|(
name|Configuration
name|conf
parameter_list|)
throws|throws
name|IOException
block|{
return|return
name|generateSplitsInfo
argument_list|(
name|conf
argument_list|,
operator|-
literal|1
argument_list|)
return|;
block|}
specifier|static
name|List
argument_list|<
name|OrcSplit
argument_list|>
name|generateSplitsInfo
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|int
name|numSplits
parameter_list|)
throws|throws
name|IOException
block|{
comment|// Use threads to resolve directories into splits.
if|if
condition|(
name|HiveConf
operator|.
name|getBoolVar
argument_list|(
name|conf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_ORC_MS_FOOTER_CACHE_ENABLED
argument_list|)
condition|)
block|{
comment|// Create HiveConf once, since this is expensive.
name|conf
operator|=
operator|new
name|HiveConf
argument_list|(
name|conf
argument_list|,
name|OrcInputFormat
operator|.
name|class
argument_list|)
expr_stmt|;
block|}
name|Context
name|context
init|=
operator|new
name|Context
argument_list|(
name|conf
argument_list|,
name|numSplits
argument_list|)
decl_stmt|;
name|boolean
name|useFileIds
init|=
name|HiveConf
operator|.
name|getBoolVar
argument_list|(
name|conf
argument_list|,
name|ConfVars
operator|.
name|HIVE_ORC_INCLUDE_FILE_ID_IN_SPLITS
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|OrcSplit
argument_list|>
name|splits
init|=
name|Lists
operator|.
name|newArrayList
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|Future
argument_list|<
name|AcidDirInfo
argument_list|>
argument_list|>
name|pathFutures
init|=
name|Lists
operator|.
name|newArrayList
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|Future
argument_list|<
name|Void
argument_list|>
argument_list|>
name|strategyFutures
init|=
name|Lists
operator|.
name|newArrayList
argument_list|()
decl_stmt|;
specifier|final
name|List
argument_list|<
name|Future
argument_list|<
name|List
argument_list|<
name|OrcSplit
argument_list|>
argument_list|>
argument_list|>
name|splitFutures
init|=
name|Lists
operator|.
name|newArrayList
argument_list|()
decl_stmt|;
comment|// multi-threaded file statuses and split strategy
name|Path
index|[]
name|paths
init|=
name|getInputPaths
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|CompletionService
argument_list|<
name|AcidDirInfo
argument_list|>
name|ecs
init|=
operator|new
name|ExecutorCompletionService
argument_list|<>
argument_list|(
name|Context
operator|.
name|threadPool
argument_list|)
decl_stmt|;
for|for
control|(
name|Path
name|dir
range|:
name|paths
control|)
block|{
name|FileSystem
name|fs
init|=
name|dir
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|FileGenerator
name|fileGenerator
init|=
operator|new
name|FileGenerator
argument_list|(
name|context
argument_list|,
name|fs
argument_list|,
name|dir
argument_list|,
name|useFileIds
argument_list|)
decl_stmt|;
name|pathFutures
operator|.
name|add
argument_list|(
name|ecs
operator|.
name|submit
argument_list|(
name|fileGenerator
argument_list|)
argument_list|)
expr_stmt|;
block|}
comment|// complete path futures and schedule split generation
try|try
block|{
for|for
control|(
name|int
name|notIndex
init|=
literal|0
init|;
name|notIndex
operator|<
name|paths
operator|.
name|length
condition|;
operator|++
name|notIndex
control|)
block|{
name|AcidDirInfo
name|adi
init|=
name|ecs
operator|.
name|take
argument_list|()
operator|.
name|get
argument_list|()
decl_stmt|;
name|SplitStrategy
argument_list|<
name|?
argument_list|>
name|splitStrategy
init|=
name|determineSplitStrategy
argument_list|(
name|context
argument_list|,
name|adi
operator|.
name|fs
argument_list|,
name|adi
operator|.
name|splitPath
argument_list|,
name|adi
operator|.
name|acidInfo
argument_list|,
name|adi
operator|.
name|baseOrOriginalFiles
argument_list|)
decl_stmt|;
if|if
condition|(
name|isDebugEnabled
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
name|splitStrategy
argument_list|)
expr_stmt|;
block|}
comment|// Hack note - different split strategies return differently typed lists, yay Java.
comment|// This works purely by magic, because we know which strategy produces which type.
if|if
condition|(
name|splitStrategy
operator|instanceof
name|ETLSplitStrategy
condition|)
block|{
name|Future
argument_list|<
name|Void
argument_list|>
name|ssFuture
init|=
operator|(
operator|(
name|ETLSplitStrategy
operator|)
name|splitStrategy
operator|)
operator|.
name|generateSplitWork
argument_list|(
name|context
argument_list|,
name|splitFutures
argument_list|)
decl_stmt|;
if|if
condition|(
name|ssFuture
operator|!=
literal|null
condition|)
block|{
name|strategyFutures
operator|.
name|add
argument_list|(
name|ssFuture
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
annotation|@
name|SuppressWarnings
argument_list|(
literal|"unchecked"
argument_list|)
name|List
argument_list|<
name|OrcSplit
argument_list|>
name|readySplits
init|=
operator|(
name|List
argument_list|<
name|OrcSplit
argument_list|>
operator|)
name|splitStrategy
operator|.
name|getSplits
argument_list|()
decl_stmt|;
name|splits
operator|.
name|addAll
argument_list|(
name|readySplits
argument_list|)
expr_stmt|;
block|}
block|}
comment|// complete split futures
for|for
control|(
name|Future
argument_list|<
name|Void
argument_list|>
name|ssFuture
range|:
name|strategyFutures
control|)
block|{
name|ssFuture
operator|.
name|get
argument_list|()
expr_stmt|;
comment|// Make sure we get exceptions strategies might have thrown.
block|}
comment|// All the split strategies are done, so it must be safe to access splitFutures.
for|for
control|(
name|Future
argument_list|<
name|List
argument_list|<
name|OrcSplit
argument_list|>
argument_list|>
name|splitFuture
range|:
name|splitFutures
control|)
block|{
name|splits
operator|.
name|addAll
argument_list|(
name|splitFuture
operator|.
name|get
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|cancelFutures
argument_list|(
name|pathFutures
argument_list|)
expr_stmt|;
name|cancelFutures
argument_list|(
name|strategyFutures
argument_list|)
expr_stmt|;
name|cancelFutures
argument_list|(
name|splitFutures
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|RuntimeException
argument_list|(
literal|"ORC split generation failed with exception: "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
if|if
condition|(
name|context
operator|.
name|cacheStripeDetails
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"FooterCacheHitRatio: "
operator|+
name|context
operator|.
name|cacheHitCounter
operator|.
name|get
argument_list|()
operator|+
literal|"/"
operator|+
name|context
operator|.
name|numFilesCounter
operator|.
name|get
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|isDebugEnabled
condition|)
block|{
for|for
control|(
name|OrcSplit
name|split
range|:
name|splits
control|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
name|split
operator|+
literal|" projected_columns_uncompressed_size: "
operator|+
name|split
operator|.
name|getColumnarProjectionSize
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|splits
return|;
block|}
specifier|private
specifier|static
parameter_list|<
name|T
parameter_list|>
name|void
name|cancelFutures
parameter_list|(
name|List
argument_list|<
name|Future
argument_list|<
name|T
argument_list|>
argument_list|>
name|futures
parameter_list|)
block|{
for|for
control|(
name|Future
argument_list|<
name|T
argument_list|>
name|future
range|:
name|futures
control|)
block|{
name|future
operator|.
name|cancel
argument_list|(
literal|true
argument_list|)
expr_stmt|;
block|}
block|}
annotation|@
name|Override
specifier|public
name|InputSplit
index|[]
name|getSplits
parameter_list|(
name|JobConf
name|job
parameter_list|,
name|int
name|numSplits
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|isDebugEnabled
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"getSplits started"
argument_list|)
expr_stmt|;
block|}
name|List
argument_list|<
name|OrcSplit
argument_list|>
name|result
init|=
name|generateSplitsInfo
argument_list|(
name|job
argument_list|,
name|numSplits
argument_list|)
decl_stmt|;
if|if
condition|(
name|isDebugEnabled
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"getSplits finished"
argument_list|)
expr_stmt|;
block|}
return|return
name|result
operator|.
name|toArray
argument_list|(
operator|new
name|InputSplit
index|[
name|result
operator|.
name|size
argument_list|()
index|]
argument_list|)
return|;
block|}
comment|/**    * FileInfo.    *    * Stores information relevant to split generation for an ORC File.    *    */
specifier|private
specifier|static
class|class
name|FileInfo
block|{
specifier|private
specifier|final
name|long
name|modificationTime
decl_stmt|;
specifier|private
specifier|final
name|long
name|size
decl_stmt|;
specifier|private
specifier|final
name|Long
name|fileId
decl_stmt|;
specifier|private
specifier|final
name|List
argument_list|<
name|StripeInformation
argument_list|>
name|stripeInfos
decl_stmt|;
specifier|private
name|FileMetaInfo
name|fileMetaInfo
decl_stmt|;
specifier|private
name|List
argument_list|<
name|StripeStatistics
argument_list|>
name|stripeStats
decl_stmt|;
specifier|private
name|List
argument_list|<
name|OrcProto
operator|.
name|ColumnStatistics
argument_list|>
name|fileStats
decl_stmt|;
specifier|private
name|List
argument_list|<
name|OrcProto
operator|.
name|Type
argument_list|>
name|types
decl_stmt|;
specifier|private
name|OrcFile
operator|.
name|WriterVersion
name|writerVersion
decl_stmt|;
name|FileInfo
parameter_list|(
name|long
name|modificationTime
parameter_list|,
name|long
name|size
parameter_list|,
name|List
argument_list|<
name|StripeInformation
argument_list|>
name|stripeInfos
parameter_list|,
name|List
argument_list|<
name|StripeStatistics
argument_list|>
name|stripeStats
parameter_list|,
name|List
argument_list|<
name|OrcProto
operator|.
name|Type
argument_list|>
name|types
parameter_list|,
name|List
argument_list|<
name|OrcProto
operator|.
name|ColumnStatistics
argument_list|>
name|fileStats
parameter_list|,
name|FileMetaInfo
name|fileMetaInfo
parameter_list|,
name|OrcFile
operator|.
name|WriterVersion
name|writerVersion
parameter_list|,
name|Long
name|fileId
parameter_list|)
block|{
name|this
operator|.
name|modificationTime
operator|=
name|modificationTime
expr_stmt|;
name|this
operator|.
name|size
operator|=
name|size
expr_stmt|;
name|this
operator|.
name|fileId
operator|=
name|fileId
expr_stmt|;
name|this
operator|.
name|stripeInfos
operator|=
name|stripeInfos
expr_stmt|;
name|this
operator|.
name|fileMetaInfo
operator|=
name|fileMetaInfo
expr_stmt|;
name|this
operator|.
name|stripeStats
operator|=
name|stripeStats
expr_stmt|;
name|this
operator|.
name|types
operator|=
name|types
expr_stmt|;
name|this
operator|.
name|fileStats
operator|=
name|fileStats
expr_stmt|;
name|this
operator|.
name|writerVersion
operator|=
name|writerVersion
expr_stmt|;
block|}
block|}
annotation|@
name|SuppressWarnings
argument_list|(
literal|"unchecked"
argument_list|)
specifier|private
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|RecordReader
argument_list|<
name|NullWritable
argument_list|,
name|OrcStruct
argument_list|>
name|createVectorizedReader
parameter_list|(
name|InputSplit
name|split
parameter_list|,
name|JobConf
name|conf
parameter_list|,
name|Reporter
name|reporter
parameter_list|)
throws|throws
name|IOException
block|{
return|return
operator|(
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|RecordReader
operator|)
operator|new
name|VectorizedOrcInputFormat
argument_list|()
operator|.
name|getRecordReader
argument_list|(
name|split
argument_list|,
name|conf
argument_list|,
name|reporter
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|public
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|RecordReader
argument_list|<
name|NullWritable
argument_list|,
name|OrcStruct
argument_list|>
name|getRecordReader
parameter_list|(
name|InputSplit
name|inputSplit
parameter_list|,
name|JobConf
name|conf
parameter_list|,
name|Reporter
name|reporter
parameter_list|)
throws|throws
name|IOException
block|{
name|boolean
name|vectorMode
init|=
name|Utilities
operator|.
name|isVectorMode
argument_list|(
name|conf
argument_list|)
decl_stmt|;
comment|// if HiveCombineInputFormat gives us FileSplits instead of OrcSplits,
comment|// we know it is not ACID. (see a check in CombineHiveInputFormat.getSplits() that assures this)
if|if
condition|(
name|inputSplit
operator|.
name|getClass
argument_list|()
operator|==
name|FileSplit
operator|.
name|class
condition|)
block|{
if|if
condition|(
name|vectorMode
condition|)
block|{
return|return
name|createVectorizedReader
argument_list|(
name|inputSplit
argument_list|,
name|conf
argument_list|,
name|reporter
argument_list|)
return|;
block|}
return|return
operator|new
name|OrcRecordReader
argument_list|(
name|OrcFile
operator|.
name|createReader
argument_list|(
operator|(
operator|(
name|FileSplit
operator|)
name|inputSplit
operator|)
operator|.
name|getPath
argument_list|()
argument_list|,
name|OrcFile
operator|.
name|readerOptions
argument_list|(
name|conf
argument_list|)
argument_list|)
argument_list|,
name|conf
argument_list|,
operator|(
name|FileSplit
operator|)
name|inputSplit
argument_list|)
return|;
block|}
name|OrcSplit
name|split
init|=
operator|(
name|OrcSplit
operator|)
name|inputSplit
decl_stmt|;
name|reporter
operator|.
name|setStatus
argument_list|(
name|inputSplit
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
name|Options
name|options
init|=
operator|new
name|Options
argument_list|(
name|conf
argument_list|)
operator|.
name|reporter
argument_list|(
name|reporter
argument_list|)
decl_stmt|;
specifier|final
name|RowReader
argument_list|<
name|OrcStruct
argument_list|>
name|inner
init|=
name|getReader
argument_list|(
name|inputSplit
argument_list|,
name|options
argument_list|)
decl_stmt|;
comment|/*Even though there are no delta files, we still need to produce row ids so that an     * UPDATE or DELETE statement would work on a table which didn't have any previous updates*/
if|if
condition|(
name|split
operator|.
name|isOriginal
argument_list|()
operator|&&
name|split
operator|.
name|getDeltas
argument_list|()
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
if|if
condition|(
name|vectorMode
condition|)
block|{
return|return
name|createVectorizedReader
argument_list|(
name|inputSplit
argument_list|,
name|conf
argument_list|,
name|reporter
argument_list|)
return|;
block|}
else|else
block|{
return|return
operator|new
name|NullKeyRecordReader
argument_list|(
name|inner
argument_list|,
name|conf
argument_list|)
return|;
block|}
block|}
if|if
condition|(
name|vectorMode
condition|)
block|{
return|return
operator|(
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|RecordReader
operator|)
operator|new
name|VectorizedOrcAcidRowReader
argument_list|(
name|inner
argument_list|,
name|conf
argument_list|,
operator|(
name|FileSplit
operator|)
name|inputSplit
argument_list|)
return|;
block|}
return|return
operator|new
name|NullKeyRecordReader
argument_list|(
name|inner
argument_list|,
name|conf
argument_list|)
return|;
block|}
comment|/**    * Return a RecordReader that is compatible with the Hive 0.12 reader    * with NullWritable for the key instead of RecordIdentifier.    */
specifier|public
specifier|static
specifier|final
class|class
name|NullKeyRecordReader
implements|implements
name|AcidRecordReader
argument_list|<
name|NullWritable
argument_list|,
name|OrcStruct
argument_list|>
block|{
specifier|private
specifier|final
name|RecordIdentifier
name|id
decl_stmt|;
specifier|private
specifier|final
name|RowReader
argument_list|<
name|OrcStruct
argument_list|>
name|inner
decl_stmt|;
specifier|public
name|RecordIdentifier
name|getRecordIdentifier
parameter_list|()
block|{
return|return
name|id
return|;
block|}
specifier|private
name|NullKeyRecordReader
parameter_list|(
name|RowReader
argument_list|<
name|OrcStruct
argument_list|>
name|inner
parameter_list|,
name|Configuration
name|conf
parameter_list|)
block|{
name|this
operator|.
name|inner
operator|=
name|inner
expr_stmt|;
name|id
operator|=
name|inner
operator|.
name|createKey
argument_list|()
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|next
parameter_list|(
name|NullWritable
name|nullWritable
parameter_list|,
name|OrcStruct
name|orcStruct
parameter_list|)
throws|throws
name|IOException
block|{
return|return
name|inner
operator|.
name|next
argument_list|(
name|id
argument_list|,
name|orcStruct
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|public
name|NullWritable
name|createKey
parameter_list|()
block|{
return|return
name|NullWritable
operator|.
name|get
argument_list|()
return|;
block|}
annotation|@
name|Override
specifier|public
name|OrcStruct
name|createValue
parameter_list|()
block|{
return|return
name|inner
operator|.
name|createValue
argument_list|()
return|;
block|}
annotation|@
name|Override
specifier|public
name|long
name|getPos
parameter_list|()
throws|throws
name|IOException
block|{
return|return
name|inner
operator|.
name|getPos
argument_list|()
return|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|close
parameter_list|()
throws|throws
name|IOException
block|{
name|inner
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|float
name|getProgress
parameter_list|()
throws|throws
name|IOException
block|{
return|return
name|inner
operator|.
name|getProgress
argument_list|()
return|;
block|}
block|}
annotation|@
name|Override
specifier|public
name|RowReader
argument_list|<
name|OrcStruct
argument_list|>
name|getReader
parameter_list|(
name|InputSplit
name|inputSplit
parameter_list|,
name|Options
name|options
parameter_list|)
throws|throws
name|IOException
block|{
specifier|final
name|OrcSplit
name|split
init|=
operator|(
name|OrcSplit
operator|)
name|inputSplit
decl_stmt|;
specifier|final
name|Path
name|path
init|=
name|split
operator|.
name|getPath
argument_list|()
decl_stmt|;
name|Path
name|root
decl_stmt|;
if|if
condition|(
name|split
operator|.
name|hasBase
argument_list|()
condition|)
block|{
if|if
condition|(
name|split
operator|.
name|isOriginal
argument_list|()
condition|)
block|{
name|root
operator|=
name|path
operator|.
name|getParent
argument_list|()
expr_stmt|;
block|}
else|else
block|{
name|root
operator|=
name|path
operator|.
name|getParent
argument_list|()
operator|.
name|getParent
argument_list|()
expr_stmt|;
block|}
block|}
else|else
block|{
name|root
operator|=
name|path
expr_stmt|;
block|}
specifier|final
name|Path
index|[]
name|deltas
init|=
name|AcidUtils
operator|.
name|deserializeDeltas
argument_list|(
name|root
argument_list|,
name|split
operator|.
name|getDeltas
argument_list|()
argument_list|)
decl_stmt|;
specifier|final
name|Configuration
name|conf
init|=
name|options
operator|.
name|getConfiguration
argument_list|()
decl_stmt|;
specifier|final
name|Reader
name|reader
decl_stmt|;
specifier|final
name|int
name|bucket
decl_stmt|;
name|Reader
operator|.
name|Options
name|readOptions
init|=
operator|new
name|Reader
operator|.
name|Options
argument_list|()
decl_stmt|;
name|readOptions
operator|.
name|range
argument_list|(
name|split
operator|.
name|getStart
argument_list|()
argument_list|,
name|split
operator|.
name|getLength
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|split
operator|.
name|hasBase
argument_list|()
condition|)
block|{
name|bucket
operator|=
name|AcidUtils
operator|.
name|parseBaseBucketFilename
argument_list|(
name|split
operator|.
name|getPath
argument_list|()
argument_list|,
name|conf
argument_list|)
operator|.
name|getBucket
argument_list|()
expr_stmt|;
name|reader
operator|=
name|OrcFile
operator|.
name|createReader
argument_list|(
name|path
argument_list|,
name|OrcFile
operator|.
name|readerOptions
argument_list|(
name|conf
argument_list|)
argument_list|)
expr_stmt|;
specifier|final
name|List
argument_list|<
name|OrcProto
operator|.
name|Type
argument_list|>
name|types
init|=
name|reader
operator|.
name|getTypes
argument_list|()
decl_stmt|;
name|readOptions
operator|.
name|include
argument_list|(
name|genIncludedColumns
argument_list|(
name|types
argument_list|,
name|conf
argument_list|,
name|split
operator|.
name|isOriginal
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
name|setSearchArgument
argument_list|(
name|readOptions
argument_list|,
name|types
argument_list|,
name|conf
argument_list|,
name|split
operator|.
name|isOriginal
argument_list|()
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|bucket
operator|=
operator|(
name|int
operator|)
name|split
operator|.
name|getStart
argument_list|()
expr_stmt|;
name|reader
operator|=
literal|null
expr_stmt|;
if|if
condition|(
name|deltas
operator|!=
literal|null
operator|&&
name|deltas
operator|.
name|length
operator|>
literal|0
condition|)
block|{
name|Path
name|bucketPath
init|=
name|AcidUtils
operator|.
name|createBucketFile
argument_list|(
name|deltas
index|[
literal|0
index|]
argument_list|,
name|bucket
argument_list|)
decl_stmt|;
name|OrcFile
operator|.
name|ReaderOptions
name|readerOptions
init|=
name|OrcFile
operator|.
name|readerOptions
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|FileSystem
name|fs
init|=
name|readerOptions
operator|.
name|getFilesystem
argument_list|()
decl_stmt|;
if|if
condition|(
name|fs
operator|==
literal|null
condition|)
block|{
name|fs
operator|=
name|path
operator|.
name|getFileSystem
argument_list|(
name|options
operator|.
name|getConfiguration
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|fs
operator|.
name|exists
argument_list|(
name|bucketPath
argument_list|)
condition|)
block|{
comment|/* w/o schema evolution (which ACID doesn't support yet) all delta         files have the same schema, so choosing the 1st one*/
specifier|final
name|List
argument_list|<
name|OrcProto
operator|.
name|Type
argument_list|>
name|types
init|=
name|OrcFile
operator|.
name|createReader
argument_list|(
name|bucketPath
argument_list|,
name|readerOptions
argument_list|)
operator|.
name|getTypes
argument_list|()
decl_stmt|;
name|readOptions
operator|.
name|include
argument_list|(
name|genIncludedColumns
argument_list|(
name|types
argument_list|,
name|conf
argument_list|,
name|split
operator|.
name|isOriginal
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
name|setSearchArgument
argument_list|(
name|readOptions
argument_list|,
name|types
argument_list|,
name|conf
argument_list|,
name|split
operator|.
name|isOriginal
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
block|}
name|String
name|txnString
init|=
name|conf
operator|.
name|get
argument_list|(
name|ValidTxnList
operator|.
name|VALID_TXNS_KEY
argument_list|,
name|Long
operator|.
name|MAX_VALUE
operator|+
literal|":"
argument_list|)
decl_stmt|;
name|ValidTxnList
name|validTxnList
init|=
operator|new
name|ValidReadTxnList
argument_list|(
name|txnString
argument_list|)
decl_stmt|;
specifier|final
name|OrcRawRecordMerger
name|records
init|=
operator|new
name|OrcRawRecordMerger
argument_list|(
name|conf
argument_list|,
literal|true
argument_list|,
name|reader
argument_list|,
name|split
operator|.
name|isOriginal
argument_list|()
argument_list|,
name|bucket
argument_list|,
name|validTxnList
argument_list|,
name|readOptions
argument_list|,
name|deltas
argument_list|)
decl_stmt|;
return|return
operator|new
name|RowReader
argument_list|<
name|OrcStruct
argument_list|>
argument_list|()
block|{
name|OrcStruct
name|innerRecord
init|=
name|records
operator|.
name|createValue
argument_list|()
decl_stmt|;
annotation|@
name|Override
specifier|public
name|ObjectInspector
name|getObjectInspector
parameter_list|()
block|{
return|return
operator|(
operator|(
name|StructObjectInspector
operator|)
name|records
operator|.
name|getObjectInspector
argument_list|()
operator|)
operator|.
name|getAllStructFieldRefs
argument_list|()
operator|.
name|get
argument_list|(
name|OrcRecordUpdater
operator|.
name|ROW
argument_list|)
operator|.
name|getFieldObjectInspector
argument_list|()
return|;
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|next
parameter_list|(
name|RecordIdentifier
name|recordIdentifier
parameter_list|,
name|OrcStruct
name|orcStruct
parameter_list|)
throws|throws
name|IOException
block|{
name|boolean
name|result
decl_stmt|;
comment|// filter out the deleted records
do|do
block|{
name|result
operator|=
name|records
operator|.
name|next
argument_list|(
name|recordIdentifier
argument_list|,
name|innerRecord
argument_list|)
expr_stmt|;
block|}
do|while
condition|(
name|result
operator|&&
name|OrcRecordUpdater
operator|.
name|getOperation
argument_list|(
name|innerRecord
argument_list|)
operator|==
name|OrcRecordUpdater
operator|.
name|DELETE_OPERATION
condition|)
do|;
if|if
condition|(
name|result
condition|)
block|{
comment|// swap the fields with the passed in orcStruct
name|orcStruct
operator|.
name|linkFields
argument_list|(
name|OrcRecordUpdater
operator|.
name|getRow
argument_list|(
name|innerRecord
argument_list|)
argument_list|)
expr_stmt|;
block|}
return|return
name|result
return|;
block|}
annotation|@
name|Override
specifier|public
name|RecordIdentifier
name|createKey
parameter_list|()
block|{
return|return
name|records
operator|.
name|createKey
argument_list|()
return|;
block|}
annotation|@
name|Override
specifier|public
name|OrcStruct
name|createValue
parameter_list|()
block|{
return|return
operator|new
name|OrcStruct
argument_list|(
name|records
operator|.
name|getColumns
argument_list|()
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|public
name|long
name|getPos
parameter_list|()
throws|throws
name|IOException
block|{
return|return
name|records
operator|.
name|getPos
argument_list|()
return|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|close
parameter_list|()
throws|throws
name|IOException
block|{
name|records
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|float
name|getProgress
parameter_list|()
throws|throws
name|IOException
block|{
return|return
name|records
operator|.
name|getProgress
argument_list|()
return|;
block|}
block|}
return|;
block|}
specifier|static
name|Path
name|findOriginalBucket
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|Path
name|directory
parameter_list|,
name|int
name|bucket
parameter_list|)
throws|throws
name|IOException
block|{
for|for
control|(
name|FileStatus
name|stat
range|:
name|fs
operator|.
name|listStatus
argument_list|(
name|directory
argument_list|)
control|)
block|{
name|String
name|name
init|=
name|stat
operator|.
name|getPath
argument_list|()
operator|.
name|getName
argument_list|()
decl_stmt|;
name|String
name|numberPart
init|=
name|name
operator|.
name|substring
argument_list|(
literal|0
argument_list|,
name|name
operator|.
name|indexOf
argument_list|(
literal|'_'
argument_list|)
argument_list|)
decl_stmt|;
if|if
condition|(
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|lang3
operator|.
name|StringUtils
operator|.
name|isNumeric
argument_list|(
name|numberPart
argument_list|)
operator|&&
name|Integer
operator|.
name|parseInt
argument_list|(
name|numberPart
argument_list|)
operator|==
name|bucket
condition|)
block|{
return|return
name|stat
operator|.
name|getPath
argument_list|()
return|;
block|}
block|}
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
literal|"Can't find bucket "
operator|+
name|bucket
operator|+
literal|" in "
operator|+
name|directory
argument_list|)
throw|;
block|}
specifier|public
specifier|static
name|boolean
index|[]
name|pickStripesViaTranslatedSarg
parameter_list|(
name|SearchArgument
name|sarg
parameter_list|,
name|WriterVersion
name|writerVersion
parameter_list|,
name|List
argument_list|<
name|OrcProto
operator|.
name|Type
argument_list|>
name|types
parameter_list|,
name|List
argument_list|<
name|StripeStatistics
argument_list|>
name|stripeStats
parameter_list|,
name|int
name|stripeCount
parameter_list|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Translated ORC pushdown predicate: "
operator|+
name|sarg
argument_list|)
expr_stmt|;
assert|assert
name|sarg
operator|!=
literal|null
assert|;
if|if
condition|(
name|stripeStats
operator|==
literal|null
operator|||
name|writerVersion
operator|==
name|OrcFile
operator|.
name|WriterVersion
operator|.
name|ORIGINAL
condition|)
block|{
return|return
literal|null
return|;
comment|// only do split pruning if HIVE-8732 has been fixed in the writer
block|}
comment|// eliminate stripes that doesn't satisfy the predicate condition
name|List
argument_list|<
name|PredicateLeaf
argument_list|>
name|sargLeaves
init|=
name|sarg
operator|.
name|getLeaves
argument_list|()
decl_stmt|;
name|int
index|[]
name|filterColumns
init|=
name|RecordReaderImpl
operator|.
name|mapTranslatedSargColumns
argument_list|(
name|types
argument_list|,
name|sargLeaves
argument_list|)
decl_stmt|;
return|return
name|pickStripesInternal
argument_list|(
name|sarg
argument_list|,
name|filterColumns
argument_list|,
name|stripeStats
argument_list|,
name|stripeCount
argument_list|,
literal|null
argument_list|)
return|;
block|}
specifier|private
specifier|static
name|boolean
index|[]
name|pickStripes
parameter_list|(
name|SearchArgument
name|sarg
parameter_list|,
name|String
index|[]
name|sargColNames
parameter_list|,
name|WriterVersion
name|writerVersion
parameter_list|,
name|boolean
name|isOriginal
parameter_list|,
name|List
argument_list|<
name|StripeStatistics
argument_list|>
name|stripeStats
parameter_list|,
name|int
name|stripeCount
parameter_list|,
name|Path
name|filePath
parameter_list|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"ORC pushdown predicate: "
operator|+
name|sarg
argument_list|)
expr_stmt|;
if|if
condition|(
name|sarg
operator|==
literal|null
operator|||
name|stripeStats
operator|==
literal|null
operator|||
name|writerVersion
operator|==
name|OrcFile
operator|.
name|WriterVersion
operator|.
name|ORIGINAL
condition|)
block|{
return|return
literal|null
return|;
comment|// only do split pruning if HIVE-8732 has been fixed in the writer
block|}
comment|// eliminate stripes that doesn't satisfy the predicate condition
name|List
argument_list|<
name|PredicateLeaf
argument_list|>
name|sargLeaves
init|=
name|sarg
operator|.
name|getLeaves
argument_list|()
decl_stmt|;
name|int
index|[]
name|filterColumns
init|=
name|RecordReaderImpl
operator|.
name|mapSargColumnsToOrcInternalColIdx
argument_list|(
name|sargLeaves
argument_list|,
name|sargColNames
argument_list|,
name|getRootColumn
argument_list|(
name|isOriginal
argument_list|)
argument_list|)
decl_stmt|;
return|return
name|pickStripesInternal
argument_list|(
name|sarg
argument_list|,
name|filterColumns
argument_list|,
name|stripeStats
argument_list|,
name|stripeCount
argument_list|,
name|filePath
argument_list|)
return|;
block|}
specifier|private
specifier|static
name|boolean
index|[]
name|pickStripesInternal
parameter_list|(
name|SearchArgument
name|sarg
parameter_list|,
name|int
index|[]
name|filterColumns
parameter_list|,
name|List
argument_list|<
name|StripeStatistics
argument_list|>
name|stripeStats
parameter_list|,
name|int
name|stripeCount
parameter_list|,
name|Path
name|filePath
parameter_list|)
block|{
name|boolean
index|[]
name|includeStripe
init|=
operator|new
name|boolean
index|[
name|stripeCount
index|]
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|includeStripe
operator|.
name|length
condition|;
operator|++
name|i
control|)
block|{
name|includeStripe
index|[
name|i
index|]
operator|=
operator|(
name|i
operator|>=
name|stripeStats
operator|.
name|size
argument_list|()
operator|)
operator|||
name|isStripeSatisfyPredicate
argument_list|(
name|stripeStats
operator|.
name|get
argument_list|(
name|i
argument_list|)
argument_list|,
name|sarg
argument_list|,
name|filterColumns
argument_list|)
expr_stmt|;
if|if
condition|(
name|isDebugEnabled
operator|&&
operator|!
name|includeStripe
index|[
name|i
index|]
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Eliminating ORC stripe-"
operator|+
name|i
operator|+
literal|" of file '"
operator|+
name|filePath
operator|+
literal|"'  as it did not satisfy predicate condition."
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|includeStripe
return|;
block|}
specifier|private
specifier|static
name|boolean
name|isStripeSatisfyPredicate
parameter_list|(
name|StripeStatistics
name|stripeStatistics
parameter_list|,
name|SearchArgument
name|sarg
parameter_list|,
name|int
index|[]
name|filterColumns
parameter_list|)
block|{
name|List
argument_list|<
name|PredicateLeaf
argument_list|>
name|predLeaves
init|=
name|sarg
operator|.
name|getLeaves
argument_list|()
decl_stmt|;
name|TruthValue
index|[]
name|truthValues
init|=
operator|new
name|TruthValue
index|[
name|predLeaves
operator|.
name|size
argument_list|()
index|]
decl_stmt|;
for|for
control|(
name|int
name|pred
init|=
literal|0
init|;
name|pred
operator|<
name|truthValues
operator|.
name|length
condition|;
name|pred
operator|++
control|)
block|{
if|if
condition|(
name|filterColumns
index|[
name|pred
index|]
operator|!=
operator|-
literal|1
condition|)
block|{
comment|// column statistics at index 0 contains only the number of rows
name|ColumnStatistics
name|stats
init|=
name|stripeStatistics
operator|.
name|getColumnStatistics
argument_list|()
index|[
name|filterColumns
index|[
name|pred
index|]
index|]
decl_stmt|;
name|truthValues
index|[
name|pred
index|]
operator|=
name|RecordReaderImpl
operator|.
name|evaluatePredicate
argument_list|(
name|stats
argument_list|,
name|predLeaves
operator|.
name|get
argument_list|(
name|pred
argument_list|)
argument_list|,
literal|null
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// parition column case.
comment|// partition filter will be evaluated by partition pruner so
comment|// we will not evaluate partition filter here.
name|truthValues
index|[
name|pred
index|]
operator|=
name|TruthValue
operator|.
name|YES_NO_NULL
expr_stmt|;
block|}
block|}
return|return
name|sarg
operator|.
name|evaluate
argument_list|(
name|truthValues
argument_list|)
operator|.
name|isNeeded
argument_list|()
return|;
block|}
annotation|@
name|VisibleForTesting
specifier|static
name|SplitStrategy
name|determineSplitStrategy
parameter_list|(
name|Context
name|context
parameter_list|,
name|FileSystem
name|fs
parameter_list|,
name|Path
name|dir
parameter_list|,
name|AcidUtils
operator|.
name|Directory
name|dirInfo
parameter_list|,
name|List
argument_list|<
name|HdfsFileStatusWithId
argument_list|>
name|baseOrOriginalFiles
parameter_list|)
block|{
name|Path
name|base
init|=
name|dirInfo
operator|.
name|getBaseDirectory
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|HdfsFileStatusWithId
argument_list|>
name|original
init|=
name|dirInfo
operator|.
name|getOriginalFiles
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|DeltaMetaData
argument_list|>
name|deltas
init|=
name|AcidUtils
operator|.
name|serializeDeltas
argument_list|(
name|dirInfo
operator|.
name|getCurrentDirectories
argument_list|()
argument_list|)
decl_stmt|;
name|boolean
index|[]
name|covered
init|=
operator|new
name|boolean
index|[
name|context
operator|.
name|numBuckets
index|]
decl_stmt|;
name|boolean
name|isOriginal
init|=
name|base
operator|==
literal|null
decl_stmt|;
comment|// if we have a base to work from
if|if
condition|(
name|base
operator|!=
literal|null
operator|||
operator|!
name|original
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|long
name|totalFileSize
init|=
literal|0
decl_stmt|;
for|for
control|(
name|HdfsFileStatusWithId
name|child
range|:
name|baseOrOriginalFiles
control|)
block|{
name|totalFileSize
operator|+=
name|child
operator|.
name|getFileStatus
argument_list|()
operator|.
name|getLen
argument_list|()
expr_stmt|;
name|AcidOutputFormat
operator|.
name|Options
name|opts
init|=
name|AcidUtils
operator|.
name|parseBaseBucketFilename
argument_list|(
name|child
operator|.
name|getFileStatus
argument_list|()
operator|.
name|getPath
argument_list|()
argument_list|,
name|context
operator|.
name|conf
argument_list|)
decl_stmt|;
name|int
name|b
init|=
name|opts
operator|.
name|getBucket
argument_list|()
decl_stmt|;
comment|// If the bucket is in the valid range, mark it as covered.
comment|// I wish Hive actually enforced bucketing all of the time.
if|if
condition|(
name|b
operator|>=
literal|0
operator|&&
name|b
operator|<
name|covered
operator|.
name|length
condition|)
block|{
name|covered
index|[
name|b
index|]
operator|=
literal|true
expr_stmt|;
block|}
block|}
name|int
name|numFiles
init|=
name|baseOrOriginalFiles
operator|.
name|size
argument_list|()
decl_stmt|;
name|long
name|avgFileSize
init|=
name|totalFileSize
operator|/
name|numFiles
decl_stmt|;
name|int
name|totalFiles
init|=
name|context
operator|.
name|numFilesCounter
operator|.
name|addAndGet
argument_list|(
name|numFiles
argument_list|)
decl_stmt|;
switch|switch
condition|(
name|context
operator|.
name|splitStrategyKind
condition|)
block|{
case|case
name|BI
case|:
comment|// BI strategy requested through config
return|return
operator|new
name|BISplitStrategy
argument_list|(
name|context
argument_list|,
name|fs
argument_list|,
name|dir
argument_list|,
name|baseOrOriginalFiles
argument_list|,
name|isOriginal
argument_list|,
name|deltas
argument_list|,
name|covered
argument_list|)
return|;
case|case
name|ETL
case|:
comment|// ETL strategy requested through config
return|return
operator|new
name|ETLSplitStrategy
argument_list|(
name|context
argument_list|,
name|fs
argument_list|,
name|dir
argument_list|,
name|baseOrOriginalFiles
argument_list|,
name|isOriginal
argument_list|,
name|deltas
argument_list|,
name|covered
argument_list|)
return|;
default|default:
comment|// HYBRID strategy
if|if
condition|(
name|avgFileSize
operator|>
name|context
operator|.
name|maxSize
operator|||
name|totalFiles
operator|<=
name|context
operator|.
name|minSplits
condition|)
block|{
return|return
operator|new
name|ETLSplitStrategy
argument_list|(
name|context
argument_list|,
name|fs
argument_list|,
name|dir
argument_list|,
name|baseOrOriginalFiles
argument_list|,
name|isOriginal
argument_list|,
name|deltas
argument_list|,
name|covered
argument_list|)
return|;
block|}
else|else
block|{
return|return
operator|new
name|BISplitStrategy
argument_list|(
name|context
argument_list|,
name|fs
argument_list|,
name|dir
argument_list|,
name|baseOrOriginalFiles
argument_list|,
name|isOriginal
argument_list|,
name|deltas
argument_list|,
name|covered
argument_list|)
return|;
block|}
block|}
block|}
else|else
block|{
comment|// no base, only deltas
return|return
operator|new
name|ACIDSplitStrategy
argument_list|(
name|dir
argument_list|,
name|context
operator|.
name|numBuckets
argument_list|,
name|deltas
argument_list|,
name|covered
argument_list|)
return|;
block|}
block|}
annotation|@
name|Override
specifier|public
name|RawReader
argument_list|<
name|OrcStruct
argument_list|>
name|getRawReader
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|boolean
name|collapseEvents
parameter_list|,
name|int
name|bucket
parameter_list|,
name|ValidTxnList
name|validTxnList
parameter_list|,
name|Path
name|baseDirectory
parameter_list|,
name|Path
index|[]
name|deltaDirectory
parameter_list|)
throws|throws
name|IOException
block|{
name|Reader
name|reader
init|=
literal|null
decl_stmt|;
name|boolean
name|isOriginal
init|=
literal|false
decl_stmt|;
if|if
condition|(
name|baseDirectory
operator|!=
literal|null
condition|)
block|{
name|Path
name|bucketFile
decl_stmt|;
if|if
condition|(
name|baseDirectory
operator|.
name|getName
argument_list|()
operator|.
name|startsWith
argument_list|(
name|AcidUtils
operator|.
name|BASE_PREFIX
argument_list|)
condition|)
block|{
name|bucketFile
operator|=
name|AcidUtils
operator|.
name|createBucketFile
argument_list|(
name|baseDirectory
argument_list|,
name|bucket
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|isOriginal
operator|=
literal|true
expr_stmt|;
name|bucketFile
operator|=
name|findOriginalBucket
argument_list|(
name|baseDirectory
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
argument_list|,
name|baseDirectory
argument_list|,
name|bucket
argument_list|)
expr_stmt|;
block|}
name|reader
operator|=
name|OrcFile
operator|.
name|createReader
argument_list|(
name|bucketFile
argument_list|,
name|OrcFile
operator|.
name|readerOptions
argument_list|(
name|conf
argument_list|)
argument_list|)
expr_stmt|;
block|}
return|return
operator|new
name|OrcRawRecordMerger
argument_list|(
name|conf
argument_list|,
name|collapseEvents
argument_list|,
name|reader
argument_list|,
name|isOriginal
argument_list|,
name|bucket
argument_list|,
name|validTxnList
argument_list|,
operator|new
name|Reader
operator|.
name|Options
argument_list|()
argument_list|,
name|deltaDirectory
argument_list|)
return|;
block|}
comment|/**    * Represents footer cache.    */
specifier|public
interface|interface
name|FooterCache
block|{
name|FileInfo
index|[]
name|getAndValidate
parameter_list|(
name|List
argument_list|<
name|HdfsFileStatusWithId
argument_list|>
name|files
parameter_list|)
throws|throws
name|IOException
function_decl|;
name|boolean
name|isBlocking
parameter_list|()
function_decl|;
name|void
name|put
parameter_list|(
name|Long
name|fileId
parameter_list|,
name|FileStatus
name|file
parameter_list|,
name|FileMetaInfo
name|fileMetaInfo
parameter_list|,
name|Reader
name|orcReader
parameter_list|)
throws|throws
name|IOException
function_decl|;
block|}
comment|/** Local footer cache using Guava. Stores convoluted Java objects. */
specifier|private
specifier|static
class|class
name|LocalCache
implements|implements
name|FooterCache
block|{
specifier|private
name|Cache
argument_list|<
name|Path
argument_list|,
name|FileInfo
argument_list|>
name|cache
decl_stmt|;
specifier|public
name|LocalCache
parameter_list|(
name|int
name|numThreads
parameter_list|,
name|int
name|cacheStripeDetailsSize
parameter_list|)
block|{
name|cache
operator|=
name|CacheBuilder
operator|.
name|newBuilder
argument_list|()
operator|.
name|concurrencyLevel
argument_list|(
name|numThreads
argument_list|)
operator|.
name|initialCapacity
argument_list|(
name|cacheStripeDetailsSize
argument_list|)
operator|.
name|maximumSize
argument_list|(
name|cacheStripeDetailsSize
argument_list|)
operator|.
name|softValues
argument_list|()
operator|.
name|build
argument_list|()
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|FileInfo
index|[]
name|getAndValidate
parameter_list|(
name|List
argument_list|<
name|HdfsFileStatusWithId
argument_list|>
name|files
parameter_list|)
block|{
comment|// TODO: should local cache also be by fileId? Preserve the original logic for now.
name|FileInfo
index|[]
name|result
init|=
operator|new
name|FileInfo
index|[
name|files
operator|.
name|size
argument_list|()
index|]
decl_stmt|;
name|int
name|i
init|=
operator|-
literal|1
decl_stmt|;
for|for
control|(
name|HdfsFileStatusWithId
name|fileWithId
range|:
name|files
control|)
block|{
operator|++
name|i
expr_stmt|;
name|FileStatus
name|file
init|=
name|fileWithId
operator|.
name|getFileStatus
argument_list|()
decl_stmt|;
name|Path
name|path
init|=
name|file
operator|.
name|getPath
argument_list|()
decl_stmt|;
name|Long
name|fileId
init|=
name|fileWithId
operator|.
name|getFileId
argument_list|()
decl_stmt|;
name|FileInfo
name|fileInfo
init|=
name|cache
operator|.
name|getIfPresent
argument_list|(
name|path
argument_list|)
decl_stmt|;
if|if
condition|(
name|isDebugEnabled
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Info "
operator|+
operator|(
name|fileInfo
operator|==
literal|null
condition|?
literal|"not "
else|:
literal|""
operator|)
operator|+
literal|"cached for path: "
operator|+
name|path
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|fileInfo
operator|==
literal|null
condition|)
continue|continue;
if|if
condition|(
operator|(
name|fileId
operator|!=
literal|null
operator|&&
name|fileInfo
operator|.
name|fileId
operator|!=
literal|null
operator|&&
name|fileId
operator|==
name|fileInfo
operator|.
name|fileId
operator|)
operator|||
operator|(
name|fileInfo
operator|.
name|modificationTime
operator|==
name|file
operator|.
name|getModificationTime
argument_list|()
operator|&&
name|fileInfo
operator|.
name|size
operator|==
name|file
operator|.
name|getLen
argument_list|()
operator|)
condition|)
block|{
name|result
index|[
name|i
index|]
operator|=
name|fileInfo
expr_stmt|;
continue|continue;
block|}
comment|// Invalidate
name|cache
operator|.
name|invalidate
argument_list|(
name|path
argument_list|)
expr_stmt|;
if|if
condition|(
name|isDebugEnabled
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Meta-Info for : "
operator|+
name|path
operator|+
literal|" changed. CachedModificationTime: "
operator|+
name|fileInfo
operator|.
name|modificationTime
operator|+
literal|", CurrentModificationTime: "
operator|+
name|file
operator|.
name|getModificationTime
argument_list|()
operator|+
literal|", CachedLength: "
operator|+
name|fileInfo
operator|.
name|size
operator|+
literal|", CurrentLength: "
operator|+
name|file
operator|.
name|getLen
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|result
return|;
block|}
specifier|public
name|void
name|put
parameter_list|(
name|Path
name|path
parameter_list|,
name|FileInfo
name|fileInfo
parameter_list|)
block|{
name|cache
operator|.
name|put
argument_list|(
name|path
argument_list|,
name|fileInfo
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|put
parameter_list|(
name|Long
name|fileId
parameter_list|,
name|FileStatus
name|file
parameter_list|,
name|FileMetaInfo
name|fileMetaInfo
parameter_list|,
name|Reader
name|orcReader
parameter_list|)
throws|throws
name|IOException
block|{
name|cache
operator|.
name|put
argument_list|(
name|file
operator|.
name|getPath
argument_list|()
argument_list|,
operator|new
name|FileInfo
argument_list|(
name|file
operator|.
name|getModificationTime
argument_list|()
argument_list|,
name|file
operator|.
name|getLen
argument_list|()
argument_list|,
name|orcReader
operator|.
name|getStripes
argument_list|()
argument_list|,
name|orcReader
operator|.
name|getStripeStatistics
argument_list|()
argument_list|,
name|orcReader
operator|.
name|getTypes
argument_list|()
argument_list|,
name|orcReader
operator|.
name|getOrcProtoFileStatistics
argument_list|()
argument_list|,
name|fileMetaInfo
argument_list|,
name|orcReader
operator|.
name|getWriterVersion
argument_list|()
argument_list|,
name|fileId
argument_list|)
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|isBlocking
parameter_list|()
block|{
return|return
literal|false
return|;
block|}
block|}
comment|/** Metastore-based footer cache storing serialized footers. Also has a local cache. */
specifier|public
specifier|static
class|class
name|MetastoreCache
implements|implements
name|FooterCache
block|{
specifier|private
specifier|final
name|LocalCache
name|localCache
decl_stmt|;
specifier|private
name|boolean
name|isWarnLogged
init|=
literal|false
decl_stmt|;
specifier|private
name|HiveConf
name|conf
decl_stmt|;
specifier|public
name|MetastoreCache
parameter_list|(
name|LocalCache
name|lc
parameter_list|)
block|{
name|localCache
operator|=
name|lc
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|FileInfo
index|[]
name|getAndValidate
parameter_list|(
name|List
argument_list|<
name|HdfsFileStatusWithId
argument_list|>
name|files
parameter_list|)
throws|throws
name|IOException
block|{
comment|// First, check the local cache.
name|FileInfo
index|[]
name|result
init|=
name|localCache
operator|.
name|getAndValidate
argument_list|(
name|files
argument_list|)
decl_stmt|;
assert|assert
name|result
operator|.
name|length
operator|==
name|files
operator|.
name|size
argument_list|()
assert|;
comment|// This is an unfortunate consequence of batching/iterating thru MS results.
comment|// TODO: maybe have a direct map call for small lists if this becomes a perf issue.
name|HashMap
argument_list|<
name|Long
argument_list|,
name|Integer
argument_list|>
name|posMap
init|=
operator|new
name|HashMap
argument_list|<>
argument_list|(
name|files
operator|.
name|size
argument_list|()
argument_list|)
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|result
operator|.
name|length
condition|;
operator|++
name|i
control|)
block|{
if|if
condition|(
name|result
index|[
name|i
index|]
operator|!=
literal|null
condition|)
continue|continue;
name|HdfsFileStatusWithId
name|file
init|=
name|files
operator|.
name|get
argument_list|(
name|i
argument_list|)
decl_stmt|;
name|Long
name|fileId
init|=
name|file
operator|.
name|getFileId
argument_list|()
decl_stmt|;
if|if
condition|(
name|fileId
operator|==
literal|null
condition|)
block|{
if|if
condition|(
operator|!
name|isWarnLogged
operator|||
name|isDebugEnabled
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Not using metastore cache because fileId is missing: "
operator|+
name|file
operator|.
name|getFileStatus
argument_list|()
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
name|isWarnLogged
operator|=
literal|true
expr_stmt|;
block|}
continue|continue;
block|}
name|posMap
operator|.
name|put
argument_list|(
name|fileId
argument_list|,
name|i
argument_list|)
expr_stmt|;
block|}
name|Iterator
argument_list|<
name|Entry
argument_list|<
name|Long
argument_list|,
name|ByteBuffer
argument_list|>
argument_list|>
name|iter
init|=
literal|null
decl_stmt|;
name|Hive
name|hive
decl_stmt|;
try|try
block|{
name|hive
operator|=
name|getHive
argument_list|()
expr_stmt|;
name|iter
operator|=
name|hive
operator|.
name|getFileMetadata
argument_list|(
name|Lists
operator|.
name|newArrayList
argument_list|(
name|posMap
operator|.
name|keySet
argument_list|()
argument_list|)
argument_list|,
name|conf
argument_list|)
operator|.
name|iterator
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|HiveException
name|ex
parameter_list|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
name|ex
argument_list|)
throw|;
block|}
name|List
argument_list|<
name|Long
argument_list|>
name|corruptIds
init|=
literal|null
decl_stmt|;
while|while
condition|(
name|iter
operator|.
name|hasNext
argument_list|()
condition|)
block|{
name|Entry
argument_list|<
name|Long
argument_list|,
name|ByteBuffer
argument_list|>
name|e
init|=
name|iter
operator|.
name|next
argument_list|()
decl_stmt|;
name|int
name|ix
init|=
name|posMap
operator|.
name|get
argument_list|(
name|e
operator|.
name|getKey
argument_list|()
argument_list|)
decl_stmt|;
assert|assert
name|result
index|[
name|ix
index|]
operator|==
literal|null
assert|;
name|HdfsFileStatusWithId
name|file
init|=
name|files
operator|.
name|get
argument_list|(
name|ix
argument_list|)
decl_stmt|;
assert|assert
name|file
operator|.
name|getFileId
argument_list|()
operator|==
name|e
operator|.
name|getKey
argument_list|()
assert|;
name|result
index|[
name|ix
index|]
operator|=
name|createFileInfoFromMs
argument_list|(
name|file
argument_list|,
name|e
operator|.
name|getValue
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|result
index|[
name|ix
index|]
operator|==
literal|null
condition|)
block|{
if|if
condition|(
name|corruptIds
operator|==
literal|null
condition|)
block|{
name|corruptIds
operator|=
operator|new
name|ArrayList
argument_list|<>
argument_list|()
expr_stmt|;
block|}
name|corruptIds
operator|.
name|add
argument_list|(
name|file
operator|.
name|getFileId
argument_list|()
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|localCache
operator|.
name|put
argument_list|(
name|file
operator|.
name|getFileStatus
argument_list|()
operator|.
name|getPath
argument_list|()
argument_list|,
name|result
index|[
name|ix
index|]
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
name|corruptIds
operator|!=
literal|null
condition|)
block|{
try|try
block|{
name|hive
operator|.
name|clearFileMetadata
argument_list|(
name|corruptIds
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|HiveException
name|ex
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Failed to clear corrupt cache data"
argument_list|,
name|ex
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|result
return|;
block|}
specifier|private
name|Hive
name|getHive
parameter_list|()
throws|throws
name|HiveException
block|{
comment|// TODO: we wish we could cache the Hive object, but it's not thread safe, and each
comment|//       threadlocal we "cache" would need to be reinitialized for every query. This is
comment|//       a huge PITA. Hive object will be cached internally, but the compat check will be
comment|//       done every time inside get().
return|return
name|Hive
operator|.
name|getWithFastCheck
argument_list|(
name|conf
argument_list|)
return|;
block|}
specifier|private
specifier|static
name|FileInfo
name|createFileInfoFromMs
parameter_list|(
name|HdfsFileStatusWithId
name|file
parameter_list|,
name|ByteBuffer
name|bb
parameter_list|)
throws|throws
name|IOException
block|{
name|FileStatus
name|fs
init|=
name|file
operator|.
name|getFileStatus
argument_list|()
decl_stmt|;
name|ReaderImpl
operator|.
name|FooterInfo
name|fi
init|=
literal|null
decl_stmt|;
name|ByteBuffer
name|original
init|=
name|bb
operator|.
name|duplicate
argument_list|()
decl_stmt|;
try|try
block|{
name|fi
operator|=
name|ReaderImpl
operator|.
name|extractMetaInfoFromFooter
argument_list|(
name|bb
argument_list|,
name|fs
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|ex
parameter_list|)
block|{
name|byte
index|[]
name|data
init|=
operator|new
name|byte
index|[
name|original
operator|.
name|remaining
argument_list|()
index|]
decl_stmt|;
name|System
operator|.
name|arraycopy
argument_list|(
name|original
operator|.
name|array
argument_list|()
argument_list|,
name|original
operator|.
name|arrayOffset
argument_list|()
operator|+
name|original
operator|.
name|position
argument_list|()
argument_list|,
name|data
argument_list|,
literal|0
argument_list|,
name|data
operator|.
name|length
argument_list|)
expr_stmt|;
name|String
name|msg
init|=
literal|"Failed to parse the footer stored in cache for file ID "
operator|+
name|file
operator|.
name|getFileId
argument_list|()
operator|+
literal|" "
operator|+
name|original
operator|+
literal|" [ "
operator|+
name|Hex
operator|.
name|encodeHexString
argument_list|(
name|data
argument_list|)
operator|+
literal|" ]"
decl_stmt|;
name|LOG
operator|.
name|error
argument_list|(
name|msg
argument_list|,
name|ex
argument_list|)
expr_stmt|;
return|return
literal|null
return|;
block|}
return|return
operator|new
name|FileInfo
argument_list|(
name|fs
operator|.
name|getModificationTime
argument_list|()
argument_list|,
name|fs
operator|.
name|getLen
argument_list|()
argument_list|,
name|fi
operator|.
name|getStripes
argument_list|()
argument_list|,
name|fi
operator|.
name|getMetadata
argument_list|()
argument_list|,
name|fi
operator|.
name|getFooter
argument_list|()
operator|.
name|getTypesList
argument_list|()
argument_list|,
name|fi
operator|.
name|getFooter
argument_list|()
operator|.
name|getStatisticsList
argument_list|()
argument_list|,
name|fi
operator|.
name|getFileMetaInfo
argument_list|()
argument_list|,
name|fi
operator|.
name|getFileMetaInfo
argument_list|()
operator|.
name|writerVersion
argument_list|,
name|file
operator|.
name|getFileId
argument_list|()
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|put
parameter_list|(
name|Long
name|fileId
parameter_list|,
name|FileStatus
name|file
parameter_list|,
name|FileMetaInfo
name|fileMetaInfo
parameter_list|,
name|Reader
name|orcReader
parameter_list|)
throws|throws
name|IOException
block|{
name|localCache
operator|.
name|put
argument_list|(
name|fileId
argument_list|,
name|file
argument_list|,
name|fileMetaInfo
argument_list|,
name|orcReader
argument_list|)
expr_stmt|;
if|if
condition|(
name|fileId
operator|!=
literal|null
condition|)
block|{
try|try
block|{
name|getHive
argument_list|()
operator|.
name|putFileMetadata
argument_list|(
name|Lists
operator|.
name|newArrayList
argument_list|(
name|fileId
argument_list|)
argument_list|,
name|Lists
operator|.
name|newArrayList
argument_list|(
operator|(
operator|(
name|ReaderImpl
operator|)
name|orcReader
operator|)
operator|.
name|getSerializedFileFooter
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|HiveException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
block|}
specifier|public
name|void
name|configure
parameter_list|(
name|HiveConf
name|queryConfig
parameter_list|)
block|{
name|this
operator|.
name|conf
operator|=
name|queryConfig
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|isBlocking
parameter_list|()
block|{
return|return
literal|true
return|;
block|}
block|}
block|}
end_class

end_unit

