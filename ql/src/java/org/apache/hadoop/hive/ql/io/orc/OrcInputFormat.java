begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/**  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|orc
package|;
end_package

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Arrays
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|ExecutorService
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|Executors
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|TimeUnit
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|atomic
operator|.
name|AtomicInteger
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|Log
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|LogFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configuration
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|BlockLocation
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileStatus
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|ValidTxnList
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|ValidTxnListImpl
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
operator|.
name|ConfVars
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|hive_metastoreConstants
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|Utilities
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|vector
operator|.
name|VectorizedInputFormatInterface
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|AcidInputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|AcidOutputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|AcidUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|InputFormatChecker
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|RecordIdentifier
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|StatsProvidingRecordReader
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|sarg
operator|.
name|PredicateLeaf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|sarg
operator|.
name|SearchArgument
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|log
operator|.
name|PerfLogger
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|sarg
operator|.
name|SearchArgument
operator|.
name|TruthValue
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|TableScanDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|ColumnProjectionUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|SerDeStats
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|objectinspector
operator|.
name|ObjectInspector
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|objectinspector
operator|.
name|StructObjectInspector
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|shims
operator|.
name|HadoopShims
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|shims
operator|.
name|ShimLoader
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|LongWritable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|NullWritable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|FileSplit
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|InputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|InputSplit
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|InvalidInputException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|JobConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|Reporter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|StringUtils
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|cache
operator|.
name|Cache
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|cache
operator|.
name|CacheBuilder
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|util
operator|.
name|concurrent
operator|.
name|ThreadFactoryBuilder
import|;
end_import

begin_comment
comment|/**  * A MapReduce/Hive input format for ORC files.  *<p>  * This class implements both the classic InputFormat, which stores the rows  * directly, and AcidInputFormat, which stores a series of events with the  * following schema:  *<pre>  *   class AcidEvent&lt;ROW&gt; {  *     enum ACTION {INSERT, UPDATE, DELETE}  *     ACTION operation;  *     long originalTransaction;  *     int bucket;  *     long rowId;  *     long currentTransaction;  *     ROW row;  *   }  *</pre>  * Each AcidEvent object corresponds to an update event. The  * originalTransaction, bucket, and rowId are the unique identifier for the row.  * The operation and currentTransaction are the operation and the transaction  * that added this event. Insert and update events include the entire row, while  * delete events have null for row.  */
end_comment

begin_class
specifier|public
class|class
name|OrcInputFormat
implements|implements
name|InputFormat
argument_list|<
name|NullWritable
argument_list|,
name|OrcStruct
argument_list|>
implements|,
name|InputFormatChecker
implements|,
name|VectorizedInputFormatInterface
implements|,
name|AcidInputFormat
argument_list|<
name|OrcStruct
argument_list|>
block|{
specifier|private
specifier|static
specifier|final
name|Log
name|LOG
init|=
name|LogFactory
operator|.
name|getLog
argument_list|(
name|OrcInputFormat
operator|.
name|class
argument_list|)
decl_stmt|;
specifier|static
specifier|final
name|HadoopShims
name|SHIMS
init|=
name|ShimLoader
operator|.
name|getHadoopShims
argument_list|()
decl_stmt|;
specifier|static
specifier|final
name|String
name|MIN_SPLIT_SIZE
init|=
name|SHIMS
operator|.
name|getHadoopConfNames
argument_list|()
operator|.
name|get
argument_list|(
literal|"MAPREDMINSPLITSIZE"
argument_list|)
decl_stmt|;
specifier|static
specifier|final
name|String
name|MAX_SPLIT_SIZE
init|=
name|SHIMS
operator|.
name|getHadoopConfNames
argument_list|()
operator|.
name|get
argument_list|(
literal|"MAPREDMAXSPLITSIZE"
argument_list|)
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|long
name|DEFAULT_MIN_SPLIT_SIZE
init|=
literal|16
operator|*
literal|1024
operator|*
literal|1024
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|long
name|DEFAULT_MAX_SPLIT_SIZE
init|=
literal|256
operator|*
literal|1024
operator|*
literal|1024
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|PerfLogger
name|perfLogger
init|=
name|PerfLogger
operator|.
name|getPerfLogger
argument_list|()
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|String
name|CLASS_NAME
init|=
name|ReaderImpl
operator|.
name|class
operator|.
name|getName
argument_list|()
decl_stmt|;
comment|/**    * When picking the hosts for a split that crosses block boundaries,    * any drop any host that has fewer than MIN_INCLUDED_LOCATION of the    * number of bytes available on the host with the most.    * If host1 has 10MB of the split, host2 has 20MB, and host3 has 18MB the    * split will contain host2 (100% of host2) and host3 (90% of host2). Host1    * with 50% will be dropped.    */
specifier|private
specifier|static
specifier|final
name|double
name|MIN_INCLUDED_LOCATION
init|=
literal|0.80
decl_stmt|;
specifier|private
specifier|static
class|class
name|OrcRecordReader
implements|implements
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|RecordReader
argument_list|<
name|NullWritable
argument_list|,
name|OrcStruct
argument_list|>
implements|,
name|StatsProvidingRecordReader
block|{
specifier|private
specifier|final
name|RecordReader
name|reader
decl_stmt|;
specifier|private
specifier|final
name|long
name|offset
decl_stmt|;
specifier|private
specifier|final
name|long
name|length
decl_stmt|;
specifier|private
specifier|final
name|int
name|numColumns
decl_stmt|;
specifier|private
name|float
name|progress
init|=
literal|0.0f
decl_stmt|;
specifier|private
specifier|final
name|Reader
name|file
decl_stmt|;
specifier|private
specifier|final
name|SerDeStats
name|stats
decl_stmt|;
name|OrcRecordReader
parameter_list|(
name|Reader
name|file
parameter_list|,
name|Configuration
name|conf
parameter_list|,
name|FileSplit
name|split
parameter_list|)
throws|throws
name|IOException
block|{
name|List
argument_list|<
name|OrcProto
operator|.
name|Type
argument_list|>
name|types
init|=
name|file
operator|.
name|getTypes
argument_list|()
decl_stmt|;
name|this
operator|.
name|file
operator|=
name|file
expr_stmt|;
name|numColumns
operator|=
operator|(
name|types
operator|.
name|size
argument_list|()
operator|==
literal|0
operator|)
condition|?
literal|0
else|:
name|types
operator|.
name|get
argument_list|(
literal|0
argument_list|)
operator|.
name|getSubtypesCount
argument_list|()
expr_stmt|;
name|this
operator|.
name|offset
operator|=
name|split
operator|.
name|getStart
argument_list|()
expr_stmt|;
name|this
operator|.
name|length
operator|=
name|split
operator|.
name|getLength
argument_list|()
expr_stmt|;
name|this
operator|.
name|reader
operator|=
name|createReaderFromFile
argument_list|(
name|file
argument_list|,
name|conf
argument_list|,
name|offset
argument_list|,
name|length
argument_list|)
expr_stmt|;
name|this
operator|.
name|stats
operator|=
operator|new
name|SerDeStats
argument_list|()
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|next
parameter_list|(
name|NullWritable
name|key
parameter_list|,
name|OrcStruct
name|value
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|reader
operator|.
name|hasNext
argument_list|()
condition|)
block|{
name|reader
operator|.
name|next
argument_list|(
name|value
argument_list|)
expr_stmt|;
name|progress
operator|=
name|reader
operator|.
name|getProgress
argument_list|()
expr_stmt|;
return|return
literal|true
return|;
block|}
else|else
block|{
return|return
literal|false
return|;
block|}
block|}
annotation|@
name|Override
specifier|public
name|NullWritable
name|createKey
parameter_list|()
block|{
return|return
name|NullWritable
operator|.
name|get
argument_list|()
return|;
block|}
annotation|@
name|Override
specifier|public
name|OrcStruct
name|createValue
parameter_list|()
block|{
return|return
operator|new
name|OrcStruct
argument_list|(
name|numColumns
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|public
name|long
name|getPos
parameter_list|()
throws|throws
name|IOException
block|{
return|return
name|offset
operator|+
call|(
name|long
call|)
argument_list|(
name|progress
operator|*
name|length
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|close
parameter_list|()
throws|throws
name|IOException
block|{
name|reader
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|float
name|getProgress
parameter_list|()
throws|throws
name|IOException
block|{
return|return
name|progress
return|;
block|}
annotation|@
name|Override
specifier|public
name|SerDeStats
name|getStats
parameter_list|()
block|{
name|stats
operator|.
name|setRawDataSize
argument_list|(
name|file
operator|.
name|getRawDataSize
argument_list|()
argument_list|)
expr_stmt|;
name|stats
operator|.
name|setRowCount
argument_list|(
name|file
operator|.
name|getNumberOfRows
argument_list|()
argument_list|)
expr_stmt|;
return|return
name|stats
return|;
block|}
block|}
comment|/**    * Get the root column for the row. In ACID format files, it is offset by    * the extra metadata columns.    * @param isOriginal is the file in the original format?    * @return the column number for the root of row.    */
specifier|private
specifier|static
name|int
name|getRootColumn
parameter_list|(
name|boolean
name|isOriginal
parameter_list|)
block|{
return|return
name|isOriginal
condition|?
literal|0
else|:
operator|(
name|OrcRecordUpdater
operator|.
name|ROW
operator|+
literal|1
operator|)
return|;
block|}
specifier|public
specifier|static
name|RecordReader
name|createReaderFromFile
parameter_list|(
name|Reader
name|file
parameter_list|,
name|Configuration
name|conf
parameter_list|,
name|long
name|offset
parameter_list|,
name|long
name|length
parameter_list|)
throws|throws
name|IOException
block|{
name|Reader
operator|.
name|Options
name|options
init|=
operator|new
name|Reader
operator|.
name|Options
argument_list|()
operator|.
name|range
argument_list|(
name|offset
argument_list|,
name|length
argument_list|)
decl_stmt|;
name|boolean
name|isOriginal
init|=
operator|!
name|file
operator|.
name|hasMetadataValue
argument_list|(
name|OrcRecordUpdater
operator|.
name|ACID_KEY_INDEX_NAME
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|OrcProto
operator|.
name|Type
argument_list|>
name|types
init|=
name|file
operator|.
name|getTypes
argument_list|()
decl_stmt|;
name|setIncludedColumns
argument_list|(
name|options
argument_list|,
name|types
argument_list|,
name|conf
argument_list|,
name|isOriginal
argument_list|)
expr_stmt|;
name|setSearchArgument
argument_list|(
name|options
argument_list|,
name|types
argument_list|,
name|conf
argument_list|,
name|isOriginal
argument_list|)
expr_stmt|;
return|return
name|file
operator|.
name|rowsOptions
argument_list|(
name|options
argument_list|)
return|;
block|}
comment|/**    * Recurse down into a type subtree turning on all of the sub-columns.    * @param types the types of the file    * @param result the global view of columns that should be included    * @param typeId the root of tree to enable    * @param rootColumn the top column    */
specifier|private
specifier|static
name|void
name|includeColumnRecursive
parameter_list|(
name|List
argument_list|<
name|OrcProto
operator|.
name|Type
argument_list|>
name|types
parameter_list|,
name|boolean
index|[]
name|result
parameter_list|,
name|int
name|typeId
parameter_list|,
name|int
name|rootColumn
parameter_list|)
block|{
name|result
index|[
name|typeId
operator|-
name|rootColumn
index|]
operator|=
literal|true
expr_stmt|;
name|OrcProto
operator|.
name|Type
name|type
init|=
name|types
operator|.
name|get
argument_list|(
name|typeId
argument_list|)
decl_stmt|;
name|int
name|children
init|=
name|type
operator|.
name|getSubtypesCount
argument_list|()
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|children
condition|;
operator|++
name|i
control|)
block|{
name|includeColumnRecursive
argument_list|(
name|types
argument_list|,
name|result
argument_list|,
name|type
operator|.
name|getSubtypes
argument_list|(
name|i
argument_list|)
argument_list|,
name|rootColumn
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * Take the configuration and figure out which columns we need to include.    * @param options the options to update    * @param types the types for the file    * @param conf the configuration    * @param isOriginal is the file in the original format?    */
specifier|static
name|void
name|setIncludedColumns
parameter_list|(
name|Reader
operator|.
name|Options
name|options
parameter_list|,
name|List
argument_list|<
name|OrcProto
operator|.
name|Type
argument_list|>
name|types
parameter_list|,
name|Configuration
name|conf
parameter_list|,
name|boolean
name|isOriginal
parameter_list|)
block|{
name|int
name|rootColumn
init|=
name|getRootColumn
argument_list|(
name|isOriginal
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|ColumnProjectionUtils
operator|.
name|isReadAllColumns
argument_list|(
name|conf
argument_list|)
condition|)
block|{
name|int
name|numColumns
init|=
name|types
operator|.
name|size
argument_list|()
operator|-
name|rootColumn
decl_stmt|;
name|boolean
index|[]
name|result
init|=
operator|new
name|boolean
index|[
name|numColumns
index|]
decl_stmt|;
name|result
index|[
literal|0
index|]
operator|=
literal|true
expr_stmt|;
name|OrcProto
operator|.
name|Type
name|root
init|=
name|types
operator|.
name|get
argument_list|(
name|rootColumn
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|Integer
argument_list|>
name|included
init|=
name|ColumnProjectionUtils
operator|.
name|getReadColumnIDs
argument_list|(
name|conf
argument_list|)
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|root
operator|.
name|getSubtypesCount
argument_list|()
condition|;
operator|++
name|i
control|)
block|{
if|if
condition|(
name|included
operator|.
name|contains
argument_list|(
name|i
argument_list|)
condition|)
block|{
name|includeColumnRecursive
argument_list|(
name|types
argument_list|,
name|result
argument_list|,
name|root
operator|.
name|getSubtypes
argument_list|(
name|i
argument_list|)
argument_list|,
name|rootColumn
argument_list|)
expr_stmt|;
block|}
block|}
name|options
operator|.
name|include
argument_list|(
name|result
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|options
operator|.
name|include
argument_list|(
literal|null
argument_list|)
expr_stmt|;
block|}
block|}
specifier|static
name|void
name|setSearchArgument
parameter_list|(
name|Reader
operator|.
name|Options
name|options
parameter_list|,
name|List
argument_list|<
name|OrcProto
operator|.
name|Type
argument_list|>
name|types
parameter_list|,
name|Configuration
name|conf
parameter_list|,
name|boolean
name|isOriginal
parameter_list|)
block|{
name|int
name|rootColumn
init|=
name|getRootColumn
argument_list|(
name|isOriginal
argument_list|)
decl_stmt|;
name|String
name|serializedPushdown
init|=
name|conf
operator|.
name|get
argument_list|(
name|TableScanDesc
operator|.
name|FILTER_EXPR_CONF_STR
argument_list|)
decl_stmt|;
name|String
name|columnNamesString
init|=
name|conf
operator|.
name|get
argument_list|(
name|ColumnProjectionUtils
operator|.
name|READ_COLUMN_NAMES_CONF_STR
argument_list|)
decl_stmt|;
if|if
condition|(
name|serializedPushdown
operator|==
literal|null
operator|||
name|columnNamesString
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"No ORC pushdown predicate"
argument_list|)
expr_stmt|;
name|options
operator|.
name|searchArgument
argument_list|(
literal|null
argument_list|,
literal|null
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|SearchArgument
name|sarg
init|=
name|SearchArgument
operator|.
name|FACTORY
operator|.
name|create
argument_list|(
name|Utilities
operator|.
name|deserializeExpression
argument_list|(
name|serializedPushdown
argument_list|)
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"ORC pushdown predicate: "
operator|+
name|sarg
argument_list|)
expr_stmt|;
name|String
index|[]
name|neededColumnNames
init|=
name|columnNamesString
operator|.
name|split
argument_list|(
literal|","
argument_list|)
decl_stmt|;
name|String
index|[]
name|columnNames
init|=
operator|new
name|String
index|[
name|types
operator|.
name|size
argument_list|()
operator|-
name|rootColumn
index|]
decl_stmt|;
name|boolean
index|[]
name|includedColumns
init|=
name|options
operator|.
name|getInclude
argument_list|()
decl_stmt|;
name|int
name|i
init|=
literal|0
decl_stmt|;
for|for
control|(
name|int
name|columnId
range|:
name|types
operator|.
name|get
argument_list|(
name|rootColumn
argument_list|)
operator|.
name|getSubtypesList
argument_list|()
control|)
block|{
if|if
condition|(
name|includedColumns
operator|==
literal|null
operator|||
name|includedColumns
index|[
name|columnId
index|]
condition|)
block|{
comment|// this is guaranteed to be positive because types only have children
comment|// ids greater than their own id.
name|columnNames
index|[
name|columnId
operator|-
name|rootColumn
index|]
operator|=
name|neededColumnNames
index|[
name|i
operator|++
index|]
expr_stmt|;
block|}
block|}
name|options
operator|.
name|searchArgument
argument_list|(
name|sarg
argument_list|,
name|columnNames
argument_list|)
expr_stmt|;
block|}
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|validateInput
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|HiveConf
name|conf
parameter_list|,
name|ArrayList
argument_list|<
name|FileStatus
argument_list|>
name|files
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|Utilities
operator|.
name|isVectorMode
argument_list|(
name|conf
argument_list|)
condition|)
block|{
return|return
operator|new
name|VectorizedOrcInputFormat
argument_list|()
operator|.
name|validateInput
argument_list|(
name|fs
argument_list|,
name|conf
argument_list|,
name|files
argument_list|)
return|;
block|}
if|if
condition|(
name|files
operator|.
name|size
argument_list|()
operator|<=
literal|0
condition|)
block|{
return|return
literal|false
return|;
block|}
for|for
control|(
name|FileStatus
name|file
range|:
name|files
control|)
block|{
try|try
block|{
name|OrcFile
operator|.
name|createReader
argument_list|(
name|file
operator|.
name|getPath
argument_list|()
argument_list|,
name|OrcFile
operator|.
name|readerOptions
argument_list|(
name|conf
argument_list|)
operator|.
name|filesystem
argument_list|(
name|fs
argument_list|)
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
return|return
literal|false
return|;
block|}
block|}
return|return
literal|true
return|;
block|}
comment|/**    * Get the list of input {@link Path}s for the map-reduce job.    *    * @param conf The configuration of the job    * @return the list of input {@link Path}s for the map-reduce job.    */
specifier|static
name|Path
index|[]
name|getInputPaths
parameter_list|(
name|Configuration
name|conf
parameter_list|)
throws|throws
name|IOException
block|{
name|String
name|dirs
init|=
name|conf
operator|.
name|get
argument_list|(
literal|"mapred.input.dir"
argument_list|)
decl_stmt|;
if|if
condition|(
name|dirs
operator|==
literal|null
condition|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Configuration mapred.input.dir is not defined."
argument_list|)
throw|;
block|}
name|String
index|[]
name|list
init|=
name|StringUtils
operator|.
name|split
argument_list|(
name|dirs
argument_list|)
decl_stmt|;
name|Path
index|[]
name|result
init|=
operator|new
name|Path
index|[
name|list
operator|.
name|length
index|]
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|list
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|result
index|[
name|i
index|]
operator|=
operator|new
name|Path
argument_list|(
name|StringUtils
operator|.
name|unEscapeString
argument_list|(
name|list
index|[
name|i
index|]
argument_list|)
argument_list|)
expr_stmt|;
block|}
return|return
name|result
return|;
block|}
comment|/**    * The global information about the split generation that we pass around to    * the different worker threads.    */
specifier|static
class|class
name|Context
block|{
specifier|private
specifier|final
name|Configuration
name|conf
decl_stmt|;
specifier|private
specifier|static
name|Cache
argument_list|<
name|Path
argument_list|,
name|FileInfo
argument_list|>
name|footerCache
decl_stmt|;
specifier|private
specifier|final
name|ExecutorService
name|threadPool
decl_stmt|;
specifier|private
specifier|final
name|List
argument_list|<
name|OrcSplit
argument_list|>
name|splits
init|=
operator|new
name|ArrayList
argument_list|<
name|OrcSplit
argument_list|>
argument_list|(
literal|10000
argument_list|)
decl_stmt|;
specifier|private
specifier|final
name|int
name|numBuckets
decl_stmt|;
specifier|private
specifier|final
name|List
argument_list|<
name|Throwable
argument_list|>
name|errors
init|=
operator|new
name|ArrayList
argument_list|<
name|Throwable
argument_list|>
argument_list|()
decl_stmt|;
specifier|private
specifier|final
name|long
name|maxSize
decl_stmt|;
specifier|private
specifier|final
name|long
name|minSize
decl_stmt|;
specifier|private
specifier|final
name|boolean
name|footerInSplits
decl_stmt|;
specifier|private
specifier|final
name|boolean
name|cacheStripeDetails
decl_stmt|;
specifier|private
specifier|final
name|AtomicInteger
name|cacheHitCounter
init|=
operator|new
name|AtomicInteger
argument_list|(
literal|0
argument_list|)
decl_stmt|;
specifier|private
specifier|final
name|AtomicInteger
name|numFilesCounter
init|=
operator|new
name|AtomicInteger
argument_list|(
literal|0
argument_list|)
decl_stmt|;
specifier|private
name|Throwable
name|fatalError
init|=
literal|null
decl_stmt|;
specifier|private
name|ValidTxnList
name|transactionList
decl_stmt|;
comment|/**      * A count of the number of threads that may create more work for the      * thread pool.      */
specifier|private
name|int
name|schedulers
init|=
literal|0
decl_stmt|;
name|Context
parameter_list|(
name|Configuration
name|conf
parameter_list|)
block|{
name|this
operator|.
name|conf
operator|=
name|conf
expr_stmt|;
name|minSize
operator|=
name|conf
operator|.
name|getLong
argument_list|(
name|MIN_SPLIT_SIZE
argument_list|,
name|DEFAULT_MIN_SPLIT_SIZE
argument_list|)
expr_stmt|;
name|maxSize
operator|=
name|conf
operator|.
name|getLong
argument_list|(
name|MAX_SPLIT_SIZE
argument_list|,
name|DEFAULT_MAX_SPLIT_SIZE
argument_list|)
expr_stmt|;
name|footerInSplits
operator|=
name|HiveConf
operator|.
name|getBoolVar
argument_list|(
name|conf
argument_list|,
name|ConfVars
operator|.
name|HIVE_ORC_INCLUDE_FILE_FOOTER_IN_SPLITS
argument_list|)
expr_stmt|;
name|numBuckets
operator|=
name|Math
operator|.
name|max
argument_list|(
name|conf
operator|.
name|getInt
argument_list|(
name|hive_metastoreConstants
operator|.
name|BUCKET_COUNT
argument_list|,
literal|0
argument_list|)
argument_list|,
literal|0
argument_list|)
expr_stmt|;
name|int
name|cacheStripeDetailsSize
init|=
name|HiveConf
operator|.
name|getIntVar
argument_list|(
name|conf
argument_list|,
name|ConfVars
operator|.
name|HIVE_ORC_CACHE_STRIPE_DETAILS_SIZE
argument_list|)
decl_stmt|;
name|int
name|numThreads
init|=
name|HiveConf
operator|.
name|getIntVar
argument_list|(
name|conf
argument_list|,
name|ConfVars
operator|.
name|HIVE_ORC_COMPUTE_SPLITS_NUM_THREADS
argument_list|)
decl_stmt|;
name|cacheStripeDetails
operator|=
operator|(
name|cacheStripeDetailsSize
operator|>
literal|0
operator|)
expr_stmt|;
name|threadPool
operator|=
name|Executors
operator|.
name|newFixedThreadPool
argument_list|(
name|numThreads
argument_list|,
operator|new
name|ThreadFactoryBuilder
argument_list|()
operator|.
name|setDaemon
argument_list|(
literal|true
argument_list|)
operator|.
name|setNameFormat
argument_list|(
literal|"ORC_GET_SPLITS #%d"
argument_list|)
operator|.
name|build
argument_list|()
argument_list|)
expr_stmt|;
synchronized|synchronized
init|(
name|Context
operator|.
name|class
init|)
block|{
if|if
condition|(
name|footerCache
operator|==
literal|null
operator|&&
name|cacheStripeDetails
condition|)
block|{
name|footerCache
operator|=
name|CacheBuilder
operator|.
name|newBuilder
argument_list|()
operator|.
name|concurrencyLevel
argument_list|(
name|numThreads
argument_list|)
operator|.
name|initialCapacity
argument_list|(
name|cacheStripeDetailsSize
argument_list|)
operator|.
name|softValues
argument_list|()
operator|.
name|build
argument_list|()
expr_stmt|;
block|}
block|}
name|String
name|value
init|=
name|conf
operator|.
name|get
argument_list|(
name|ValidTxnList
operator|.
name|VALID_TXNS_KEY
argument_list|,
name|Long
operator|.
name|MAX_VALUE
operator|+
literal|":"
argument_list|)
decl_stmt|;
name|transactionList
operator|=
operator|new
name|ValidTxnListImpl
argument_list|(
name|value
argument_list|)
expr_stmt|;
block|}
name|int
name|getSchedulers
parameter_list|()
block|{
return|return
name|schedulers
return|;
block|}
comment|/**      * Get the Nth split.      * @param index if index>= 0, count from the front, otherwise count from      *     the back.      * @return the Nth file split      */
name|OrcSplit
name|getResult
parameter_list|(
name|int
name|index
parameter_list|)
block|{
if|if
condition|(
name|index
operator|>=
literal|0
condition|)
block|{
return|return
name|splits
operator|.
name|get
argument_list|(
name|index
argument_list|)
return|;
block|}
else|else
block|{
return|return
name|splits
operator|.
name|get
argument_list|(
name|splits
operator|.
name|size
argument_list|()
operator|+
name|index
argument_list|)
return|;
block|}
block|}
name|List
argument_list|<
name|Throwable
argument_list|>
name|getErrors
parameter_list|()
block|{
return|return
name|errors
return|;
block|}
comment|/**      * Add a unit of work.      * @param runnable the object to run      */
specifier|synchronized
name|void
name|schedule
parameter_list|(
name|Runnable
name|runnable
parameter_list|)
block|{
if|if
condition|(
name|fatalError
operator|==
literal|null
condition|)
block|{
if|if
condition|(
name|runnable
operator|instanceof
name|FileGenerator
operator|||
name|runnable
operator|instanceof
name|SplitGenerator
condition|)
block|{
name|schedulers
operator|+=
literal|1
expr_stmt|;
block|}
name|threadPool
operator|.
name|execute
argument_list|(
name|runnable
argument_list|)
expr_stmt|;
block|}
else|else
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
literal|"serious problem"
argument_list|,
name|fatalError
argument_list|)
throw|;
block|}
block|}
comment|/**      * Mark a worker that may generate more work as done.      */
specifier|synchronized
name|void
name|decrementSchedulers
parameter_list|()
block|{
name|schedulers
operator|-=
literal|1
expr_stmt|;
if|if
condition|(
name|schedulers
operator|==
literal|0
condition|)
block|{
name|notify
argument_list|()
expr_stmt|;
block|}
block|}
specifier|synchronized
name|void
name|notifyOnNonIOException
parameter_list|(
name|Throwable
name|th
parameter_list|)
block|{
name|fatalError
operator|=
name|th
expr_stmt|;
name|notify
argument_list|()
expr_stmt|;
block|}
comment|/**      * Wait until all of the tasks are done. It waits until all of the      * threads that may create more work are done and then shuts down the      * thread pool and waits for the final threads to finish.      */
specifier|synchronized
name|void
name|waitForTasks
parameter_list|()
block|{
try|try
block|{
while|while
condition|(
name|schedulers
operator|!=
literal|0
condition|)
block|{
name|wait
argument_list|()
expr_stmt|;
if|if
condition|(
name|fatalError
operator|!=
literal|null
condition|)
block|{
name|threadPool
operator|.
name|shutdownNow
argument_list|()
expr_stmt|;
throw|throw
operator|new
name|RuntimeException
argument_list|(
literal|"serious problem"
argument_list|,
name|fatalError
argument_list|)
throw|;
block|}
block|}
name|threadPool
operator|.
name|shutdown
argument_list|()
expr_stmt|;
name|threadPool
operator|.
name|awaitTermination
argument_list|(
name|Long
operator|.
name|MAX_VALUE
argument_list|,
name|TimeUnit
operator|.
name|DAYS
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|InterruptedException
name|ie
parameter_list|)
block|{
throw|throw
operator|new
name|IllegalStateException
argument_list|(
literal|"interrupted"
argument_list|,
name|ie
argument_list|)
throw|;
block|}
block|}
block|}
comment|/**    * Given a directory, get the list of files and blocks in those files.    * A thread is used for each directory.    */
specifier|static
specifier|final
class|class
name|FileGenerator
implements|implements
name|Runnable
block|{
specifier|private
specifier|final
name|Context
name|context
decl_stmt|;
specifier|private
specifier|final
name|FileSystem
name|fs
decl_stmt|;
specifier|private
specifier|final
name|Path
name|dir
decl_stmt|;
name|FileGenerator
parameter_list|(
name|Context
name|context
parameter_list|,
name|FileSystem
name|fs
parameter_list|,
name|Path
name|dir
parameter_list|)
block|{
name|this
operator|.
name|context
operator|=
name|context
expr_stmt|;
name|this
operator|.
name|fs
operator|=
name|fs
expr_stmt|;
name|this
operator|.
name|dir
operator|=
name|dir
expr_stmt|;
block|}
specifier|private
name|void
name|scheduleSplits
parameter_list|(
name|FileStatus
name|file
parameter_list|,
name|boolean
name|isOriginal
parameter_list|,
name|boolean
name|hasBase
parameter_list|,
name|List
argument_list|<
name|Long
argument_list|>
name|deltas
parameter_list|)
throws|throws
name|IOException
block|{
name|FileInfo
name|info
init|=
literal|null
decl_stmt|;
if|if
condition|(
name|context
operator|.
name|cacheStripeDetails
condition|)
block|{
name|info
operator|=
name|verifyCachedFileInfo
argument_list|(
name|file
argument_list|)
expr_stmt|;
block|}
operator|new
name|SplitGenerator
argument_list|(
name|context
argument_list|,
name|fs
argument_list|,
name|file
argument_list|,
name|info
argument_list|,
name|isOriginal
argument_list|,
name|deltas
argument_list|,
name|hasBase
argument_list|)
operator|.
name|schedule
argument_list|()
expr_stmt|;
block|}
comment|/**      * For each path, get the list of files and blocks that they consist of.      */
annotation|@
name|Override
specifier|public
name|void
name|run
parameter_list|()
block|{
try|try
block|{
name|AcidUtils
operator|.
name|Directory
name|dirInfo
init|=
name|AcidUtils
operator|.
name|getAcidState
argument_list|(
name|dir
argument_list|,
name|context
operator|.
name|conf
argument_list|,
name|context
operator|.
name|transactionList
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|Long
argument_list|>
name|deltas
init|=
name|AcidUtils
operator|.
name|serializeDeltas
argument_list|(
name|dirInfo
operator|.
name|getCurrentDirectories
argument_list|()
argument_list|)
decl_stmt|;
name|Path
name|base
init|=
name|dirInfo
operator|.
name|getBaseDirectory
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|FileStatus
argument_list|>
name|original
init|=
name|dirInfo
operator|.
name|getOriginalFiles
argument_list|()
decl_stmt|;
name|boolean
index|[]
name|covered
init|=
operator|new
name|boolean
index|[
name|context
operator|.
name|numBuckets
index|]
decl_stmt|;
name|boolean
name|isOriginal
init|=
name|base
operator|==
literal|null
decl_stmt|;
comment|// if we have a base to work from
if|if
condition|(
name|base
operator|!=
literal|null
operator|||
operator|!
name|original
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
comment|// find the base files (original or new style)
name|List
argument_list|<
name|FileStatus
argument_list|>
name|children
init|=
name|original
decl_stmt|;
if|if
condition|(
name|base
operator|!=
literal|null
condition|)
block|{
name|children
operator|=
name|SHIMS
operator|.
name|listLocatedStatus
argument_list|(
name|fs
argument_list|,
name|base
argument_list|,
name|AcidUtils
operator|.
name|hiddenFileFilter
argument_list|)
expr_stmt|;
block|}
comment|// for each child, schedule splits and mark off the bucket
for|for
control|(
name|FileStatus
name|child
range|:
name|children
control|)
block|{
name|AcidOutputFormat
operator|.
name|Options
name|opts
init|=
name|AcidUtils
operator|.
name|parseBaseBucketFilename
argument_list|(
name|child
operator|.
name|getPath
argument_list|()
argument_list|,
name|context
operator|.
name|conf
argument_list|)
decl_stmt|;
name|scheduleSplits
argument_list|(
name|child
argument_list|,
name|isOriginal
argument_list|,
literal|true
argument_list|,
name|deltas
argument_list|)
expr_stmt|;
name|int
name|b
init|=
name|opts
operator|.
name|getBucket
argument_list|()
decl_stmt|;
comment|// If the bucket is in the valid range, mark it as covered.
comment|// I wish Hive actually enforced bucketing all of the time.
if|if
condition|(
name|b
operator|>=
literal|0
operator|&&
name|b
operator|<
name|covered
operator|.
name|length
condition|)
block|{
name|covered
index|[
name|b
index|]
operator|=
literal|true
expr_stmt|;
block|}
block|}
block|}
comment|// Generate a split for any buckets that weren't covered.
comment|// This happens in the case where a bucket just has deltas and no
comment|// base.
for|for
control|(
name|int
name|b
init|=
literal|0
init|;
name|b
operator|<
name|context
operator|.
name|numBuckets
condition|;
operator|++
name|b
control|)
block|{
if|if
condition|(
operator|!
name|covered
index|[
name|b
index|]
condition|)
block|{
name|context
operator|.
name|splits
operator|.
name|add
argument_list|(
operator|new
name|OrcSplit
argument_list|(
name|dir
argument_list|,
name|b
argument_list|,
literal|0
argument_list|,
operator|new
name|String
index|[
literal|0
index|]
argument_list|,
literal|null
argument_list|,
literal|false
argument_list|,
literal|false
argument_list|,
name|deltas
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
block|}
catch|catch
parameter_list|(
name|Throwable
name|th
parameter_list|)
block|{
if|if
condition|(
operator|!
operator|(
name|th
operator|instanceof
name|IOException
operator|)
condition|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Unexpected Exception"
argument_list|,
name|th
argument_list|)
expr_stmt|;
block|}
synchronized|synchronized
init|(
name|context
operator|.
name|errors
init|)
block|{
name|context
operator|.
name|errors
operator|.
name|add
argument_list|(
name|th
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
operator|!
operator|(
name|th
operator|instanceof
name|IOException
operator|)
condition|)
block|{
name|context
operator|.
name|notifyOnNonIOException
argument_list|(
name|th
argument_list|)
expr_stmt|;
block|}
block|}
finally|finally
block|{
name|context
operator|.
name|decrementSchedulers
argument_list|()
expr_stmt|;
block|}
block|}
specifier|private
name|FileInfo
name|verifyCachedFileInfo
parameter_list|(
name|FileStatus
name|file
parameter_list|)
block|{
name|context
operator|.
name|numFilesCounter
operator|.
name|incrementAndGet
argument_list|()
expr_stmt|;
name|FileInfo
name|fileInfo
init|=
name|Context
operator|.
name|footerCache
operator|.
name|getIfPresent
argument_list|(
name|file
operator|.
name|getPath
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|fileInfo
operator|!=
literal|null
condition|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Info cached for path: "
operator|+
name|file
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|fileInfo
operator|.
name|modificationTime
operator|==
name|file
operator|.
name|getModificationTime
argument_list|()
operator|&&
name|fileInfo
operator|.
name|size
operator|==
name|file
operator|.
name|getLen
argument_list|()
condition|)
block|{
comment|// Cached copy is valid
name|context
operator|.
name|cacheHitCounter
operator|.
name|incrementAndGet
argument_list|()
expr_stmt|;
return|return
name|fileInfo
return|;
block|}
else|else
block|{
comment|// Invalidate
name|Context
operator|.
name|footerCache
operator|.
name|invalidate
argument_list|(
name|file
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Meta-Info for : "
operator|+
name|file
operator|.
name|getPath
argument_list|()
operator|+
literal|" changed. CachedModificationTime: "
operator|+
name|fileInfo
operator|.
name|modificationTime
operator|+
literal|", CurrentModificationTime: "
operator|+
name|file
operator|.
name|getModificationTime
argument_list|()
operator|+
literal|", CachedLength: "
operator|+
name|fileInfo
operator|.
name|size
operator|+
literal|", CurrentLength: "
operator|+
name|file
operator|.
name|getLen
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
block|}
else|else
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Info not cached for path: "
operator|+
name|file
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
return|return
literal|null
return|;
block|}
block|}
comment|/**    * Split the stripes of a given file into input splits.    * A thread is used for each file.    */
specifier|static
specifier|final
class|class
name|SplitGenerator
implements|implements
name|Runnable
block|{
specifier|private
specifier|final
name|Context
name|context
decl_stmt|;
specifier|private
specifier|final
name|FileSystem
name|fs
decl_stmt|;
specifier|private
specifier|final
name|FileStatus
name|file
decl_stmt|;
specifier|private
specifier|final
name|long
name|blockSize
decl_stmt|;
specifier|private
specifier|final
name|BlockLocation
index|[]
name|locations
decl_stmt|;
specifier|private
specifier|final
name|FileInfo
name|fileInfo
decl_stmt|;
specifier|private
name|List
argument_list|<
name|StripeInformation
argument_list|>
name|stripes
decl_stmt|;
specifier|private
name|ReaderImpl
operator|.
name|FileMetaInfo
name|fileMetaInfo
decl_stmt|;
specifier|private
name|Metadata
name|metadata
decl_stmt|;
specifier|private
name|List
argument_list|<
name|OrcProto
operator|.
name|Type
argument_list|>
name|types
decl_stmt|;
specifier|private
specifier|final
name|boolean
name|isOriginal
decl_stmt|;
specifier|private
specifier|final
name|List
argument_list|<
name|Long
argument_list|>
name|deltas
decl_stmt|;
specifier|private
specifier|final
name|boolean
name|hasBase
decl_stmt|;
name|SplitGenerator
parameter_list|(
name|Context
name|context
parameter_list|,
name|FileSystem
name|fs
parameter_list|,
name|FileStatus
name|file
parameter_list|,
name|FileInfo
name|fileInfo
parameter_list|,
name|boolean
name|isOriginal
parameter_list|,
name|List
argument_list|<
name|Long
argument_list|>
name|deltas
parameter_list|,
name|boolean
name|hasBase
parameter_list|)
throws|throws
name|IOException
block|{
name|this
operator|.
name|context
operator|=
name|context
expr_stmt|;
name|this
operator|.
name|fs
operator|=
name|fs
expr_stmt|;
name|this
operator|.
name|file
operator|=
name|file
expr_stmt|;
name|this
operator|.
name|blockSize
operator|=
name|file
operator|.
name|getBlockSize
argument_list|()
expr_stmt|;
name|this
operator|.
name|fileInfo
operator|=
name|fileInfo
expr_stmt|;
name|locations
operator|=
name|SHIMS
operator|.
name|getLocations
argument_list|(
name|fs
argument_list|,
name|file
argument_list|)
expr_stmt|;
name|this
operator|.
name|isOriginal
operator|=
name|isOriginal
expr_stmt|;
name|this
operator|.
name|deltas
operator|=
name|deltas
expr_stmt|;
name|this
operator|.
name|hasBase
operator|=
name|hasBase
expr_stmt|;
block|}
name|Path
name|getPath
parameter_list|()
block|{
return|return
name|file
operator|.
name|getPath
argument_list|()
return|;
block|}
name|void
name|schedule
parameter_list|()
throws|throws
name|IOException
block|{
if|if
condition|(
name|locations
operator|.
name|length
operator|==
literal|1
operator|&&
name|file
operator|.
name|getLen
argument_list|()
operator|<
name|context
operator|.
name|maxSize
condition|)
block|{
name|String
index|[]
name|hosts
init|=
name|locations
index|[
literal|0
index|]
operator|.
name|getHosts
argument_list|()
decl_stmt|;
synchronized|synchronized
init|(
name|context
operator|.
name|splits
init|)
block|{
name|context
operator|.
name|splits
operator|.
name|add
argument_list|(
operator|new
name|OrcSplit
argument_list|(
name|file
operator|.
name|getPath
argument_list|()
argument_list|,
literal|0
argument_list|,
name|file
operator|.
name|getLen
argument_list|()
argument_list|,
name|hosts
argument_list|,
name|fileMetaInfo
argument_list|,
name|isOriginal
argument_list|,
name|hasBase
argument_list|,
name|deltas
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
comment|// if it requires a compute task
name|context
operator|.
name|schedule
argument_list|(
name|this
argument_list|)
expr_stmt|;
block|}
block|}
annotation|@
name|Override
specifier|public
name|String
name|toString
parameter_list|()
block|{
return|return
literal|"splitter("
operator|+
name|file
operator|.
name|getPath
argument_list|()
operator|+
literal|")"
return|;
block|}
comment|/**      * Compute the number of bytes that overlap between the two ranges.      * @param offset1 start of range1      * @param length1 length of range1      * @param offset2 start of range2      * @param length2 length of range2      * @return the number of bytes in the overlap range      */
specifier|static
name|long
name|getOverlap
parameter_list|(
name|long
name|offset1
parameter_list|,
name|long
name|length1
parameter_list|,
name|long
name|offset2
parameter_list|,
name|long
name|length2
parameter_list|)
block|{
name|long
name|end1
init|=
name|offset1
operator|+
name|length1
decl_stmt|;
name|long
name|end2
init|=
name|offset2
operator|+
name|length2
decl_stmt|;
if|if
condition|(
name|end2
operator|<=
name|offset1
operator|||
name|end1
operator|<=
name|offset2
condition|)
block|{
return|return
literal|0
return|;
block|}
else|else
block|{
return|return
name|Math
operator|.
name|min
argument_list|(
name|end1
argument_list|,
name|end2
argument_list|)
operator|-
name|Math
operator|.
name|max
argument_list|(
name|offset1
argument_list|,
name|offset2
argument_list|)
return|;
block|}
block|}
comment|/**      * Create an input split over the given range of bytes. The location of the      * split is based on where the majority of the byte are coming from. ORC      * files are unlikely to have splits that cross between blocks because they      * are written with large block sizes.      * @param offset the start of the split      * @param length the length of the split      * @param fileMetaInfo file metadata from footer and postscript      * @throws IOException      */
name|void
name|createSplit
parameter_list|(
name|long
name|offset
parameter_list|,
name|long
name|length
parameter_list|,
name|ReaderImpl
operator|.
name|FileMetaInfo
name|fileMetaInfo
parameter_list|)
throws|throws
name|IOException
block|{
name|String
index|[]
name|hosts
decl_stmt|;
if|if
condition|(
operator|(
name|offset
operator|%
name|blockSize
operator|)
operator|+
name|length
operator|<=
name|blockSize
condition|)
block|{
comment|// handle the single block case
name|hosts
operator|=
name|locations
index|[
call|(
name|int
call|)
argument_list|(
name|offset
operator|/
name|blockSize
argument_list|)
index|]
operator|.
name|getHosts
argument_list|()
expr_stmt|;
block|}
else|else
block|{
comment|// Calculate the number of bytes in the split that are local to each
comment|// host.
name|Map
argument_list|<
name|String
argument_list|,
name|LongWritable
argument_list|>
name|sizes
init|=
operator|new
name|HashMap
argument_list|<
name|String
argument_list|,
name|LongWritable
argument_list|>
argument_list|()
decl_stmt|;
name|long
name|maxSize
init|=
literal|0
decl_stmt|;
for|for
control|(
name|BlockLocation
name|block
range|:
name|locations
control|)
block|{
name|long
name|overlap
init|=
name|getOverlap
argument_list|(
name|offset
argument_list|,
name|length
argument_list|,
name|block
operator|.
name|getOffset
argument_list|()
argument_list|,
name|block
operator|.
name|getLength
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|overlap
operator|>
literal|0
condition|)
block|{
for|for
control|(
name|String
name|host
range|:
name|block
operator|.
name|getHosts
argument_list|()
control|)
block|{
name|LongWritable
name|val
init|=
name|sizes
operator|.
name|get
argument_list|(
name|host
argument_list|)
decl_stmt|;
if|if
condition|(
name|val
operator|==
literal|null
condition|)
block|{
name|val
operator|=
operator|new
name|LongWritable
argument_list|()
expr_stmt|;
name|sizes
operator|.
name|put
argument_list|(
name|host
argument_list|,
name|val
argument_list|)
expr_stmt|;
block|}
name|val
operator|.
name|set
argument_list|(
name|val
operator|.
name|get
argument_list|()
operator|+
name|overlap
argument_list|)
expr_stmt|;
name|maxSize
operator|=
name|Math
operator|.
name|max
argument_list|(
name|maxSize
argument_list|,
name|val
operator|.
name|get
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
block|}
comment|// filter the list of locations to those that have at least 80% of the
comment|// max
name|long
name|threshold
init|=
call|(
name|long
call|)
argument_list|(
name|maxSize
operator|*
name|MIN_INCLUDED_LOCATION
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|hostList
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
comment|// build the locations in a predictable order to simplify testing
for|for
control|(
name|BlockLocation
name|block
range|:
name|locations
control|)
block|{
for|for
control|(
name|String
name|host
range|:
name|block
operator|.
name|getHosts
argument_list|()
control|)
block|{
if|if
condition|(
name|sizes
operator|.
name|containsKey
argument_list|(
name|host
argument_list|)
condition|)
block|{
if|if
condition|(
name|sizes
operator|.
name|get
argument_list|(
name|host
argument_list|)
operator|.
name|get
argument_list|()
operator|>=
name|threshold
condition|)
block|{
name|hostList
operator|.
name|add
argument_list|(
name|host
argument_list|)
expr_stmt|;
block|}
name|sizes
operator|.
name|remove
argument_list|(
name|host
argument_list|)
expr_stmt|;
block|}
block|}
block|}
name|hosts
operator|=
operator|new
name|String
index|[
name|hostList
operator|.
name|size
argument_list|()
index|]
expr_stmt|;
name|hostList
operator|.
name|toArray
argument_list|(
name|hosts
argument_list|)
expr_stmt|;
block|}
synchronized|synchronized
init|(
name|context
operator|.
name|splits
init|)
block|{
name|context
operator|.
name|splits
operator|.
name|add
argument_list|(
operator|new
name|OrcSplit
argument_list|(
name|file
operator|.
name|getPath
argument_list|()
argument_list|,
name|offset
argument_list|,
name|length
argument_list|,
name|hosts
argument_list|,
name|fileMetaInfo
argument_list|,
name|isOriginal
argument_list|,
name|hasBase
argument_list|,
name|deltas
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**      * Divide the adjacent stripes in the file into input splits based on the      * block size and the configured minimum and maximum sizes.      */
annotation|@
name|Override
specifier|public
name|void
name|run
parameter_list|()
block|{
try|try
block|{
name|populateAndCacheStripeDetails
argument_list|()
expr_stmt|;
comment|// figure out which stripes we need to read
name|boolean
index|[]
name|includeStripe
init|=
literal|null
decl_stmt|;
comment|// we can't eliminate stripes if there are deltas because the
comment|// deltas may change the rows making them match the predicate.
if|if
condition|(
name|deltas
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|Reader
operator|.
name|Options
name|options
init|=
operator|new
name|Reader
operator|.
name|Options
argument_list|()
decl_stmt|;
name|setIncludedColumns
argument_list|(
name|options
argument_list|,
name|types
argument_list|,
name|context
operator|.
name|conf
argument_list|,
name|isOriginal
argument_list|)
expr_stmt|;
name|setSearchArgument
argument_list|(
name|options
argument_list|,
name|types
argument_list|,
name|context
operator|.
name|conf
argument_list|,
name|isOriginal
argument_list|)
expr_stmt|;
if|if
condition|(
name|options
operator|.
name|getSearchArgument
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|SearchArgument
name|sarg
init|=
name|options
operator|.
name|getSearchArgument
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|PredicateLeaf
argument_list|>
name|sargLeaves
init|=
name|sarg
operator|.
name|getLeaves
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|StripeStatistics
argument_list|>
name|stripeStats
init|=
name|metadata
operator|.
name|getStripeStatistics
argument_list|()
decl_stmt|;
name|int
index|[]
name|filterColumns
init|=
name|RecordReaderImpl
operator|.
name|mapSargColumns
argument_list|(
name|sargLeaves
argument_list|,
name|options
operator|.
name|getColumnNames
argument_list|()
argument_list|,
name|getRootColumn
argument_list|(
name|isOriginal
argument_list|)
argument_list|)
decl_stmt|;
if|if
condition|(
name|stripeStats
operator|!=
literal|null
condition|)
block|{
comment|// eliminate stripes that doesn't satisfy the predicate condition
name|includeStripe
operator|=
operator|new
name|boolean
index|[
name|stripes
operator|.
name|size
argument_list|()
index|]
expr_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|stripes
operator|.
name|size
argument_list|()
condition|;
operator|++
name|i
control|)
block|{
name|includeStripe
index|[
name|i
index|]
operator|=
operator|(
name|i
operator|>=
name|stripeStats
operator|.
name|size
argument_list|()
operator|)
operator|||
name|isStripeSatisfyPredicate
argument_list|(
name|stripeStats
operator|.
name|get
argument_list|(
name|i
argument_list|)
argument_list|,
name|sarg
argument_list|,
name|filterColumns
argument_list|)
expr_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
operator|&&
operator|!
name|includeStripe
index|[
name|i
index|]
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Eliminating ORC stripe-"
operator|+
name|i
operator|+
literal|" of file '"
operator|+
name|file
operator|.
name|getPath
argument_list|()
operator|+
literal|"'  as it did not satisfy "
operator|+
literal|"predicate condition."
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
block|}
comment|// if we didn't have predicate pushdown, read everything
if|if
condition|(
name|includeStripe
operator|==
literal|null
condition|)
block|{
name|includeStripe
operator|=
operator|new
name|boolean
index|[
name|stripes
operator|.
name|size
argument_list|()
index|]
expr_stmt|;
name|Arrays
operator|.
name|fill
argument_list|(
name|includeStripe
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
name|long
name|currentOffset
init|=
operator|-
literal|1
decl_stmt|;
name|long
name|currentLength
init|=
literal|0
decl_stmt|;
name|int
name|idx
init|=
operator|-
literal|1
decl_stmt|;
for|for
control|(
name|StripeInformation
name|stripe
range|:
name|stripes
control|)
block|{
name|idx
operator|++
expr_stmt|;
if|if
condition|(
operator|!
name|includeStripe
index|[
name|idx
index|]
condition|)
block|{
comment|// create split for the previous unfinished stripe
if|if
condition|(
name|currentOffset
operator|!=
operator|-
literal|1
condition|)
block|{
name|createSplit
argument_list|(
name|currentOffset
argument_list|,
name|currentLength
argument_list|,
name|fileMetaInfo
argument_list|)
expr_stmt|;
name|currentOffset
operator|=
operator|-
literal|1
expr_stmt|;
block|}
continue|continue;
block|}
comment|// if we are working on a stripe, over the min stripe size, and
comment|// crossed a block boundary, cut the input split here.
if|if
condition|(
name|currentOffset
operator|!=
operator|-
literal|1
operator|&&
name|currentLength
operator|>
name|context
operator|.
name|minSize
operator|&&
operator|(
name|currentOffset
operator|/
name|blockSize
operator|!=
name|stripe
operator|.
name|getOffset
argument_list|()
operator|/
name|blockSize
operator|)
condition|)
block|{
name|createSplit
argument_list|(
name|currentOffset
argument_list|,
name|currentLength
argument_list|,
name|fileMetaInfo
argument_list|)
expr_stmt|;
name|currentOffset
operator|=
operator|-
literal|1
expr_stmt|;
block|}
comment|// if we aren't building a split, start a new one.
if|if
condition|(
name|currentOffset
operator|==
operator|-
literal|1
condition|)
block|{
name|currentOffset
operator|=
name|stripe
operator|.
name|getOffset
argument_list|()
expr_stmt|;
name|currentLength
operator|=
name|stripe
operator|.
name|getLength
argument_list|()
expr_stmt|;
block|}
else|else
block|{
name|currentLength
operator|=
operator|(
name|stripe
operator|.
name|getOffset
argument_list|()
operator|+
name|stripe
operator|.
name|getLength
argument_list|()
operator|)
operator|-
name|currentOffset
expr_stmt|;
block|}
if|if
condition|(
name|currentLength
operator|>=
name|context
operator|.
name|maxSize
condition|)
block|{
name|createSplit
argument_list|(
name|currentOffset
argument_list|,
name|currentLength
argument_list|,
name|fileMetaInfo
argument_list|)
expr_stmt|;
name|currentOffset
operator|=
operator|-
literal|1
expr_stmt|;
block|}
block|}
if|if
condition|(
name|currentOffset
operator|!=
operator|-
literal|1
condition|)
block|{
name|createSplit
argument_list|(
name|currentOffset
argument_list|,
name|currentLength
argument_list|,
name|fileMetaInfo
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|Throwable
name|th
parameter_list|)
block|{
if|if
condition|(
operator|!
operator|(
name|th
operator|instanceof
name|IOException
operator|)
condition|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Unexpected Exception"
argument_list|,
name|th
argument_list|)
expr_stmt|;
block|}
synchronized|synchronized
init|(
name|context
operator|.
name|errors
init|)
block|{
name|context
operator|.
name|errors
operator|.
name|add
argument_list|(
name|th
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
operator|!
operator|(
name|th
operator|instanceof
name|IOException
operator|)
condition|)
block|{
name|context
operator|.
name|notifyOnNonIOException
argument_list|(
name|th
argument_list|)
expr_stmt|;
block|}
block|}
finally|finally
block|{
name|context
operator|.
name|decrementSchedulers
argument_list|()
expr_stmt|;
block|}
block|}
specifier|private
name|void
name|populateAndCacheStripeDetails
parameter_list|()
block|{
try|try
block|{
name|Reader
name|orcReader
decl_stmt|;
if|if
condition|(
name|fileInfo
operator|!=
literal|null
condition|)
block|{
name|stripes
operator|=
name|fileInfo
operator|.
name|stripeInfos
expr_stmt|;
name|fileMetaInfo
operator|=
name|fileInfo
operator|.
name|fileMetaInfo
expr_stmt|;
name|metadata
operator|=
name|fileInfo
operator|.
name|metadata
expr_stmt|;
name|types
operator|=
name|fileInfo
operator|.
name|types
expr_stmt|;
comment|// For multiple runs, in case sendSplitsInFooter changes
if|if
condition|(
name|fileMetaInfo
operator|==
literal|null
operator|&&
name|context
operator|.
name|footerInSplits
condition|)
block|{
name|orcReader
operator|=
name|OrcFile
operator|.
name|createReader
argument_list|(
name|file
operator|.
name|getPath
argument_list|()
argument_list|,
name|OrcFile
operator|.
name|readerOptions
argument_list|(
name|context
operator|.
name|conf
argument_list|)
operator|.
name|filesystem
argument_list|(
name|fs
argument_list|)
argument_list|)
expr_stmt|;
name|fileInfo
operator|.
name|fileMetaInfo
operator|=
operator|(
operator|(
name|ReaderImpl
operator|)
name|orcReader
operator|)
operator|.
name|getFileMetaInfo
argument_list|()
expr_stmt|;
name|fileInfo
operator|.
name|metadata
operator|=
name|orcReader
operator|.
name|getMetadata
argument_list|()
expr_stmt|;
name|fileInfo
operator|.
name|types
operator|=
name|orcReader
operator|.
name|getTypes
argument_list|()
expr_stmt|;
block|}
block|}
else|else
block|{
name|orcReader
operator|=
name|OrcFile
operator|.
name|createReader
argument_list|(
name|file
operator|.
name|getPath
argument_list|()
argument_list|,
name|OrcFile
operator|.
name|readerOptions
argument_list|(
name|context
operator|.
name|conf
argument_list|)
operator|.
name|filesystem
argument_list|(
name|fs
argument_list|)
argument_list|)
expr_stmt|;
name|stripes
operator|=
name|orcReader
operator|.
name|getStripes
argument_list|()
expr_stmt|;
name|metadata
operator|=
name|orcReader
operator|.
name|getMetadata
argument_list|()
expr_stmt|;
name|types
operator|=
name|orcReader
operator|.
name|getTypes
argument_list|()
expr_stmt|;
name|fileMetaInfo
operator|=
name|context
operator|.
name|footerInSplits
condition|?
operator|(
operator|(
name|ReaderImpl
operator|)
name|orcReader
operator|)
operator|.
name|getFileMetaInfo
argument_list|()
else|:
literal|null
expr_stmt|;
if|if
condition|(
name|context
operator|.
name|cacheStripeDetails
condition|)
block|{
comment|// Populate into cache.
name|Context
operator|.
name|footerCache
operator|.
name|put
argument_list|(
name|file
operator|.
name|getPath
argument_list|()
argument_list|,
operator|new
name|FileInfo
argument_list|(
name|file
operator|.
name|getModificationTime
argument_list|()
argument_list|,
name|file
operator|.
name|getLen
argument_list|()
argument_list|,
name|stripes
argument_list|,
name|metadata
argument_list|,
name|types
argument_list|,
name|fileMetaInfo
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
block|}
catch|catch
parameter_list|(
name|Throwable
name|th
parameter_list|)
block|{
if|if
condition|(
operator|!
operator|(
name|th
operator|instanceof
name|IOException
operator|)
condition|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Unexpected Exception"
argument_list|,
name|th
argument_list|)
expr_stmt|;
block|}
synchronized|synchronized
init|(
name|context
operator|.
name|errors
init|)
block|{
name|context
operator|.
name|errors
operator|.
name|add
argument_list|(
name|th
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
operator|!
operator|(
name|th
operator|instanceof
name|IOException
operator|)
condition|)
block|{
name|context
operator|.
name|notifyOnNonIOException
argument_list|(
name|th
argument_list|)
expr_stmt|;
block|}
block|}
block|}
specifier|private
name|boolean
name|isStripeSatisfyPredicate
parameter_list|(
name|StripeStatistics
name|stripeStatistics
parameter_list|,
name|SearchArgument
name|sarg
parameter_list|,
name|int
index|[]
name|filterColumns
parameter_list|)
block|{
name|List
argument_list|<
name|PredicateLeaf
argument_list|>
name|predLeaves
init|=
name|sarg
operator|.
name|getLeaves
argument_list|()
decl_stmt|;
name|TruthValue
index|[]
name|truthValues
init|=
operator|new
name|TruthValue
index|[
name|predLeaves
operator|.
name|size
argument_list|()
index|]
decl_stmt|;
for|for
control|(
name|int
name|pred
init|=
literal|0
init|;
name|pred
operator|<
name|truthValues
operator|.
name|length
condition|;
name|pred
operator|++
control|)
block|{
if|if
condition|(
name|filterColumns
index|[
name|pred
index|]
operator|!=
operator|-
literal|1
condition|)
block|{
comment|// column statistics at index 0 contains only the number of rows
name|ColumnStatistics
name|stats
init|=
name|stripeStatistics
operator|.
name|getColumnStatistics
argument_list|()
index|[
name|filterColumns
index|[
name|pred
index|]
index|]
decl_stmt|;
name|Object
name|minValue
init|=
name|RecordReaderImpl
operator|.
name|getMin
argument_list|(
name|stats
argument_list|)
decl_stmt|;
name|Object
name|maxValue
init|=
name|RecordReaderImpl
operator|.
name|getMax
argument_list|(
name|stats
argument_list|)
decl_stmt|;
name|truthValues
index|[
name|pred
index|]
operator|=
name|RecordReaderImpl
operator|.
name|evaluatePredicateRange
argument_list|(
name|predLeaves
operator|.
name|get
argument_list|(
name|pred
argument_list|)
argument_list|,
name|minValue
argument_list|,
name|maxValue
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// parition column case.
comment|// partition filter will be evaluated by partition pruner so
comment|// we will not evaluate partition filter here.
name|truthValues
index|[
name|pred
index|]
operator|=
name|TruthValue
operator|.
name|YES_NO_NULL
expr_stmt|;
block|}
block|}
return|return
name|sarg
operator|.
name|evaluate
argument_list|(
name|truthValues
argument_list|)
operator|.
name|isNeeded
argument_list|()
return|;
block|}
block|}
specifier|static
name|List
argument_list|<
name|OrcSplit
argument_list|>
name|generateSplitsInfo
parameter_list|(
name|Configuration
name|conf
parameter_list|)
throws|throws
name|IOException
block|{
comment|// use threads to resolve directories into splits
name|Context
name|context
init|=
operator|new
name|Context
argument_list|(
name|conf
argument_list|)
decl_stmt|;
for|for
control|(
name|Path
name|dir
range|:
name|getInputPaths
argument_list|(
name|conf
argument_list|)
control|)
block|{
name|FileSystem
name|fs
init|=
name|dir
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|context
operator|.
name|schedule
argument_list|(
operator|new
name|FileGenerator
argument_list|(
name|context
argument_list|,
name|fs
argument_list|,
name|dir
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|context
operator|.
name|waitForTasks
argument_list|()
expr_stmt|;
comment|// deal with exceptions
if|if
condition|(
operator|!
name|context
operator|.
name|errors
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|List
argument_list|<
name|IOException
argument_list|>
name|errors
init|=
operator|new
name|ArrayList
argument_list|<
name|IOException
argument_list|>
argument_list|(
name|context
operator|.
name|errors
operator|.
name|size
argument_list|()
argument_list|)
decl_stmt|;
for|for
control|(
name|Throwable
name|th
range|:
name|context
operator|.
name|errors
control|)
block|{
if|if
condition|(
name|th
operator|instanceof
name|IOException
condition|)
block|{
name|errors
operator|.
name|add
argument_list|(
operator|(
name|IOException
operator|)
name|th
argument_list|)
expr_stmt|;
block|}
else|else
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
literal|"serious problem"
argument_list|,
name|th
argument_list|)
throw|;
block|}
block|}
throw|throw
operator|new
name|InvalidInputException
argument_list|(
name|errors
argument_list|)
throw|;
block|}
if|if
condition|(
name|context
operator|.
name|cacheStripeDetails
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"FooterCacheHitRatio: "
operator|+
name|context
operator|.
name|cacheHitCounter
operator|.
name|get
argument_list|()
operator|+
literal|"/"
operator|+
name|context
operator|.
name|numFilesCounter
operator|.
name|get
argument_list|()
argument_list|)
expr_stmt|;
block|}
return|return
name|context
operator|.
name|splits
return|;
block|}
annotation|@
name|Override
specifier|public
name|InputSplit
index|[]
name|getSplits
parameter_list|(
name|JobConf
name|job
parameter_list|,
name|int
name|numSplits
parameter_list|)
throws|throws
name|IOException
block|{
name|perfLogger
operator|.
name|PerfLogBegin
argument_list|(
name|CLASS_NAME
argument_list|,
name|PerfLogger
operator|.
name|ORC_GET_SPLITS
argument_list|)
expr_stmt|;
name|List
argument_list|<
name|OrcSplit
argument_list|>
name|result
init|=
name|generateSplitsInfo
argument_list|(
name|job
argument_list|)
decl_stmt|;
name|perfLogger
operator|.
name|PerfLogEnd
argument_list|(
name|CLASS_NAME
argument_list|,
name|PerfLogger
operator|.
name|ORC_GET_SPLITS
argument_list|)
expr_stmt|;
return|return
name|result
operator|.
name|toArray
argument_list|(
operator|new
name|InputSplit
index|[
name|result
operator|.
name|size
argument_list|()
index|]
argument_list|)
return|;
block|}
comment|/**    * FileInfo.    *    * Stores information relevant to split generation for an ORC File.    *    */
specifier|private
specifier|static
class|class
name|FileInfo
block|{
name|long
name|modificationTime
decl_stmt|;
name|long
name|size
decl_stmt|;
name|List
argument_list|<
name|StripeInformation
argument_list|>
name|stripeInfos
decl_stmt|;
name|ReaderImpl
operator|.
name|FileMetaInfo
name|fileMetaInfo
decl_stmt|;
name|Metadata
name|metadata
decl_stmt|;
name|List
argument_list|<
name|OrcProto
operator|.
name|Type
argument_list|>
name|types
decl_stmt|;
name|FileInfo
parameter_list|(
name|long
name|modificationTime
parameter_list|,
name|long
name|size
parameter_list|,
name|List
argument_list|<
name|StripeInformation
argument_list|>
name|stripeInfos
parameter_list|,
name|Metadata
name|metadata
parameter_list|,
name|List
argument_list|<
name|OrcProto
operator|.
name|Type
argument_list|>
name|types
parameter_list|,
name|ReaderImpl
operator|.
name|FileMetaInfo
name|fileMetaInfo
parameter_list|)
block|{
name|this
operator|.
name|modificationTime
operator|=
name|modificationTime
expr_stmt|;
name|this
operator|.
name|size
operator|=
name|size
expr_stmt|;
name|this
operator|.
name|stripeInfos
operator|=
name|stripeInfos
expr_stmt|;
name|this
operator|.
name|fileMetaInfo
operator|=
name|fileMetaInfo
expr_stmt|;
name|this
operator|.
name|metadata
operator|=
name|metadata
expr_stmt|;
name|this
operator|.
name|types
operator|=
name|types
expr_stmt|;
block|}
block|}
annotation|@
name|SuppressWarnings
argument_list|(
literal|"unchecked"
argument_list|)
specifier|private
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|RecordReader
argument_list|<
name|NullWritable
argument_list|,
name|OrcStruct
argument_list|>
name|createVectorizedReader
parameter_list|(
name|InputSplit
name|split
parameter_list|,
name|JobConf
name|conf
parameter_list|,
name|Reporter
name|reporter
parameter_list|)
throws|throws
name|IOException
block|{
return|return
operator|(
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|RecordReader
operator|)
operator|new
name|VectorizedOrcInputFormat
argument_list|()
operator|.
name|getRecordReader
argument_list|(
name|split
argument_list|,
name|conf
argument_list|,
name|reporter
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|public
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|RecordReader
argument_list|<
name|NullWritable
argument_list|,
name|OrcStruct
argument_list|>
name|getRecordReader
parameter_list|(
name|InputSplit
name|inputSplit
parameter_list|,
name|JobConf
name|conf
parameter_list|,
name|Reporter
name|reporter
parameter_list|)
throws|throws
name|IOException
block|{
name|boolean
name|vectorMode
init|=
name|Utilities
operator|.
name|isVectorMode
argument_list|(
name|conf
argument_list|)
decl_stmt|;
comment|// if HiveCombineInputFormat gives us FileSplits instead of OrcSplits,
comment|// we know it is not ACID.
if|if
condition|(
name|inputSplit
operator|.
name|getClass
argument_list|()
operator|==
name|FileSplit
operator|.
name|class
condition|)
block|{
if|if
condition|(
name|vectorMode
condition|)
block|{
return|return
name|createVectorizedReader
argument_list|(
name|inputSplit
argument_list|,
name|conf
argument_list|,
name|reporter
argument_list|)
return|;
block|}
return|return
operator|new
name|OrcRecordReader
argument_list|(
name|OrcFile
operator|.
name|createReader
argument_list|(
operator|(
operator|(
name|FileSplit
operator|)
name|inputSplit
operator|)
operator|.
name|getPath
argument_list|()
argument_list|,
name|OrcFile
operator|.
name|readerOptions
argument_list|(
name|conf
argument_list|)
argument_list|)
argument_list|,
name|conf
argument_list|,
operator|(
name|FileSplit
operator|)
name|inputSplit
argument_list|)
return|;
block|}
name|OrcSplit
name|split
init|=
operator|(
name|OrcSplit
operator|)
name|inputSplit
decl_stmt|;
comment|// TODO vectorized reader doesn't work with the new format yet
if|if
condition|(
name|vectorMode
condition|)
block|{
if|if
condition|(
operator|!
name|split
operator|.
name|getDeltas
argument_list|()
operator|.
name|isEmpty
argument_list|()
operator|||
operator|!
name|split
operator|.
name|isOriginal
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Vectorization and ACID tables are incompatible."
argument_list|)
throw|;
block|}
return|return
name|createVectorizedReader
argument_list|(
name|inputSplit
argument_list|,
name|conf
argument_list|,
name|reporter
argument_list|)
return|;
block|}
name|reporter
operator|.
name|setStatus
argument_list|(
name|inputSplit
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
comment|// if we are strictly old-school, just use the old code
if|if
condition|(
name|split
operator|.
name|isOriginal
argument_list|()
operator|&&
name|split
operator|.
name|getDeltas
argument_list|()
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
return|return
operator|new
name|OrcRecordReader
argument_list|(
name|OrcFile
operator|.
name|createReader
argument_list|(
name|split
operator|.
name|getPath
argument_list|()
argument_list|,
name|OrcFile
operator|.
name|readerOptions
argument_list|(
name|conf
argument_list|)
argument_list|)
argument_list|,
name|conf
argument_list|,
name|split
argument_list|)
return|;
block|}
name|Options
name|options
init|=
operator|new
name|Options
argument_list|(
name|conf
argument_list|)
operator|.
name|reporter
argument_list|(
name|reporter
argument_list|)
decl_stmt|;
specifier|final
name|RowReader
argument_list|<
name|OrcStruct
argument_list|>
name|inner
init|=
name|getReader
argument_list|(
name|inputSplit
argument_list|,
name|options
argument_list|)
decl_stmt|;
specifier|final
name|RecordIdentifier
name|id
init|=
name|inner
operator|.
name|createKey
argument_list|()
decl_stmt|;
comment|// Return a RecordReader that is compatible with the Hive 0.12 reader
comment|// with NullWritable for the key instead of RecordIdentifier.
return|return
operator|new
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|RecordReader
argument_list|<
name|NullWritable
argument_list|,
name|OrcStruct
argument_list|>
argument_list|()
block|{
annotation|@
name|Override
specifier|public
name|boolean
name|next
parameter_list|(
name|NullWritable
name|nullWritable
parameter_list|,
name|OrcStruct
name|orcStruct
parameter_list|)
throws|throws
name|IOException
block|{
return|return
name|inner
operator|.
name|next
argument_list|(
name|id
argument_list|,
name|orcStruct
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|public
name|NullWritable
name|createKey
parameter_list|()
block|{
return|return
name|NullWritable
operator|.
name|get
argument_list|()
return|;
block|}
annotation|@
name|Override
specifier|public
name|OrcStruct
name|createValue
parameter_list|()
block|{
return|return
name|inner
operator|.
name|createValue
argument_list|()
return|;
block|}
annotation|@
name|Override
specifier|public
name|long
name|getPos
parameter_list|()
throws|throws
name|IOException
block|{
return|return
name|inner
operator|.
name|getPos
argument_list|()
return|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|close
parameter_list|()
throws|throws
name|IOException
block|{
name|inner
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|float
name|getProgress
parameter_list|()
throws|throws
name|IOException
block|{
return|return
name|inner
operator|.
name|getProgress
argument_list|()
return|;
block|}
block|}
return|;
block|}
annotation|@
name|Override
specifier|public
name|RowReader
argument_list|<
name|OrcStruct
argument_list|>
name|getReader
parameter_list|(
name|InputSplit
name|inputSplit
parameter_list|,
name|Options
name|options
parameter_list|)
throws|throws
name|IOException
block|{
specifier|final
name|OrcSplit
name|split
init|=
operator|(
name|OrcSplit
operator|)
name|inputSplit
decl_stmt|;
specifier|final
name|Path
name|path
init|=
name|split
operator|.
name|getPath
argument_list|()
decl_stmt|;
name|Path
name|root
decl_stmt|;
if|if
condition|(
name|split
operator|.
name|hasBase
argument_list|()
condition|)
block|{
if|if
condition|(
name|split
operator|.
name|isOriginal
argument_list|()
condition|)
block|{
name|root
operator|=
name|path
operator|.
name|getParent
argument_list|()
expr_stmt|;
block|}
else|else
block|{
name|root
operator|=
name|path
operator|.
name|getParent
argument_list|()
operator|.
name|getParent
argument_list|()
expr_stmt|;
block|}
block|}
else|else
block|{
name|root
operator|=
name|path
expr_stmt|;
block|}
specifier|final
name|Path
index|[]
name|deltas
init|=
name|AcidUtils
operator|.
name|deserializeDeltas
argument_list|(
name|root
argument_list|,
name|split
operator|.
name|getDeltas
argument_list|()
argument_list|)
decl_stmt|;
specifier|final
name|Configuration
name|conf
init|=
name|options
operator|.
name|getConfiguration
argument_list|()
decl_stmt|;
specifier|final
name|Reader
name|reader
decl_stmt|;
specifier|final
name|int
name|bucket
decl_stmt|;
name|Reader
operator|.
name|Options
name|readOptions
init|=
operator|new
name|Reader
operator|.
name|Options
argument_list|()
decl_stmt|;
name|readOptions
operator|.
name|range
argument_list|(
name|split
operator|.
name|getStart
argument_list|()
argument_list|,
name|split
operator|.
name|getLength
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|split
operator|.
name|hasBase
argument_list|()
condition|)
block|{
name|bucket
operator|=
name|AcidUtils
operator|.
name|parseBaseBucketFilename
argument_list|(
name|split
operator|.
name|getPath
argument_list|()
argument_list|,
name|conf
argument_list|)
operator|.
name|getBucket
argument_list|()
expr_stmt|;
name|reader
operator|=
name|OrcFile
operator|.
name|createReader
argument_list|(
name|path
argument_list|,
name|OrcFile
operator|.
name|readerOptions
argument_list|(
name|conf
argument_list|)
argument_list|)
expr_stmt|;
specifier|final
name|List
argument_list|<
name|OrcProto
operator|.
name|Type
argument_list|>
name|types
init|=
name|reader
operator|.
name|getTypes
argument_list|()
decl_stmt|;
name|setIncludedColumns
argument_list|(
name|readOptions
argument_list|,
name|types
argument_list|,
name|conf
argument_list|,
name|split
operator|.
name|isOriginal
argument_list|()
argument_list|)
expr_stmt|;
name|setSearchArgument
argument_list|(
name|readOptions
argument_list|,
name|types
argument_list|,
name|conf
argument_list|,
name|split
operator|.
name|isOriginal
argument_list|()
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|bucket
operator|=
operator|(
name|int
operator|)
name|split
operator|.
name|getStart
argument_list|()
expr_stmt|;
name|reader
operator|=
literal|null
expr_stmt|;
block|}
name|String
name|txnString
init|=
name|conf
operator|.
name|get
argument_list|(
name|ValidTxnList
operator|.
name|VALID_TXNS_KEY
argument_list|,
name|Long
operator|.
name|MAX_VALUE
operator|+
literal|":"
argument_list|)
decl_stmt|;
name|ValidTxnList
name|validTxnList
init|=
operator|new
name|ValidTxnListImpl
argument_list|(
name|txnString
argument_list|)
decl_stmt|;
specifier|final
name|OrcRawRecordMerger
name|records
init|=
operator|new
name|OrcRawRecordMerger
argument_list|(
name|conf
argument_list|,
literal|true
argument_list|,
name|reader
argument_list|,
name|split
operator|.
name|isOriginal
argument_list|()
argument_list|,
name|bucket
argument_list|,
name|validTxnList
argument_list|,
name|readOptions
argument_list|,
name|deltas
argument_list|)
decl_stmt|;
return|return
operator|new
name|RowReader
argument_list|<
name|OrcStruct
argument_list|>
argument_list|()
block|{
name|OrcStruct
name|innerRecord
init|=
name|records
operator|.
name|createValue
argument_list|()
decl_stmt|;
annotation|@
name|Override
specifier|public
name|ObjectInspector
name|getObjectInspector
parameter_list|()
block|{
return|return
operator|(
operator|(
name|StructObjectInspector
operator|)
name|reader
operator|.
name|getObjectInspector
argument_list|()
operator|)
operator|.
name|getAllStructFieldRefs
argument_list|()
operator|.
name|get
argument_list|(
name|OrcRecordUpdater
operator|.
name|ROW
argument_list|)
operator|.
name|getFieldObjectInspector
argument_list|()
return|;
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|next
parameter_list|(
name|RecordIdentifier
name|recordIdentifier
parameter_list|,
name|OrcStruct
name|orcStruct
parameter_list|)
throws|throws
name|IOException
block|{
name|boolean
name|result
decl_stmt|;
comment|// filter out the deleted records
do|do
block|{
name|result
operator|=
name|records
operator|.
name|next
argument_list|(
name|recordIdentifier
argument_list|,
name|innerRecord
argument_list|)
expr_stmt|;
block|}
do|while
condition|(
name|result
operator|&&
name|OrcRecordUpdater
operator|.
name|getOperation
argument_list|(
name|innerRecord
argument_list|)
operator|==
name|OrcRecordUpdater
operator|.
name|DELETE_OPERATION
condition|)
do|;
if|if
condition|(
name|result
condition|)
block|{
comment|// swap the fields with the passed in orcStruct
name|orcStruct
operator|.
name|linkFields
argument_list|(
name|OrcRecordUpdater
operator|.
name|getRow
argument_list|(
name|innerRecord
argument_list|)
argument_list|)
expr_stmt|;
block|}
return|return
name|result
return|;
block|}
annotation|@
name|Override
specifier|public
name|RecordIdentifier
name|createKey
parameter_list|()
block|{
return|return
name|records
operator|.
name|createKey
argument_list|()
return|;
block|}
annotation|@
name|Override
specifier|public
name|OrcStruct
name|createValue
parameter_list|()
block|{
return|return
operator|new
name|OrcStruct
argument_list|(
name|records
operator|.
name|getColumns
argument_list|()
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|public
name|long
name|getPos
parameter_list|()
throws|throws
name|IOException
block|{
return|return
name|records
operator|.
name|getPos
argument_list|()
return|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|close
parameter_list|()
throws|throws
name|IOException
block|{
name|records
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|float
name|getProgress
parameter_list|()
throws|throws
name|IOException
block|{
return|return
name|records
operator|.
name|getProgress
argument_list|()
return|;
block|}
block|}
return|;
block|}
specifier|static
name|Path
name|findOriginalBucket
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|Path
name|directory
parameter_list|,
name|int
name|bucket
parameter_list|)
throws|throws
name|IOException
block|{
for|for
control|(
name|FileStatus
name|stat
range|:
name|fs
operator|.
name|listStatus
argument_list|(
name|directory
argument_list|)
control|)
block|{
name|String
name|name
init|=
name|stat
operator|.
name|getPath
argument_list|()
operator|.
name|getName
argument_list|()
decl_stmt|;
if|if
condition|(
name|Integer
operator|.
name|parseInt
argument_list|(
name|name
operator|.
name|substring
argument_list|(
literal|0
argument_list|,
name|name
operator|.
name|indexOf
argument_list|(
literal|'_'
argument_list|)
argument_list|)
argument_list|)
operator|==
name|bucket
condition|)
block|{
return|return
name|stat
operator|.
name|getPath
argument_list|()
return|;
block|}
block|}
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
literal|"Can't find bucket "
operator|+
name|bucket
operator|+
literal|" in "
operator|+
name|directory
argument_list|)
throw|;
block|}
annotation|@
name|Override
specifier|public
name|RawReader
argument_list|<
name|OrcStruct
argument_list|>
name|getRawReader
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|boolean
name|collapseEvents
parameter_list|,
name|int
name|bucket
parameter_list|,
name|ValidTxnList
name|validTxnList
parameter_list|,
name|Path
name|baseDirectory
parameter_list|,
name|Path
index|[]
name|deltaDirectory
parameter_list|)
throws|throws
name|IOException
block|{
name|Reader
name|reader
init|=
literal|null
decl_stmt|;
name|boolean
name|isOriginal
init|=
literal|false
decl_stmt|;
if|if
condition|(
name|baseDirectory
operator|!=
literal|null
condition|)
block|{
name|Path
name|bucketFile
decl_stmt|;
if|if
condition|(
name|baseDirectory
operator|.
name|getName
argument_list|()
operator|.
name|startsWith
argument_list|(
name|AcidUtils
operator|.
name|BASE_PREFIX
argument_list|)
condition|)
block|{
name|bucketFile
operator|=
name|AcidUtils
operator|.
name|createBucketFile
argument_list|(
name|baseDirectory
argument_list|,
name|bucket
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|isOriginal
operator|=
literal|true
expr_stmt|;
name|bucketFile
operator|=
name|findOriginalBucket
argument_list|(
name|baseDirectory
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
argument_list|,
name|baseDirectory
argument_list|,
name|bucket
argument_list|)
expr_stmt|;
block|}
name|reader
operator|=
name|OrcFile
operator|.
name|createReader
argument_list|(
name|bucketFile
argument_list|,
name|OrcFile
operator|.
name|readerOptions
argument_list|(
name|conf
argument_list|)
argument_list|)
expr_stmt|;
block|}
return|return
operator|new
name|OrcRawRecordMerger
argument_list|(
name|conf
argument_list|,
name|collapseEvents
argument_list|,
name|reader
argument_list|,
name|isOriginal
argument_list|,
name|bucket
argument_list|,
name|validTxnList
argument_list|,
operator|new
name|Reader
operator|.
name|Options
argument_list|()
argument_list|,
name|deltaDirectory
argument_list|)
return|;
block|}
block|}
end_class

end_unit

