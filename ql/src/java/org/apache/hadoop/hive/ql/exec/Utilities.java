begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/**  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
package|;
end_package

begin_import
import|import
name|java
operator|.
name|beans
operator|.
name|DefaultPersistenceDelegate
import|;
end_import

begin_import
import|import
name|java
operator|.
name|beans
operator|.
name|Encoder
import|;
end_import

begin_import
import|import
name|java
operator|.
name|beans
operator|.
name|ExceptionListener
import|;
end_import

begin_import
import|import
name|java
operator|.
name|beans
operator|.
name|Expression
import|;
end_import

begin_import
import|import
name|java
operator|.
name|beans
operator|.
name|PersistenceDelegate
import|;
end_import

begin_import
import|import
name|java
operator|.
name|beans
operator|.
name|Statement
import|;
end_import

begin_import
import|import
name|java
operator|.
name|beans
operator|.
name|XMLDecoder
import|;
end_import

begin_import
import|import
name|java
operator|.
name|beans
operator|.
name|XMLEncoder
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|BufferedReader
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|ByteArrayInputStream
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|ByteArrayOutputStream
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|DataInput
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|EOFException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|File
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|FileInputStream
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|FileNotFoundException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|InputStream
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|InputStreamReader
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|OutputStream
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|PrintStream
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|Serializable
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|UnsupportedEncodingException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|net
operator|.
name|URI
import|;
end_import

begin_import
import|import
name|java
operator|.
name|net
operator|.
name|URL
import|;
end_import

begin_import
import|import
name|java
operator|.
name|net
operator|.
name|URLClassLoader
import|;
end_import

begin_import
import|import
name|java
operator|.
name|security
operator|.
name|MessageDigest
import|;
end_import

begin_import
import|import
name|java
operator|.
name|security
operator|.
name|NoSuchAlgorithmException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|sql
operator|.
name|Connection
import|;
end_import

begin_import
import|import
name|java
operator|.
name|sql
operator|.
name|DriverManager
import|;
end_import

begin_import
import|import
name|java
operator|.
name|sql
operator|.
name|PreparedStatement
import|;
end_import

begin_import
import|import
name|java
operator|.
name|sql
operator|.
name|SQLException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|sql
operator|.
name|SQLTransientException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|sql
operator|.
name|Timestamp
import|;
end_import

begin_import
import|import
name|java
operator|.
name|text
operator|.
name|SimpleDateFormat
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Arrays
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Calendar
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Collection
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Collections
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Date
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashSet
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Iterator
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|LinkedHashMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|LinkedList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Properties
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Random
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Set
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|UUID
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|ConcurrentHashMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|ExecutionException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|Future
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|LinkedBlockingQueue
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|ThreadPoolExecutor
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|TimeUnit
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|regex
operator|.
name|Matcher
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|regex
operator|.
name|Pattern
import|;
end_import

begin_import
import|import
name|org
operator|.
name|antlr
operator|.
name|runtime
operator|.
name|CommonToken
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|codec
operator|.
name|binary
operator|.
name|Base64
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|lang
operator|.
name|StringUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|lang
operator|.
name|WordUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|Log
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|LogFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configuration
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|filecache
operator|.
name|DistributedCache
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|ContentSummary
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileStatus
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|PathFilter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|HiveInterruptCallback
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|HiveInterruptUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|HiveStatsUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
operator|.
name|ConfVars
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|Warehouse
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|FieldSchema
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Order
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|hive_metastoreConstants
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|Context
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|ErrorMsg
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|QueryPlan
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|mr
operator|.
name|ExecDriver
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|mr
operator|.
name|ExecMapper
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|mr
operator|.
name|ExecReducer
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|mr
operator|.
name|MapRedTask
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|tez
operator|.
name|TezTask
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|ContentSummaryInputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|FSRecordWriter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|HiveFileFormatUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|HiveIgnoreKeyTextOutputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|HiveInputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|HiveOutputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|HiveSequenceFileOutputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|OneNullRowInputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|RCFile
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|ReworkMapredInputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|rcfile
operator|.
name|merge
operator|.
name|MergeWork
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|rcfile
operator|.
name|merge
operator|.
name|RCFileMergeMapper
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|rcfile
operator|.
name|stats
operator|.
name|PartialScanMapper
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|rcfile
operator|.
name|stats
operator|.
name|PartialScanWork
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|rcfile
operator|.
name|truncate
operator|.
name|ColumnTruncateMapper
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|rcfile
operator|.
name|truncate
operator|.
name|ColumnTruncateWork
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|log
operator|.
name|PerfLogger
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|HiveException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|HiveStorageHandler
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|HiveUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|InputEstimator
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|Partition
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|Table
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|parse
operator|.
name|SemanticException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|BaseWork
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|DynamicPartitionCtx
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|ExprNodeGenericFuncDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|FileSinkDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|GroupByDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|MapWork
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|MapredWork
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|OperatorDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|PartitionDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|PlanUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|PlanUtils
operator|.
name|ExpressionTypes
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|ReduceWork
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|TableDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|api
operator|.
name|Adjacency
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|api
operator|.
name|Graph
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|session
operator|.
name|SessionState
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|stats
operator|.
name|StatsFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|stats
operator|.
name|StatsPublisher
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde
operator|.
name|serdeConstants
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|SerDeException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|Serializer
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|lazy
operator|.
name|LazySimpleSerDe
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|shims
operator|.
name|ShimLoader
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|IOUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|SequenceFile
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|SequenceFile
operator|.
name|CompressionType
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|Text
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|Writable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|WritableComparable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|compress
operator|.
name|CompressionCodec
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|compress
operator|.
name|DefaultCodec
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|FileInputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|FileOutputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|InputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|JobConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|RecordReader
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|Reporter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|SequenceFileInputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|SequenceFileOutputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|ReflectionUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|Shell
import|;
end_import

begin_import
import|import
name|com
operator|.
name|esotericsoftware
operator|.
name|kryo
operator|.
name|Kryo
import|;
end_import

begin_import
import|import
name|com
operator|.
name|esotericsoftware
operator|.
name|kryo
operator|.
name|io
operator|.
name|Input
import|;
end_import

begin_import
import|import
name|com
operator|.
name|esotericsoftware
operator|.
name|kryo
operator|.
name|io
operator|.
name|Output
import|;
end_import

begin_import
import|import
name|com
operator|.
name|esotericsoftware
operator|.
name|kryo
operator|.
name|serializers
operator|.
name|FieldSerializer
import|;
end_import

begin_comment
comment|/**  * Utilities.  *  */
end_comment

begin_class
annotation|@
name|SuppressWarnings
argument_list|(
literal|"nls"
argument_list|)
specifier|public
specifier|final
class|class
name|Utilities
block|{
comment|/**    * The object in the reducer are composed of these top level fields.    */
specifier|public
specifier|static
name|String
name|HADOOP_LOCAL_FS
init|=
literal|"file:///"
decl_stmt|;
specifier|public
specifier|static
name|String
name|MAP_PLAN_NAME
init|=
literal|"map.xml"
decl_stmt|;
specifier|public
specifier|static
name|String
name|REDUCE_PLAN_NAME
init|=
literal|"reduce.xml"
decl_stmt|;
specifier|public
specifier|static
specifier|final
name|String
name|MAPRED_MAPPER_CLASS
init|=
literal|"mapred.mapper.class"
decl_stmt|;
specifier|public
specifier|static
specifier|final
name|String
name|MAPRED_REDUCER_CLASS
init|=
literal|"mapred.reducer.class"
decl_stmt|;
comment|/**    * ReduceField:    * KEY: record key    * VALUE: record value    */
specifier|public
specifier|static
enum|enum
name|ReduceField
block|{
name|KEY
block|,
name|VALUE
block|}
empty_stmt|;
specifier|public
specifier|static
name|List
argument_list|<
name|String
argument_list|>
name|reduceFieldNameList
decl_stmt|;
static|static
block|{
name|reduceFieldNameList
operator|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
expr_stmt|;
for|for
control|(
name|ReduceField
name|r
range|:
name|ReduceField
operator|.
name|values
argument_list|()
control|)
block|{
name|reduceFieldNameList
operator|.
name|add
argument_list|(
name|r
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
specifier|private
name|Utilities
parameter_list|()
block|{
comment|// prevent instantiation
block|}
specifier|private
specifier|static
name|Map
argument_list|<
name|Path
argument_list|,
name|BaseWork
argument_list|>
name|gWorkMap
init|=
name|Collections
operator|.
name|synchronizedMap
argument_list|(
operator|new
name|HashMap
argument_list|<
name|Path
argument_list|,
name|BaseWork
argument_list|>
argument_list|()
argument_list|)
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|String
name|CLASS_NAME
init|=
name|Utilities
operator|.
name|class
operator|.
name|getName
argument_list|()
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|Log
name|LOG
init|=
name|LogFactory
operator|.
name|getLog
argument_list|(
name|CLASS_NAME
argument_list|)
decl_stmt|;
specifier|public
specifier|static
name|void
name|clearWork
parameter_list|(
name|Configuration
name|conf
parameter_list|)
block|{
name|Path
name|mapPath
init|=
name|getPlanPath
argument_list|(
name|conf
argument_list|,
name|MAP_PLAN_NAME
argument_list|)
decl_stmt|;
name|Path
name|reducePath
init|=
name|getPlanPath
argument_list|(
name|conf
argument_list|,
name|REDUCE_PLAN_NAME
argument_list|)
decl_stmt|;
comment|// if the plan path hasn't been initialized just return, nothing to clean.
if|if
condition|(
name|mapPath
operator|==
literal|null
operator|||
name|reducePath
operator|==
literal|null
condition|)
block|{
return|return;
block|}
try|try
block|{
name|FileSystem
name|fs
init|=
name|mapPath
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
if|if
condition|(
name|fs
operator|.
name|exists
argument_list|(
name|mapPath
argument_list|)
condition|)
block|{
name|fs
operator|.
name|delete
argument_list|(
name|mapPath
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|fs
operator|.
name|exists
argument_list|(
name|reducePath
argument_list|)
condition|)
block|{
name|fs
operator|.
name|delete
argument_list|(
name|reducePath
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Failed to clean-up tmp directories."
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
comment|// where a single process works with multiple plans - we must clear
comment|// the cache before working with the next plan.
if|if
condition|(
name|mapPath
operator|!=
literal|null
condition|)
block|{
name|gWorkMap
operator|.
name|remove
argument_list|(
name|mapPath
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|reducePath
operator|!=
literal|null
condition|)
block|{
name|gWorkMap
operator|.
name|remove
argument_list|(
name|reducePath
argument_list|)
expr_stmt|;
block|}
block|}
block|}
specifier|public
specifier|static
name|MapredWork
name|getMapRedWork
parameter_list|(
name|Configuration
name|conf
parameter_list|)
block|{
name|MapredWork
name|w
init|=
operator|new
name|MapredWork
argument_list|()
decl_stmt|;
name|w
operator|.
name|setMapWork
argument_list|(
name|getMapWork
argument_list|(
name|conf
argument_list|)
argument_list|)
expr_stmt|;
name|w
operator|.
name|setReduceWork
argument_list|(
name|getReduceWork
argument_list|(
name|conf
argument_list|)
argument_list|)
expr_stmt|;
return|return
name|w
return|;
block|}
specifier|public
specifier|static
name|void
name|setMapWork
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|MapWork
name|work
parameter_list|)
block|{
name|setBaseWork
argument_list|(
name|conf
argument_list|,
name|MAP_PLAN_NAME
argument_list|,
name|work
argument_list|)
expr_stmt|;
block|}
specifier|public
specifier|static
name|MapWork
name|getMapWork
parameter_list|(
name|Configuration
name|conf
parameter_list|)
block|{
return|return
operator|(
name|MapWork
operator|)
name|getBaseWork
argument_list|(
name|conf
argument_list|,
name|MAP_PLAN_NAME
argument_list|)
return|;
block|}
specifier|public
specifier|static
name|void
name|setReduceWork
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|ReduceWork
name|work
parameter_list|)
block|{
name|setBaseWork
argument_list|(
name|conf
argument_list|,
name|REDUCE_PLAN_NAME
argument_list|,
name|work
argument_list|)
expr_stmt|;
block|}
specifier|public
specifier|static
name|ReduceWork
name|getReduceWork
parameter_list|(
name|Configuration
name|conf
parameter_list|)
block|{
return|return
operator|(
name|ReduceWork
operator|)
name|getBaseWork
argument_list|(
name|conf
argument_list|,
name|REDUCE_PLAN_NAME
argument_list|)
return|;
block|}
comment|/**    * Pushes work into the global work map    */
specifier|public
specifier|static
name|void
name|setBaseWork
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|String
name|name
parameter_list|,
name|BaseWork
name|work
parameter_list|)
block|{
name|Path
name|path
init|=
name|getPlanPath
argument_list|(
name|conf
argument_list|,
name|name
argument_list|)
decl_stmt|;
name|gWorkMap
operator|.
name|put
argument_list|(
name|path
argument_list|,
name|work
argument_list|)
expr_stmt|;
block|}
comment|/**    * Returns the Map or Reduce plan    * Side effect: the BaseWork returned is also placed in the gWorkMap    * @param conf    * @param name    * @return BaseWork based on the name supplied will return null if name is null    * @throws RuntimeException if the configuration files are not proper or if plan can not be loaded    */
specifier|private
specifier|static
name|BaseWork
name|getBaseWork
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|String
name|name
parameter_list|)
block|{
name|BaseWork
name|gWork
init|=
literal|null
decl_stmt|;
name|Path
name|path
init|=
literal|null
decl_stmt|;
name|InputStream
name|in
init|=
literal|null
decl_stmt|;
try|try
block|{
name|path
operator|=
name|getPlanPath
argument_list|(
name|conf
argument_list|,
name|name
argument_list|)
expr_stmt|;
assert|assert
name|path
operator|!=
literal|null
assert|;
if|if
condition|(
operator|!
name|gWorkMap
operator|.
name|containsKey
argument_list|(
name|path
argument_list|)
condition|)
block|{
name|Path
name|localPath
decl_stmt|;
if|if
condition|(
name|ShimLoader
operator|.
name|getHadoopShims
argument_list|()
operator|.
name|isLocalMode
argument_list|(
name|conf
argument_list|)
condition|)
block|{
name|localPath
operator|=
name|path
expr_stmt|;
block|}
else|else
block|{
name|localPath
operator|=
operator|new
name|Path
argument_list|(
name|name
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|HiveConf
operator|.
name|getBoolVar
argument_list|(
name|conf
argument_list|,
name|ConfVars
operator|.
name|HIVE_RPC_QUERY_PLAN
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Loading plan from string: "
operator|+
name|path
operator|.
name|toUri
argument_list|()
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
name|String
name|planString
init|=
name|conf
operator|.
name|get
argument_list|(
name|path
operator|.
name|toUri
argument_list|()
operator|.
name|getPath
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|planString
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Could not find plan string in conf"
argument_list|)
expr_stmt|;
return|return
literal|null
return|;
block|}
name|byte
index|[]
name|planBytes
init|=
name|Base64
operator|.
name|decodeBase64
argument_list|(
name|planString
argument_list|)
decl_stmt|;
name|in
operator|=
operator|new
name|ByteArrayInputStream
argument_list|(
name|planBytes
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|in
operator|=
operator|new
name|FileInputStream
argument_list|(
name|localPath
operator|.
name|toUri
argument_list|()
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|MAP_PLAN_NAME
operator|.
name|equals
argument_list|(
name|name
argument_list|)
condition|)
block|{
if|if
condition|(
name|ExecMapper
operator|.
name|class
operator|.
name|getName
argument_list|()
operator|.
name|equals
argument_list|(
name|conf
operator|.
name|get
argument_list|(
name|MAPRED_MAPPER_CLASS
argument_list|)
argument_list|)
condition|)
block|{
name|gWork
operator|=
name|deserializePlan
argument_list|(
name|in
argument_list|,
name|MapWork
operator|.
name|class
argument_list|,
name|conf
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|RCFileMergeMapper
operator|.
name|class
operator|.
name|getName
argument_list|()
operator|.
name|equals
argument_list|(
name|conf
operator|.
name|get
argument_list|(
name|MAPRED_MAPPER_CLASS
argument_list|)
argument_list|)
condition|)
block|{
name|gWork
operator|=
name|deserializePlan
argument_list|(
name|in
argument_list|,
name|MergeWork
operator|.
name|class
argument_list|,
name|conf
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|ColumnTruncateMapper
operator|.
name|class
operator|.
name|getName
argument_list|()
operator|.
name|equals
argument_list|(
name|conf
operator|.
name|get
argument_list|(
name|MAPRED_MAPPER_CLASS
argument_list|)
argument_list|)
condition|)
block|{
name|gWork
operator|=
name|deserializePlan
argument_list|(
name|in
argument_list|,
name|ColumnTruncateWork
operator|.
name|class
argument_list|,
name|conf
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|PartialScanMapper
operator|.
name|class
operator|.
name|getName
argument_list|()
operator|.
name|equals
argument_list|(
name|conf
operator|.
name|get
argument_list|(
name|MAPRED_MAPPER_CLASS
argument_list|)
argument_list|)
condition|)
block|{
name|gWork
operator|=
name|deserializePlan
argument_list|(
name|in
argument_list|,
name|PartialScanWork
operator|.
name|class
argument_list|,
name|conf
argument_list|)
expr_stmt|;
block|}
else|else
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
literal|"unable to determine work from configuration ."
operator|+
name|MAPRED_MAPPER_CLASS
operator|+
literal|" was "
operator|+
name|conf
operator|.
name|get
argument_list|(
name|MAPRED_MAPPER_CLASS
argument_list|)
argument_list|)
throw|;
block|}
block|}
elseif|else
if|if
condition|(
name|REDUCE_PLAN_NAME
operator|.
name|equals
argument_list|(
name|name
argument_list|)
condition|)
block|{
if|if
condition|(
name|ExecReducer
operator|.
name|class
operator|.
name|getName
argument_list|()
operator|.
name|equals
argument_list|(
name|conf
operator|.
name|get
argument_list|(
name|MAPRED_REDUCER_CLASS
argument_list|)
argument_list|)
condition|)
block|{
name|gWork
operator|=
name|deserializePlan
argument_list|(
name|in
argument_list|,
name|ReduceWork
operator|.
name|class
argument_list|,
name|conf
argument_list|)
expr_stmt|;
block|}
else|else
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
literal|"unable to determine work from configuration ."
operator|+
name|MAPRED_REDUCER_CLASS
operator|+
literal|" was "
operator|+
name|conf
operator|.
name|get
argument_list|(
name|MAPRED_REDUCER_CLASS
argument_list|)
argument_list|)
throw|;
block|}
block|}
name|gWorkMap
operator|.
name|put
argument_list|(
name|path
argument_list|,
name|gWork
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Found plan in cache."
argument_list|)
expr_stmt|;
name|gWork
operator|=
name|gWorkMap
operator|.
name|get
argument_list|(
name|path
argument_list|)
expr_stmt|;
block|}
return|return
name|gWork
return|;
block|}
catch|catch
parameter_list|(
name|FileNotFoundException
name|fnf
parameter_list|)
block|{
comment|// happens. e.g.: no reduce work.
name|LOG
operator|.
name|debug
argument_list|(
literal|"No plan file found: "
operator|+
name|path
argument_list|)
expr_stmt|;
return|return
literal|null
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Failed to load plan: "
operator|+
name|path
argument_list|,
name|e
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|RuntimeException
argument_list|(
name|e
argument_list|)
throw|;
block|}
finally|finally
block|{
if|if
condition|(
name|in
operator|!=
literal|null
condition|)
block|{
try|try
block|{
name|in
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|cantBlameMeForTrying
parameter_list|)
block|{ }
block|}
block|}
block|}
specifier|public
specifier|static
name|void
name|setWorkflowAdjacencies
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|QueryPlan
name|plan
parameter_list|)
block|{
try|try
block|{
name|Graph
name|stageGraph
init|=
name|plan
operator|.
name|getQueryPlan
argument_list|()
operator|.
name|getStageGraph
argument_list|()
decl_stmt|;
if|if
condition|(
name|stageGraph
operator|==
literal|null
condition|)
block|{
return|return;
block|}
name|List
argument_list|<
name|Adjacency
argument_list|>
name|adjList
init|=
name|stageGraph
operator|.
name|getAdjacencyList
argument_list|()
decl_stmt|;
if|if
condition|(
name|adjList
operator|==
literal|null
condition|)
block|{
return|return;
block|}
for|for
control|(
name|Adjacency
name|adj
range|:
name|adjList
control|)
block|{
name|List
argument_list|<
name|String
argument_list|>
name|children
init|=
name|adj
operator|.
name|getChildren
argument_list|()
decl_stmt|;
if|if
condition|(
name|children
operator|==
literal|null
operator|||
name|children
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
return|return;
block|}
name|conf
operator|.
name|setStrings
argument_list|(
literal|"mapreduce.workflow.adjacency."
operator|+
name|adj
operator|.
name|getNode
argument_list|()
argument_list|,
name|children
operator|.
name|toArray
argument_list|(
operator|new
name|String
index|[
name|children
operator|.
name|size
argument_list|()
index|]
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{     }
block|}
specifier|public
specifier|static
name|List
argument_list|<
name|String
argument_list|>
name|getFieldSchemaString
parameter_list|(
name|List
argument_list|<
name|FieldSchema
argument_list|>
name|fl
parameter_list|)
block|{
if|if
condition|(
name|fl
operator|==
literal|null
condition|)
block|{
return|return
literal|null
return|;
block|}
name|ArrayList
argument_list|<
name|String
argument_list|>
name|ret
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|FieldSchema
name|f
range|:
name|fl
control|)
block|{
name|ret
operator|.
name|add
argument_list|(
name|f
operator|.
name|getName
argument_list|()
operator|+
literal|" "
operator|+
name|f
operator|.
name|getType
argument_list|()
operator|+
operator|(
name|f
operator|.
name|getComment
argument_list|()
operator|!=
literal|null
condition|?
operator|(
literal|" "
operator|+
name|f
operator|.
name|getComment
argument_list|()
operator|)
else|:
literal|""
operator|)
argument_list|)
expr_stmt|;
block|}
return|return
name|ret
return|;
block|}
comment|/**    * Java 1.5 workaround. From http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=5015403    */
specifier|public
specifier|static
class|class
name|EnumDelegate
extends|extends
name|DefaultPersistenceDelegate
block|{
annotation|@
name|Override
specifier|protected
name|Expression
name|instantiate
parameter_list|(
name|Object
name|oldInstance
parameter_list|,
name|Encoder
name|out
parameter_list|)
block|{
return|return
operator|new
name|Expression
argument_list|(
name|Enum
operator|.
name|class
argument_list|,
literal|"valueOf"
argument_list|,
operator|new
name|Object
index|[]
block|{
name|oldInstance
operator|.
name|getClass
argument_list|()
block|,
operator|(
operator|(
name|Enum
argument_list|<
name|?
argument_list|>
operator|)
name|oldInstance
operator|)
operator|.
name|name
argument_list|()
block|}
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|protected
name|boolean
name|mutatesTo
parameter_list|(
name|Object
name|oldInstance
parameter_list|,
name|Object
name|newInstance
parameter_list|)
block|{
return|return
name|oldInstance
operator|==
name|newInstance
return|;
block|}
block|}
specifier|public
specifier|static
class|class
name|MapDelegate
extends|extends
name|DefaultPersistenceDelegate
block|{
annotation|@
name|Override
specifier|protected
name|Expression
name|instantiate
parameter_list|(
name|Object
name|oldInstance
parameter_list|,
name|Encoder
name|out
parameter_list|)
block|{
name|Map
name|oldMap
init|=
operator|(
name|Map
operator|)
name|oldInstance
decl_stmt|;
name|HashMap
name|newMap
init|=
operator|new
name|HashMap
argument_list|(
name|oldMap
argument_list|)
decl_stmt|;
return|return
operator|new
name|Expression
argument_list|(
name|newMap
argument_list|,
name|HashMap
operator|.
name|class
argument_list|,
literal|"new"
argument_list|,
operator|new
name|Object
index|[]
block|{}
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|protected
name|boolean
name|mutatesTo
parameter_list|(
name|Object
name|oldInstance
parameter_list|,
name|Object
name|newInstance
parameter_list|)
block|{
return|return
literal|false
return|;
block|}
annotation|@
name|Override
specifier|protected
name|void
name|initialize
parameter_list|(
name|Class
argument_list|<
name|?
argument_list|>
name|type
parameter_list|,
name|Object
name|oldInstance
parameter_list|,
name|Object
name|newInstance
parameter_list|,
name|Encoder
name|out
parameter_list|)
block|{
name|java
operator|.
name|util
operator|.
name|Collection
name|oldO
init|=
operator|(
name|java
operator|.
name|util
operator|.
name|Collection
operator|)
name|oldInstance
decl_stmt|;
name|java
operator|.
name|util
operator|.
name|Collection
name|newO
init|=
operator|(
name|java
operator|.
name|util
operator|.
name|Collection
operator|)
name|newInstance
decl_stmt|;
if|if
condition|(
name|newO
operator|.
name|size
argument_list|()
operator|!=
literal|0
condition|)
block|{
name|out
operator|.
name|writeStatement
argument_list|(
operator|new
name|Statement
argument_list|(
name|oldInstance
argument_list|,
literal|"clear"
argument_list|,
operator|new
name|Object
index|[]
block|{}
argument_list|)
argument_list|)
expr_stmt|;
block|}
for|for
control|(
name|Iterator
name|i
init|=
name|oldO
operator|.
name|iterator
argument_list|()
init|;
name|i
operator|.
name|hasNext
argument_list|()
condition|;
control|)
block|{
name|out
operator|.
name|writeStatement
argument_list|(
operator|new
name|Statement
argument_list|(
name|oldInstance
argument_list|,
literal|"add"
argument_list|,
operator|new
name|Object
index|[]
block|{
name|i
operator|.
name|next
argument_list|()
block|}
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
block|}
specifier|public
specifier|static
class|class
name|SetDelegate
extends|extends
name|DefaultPersistenceDelegate
block|{
annotation|@
name|Override
specifier|protected
name|Expression
name|instantiate
parameter_list|(
name|Object
name|oldInstance
parameter_list|,
name|Encoder
name|out
parameter_list|)
block|{
name|Set
name|oldSet
init|=
operator|(
name|Set
operator|)
name|oldInstance
decl_stmt|;
name|HashSet
name|newSet
init|=
operator|new
name|HashSet
argument_list|(
name|oldSet
argument_list|)
decl_stmt|;
return|return
operator|new
name|Expression
argument_list|(
name|newSet
argument_list|,
name|HashSet
operator|.
name|class
argument_list|,
literal|"new"
argument_list|,
operator|new
name|Object
index|[]
block|{}
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|protected
name|boolean
name|mutatesTo
parameter_list|(
name|Object
name|oldInstance
parameter_list|,
name|Object
name|newInstance
parameter_list|)
block|{
return|return
literal|false
return|;
block|}
annotation|@
name|Override
specifier|protected
name|void
name|initialize
parameter_list|(
name|Class
argument_list|<
name|?
argument_list|>
name|type
parameter_list|,
name|Object
name|oldInstance
parameter_list|,
name|Object
name|newInstance
parameter_list|,
name|Encoder
name|out
parameter_list|)
block|{
name|java
operator|.
name|util
operator|.
name|Collection
name|oldO
init|=
operator|(
name|java
operator|.
name|util
operator|.
name|Collection
operator|)
name|oldInstance
decl_stmt|;
name|java
operator|.
name|util
operator|.
name|Collection
name|newO
init|=
operator|(
name|java
operator|.
name|util
operator|.
name|Collection
operator|)
name|newInstance
decl_stmt|;
if|if
condition|(
name|newO
operator|.
name|size
argument_list|()
operator|!=
literal|0
condition|)
block|{
name|out
operator|.
name|writeStatement
argument_list|(
operator|new
name|Statement
argument_list|(
name|oldInstance
argument_list|,
literal|"clear"
argument_list|,
operator|new
name|Object
index|[]
block|{}
argument_list|)
argument_list|)
expr_stmt|;
block|}
for|for
control|(
name|Iterator
name|i
init|=
name|oldO
operator|.
name|iterator
argument_list|()
init|;
name|i
operator|.
name|hasNext
argument_list|()
condition|;
control|)
block|{
name|out
operator|.
name|writeStatement
argument_list|(
operator|new
name|Statement
argument_list|(
name|oldInstance
argument_list|,
literal|"add"
argument_list|,
operator|new
name|Object
index|[]
block|{
name|i
operator|.
name|next
argument_list|()
block|}
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
block|}
specifier|public
specifier|static
class|class
name|ListDelegate
extends|extends
name|DefaultPersistenceDelegate
block|{
annotation|@
name|Override
specifier|protected
name|Expression
name|instantiate
parameter_list|(
name|Object
name|oldInstance
parameter_list|,
name|Encoder
name|out
parameter_list|)
block|{
name|List
name|oldList
init|=
operator|(
name|List
operator|)
name|oldInstance
decl_stmt|;
name|ArrayList
name|newList
init|=
operator|new
name|ArrayList
argument_list|(
name|oldList
argument_list|)
decl_stmt|;
return|return
operator|new
name|Expression
argument_list|(
name|newList
argument_list|,
name|ArrayList
operator|.
name|class
argument_list|,
literal|"new"
argument_list|,
operator|new
name|Object
index|[]
block|{}
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|protected
name|boolean
name|mutatesTo
parameter_list|(
name|Object
name|oldInstance
parameter_list|,
name|Object
name|newInstance
parameter_list|)
block|{
return|return
literal|false
return|;
block|}
annotation|@
name|Override
specifier|protected
name|void
name|initialize
parameter_list|(
name|Class
argument_list|<
name|?
argument_list|>
name|type
parameter_list|,
name|Object
name|oldInstance
parameter_list|,
name|Object
name|newInstance
parameter_list|,
name|Encoder
name|out
parameter_list|)
block|{
name|java
operator|.
name|util
operator|.
name|Collection
name|oldO
init|=
operator|(
name|java
operator|.
name|util
operator|.
name|Collection
operator|)
name|oldInstance
decl_stmt|;
name|java
operator|.
name|util
operator|.
name|Collection
name|newO
init|=
operator|(
name|java
operator|.
name|util
operator|.
name|Collection
operator|)
name|newInstance
decl_stmt|;
if|if
condition|(
name|newO
operator|.
name|size
argument_list|()
operator|!=
literal|0
condition|)
block|{
name|out
operator|.
name|writeStatement
argument_list|(
operator|new
name|Statement
argument_list|(
name|oldInstance
argument_list|,
literal|"clear"
argument_list|,
operator|new
name|Object
index|[]
block|{}
argument_list|)
argument_list|)
expr_stmt|;
block|}
for|for
control|(
name|Iterator
name|i
init|=
name|oldO
operator|.
name|iterator
argument_list|()
init|;
name|i
operator|.
name|hasNext
argument_list|()
condition|;
control|)
block|{
name|out
operator|.
name|writeStatement
argument_list|(
operator|new
name|Statement
argument_list|(
name|oldInstance
argument_list|,
literal|"add"
argument_list|,
operator|new
name|Object
index|[]
block|{
name|i
operator|.
name|next
argument_list|()
block|}
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
block|}
comment|/**    * DatePersistenceDelegate. Needed to serialize java.util.Date    * since it is not serialization friendly.    * Also works for java.sql.Date since it derives from java.util.Date.    */
specifier|public
specifier|static
class|class
name|DatePersistenceDelegate
extends|extends
name|PersistenceDelegate
block|{
annotation|@
name|Override
specifier|protected
name|Expression
name|instantiate
parameter_list|(
name|Object
name|oldInstance
parameter_list|,
name|Encoder
name|out
parameter_list|)
block|{
name|Date
name|dateVal
init|=
operator|(
name|Date
operator|)
name|oldInstance
decl_stmt|;
name|Object
index|[]
name|args
init|=
block|{
name|dateVal
operator|.
name|getTime
argument_list|()
block|}
decl_stmt|;
return|return
operator|new
name|Expression
argument_list|(
name|dateVal
argument_list|,
name|dateVal
operator|.
name|getClass
argument_list|()
argument_list|,
literal|"new"
argument_list|,
name|args
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|protected
name|boolean
name|mutatesTo
parameter_list|(
name|Object
name|oldInstance
parameter_list|,
name|Object
name|newInstance
parameter_list|)
block|{
if|if
condition|(
name|oldInstance
operator|==
literal|null
operator|||
name|newInstance
operator|==
literal|null
condition|)
block|{
return|return
literal|false
return|;
block|}
return|return
name|oldInstance
operator|.
name|getClass
argument_list|()
operator|==
name|newInstance
operator|.
name|getClass
argument_list|()
return|;
block|}
block|}
comment|/**    * TimestampPersistenceDelegate. Needed to serialize java.sql.Timestamp since    * it is not serialization friendly.    */
specifier|public
specifier|static
class|class
name|TimestampPersistenceDelegate
extends|extends
name|DatePersistenceDelegate
block|{
annotation|@
name|Override
specifier|protected
name|void
name|initialize
parameter_list|(
name|Class
argument_list|<
name|?
argument_list|>
name|type
parameter_list|,
name|Object
name|oldInstance
parameter_list|,
name|Object
name|newInstance
parameter_list|,
name|Encoder
name|out
parameter_list|)
block|{
name|Timestamp
name|ts
init|=
operator|(
name|Timestamp
operator|)
name|oldInstance
decl_stmt|;
name|Object
index|[]
name|args
init|=
block|{
name|ts
operator|.
name|getNanos
argument_list|()
block|}
decl_stmt|;
name|Statement
name|stmt
init|=
operator|new
name|Statement
argument_list|(
name|oldInstance
argument_list|,
literal|"setNanos"
argument_list|,
name|args
argument_list|)
decl_stmt|;
name|out
operator|.
name|writeStatement
argument_list|(
name|stmt
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * Need to serialize org.antlr.runtime.CommonToken    */
specifier|public
specifier|static
class|class
name|CommonTokenDelegate
extends|extends
name|PersistenceDelegate
block|{
annotation|@
name|Override
specifier|protected
name|Expression
name|instantiate
parameter_list|(
name|Object
name|oldInstance
parameter_list|,
name|Encoder
name|out
parameter_list|)
block|{
name|CommonToken
name|ct
init|=
operator|(
name|CommonToken
operator|)
name|oldInstance
decl_stmt|;
name|Object
index|[]
name|args
init|=
block|{
name|ct
operator|.
name|getType
argument_list|()
block|,
name|ct
operator|.
name|getText
argument_list|()
block|}
decl_stmt|;
return|return
operator|new
name|Expression
argument_list|(
name|ct
argument_list|,
name|ct
operator|.
name|getClass
argument_list|()
argument_list|,
literal|"new"
argument_list|,
name|args
argument_list|)
return|;
block|}
block|}
specifier|public
specifier|static
class|class
name|PathDelegate
extends|extends
name|PersistenceDelegate
block|{
annotation|@
name|Override
specifier|protected
name|Expression
name|instantiate
parameter_list|(
name|Object
name|oldInstance
parameter_list|,
name|Encoder
name|out
parameter_list|)
block|{
name|Path
name|p
init|=
operator|(
name|Path
operator|)
name|oldInstance
decl_stmt|;
name|Object
index|[]
name|args
init|=
block|{
name|p
operator|.
name|toString
argument_list|()
block|}
decl_stmt|;
return|return
operator|new
name|Expression
argument_list|(
name|p
argument_list|,
name|p
operator|.
name|getClass
argument_list|()
argument_list|,
literal|"new"
argument_list|,
name|args
argument_list|)
return|;
block|}
block|}
specifier|public
specifier|static
name|void
name|setMapRedWork
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|MapredWork
name|w
parameter_list|,
name|String
name|hiveScratchDir
parameter_list|)
block|{
name|setMapWork
argument_list|(
name|conf
argument_list|,
name|w
operator|.
name|getMapWork
argument_list|()
argument_list|,
name|hiveScratchDir
argument_list|,
literal|true
argument_list|)
expr_stmt|;
if|if
condition|(
name|w
operator|.
name|getReduceWork
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|setReduceWork
argument_list|(
name|conf
argument_list|,
name|w
operator|.
name|getReduceWork
argument_list|()
argument_list|,
name|hiveScratchDir
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
block|}
specifier|public
specifier|static
name|Path
name|setMapWork
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|MapWork
name|w
parameter_list|,
name|String
name|hiveScratchDir
parameter_list|,
name|boolean
name|useCache
parameter_list|)
block|{
return|return
name|setBaseWork
argument_list|(
name|conf
argument_list|,
name|w
argument_list|,
name|hiveScratchDir
argument_list|,
name|MAP_PLAN_NAME
argument_list|,
name|useCache
argument_list|)
return|;
block|}
specifier|public
specifier|static
name|Path
name|setReduceWork
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|ReduceWork
name|w
parameter_list|,
name|String
name|hiveScratchDir
parameter_list|,
name|boolean
name|useCache
parameter_list|)
block|{
return|return
name|setBaseWork
argument_list|(
name|conf
argument_list|,
name|w
argument_list|,
name|hiveScratchDir
argument_list|,
name|REDUCE_PLAN_NAME
argument_list|,
name|useCache
argument_list|)
return|;
block|}
specifier|private
specifier|static
name|Path
name|setBaseWork
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|BaseWork
name|w
parameter_list|,
name|String
name|hiveScratchDir
parameter_list|,
name|String
name|name
parameter_list|,
name|boolean
name|useCache
parameter_list|)
block|{
try|try
block|{
name|setPlanPath
argument_list|(
name|conf
argument_list|,
name|hiveScratchDir
argument_list|)
expr_stmt|;
name|Path
name|planPath
init|=
name|getPlanPath
argument_list|(
name|conf
argument_list|,
name|name
argument_list|)
decl_stmt|;
name|OutputStream
name|out
decl_stmt|;
if|if
condition|(
name|HiveConf
operator|.
name|getBoolVar
argument_list|(
name|conf
argument_list|,
name|ConfVars
operator|.
name|HIVE_RPC_QUERY_PLAN
argument_list|)
condition|)
block|{
comment|// add it to the conf
name|out
operator|=
operator|new
name|ByteArrayOutputStream
argument_list|()
expr_stmt|;
name|serializePlan
argument_list|(
name|w
argument_list|,
name|out
argument_list|,
name|conf
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Setting plan: "
operator|+
name|planPath
operator|.
name|toUri
argument_list|()
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
name|conf
operator|.
name|set
argument_list|(
name|planPath
operator|.
name|toUri
argument_list|()
operator|.
name|getPath
argument_list|()
argument_list|,
name|Base64
operator|.
name|encodeBase64String
argument_list|(
operator|(
operator|(
name|ByteArrayOutputStream
operator|)
name|out
operator|)
operator|.
name|toByteArray
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// use the default file system of the conf
name|FileSystem
name|fs
init|=
name|planPath
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|out
operator|=
name|fs
operator|.
name|create
argument_list|(
name|planPath
argument_list|)
expr_stmt|;
name|serializePlan
argument_list|(
name|w
argument_list|,
name|out
argument_list|,
name|conf
argument_list|)
expr_stmt|;
comment|// Serialize the plan to the default hdfs instance
comment|// Except for hadoop local mode execution where we should be
comment|// able to get the plan directly from the cache
if|if
condition|(
name|useCache
operator|&&
operator|!
name|ShimLoader
operator|.
name|getHadoopShims
argument_list|()
operator|.
name|isLocalMode
argument_list|(
name|conf
argument_list|)
condition|)
block|{
comment|// Set up distributed cache
if|if
condition|(
operator|!
name|DistributedCache
operator|.
name|getSymlink
argument_list|(
name|conf
argument_list|)
condition|)
block|{
name|DistributedCache
operator|.
name|createSymlink
argument_list|(
name|conf
argument_list|)
expr_stmt|;
block|}
name|String
name|uriWithLink
init|=
name|planPath
operator|.
name|toUri
argument_list|()
operator|.
name|toString
argument_list|()
operator|+
literal|"#"
operator|+
name|name
decl_stmt|;
name|DistributedCache
operator|.
name|addCacheFile
argument_list|(
operator|new
name|URI
argument_list|(
name|uriWithLink
argument_list|)
argument_list|,
name|conf
argument_list|)
expr_stmt|;
comment|// set replication of the plan file to a high number. we use the same
comment|// replication factor as used by the hadoop jobclient for job.xml etc.
name|short
name|replication
init|=
operator|(
name|short
operator|)
name|conf
operator|.
name|getInt
argument_list|(
literal|"mapred.submit.replication"
argument_list|,
literal|10
argument_list|)
decl_stmt|;
name|fs
operator|.
name|setReplication
argument_list|(
name|planPath
argument_list|,
name|replication
argument_list|)
expr_stmt|;
block|}
block|}
comment|// Cache the plan in this process
name|gWorkMap
operator|.
name|put
argument_list|(
name|planPath
argument_list|,
name|w
argument_list|)
expr_stmt|;
return|return
name|planPath
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|e
operator|.
name|printStackTrace
argument_list|()
expr_stmt|;
throw|throw
operator|new
name|RuntimeException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|private
specifier|static
name|Path
name|getPlanPath
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|String
name|name
parameter_list|)
block|{
name|Path
name|planPath
init|=
name|getPlanPath
argument_list|(
name|conf
argument_list|)
decl_stmt|;
if|if
condition|(
name|planPath
operator|==
literal|null
condition|)
block|{
return|return
literal|null
return|;
block|}
return|return
operator|new
name|Path
argument_list|(
name|planPath
argument_list|,
name|name
argument_list|)
return|;
block|}
specifier|private
specifier|static
name|void
name|setPlanPath
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|String
name|hiveScratchDir
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|getPlanPath
argument_list|(
name|conf
argument_list|)
operator|==
literal|null
condition|)
block|{
comment|// this is the unique conf ID, which is kept in JobConf as part of the plan file name
name|String
name|jobID
init|=
name|UUID
operator|.
name|randomUUID
argument_list|()
operator|.
name|toString
argument_list|()
decl_stmt|;
name|Path
name|planPath
init|=
operator|new
name|Path
argument_list|(
name|hiveScratchDir
argument_list|,
name|jobID
argument_list|)
decl_stmt|;
name|FileSystem
name|fs
init|=
name|planPath
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|fs
operator|.
name|mkdirs
argument_list|(
name|planPath
argument_list|)
expr_stmt|;
name|HiveConf
operator|.
name|setVar
argument_list|(
name|conf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|PLAN
argument_list|,
name|planPath
operator|.
name|toUri
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
specifier|public
specifier|static
name|Path
name|getPlanPath
parameter_list|(
name|Configuration
name|conf
parameter_list|)
block|{
name|String
name|plan
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|conf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|PLAN
argument_list|)
decl_stmt|;
if|if
condition|(
name|plan
operator|!=
literal|null
operator|&&
operator|!
name|plan
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
return|return
operator|new
name|Path
argument_list|(
name|plan
argument_list|)
return|;
block|}
return|return
literal|null
return|;
block|}
comment|/**    * Serializes expression via Kryo.    * @param expr Expression.    * @return Bytes.    */
specifier|public
specifier|static
name|byte
index|[]
name|serializeExpressionToKryo
parameter_list|(
name|ExprNodeGenericFuncDesc
name|expr
parameter_list|)
block|{
name|ByteArrayOutputStream
name|baos
init|=
operator|new
name|ByteArrayOutputStream
argument_list|()
decl_stmt|;
name|Output
name|output
init|=
operator|new
name|Output
argument_list|(
name|baos
argument_list|)
decl_stmt|;
name|runtimeSerializationKryo
operator|.
name|get
argument_list|()
operator|.
name|writeObject
argument_list|(
name|output
argument_list|,
name|expr
argument_list|)
expr_stmt|;
name|output
operator|.
name|close
argument_list|()
expr_stmt|;
return|return
name|baos
operator|.
name|toByteArray
argument_list|()
return|;
block|}
comment|/**    * Deserializes expression from Kryo.    * @param bytes Bytes containing the expression.    * @return Expression; null if deserialization succeeded, but the result type is incorrect.    */
specifier|public
specifier|static
name|ExprNodeGenericFuncDesc
name|deserializeExpressionFromKryo
parameter_list|(
name|byte
index|[]
name|bytes
parameter_list|)
block|{
name|Input
name|inp
init|=
operator|new
name|Input
argument_list|(
operator|new
name|ByteArrayInputStream
argument_list|(
name|bytes
argument_list|)
argument_list|)
decl_stmt|;
name|ExprNodeGenericFuncDesc
name|func
init|=
name|runtimeSerializationKryo
operator|.
name|get
argument_list|()
operator|.
name|readObject
argument_list|(
name|inp
argument_list|,
name|ExprNodeGenericFuncDesc
operator|.
name|class
argument_list|)
decl_stmt|;
name|inp
operator|.
name|close
argument_list|()
expr_stmt|;
return|return
name|func
return|;
block|}
specifier|public
specifier|static
name|String
name|serializeExpression
parameter_list|(
name|ExprNodeGenericFuncDesc
name|expr
parameter_list|)
block|{
try|try
block|{
return|return
operator|new
name|String
argument_list|(
name|Base64
operator|.
name|encodeBase64
argument_list|(
name|serializeExpressionToKryo
argument_list|(
name|expr
argument_list|)
argument_list|)
argument_list|,
literal|"UTF-8"
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|UnsupportedEncodingException
name|ex
parameter_list|)
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
literal|"UTF-8 support required"
argument_list|,
name|ex
argument_list|)
throw|;
block|}
block|}
specifier|public
specifier|static
name|ExprNodeGenericFuncDesc
name|deserializeExpression
parameter_list|(
name|String
name|s
parameter_list|)
block|{
name|byte
index|[]
name|bytes
decl_stmt|;
try|try
block|{
name|bytes
operator|=
name|Base64
operator|.
name|decodeBase64
argument_list|(
name|s
operator|.
name|getBytes
argument_list|(
literal|"UTF-8"
argument_list|)
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|UnsupportedEncodingException
name|ex
parameter_list|)
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
literal|"UTF-8 support required"
argument_list|,
name|ex
argument_list|)
throw|;
block|}
return|return
name|deserializeExpressionFromKryo
argument_list|(
name|bytes
argument_list|)
return|;
block|}
specifier|public
specifier|static
class|class
name|CollectionPersistenceDelegate
extends|extends
name|DefaultPersistenceDelegate
block|{
annotation|@
name|Override
specifier|protected
name|Expression
name|instantiate
parameter_list|(
name|Object
name|oldInstance
parameter_list|,
name|Encoder
name|out
parameter_list|)
block|{
return|return
operator|new
name|Expression
argument_list|(
name|oldInstance
argument_list|,
name|oldInstance
operator|.
name|getClass
argument_list|()
argument_list|,
literal|"new"
argument_list|,
literal|null
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|protected
name|void
name|initialize
parameter_list|(
name|Class
name|type
parameter_list|,
name|Object
name|oldInstance
parameter_list|,
name|Object
name|newInstance
parameter_list|,
name|Encoder
name|out
parameter_list|)
block|{
name|Iterator
name|ite
init|=
operator|(
operator|(
name|Collection
operator|)
name|oldInstance
operator|)
operator|.
name|iterator
argument_list|()
decl_stmt|;
while|while
condition|(
name|ite
operator|.
name|hasNext
argument_list|()
condition|)
block|{
name|out
operator|.
name|writeStatement
argument_list|(
operator|new
name|Statement
argument_list|(
name|oldInstance
argument_list|,
literal|"add"
argument_list|,
operator|new
name|Object
index|[]
block|{
name|ite
operator|.
name|next
argument_list|()
block|}
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
block|}
comment|/**    * Kryo serializer for timestamp.    */
specifier|private
specifier|static
class|class
name|TimestampSerializer
extends|extends
name|com
operator|.
name|esotericsoftware
operator|.
name|kryo
operator|.
name|Serializer
argument_list|<
name|Timestamp
argument_list|>
block|{
annotation|@
name|Override
specifier|public
name|Timestamp
name|read
parameter_list|(
name|Kryo
name|kryo
parameter_list|,
name|Input
name|input
parameter_list|,
name|Class
argument_list|<
name|Timestamp
argument_list|>
name|clazz
parameter_list|)
block|{
name|Timestamp
name|ts
init|=
operator|new
name|Timestamp
argument_list|(
name|input
operator|.
name|readLong
argument_list|()
argument_list|)
decl_stmt|;
name|ts
operator|.
name|setNanos
argument_list|(
name|input
operator|.
name|readInt
argument_list|()
argument_list|)
expr_stmt|;
return|return
name|ts
return|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|write
parameter_list|(
name|Kryo
name|kryo
parameter_list|,
name|Output
name|output
parameter_list|,
name|Timestamp
name|ts
parameter_list|)
block|{
name|output
operator|.
name|writeLong
argument_list|(
name|ts
operator|.
name|getTime
argument_list|()
argument_list|)
expr_stmt|;
name|output
operator|.
name|writeInt
argument_list|(
name|ts
operator|.
name|getNanos
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
comment|/** Custom Kryo serializer for sql date, otherwise Kryo gets confused between    java.sql.Date and java.util.Date while deserializing    */
specifier|private
specifier|static
class|class
name|SqlDateSerializer
extends|extends
name|com
operator|.
name|esotericsoftware
operator|.
name|kryo
operator|.
name|Serializer
argument_list|<
name|java
operator|.
name|sql
operator|.
name|Date
argument_list|>
block|{
annotation|@
name|Override
specifier|public
name|java
operator|.
name|sql
operator|.
name|Date
name|read
parameter_list|(
name|Kryo
name|kryo
parameter_list|,
name|Input
name|input
parameter_list|,
name|Class
argument_list|<
name|java
operator|.
name|sql
operator|.
name|Date
argument_list|>
name|clazz
parameter_list|)
block|{
return|return
operator|new
name|java
operator|.
name|sql
operator|.
name|Date
argument_list|(
name|input
operator|.
name|readLong
argument_list|()
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|write
parameter_list|(
name|Kryo
name|kryo
parameter_list|,
name|Output
name|output
parameter_list|,
name|java
operator|.
name|sql
operator|.
name|Date
name|sqlDate
parameter_list|)
block|{
name|output
operator|.
name|writeLong
argument_list|(
name|sqlDate
operator|.
name|getTime
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
specifier|private
specifier|static
class|class
name|CommonTokenSerializer
extends|extends
name|com
operator|.
name|esotericsoftware
operator|.
name|kryo
operator|.
name|Serializer
argument_list|<
name|CommonToken
argument_list|>
block|{
annotation|@
name|Override
specifier|public
name|CommonToken
name|read
parameter_list|(
name|Kryo
name|kryo
parameter_list|,
name|Input
name|input
parameter_list|,
name|Class
argument_list|<
name|CommonToken
argument_list|>
name|clazz
parameter_list|)
block|{
return|return
operator|new
name|CommonToken
argument_list|(
name|input
operator|.
name|readInt
argument_list|()
argument_list|,
name|input
operator|.
name|readString
argument_list|()
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|write
parameter_list|(
name|Kryo
name|kryo
parameter_list|,
name|Output
name|output
parameter_list|,
name|CommonToken
name|token
parameter_list|)
block|{
name|output
operator|.
name|writeInt
argument_list|(
name|token
operator|.
name|getType
argument_list|()
argument_list|)
expr_stmt|;
name|output
operator|.
name|writeString
argument_list|(
name|token
operator|.
name|getText
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
specifier|private
specifier|static
class|class
name|PathSerializer
extends|extends
name|com
operator|.
name|esotericsoftware
operator|.
name|kryo
operator|.
name|Serializer
argument_list|<
name|Path
argument_list|>
block|{
annotation|@
name|Override
specifier|public
name|void
name|write
parameter_list|(
name|Kryo
name|kryo
parameter_list|,
name|Output
name|output
parameter_list|,
name|Path
name|path
parameter_list|)
block|{
name|output
operator|.
name|writeString
argument_list|(
name|path
operator|.
name|toUri
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|Path
name|read
parameter_list|(
name|Kryo
name|kryo
parameter_list|,
name|Input
name|input
parameter_list|,
name|Class
argument_list|<
name|Path
argument_list|>
name|type
parameter_list|)
block|{
return|return
operator|new
name|Path
argument_list|(
name|URI
operator|.
name|create
argument_list|(
name|input
operator|.
name|readString
argument_list|()
argument_list|)
argument_list|)
return|;
block|}
block|}
specifier|private
specifier|static
name|void
name|serializePlan
parameter_list|(
name|Object
name|plan
parameter_list|,
name|OutputStream
name|out
parameter_list|,
name|Configuration
name|conf
parameter_list|,
name|boolean
name|cloningPlan
parameter_list|)
block|{
name|PerfLogger
name|perfLogger
init|=
name|PerfLogger
operator|.
name|getPerfLogger
argument_list|()
decl_stmt|;
name|perfLogger
operator|.
name|PerfLogBegin
argument_list|(
name|CLASS_NAME
argument_list|,
name|PerfLogger
operator|.
name|SERIALIZE_PLAN
argument_list|)
expr_stmt|;
name|String
name|serializationType
init|=
name|conf
operator|.
name|get
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|PLAN_SERIALIZATION
operator|.
name|varname
argument_list|,
literal|"kryo"
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Serializing "
operator|+
name|plan
operator|.
name|getClass
argument_list|()
operator|.
name|getSimpleName
argument_list|()
operator|+
literal|" via "
operator|+
name|serializationType
argument_list|)
expr_stmt|;
if|if
condition|(
literal|"javaXML"
operator|.
name|equalsIgnoreCase
argument_list|(
name|serializationType
argument_list|)
condition|)
block|{
name|serializeObjectByJavaXML
argument_list|(
name|plan
argument_list|,
name|out
argument_list|)
expr_stmt|;
block|}
else|else
block|{
if|if
condition|(
name|cloningPlan
condition|)
block|{
name|serializeObjectByKryo
argument_list|(
name|cloningQueryPlanKryo
operator|.
name|get
argument_list|()
argument_list|,
name|plan
argument_list|,
name|out
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|serializeObjectByKryo
argument_list|(
name|runtimeSerializationKryo
operator|.
name|get
argument_list|()
argument_list|,
name|plan
argument_list|,
name|out
argument_list|)
expr_stmt|;
block|}
block|}
name|perfLogger
operator|.
name|PerfLogEnd
argument_list|(
name|CLASS_NAME
argument_list|,
name|PerfLogger
operator|.
name|SERIALIZE_PLAN
argument_list|)
expr_stmt|;
block|}
comment|/**    * Serializes the plan.    * @param plan The plan, such as QueryPlan, MapredWork, etc.    * @param out The stream to write to.    * @param conf to pick which serialization format is desired.    */
specifier|public
specifier|static
name|void
name|serializePlan
parameter_list|(
name|Object
name|plan
parameter_list|,
name|OutputStream
name|out
parameter_list|,
name|Configuration
name|conf
parameter_list|)
block|{
name|serializePlan
argument_list|(
name|plan
argument_list|,
name|out
argument_list|,
name|conf
argument_list|,
literal|false
argument_list|)
expr_stmt|;
block|}
specifier|private
specifier|static
parameter_list|<
name|T
parameter_list|>
name|T
name|deserializePlan
parameter_list|(
name|InputStream
name|in
parameter_list|,
name|Class
argument_list|<
name|T
argument_list|>
name|planClass
parameter_list|,
name|Configuration
name|conf
parameter_list|,
name|boolean
name|cloningPlan
parameter_list|)
block|{
name|PerfLogger
name|perfLogger
init|=
name|PerfLogger
operator|.
name|getPerfLogger
argument_list|()
decl_stmt|;
name|perfLogger
operator|.
name|PerfLogBegin
argument_list|(
name|CLASS_NAME
argument_list|,
name|PerfLogger
operator|.
name|DESERIALIZE_PLAN
argument_list|)
expr_stmt|;
name|T
name|plan
decl_stmt|;
name|String
name|serializationType
init|=
name|conf
operator|.
name|get
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|PLAN_SERIALIZATION
operator|.
name|varname
argument_list|,
literal|"kryo"
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Deserializing "
operator|+
name|planClass
operator|.
name|getSimpleName
argument_list|()
operator|+
literal|" via "
operator|+
name|serializationType
argument_list|)
expr_stmt|;
if|if
condition|(
literal|"javaXML"
operator|.
name|equalsIgnoreCase
argument_list|(
name|serializationType
argument_list|)
condition|)
block|{
name|plan
operator|=
name|deserializeObjectByJavaXML
argument_list|(
name|in
argument_list|)
expr_stmt|;
block|}
else|else
block|{
if|if
condition|(
name|cloningPlan
condition|)
block|{
name|plan
operator|=
name|deserializeObjectByKryo
argument_list|(
name|cloningQueryPlanKryo
operator|.
name|get
argument_list|()
argument_list|,
name|in
argument_list|,
name|planClass
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|plan
operator|=
name|deserializeObjectByKryo
argument_list|(
name|runtimeSerializationKryo
operator|.
name|get
argument_list|()
argument_list|,
name|in
argument_list|,
name|planClass
argument_list|)
expr_stmt|;
block|}
block|}
name|perfLogger
operator|.
name|PerfLogEnd
argument_list|(
name|CLASS_NAME
argument_list|,
name|PerfLogger
operator|.
name|DESERIALIZE_PLAN
argument_list|)
expr_stmt|;
return|return
name|plan
return|;
block|}
comment|/**    * Deserializes the plan.    * @param in The stream to read from.    * @return The plan, such as QueryPlan, MapredWork, etc.    * @param To know what serialization format plan is in    */
specifier|public
specifier|static
parameter_list|<
name|T
parameter_list|>
name|T
name|deserializePlan
parameter_list|(
name|InputStream
name|in
parameter_list|,
name|Class
argument_list|<
name|T
argument_list|>
name|planClass
parameter_list|,
name|Configuration
name|conf
parameter_list|)
block|{
return|return
name|deserializePlan
argument_list|(
name|in
argument_list|,
name|planClass
argument_list|,
name|conf
argument_list|,
literal|false
argument_list|)
return|;
block|}
comment|/**    * Clones using the powers of XML. Do not use unless necessary.    * @param plan The plan.    * @return The clone.    */
specifier|public
specifier|static
name|MapredWork
name|clonePlan
parameter_list|(
name|MapredWork
name|plan
parameter_list|)
block|{
comment|// TODO: need proper clone. Meanwhile, let's at least keep this horror in one place
name|PerfLogger
name|perfLogger
init|=
name|PerfLogger
operator|.
name|getPerfLogger
argument_list|()
decl_stmt|;
name|perfLogger
operator|.
name|PerfLogBegin
argument_list|(
name|CLASS_NAME
argument_list|,
name|PerfLogger
operator|.
name|CLONE_PLAN
argument_list|)
expr_stmt|;
name|ByteArrayOutputStream
name|baos
init|=
operator|new
name|ByteArrayOutputStream
argument_list|(
literal|4096
argument_list|)
decl_stmt|;
name|Configuration
name|conf
init|=
operator|new
name|HiveConf
argument_list|()
decl_stmt|;
name|serializePlan
argument_list|(
name|plan
argument_list|,
name|baos
argument_list|,
name|conf
argument_list|,
literal|true
argument_list|)
expr_stmt|;
name|MapredWork
name|newPlan
init|=
name|deserializePlan
argument_list|(
operator|new
name|ByteArrayInputStream
argument_list|(
name|baos
operator|.
name|toByteArray
argument_list|()
argument_list|)
argument_list|,
name|MapredWork
operator|.
name|class
argument_list|,
name|conf
argument_list|,
literal|true
argument_list|)
decl_stmt|;
name|perfLogger
operator|.
name|PerfLogEnd
argument_list|(
name|CLASS_NAME
argument_list|,
name|PerfLogger
operator|.
name|CLONE_PLAN
argument_list|)
expr_stmt|;
return|return
name|newPlan
return|;
block|}
comment|/**    * Serialize the object. This helper function mainly makes sure that enums,    * counters, etc are handled properly.    */
specifier|private
specifier|static
name|void
name|serializeObjectByJavaXML
parameter_list|(
name|Object
name|plan
parameter_list|,
name|OutputStream
name|out
parameter_list|)
block|{
name|XMLEncoder
name|e
init|=
operator|new
name|XMLEncoder
argument_list|(
name|out
argument_list|)
decl_stmt|;
name|e
operator|.
name|setExceptionListener
argument_list|(
operator|new
name|ExceptionListener
argument_list|()
block|{
specifier|public
name|void
name|exceptionThrown
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|RuntimeException
argument_list|(
literal|"Cannot serialize object"
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
argument_list|)
expr_stmt|;
comment|// workaround for java 1.5
name|e
operator|.
name|setPersistenceDelegate
argument_list|(
name|ExpressionTypes
operator|.
name|class
argument_list|,
operator|new
name|EnumDelegate
argument_list|()
argument_list|)
expr_stmt|;
name|e
operator|.
name|setPersistenceDelegate
argument_list|(
name|GroupByDesc
operator|.
name|Mode
operator|.
name|class
argument_list|,
operator|new
name|EnumDelegate
argument_list|()
argument_list|)
expr_stmt|;
name|e
operator|.
name|setPersistenceDelegate
argument_list|(
name|java
operator|.
name|sql
operator|.
name|Date
operator|.
name|class
argument_list|,
operator|new
name|DatePersistenceDelegate
argument_list|()
argument_list|)
expr_stmt|;
name|e
operator|.
name|setPersistenceDelegate
argument_list|(
name|Timestamp
operator|.
name|class
argument_list|,
operator|new
name|TimestampPersistenceDelegate
argument_list|()
argument_list|)
expr_stmt|;
name|e
operator|.
name|setPersistenceDelegate
argument_list|(
name|org
operator|.
name|datanucleus
operator|.
name|store
operator|.
name|types
operator|.
name|backed
operator|.
name|Map
operator|.
name|class
argument_list|,
operator|new
name|MapDelegate
argument_list|()
argument_list|)
expr_stmt|;
name|e
operator|.
name|setPersistenceDelegate
argument_list|(
name|org
operator|.
name|datanucleus
operator|.
name|store
operator|.
name|types
operator|.
name|backed
operator|.
name|List
operator|.
name|class
argument_list|,
operator|new
name|ListDelegate
argument_list|()
argument_list|)
expr_stmt|;
name|e
operator|.
name|setPersistenceDelegate
argument_list|(
name|CommonToken
operator|.
name|class
argument_list|,
operator|new
name|CommonTokenDelegate
argument_list|()
argument_list|)
expr_stmt|;
name|e
operator|.
name|setPersistenceDelegate
argument_list|(
name|Path
operator|.
name|class
argument_list|,
operator|new
name|PathDelegate
argument_list|()
argument_list|)
expr_stmt|;
name|e
operator|.
name|writeObject
argument_list|(
name|plan
argument_list|)
expr_stmt|;
name|e
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
comment|/**    * @param plan Usually of type MapredWork, MapredLocalWork etc.    * @param out stream in which serialized plan is written into    */
specifier|private
specifier|static
name|void
name|serializeObjectByKryo
parameter_list|(
name|Kryo
name|kryo
parameter_list|,
name|Object
name|plan
parameter_list|,
name|OutputStream
name|out
parameter_list|)
block|{
name|Output
name|output
init|=
operator|new
name|Output
argument_list|(
name|out
argument_list|)
decl_stmt|;
name|kryo
operator|.
name|writeObject
argument_list|(
name|output
argument_list|,
name|plan
argument_list|)
expr_stmt|;
name|output
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
comment|/**    * De-serialize an object. This helper function mainly makes sure that enums,    * counters, etc are handled properly.    */
annotation|@
name|SuppressWarnings
argument_list|(
literal|"unchecked"
argument_list|)
specifier|private
specifier|static
parameter_list|<
name|T
parameter_list|>
name|T
name|deserializeObjectByJavaXML
parameter_list|(
name|InputStream
name|in
parameter_list|)
block|{
name|XMLDecoder
name|d
init|=
literal|null
decl_stmt|;
try|try
block|{
name|d
operator|=
operator|new
name|XMLDecoder
argument_list|(
name|in
argument_list|,
literal|null
argument_list|,
literal|null
argument_list|)
expr_stmt|;
return|return
operator|(
name|T
operator|)
name|d
operator|.
name|readObject
argument_list|()
return|;
block|}
finally|finally
block|{
if|if
condition|(
literal|null
operator|!=
name|d
condition|)
block|{
name|d
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
block|}
specifier|private
specifier|static
parameter_list|<
name|T
parameter_list|>
name|T
name|deserializeObjectByKryo
parameter_list|(
name|Kryo
name|kryo
parameter_list|,
name|InputStream
name|in
parameter_list|,
name|Class
argument_list|<
name|T
argument_list|>
name|clazz
parameter_list|)
block|{
name|Input
name|inp
init|=
operator|new
name|Input
argument_list|(
name|in
argument_list|)
decl_stmt|;
name|T
name|t
init|=
name|kryo
operator|.
name|readObject
argument_list|(
name|inp
argument_list|,
name|clazz
argument_list|)
decl_stmt|;
name|inp
operator|.
name|close
argument_list|()
expr_stmt|;
return|return
name|t
return|;
block|}
comment|// Kryo is not thread-safe,
comment|// Also new Kryo() is expensive, so we want to do it just once.
specifier|private
specifier|static
name|ThreadLocal
argument_list|<
name|Kryo
argument_list|>
name|runtimeSerializationKryo
init|=
operator|new
name|ThreadLocal
argument_list|<
name|Kryo
argument_list|>
argument_list|()
block|{
annotation|@
name|Override
specifier|protected
specifier|synchronized
name|Kryo
name|initialValue
parameter_list|()
block|{
name|Kryo
name|kryo
init|=
operator|new
name|Kryo
argument_list|()
decl_stmt|;
name|kryo
operator|.
name|setClassLoader
argument_list|(
name|Thread
operator|.
name|currentThread
argument_list|()
operator|.
name|getContextClassLoader
argument_list|()
argument_list|)
expr_stmt|;
name|kryo
operator|.
name|register
argument_list|(
name|java
operator|.
name|sql
operator|.
name|Date
operator|.
name|class
argument_list|,
operator|new
name|SqlDateSerializer
argument_list|()
argument_list|)
expr_stmt|;
name|kryo
operator|.
name|register
argument_list|(
name|java
operator|.
name|sql
operator|.
name|Timestamp
operator|.
name|class
argument_list|,
operator|new
name|TimestampSerializer
argument_list|()
argument_list|)
expr_stmt|;
name|kryo
operator|.
name|register
argument_list|(
name|Path
operator|.
name|class
argument_list|,
operator|new
name|PathSerializer
argument_list|()
argument_list|)
expr_stmt|;
name|removeField
argument_list|(
name|kryo
argument_list|,
name|Operator
operator|.
name|class
argument_list|,
literal|"colExprMap"
argument_list|)
expr_stmt|;
name|removeField
argument_list|(
name|kryo
argument_list|,
name|ColumnInfo
operator|.
name|class
argument_list|,
literal|"objectInspector"
argument_list|)
expr_stmt|;
name|removeField
argument_list|(
name|kryo
argument_list|,
name|MapWork
operator|.
name|class
argument_list|,
literal|"opParseCtxMap"
argument_list|)
expr_stmt|;
name|removeField
argument_list|(
name|kryo
argument_list|,
name|MapWork
operator|.
name|class
argument_list|,
literal|"joinTree"
argument_list|)
expr_stmt|;
return|return
name|kryo
return|;
block|}
empty_stmt|;
block|}
decl_stmt|;
annotation|@
name|SuppressWarnings
argument_list|(
literal|"rawtypes"
argument_list|)
specifier|protected
specifier|static
name|void
name|removeField
parameter_list|(
name|Kryo
name|kryo
parameter_list|,
name|Class
name|type
parameter_list|,
name|String
name|fieldName
parameter_list|)
block|{
name|FieldSerializer
name|fld
init|=
operator|new
name|FieldSerializer
argument_list|(
name|kryo
argument_list|,
name|type
argument_list|)
decl_stmt|;
name|fld
operator|.
name|removeField
argument_list|(
name|fieldName
argument_list|)
expr_stmt|;
name|kryo
operator|.
name|register
argument_list|(
name|type
argument_list|,
name|fld
argument_list|)
expr_stmt|;
block|}
specifier|private
specifier|static
name|ThreadLocal
argument_list|<
name|Kryo
argument_list|>
name|cloningQueryPlanKryo
init|=
operator|new
name|ThreadLocal
argument_list|<
name|Kryo
argument_list|>
argument_list|()
block|{
annotation|@
name|Override
specifier|protected
specifier|synchronized
name|Kryo
name|initialValue
parameter_list|()
block|{
name|Kryo
name|kryo
init|=
operator|new
name|Kryo
argument_list|()
decl_stmt|;
name|kryo
operator|.
name|setClassLoader
argument_list|(
name|Thread
operator|.
name|currentThread
argument_list|()
operator|.
name|getContextClassLoader
argument_list|()
argument_list|)
expr_stmt|;
name|kryo
operator|.
name|register
argument_list|(
name|CommonToken
operator|.
name|class
argument_list|,
operator|new
name|CommonTokenSerializer
argument_list|()
argument_list|)
expr_stmt|;
name|kryo
operator|.
name|register
argument_list|(
name|java
operator|.
name|sql
operator|.
name|Date
operator|.
name|class
argument_list|,
operator|new
name|SqlDateSerializer
argument_list|()
argument_list|)
expr_stmt|;
name|kryo
operator|.
name|register
argument_list|(
name|java
operator|.
name|sql
operator|.
name|Timestamp
operator|.
name|class
argument_list|,
operator|new
name|TimestampSerializer
argument_list|()
argument_list|)
expr_stmt|;
name|kryo
operator|.
name|register
argument_list|(
name|Path
operator|.
name|class
argument_list|,
operator|new
name|PathSerializer
argument_list|()
argument_list|)
expr_stmt|;
return|return
name|kryo
return|;
block|}
empty_stmt|;
block|}
decl_stmt|;
specifier|public
specifier|static
name|TableDesc
name|defaultTd
decl_stmt|;
static|static
block|{
comment|// by default we expect ^A separated strings
comment|// This tableDesc does not provide column names. We should always use
comment|// PlanUtils.getDefaultTableDesc(String separatorCode, String columns)
comment|// or getBinarySortableTableDesc(List<FieldSchema> fieldSchemas) when
comment|// we know the column names.
name|defaultTd
operator|=
name|PlanUtils
operator|.
name|getDefaultTableDesc
argument_list|(
literal|""
operator|+
name|Utilities
operator|.
name|ctrlaCode
argument_list|)
expr_stmt|;
block|}
specifier|public
specifier|static
specifier|final
name|int
name|carriageReturnCode
init|=
literal|13
decl_stmt|;
specifier|public
specifier|static
specifier|final
name|int
name|newLineCode
init|=
literal|10
decl_stmt|;
specifier|public
specifier|static
specifier|final
name|int
name|tabCode
init|=
literal|9
decl_stmt|;
specifier|public
specifier|static
specifier|final
name|int
name|ctrlaCode
init|=
literal|1
decl_stmt|;
specifier|public
specifier|static
specifier|final
name|String
name|INDENT
init|=
literal|"  "
decl_stmt|;
comment|// Note: When DDL supports specifying what string to represent null,
comment|// we should specify "NULL" to represent null in the temp table, and then
comment|// we can make the following translation deprecated.
specifier|public
specifier|static
name|String
name|nullStringStorage
init|=
literal|"\\N"
decl_stmt|;
specifier|public
specifier|static
name|String
name|nullStringOutput
init|=
literal|"NULL"
decl_stmt|;
specifier|public
specifier|static
name|Random
name|randGen
init|=
operator|new
name|Random
argument_list|()
decl_stmt|;
comment|/**    * Gets the task id if we are running as a Hadoop job. Gets a random number otherwise.    */
specifier|public
specifier|static
name|String
name|getTaskId
parameter_list|(
name|Configuration
name|hconf
parameter_list|)
block|{
name|String
name|taskid
init|=
operator|(
name|hconf
operator|==
literal|null
operator|)
condition|?
literal|null
else|:
name|hconf
operator|.
name|get
argument_list|(
literal|"mapred.task.id"
argument_list|)
decl_stmt|;
if|if
condition|(
operator|(
name|taskid
operator|==
literal|null
operator|)
operator|||
name|taskid
operator|.
name|equals
argument_list|(
literal|""
argument_list|)
condition|)
block|{
return|return
operator|(
literal|""
operator|+
name|Math
operator|.
name|abs
argument_list|(
name|randGen
operator|.
name|nextInt
argument_list|()
argument_list|)
operator|)
return|;
block|}
else|else
block|{
comment|/*        * extract the task and attempt id from the hadoop taskid. in version 17 the leading component        * was 'task_'. thereafter the leading component is 'attempt_'. in 17 - hadoop also seems to        * have used _map_ and _reduce_ to denote map/reduce task types        */
name|String
name|ret
init|=
name|taskid
operator|.
name|replaceAll
argument_list|(
literal|".*_[mr]_"
argument_list|,
literal|""
argument_list|)
operator|.
name|replaceAll
argument_list|(
literal|".*_(map|reduce)_"
argument_list|,
literal|""
argument_list|)
decl_stmt|;
return|return
operator|(
name|ret
operator|)
return|;
block|}
block|}
specifier|public
specifier|static
name|HashMap
name|makeMap
parameter_list|(
name|Object
modifier|...
name|olist
parameter_list|)
block|{
name|HashMap
name|ret
init|=
operator|new
name|HashMap
argument_list|()
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|olist
operator|.
name|length
condition|;
name|i
operator|+=
literal|2
control|)
block|{
name|ret
operator|.
name|put
argument_list|(
name|olist
index|[
name|i
index|]
argument_list|,
name|olist
index|[
name|i
operator|+
literal|1
index|]
argument_list|)
expr_stmt|;
block|}
return|return
operator|(
name|ret
operator|)
return|;
block|}
specifier|public
specifier|static
name|Properties
name|makeProperties
parameter_list|(
name|String
modifier|...
name|olist
parameter_list|)
block|{
name|Properties
name|ret
init|=
operator|new
name|Properties
argument_list|()
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|olist
operator|.
name|length
condition|;
name|i
operator|+=
literal|2
control|)
block|{
name|ret
operator|.
name|setProperty
argument_list|(
name|olist
index|[
name|i
index|]
argument_list|,
name|olist
index|[
name|i
operator|+
literal|1
index|]
argument_list|)
expr_stmt|;
block|}
return|return
operator|(
name|ret
operator|)
return|;
block|}
specifier|public
specifier|static
name|ArrayList
name|makeList
parameter_list|(
name|Object
modifier|...
name|olist
parameter_list|)
block|{
name|ArrayList
name|ret
init|=
operator|new
name|ArrayList
argument_list|()
decl_stmt|;
for|for
control|(
name|Object
name|element
range|:
name|olist
control|)
block|{
name|ret
operator|.
name|add
argument_list|(
name|element
argument_list|)
expr_stmt|;
block|}
return|return
operator|(
name|ret
operator|)
return|;
block|}
comment|/**    * StreamPrinter.    *    */
specifier|public
specifier|static
class|class
name|StreamPrinter
extends|extends
name|Thread
block|{
name|InputStream
name|is
decl_stmt|;
name|String
name|type
decl_stmt|;
name|PrintStream
name|os
decl_stmt|;
specifier|public
name|StreamPrinter
parameter_list|(
name|InputStream
name|is
parameter_list|,
name|String
name|type
parameter_list|,
name|PrintStream
name|os
parameter_list|)
block|{
name|this
operator|.
name|is
operator|=
name|is
expr_stmt|;
name|this
operator|.
name|type
operator|=
name|type
expr_stmt|;
name|this
operator|.
name|os
operator|=
name|os
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|run
parameter_list|()
block|{
name|BufferedReader
name|br
init|=
literal|null
decl_stmt|;
try|try
block|{
name|InputStreamReader
name|isr
init|=
operator|new
name|InputStreamReader
argument_list|(
name|is
argument_list|)
decl_stmt|;
name|br
operator|=
operator|new
name|BufferedReader
argument_list|(
name|isr
argument_list|)
expr_stmt|;
name|String
name|line
init|=
literal|null
decl_stmt|;
if|if
condition|(
name|type
operator|!=
literal|null
condition|)
block|{
while|while
condition|(
operator|(
name|line
operator|=
name|br
operator|.
name|readLine
argument_list|()
operator|)
operator|!=
literal|null
condition|)
block|{
name|os
operator|.
name|println
argument_list|(
name|type
operator|+
literal|">"
operator|+
name|line
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
while|while
condition|(
operator|(
name|line
operator|=
name|br
operator|.
name|readLine
argument_list|()
operator|)
operator|!=
literal|null
condition|)
block|{
name|os
operator|.
name|println
argument_list|(
name|line
argument_list|)
expr_stmt|;
block|}
block|}
name|br
operator|.
name|close
argument_list|()
expr_stmt|;
name|br
operator|=
literal|null
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|ioe
parameter_list|)
block|{
name|ioe
operator|.
name|printStackTrace
argument_list|()
expr_stmt|;
block|}
finally|finally
block|{
name|IOUtils
operator|.
name|closeStream
argument_list|(
name|br
argument_list|)
expr_stmt|;
block|}
block|}
block|}
specifier|public
specifier|static
name|TableDesc
name|getTableDesc
parameter_list|(
name|Table
name|tbl
parameter_list|)
block|{
name|Properties
name|props
init|=
name|tbl
operator|.
name|getMetadata
argument_list|()
decl_stmt|;
name|props
operator|.
name|put
argument_list|(
name|serdeConstants
operator|.
name|SERIALIZATION_LIB
argument_list|,
name|tbl
operator|.
name|getDeserializer
argument_list|()
operator|.
name|getClass
argument_list|()
operator|.
name|getName
argument_list|()
argument_list|)
expr_stmt|;
return|return
operator|(
operator|new
name|TableDesc
argument_list|(
name|tbl
operator|.
name|getInputFormatClass
argument_list|()
argument_list|,
name|tbl
operator|.
name|getOutputFormatClass
argument_list|()
argument_list|,
name|props
argument_list|)
operator|)
return|;
block|}
comment|// column names and column types are all delimited by comma
specifier|public
specifier|static
name|TableDesc
name|getTableDesc
parameter_list|(
name|String
name|cols
parameter_list|,
name|String
name|colTypes
parameter_list|)
block|{
return|return
operator|(
operator|new
name|TableDesc
argument_list|(
name|SequenceFileInputFormat
operator|.
name|class
argument_list|,
name|HiveSequenceFileOutputFormat
operator|.
name|class
argument_list|,
name|Utilities
operator|.
name|makeProperties
argument_list|(
name|serdeConstants
operator|.
name|SERIALIZATION_FORMAT
argument_list|,
literal|""
operator|+
name|Utilities
operator|.
name|ctrlaCode
argument_list|,
name|serdeConstants
operator|.
name|LIST_COLUMNS
argument_list|,
name|cols
argument_list|,
name|serdeConstants
operator|.
name|LIST_COLUMN_TYPES
argument_list|,
name|colTypes
argument_list|,
name|serdeConstants
operator|.
name|SERIALIZATION_LIB
argument_list|,
name|LazySimpleSerDe
operator|.
name|class
operator|.
name|getName
argument_list|()
argument_list|)
argument_list|)
operator|)
return|;
block|}
specifier|public
specifier|static
name|PartitionDesc
name|getPartitionDesc
parameter_list|(
name|Partition
name|part
parameter_list|)
throws|throws
name|HiveException
block|{
return|return
operator|(
operator|new
name|PartitionDesc
argument_list|(
name|part
argument_list|)
operator|)
return|;
block|}
specifier|public
specifier|static
name|PartitionDesc
name|getPartitionDescFromTableDesc
parameter_list|(
name|TableDesc
name|tblDesc
parameter_list|,
name|Partition
name|part
parameter_list|)
throws|throws
name|HiveException
block|{
return|return
operator|new
name|PartitionDesc
argument_list|(
name|part
argument_list|,
name|tblDesc
argument_list|)
return|;
block|}
specifier|private
specifier|static
name|String
name|getOpTreeSkel_helper
parameter_list|(
name|Operator
argument_list|<
name|?
argument_list|>
name|op
parameter_list|,
name|String
name|indent
parameter_list|)
block|{
if|if
condition|(
name|op
operator|==
literal|null
condition|)
block|{
return|return
literal|""
return|;
block|}
name|StringBuilder
name|sb
init|=
operator|new
name|StringBuilder
argument_list|()
decl_stmt|;
name|sb
operator|.
name|append
argument_list|(
name|indent
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
name|op
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
literal|"\n"
argument_list|)
expr_stmt|;
if|if
condition|(
name|op
operator|.
name|getChildOperators
argument_list|()
operator|!=
literal|null
condition|)
block|{
for|for
control|(
name|Object
name|child
range|:
name|op
operator|.
name|getChildOperators
argument_list|()
control|)
block|{
name|sb
operator|.
name|append
argument_list|(
name|getOpTreeSkel_helper
argument_list|(
operator|(
name|Operator
argument_list|<
name|?
argument_list|>
operator|)
name|child
argument_list|,
name|indent
operator|+
literal|"  "
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|sb
operator|.
name|toString
argument_list|()
return|;
block|}
specifier|public
specifier|static
name|String
name|getOpTreeSkel
parameter_list|(
name|Operator
argument_list|<
name|?
argument_list|>
name|op
parameter_list|)
block|{
return|return
name|getOpTreeSkel_helper
argument_list|(
name|op
argument_list|,
literal|""
argument_list|)
return|;
block|}
specifier|private
specifier|static
name|boolean
name|isWhitespace
parameter_list|(
name|int
name|c
parameter_list|)
block|{
if|if
condition|(
name|c
operator|==
operator|-
literal|1
condition|)
block|{
return|return
literal|false
return|;
block|}
return|return
name|Character
operator|.
name|isWhitespace
argument_list|(
operator|(
name|char
operator|)
name|c
argument_list|)
return|;
block|}
specifier|public
specifier|static
name|boolean
name|contentsEqual
parameter_list|(
name|InputStream
name|is1
parameter_list|,
name|InputStream
name|is2
parameter_list|,
name|boolean
name|ignoreWhitespace
parameter_list|)
throws|throws
name|IOException
block|{
try|try
block|{
if|if
condition|(
operator|(
name|is1
operator|==
name|is2
operator|)
operator|||
operator|(
name|is1
operator|==
literal|null
operator|&&
name|is2
operator|==
literal|null
operator|)
condition|)
block|{
return|return
literal|true
return|;
block|}
if|if
condition|(
name|is1
operator|==
literal|null
operator|||
name|is2
operator|==
literal|null
condition|)
block|{
return|return
literal|false
return|;
block|}
while|while
condition|(
literal|true
condition|)
block|{
name|int
name|c1
init|=
name|is1
operator|.
name|read
argument_list|()
decl_stmt|;
while|while
condition|(
name|ignoreWhitespace
operator|&&
name|isWhitespace
argument_list|(
name|c1
argument_list|)
condition|)
block|{
name|c1
operator|=
name|is1
operator|.
name|read
argument_list|()
expr_stmt|;
block|}
name|int
name|c2
init|=
name|is2
operator|.
name|read
argument_list|()
decl_stmt|;
while|while
condition|(
name|ignoreWhitespace
operator|&&
name|isWhitespace
argument_list|(
name|c2
argument_list|)
condition|)
block|{
name|c2
operator|=
name|is2
operator|.
name|read
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
name|c1
operator|==
operator|-
literal|1
operator|&&
name|c2
operator|==
operator|-
literal|1
condition|)
block|{
return|return
literal|true
return|;
block|}
if|if
condition|(
name|c1
operator|!=
name|c2
condition|)
block|{
break|break;
block|}
block|}
block|}
catch|catch
parameter_list|(
name|FileNotFoundException
name|e
parameter_list|)
block|{
name|e
operator|.
name|printStackTrace
argument_list|()
expr_stmt|;
block|}
return|return
literal|false
return|;
block|}
comment|/**    * convert "From src insert blah blah" to "From src insert ... blah"    */
specifier|public
specifier|static
name|String
name|abbreviate
parameter_list|(
name|String
name|str
parameter_list|,
name|int
name|max
parameter_list|)
block|{
name|str
operator|=
name|str
operator|.
name|trim
argument_list|()
expr_stmt|;
name|int
name|len
init|=
name|str
operator|.
name|length
argument_list|()
decl_stmt|;
name|int
name|suffixlength
init|=
literal|20
decl_stmt|;
if|if
condition|(
name|len
operator|<=
name|max
condition|)
block|{
return|return
name|str
return|;
block|}
name|suffixlength
operator|=
name|Math
operator|.
name|min
argument_list|(
name|suffixlength
argument_list|,
operator|(
name|max
operator|-
literal|3
operator|)
operator|/
literal|2
argument_list|)
expr_stmt|;
name|String
name|rev
init|=
name|StringUtils
operator|.
name|reverse
argument_list|(
name|str
argument_list|)
decl_stmt|;
comment|// get the last few words
name|String
name|suffix
init|=
name|WordUtils
operator|.
name|abbreviate
argument_list|(
name|rev
argument_list|,
literal|0
argument_list|,
name|suffixlength
argument_list|,
literal|""
argument_list|)
decl_stmt|;
name|suffix
operator|=
name|StringUtils
operator|.
name|reverse
argument_list|(
name|suffix
argument_list|)
expr_stmt|;
comment|// first few ..
name|String
name|prefix
init|=
name|StringUtils
operator|.
name|abbreviate
argument_list|(
name|str
argument_list|,
name|max
operator|-
name|suffix
operator|.
name|length
argument_list|()
argument_list|)
decl_stmt|;
return|return
name|prefix
operator|+
name|suffix
return|;
block|}
specifier|public
specifier|static
specifier|final
name|String
name|NSTR
init|=
literal|""
decl_stmt|;
comment|/**    * StreamStatus.    *    */
specifier|public
specifier|static
enum|enum
name|StreamStatus
block|{
name|EOF
block|,
name|TERMINATED
block|}
specifier|public
specifier|static
name|StreamStatus
name|readColumn
parameter_list|(
name|DataInput
name|in
parameter_list|,
name|OutputStream
name|out
parameter_list|)
throws|throws
name|IOException
block|{
name|boolean
name|foundCrChar
init|=
literal|false
decl_stmt|;
while|while
condition|(
literal|true
condition|)
block|{
name|int
name|b
decl_stmt|;
try|try
block|{
name|b
operator|=
name|in
operator|.
name|readByte
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|EOFException
name|e
parameter_list|)
block|{
return|return
name|StreamStatus
operator|.
name|EOF
return|;
block|}
comment|// Default new line characters on windows are "CRLF" so detect if there are any windows
comment|// native newline characters and handle them.
if|if
condition|(
name|Shell
operator|.
name|WINDOWS
condition|)
block|{
comment|// if the CR is not followed by the LF on windows then add it back to the stream and
comment|// proceed with next characters in the input stream.
if|if
condition|(
name|foundCrChar
operator|&&
name|b
operator|!=
name|Utilities
operator|.
name|newLineCode
condition|)
block|{
name|out
operator|.
name|write
argument_list|(
name|Utilities
operator|.
name|carriageReturnCode
argument_list|)
expr_stmt|;
name|foundCrChar
operator|=
literal|false
expr_stmt|;
block|}
if|if
condition|(
name|b
operator|==
name|Utilities
operator|.
name|carriageReturnCode
condition|)
block|{
name|foundCrChar
operator|=
literal|true
expr_stmt|;
continue|continue;
block|}
block|}
if|if
condition|(
name|b
operator|==
name|Utilities
operator|.
name|newLineCode
condition|)
block|{
return|return
name|StreamStatus
operator|.
name|TERMINATED
return|;
block|}
name|out
operator|.
name|write
argument_list|(
name|b
argument_list|)
expr_stmt|;
block|}
comment|// Unreachable
block|}
comment|/**    * Convert an output stream to a compressed output stream based on codecs and compression options    * specified in the Job Configuration.    *    * @param jc    *          Job Configuration    * @param out    *          Output Stream to be converted into compressed output stream    * @return compressed output stream    */
specifier|public
specifier|static
name|OutputStream
name|createCompressedStream
parameter_list|(
name|JobConf
name|jc
parameter_list|,
name|OutputStream
name|out
parameter_list|)
throws|throws
name|IOException
block|{
name|boolean
name|isCompressed
init|=
name|FileOutputFormat
operator|.
name|getCompressOutput
argument_list|(
name|jc
argument_list|)
decl_stmt|;
return|return
name|createCompressedStream
argument_list|(
name|jc
argument_list|,
name|out
argument_list|,
name|isCompressed
argument_list|)
return|;
block|}
comment|/**    * Convert an output stream to a compressed output stream based on codecs codecs in the Job    * Configuration. Caller specifies directly whether file is compressed or not    *    * @param jc    *          Job Configuration    * @param out    *          Output Stream to be converted into compressed output stream    * @param isCompressed    *          whether the output stream needs to be compressed or not    * @return compressed output stream    */
specifier|public
specifier|static
name|OutputStream
name|createCompressedStream
parameter_list|(
name|JobConf
name|jc
parameter_list|,
name|OutputStream
name|out
parameter_list|,
name|boolean
name|isCompressed
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|isCompressed
condition|)
block|{
name|Class
argument_list|<
name|?
extends|extends
name|CompressionCodec
argument_list|>
name|codecClass
init|=
name|FileOutputFormat
operator|.
name|getOutputCompressorClass
argument_list|(
name|jc
argument_list|,
name|DefaultCodec
operator|.
name|class
argument_list|)
decl_stmt|;
name|CompressionCodec
name|codec
init|=
operator|(
name|CompressionCodec
operator|)
name|ReflectionUtils
operator|.
name|newInstance
argument_list|(
name|codecClass
argument_list|,
name|jc
argument_list|)
decl_stmt|;
return|return
name|codec
operator|.
name|createOutputStream
argument_list|(
name|out
argument_list|)
return|;
block|}
else|else
block|{
return|return
operator|(
name|out
operator|)
return|;
block|}
block|}
comment|/**    * Based on compression option and configured output codec - get extension for output file. This    * is only required for text files - not sequencefiles    *    * @param jc    *          Job Configuration    * @param isCompressed    *          Whether the output file is compressed or not    * @return the required file extension (example: .gz)    * @deprecated Use {@link #getFileExtension(JobConf, boolean, HiveOutputFormat)}    */
annotation|@
name|Deprecated
specifier|public
specifier|static
name|String
name|getFileExtension
parameter_list|(
name|JobConf
name|jc
parameter_list|,
name|boolean
name|isCompressed
parameter_list|)
block|{
return|return
name|getFileExtension
argument_list|(
name|jc
argument_list|,
name|isCompressed
argument_list|,
operator|new
name|HiveIgnoreKeyTextOutputFormat
argument_list|()
argument_list|)
return|;
block|}
comment|/**    * Based on compression option, output format, and configured output codec -    * get extension for output file. Text files require an extension, whereas    * others, like sequence files, do not.    *<p>    * The property<code>hive.output.file.extension</code> is used to determine    * the extension - if set, it will override other logic for choosing an    * extension.    *    * @param jc    *          Job Configuration    * @param isCompressed    *          Whether the output file is compressed or not    * @param hiveOutputFormat    *          The output format, used to detect if the format is text    * @return the required file extension (example: .gz)    */
specifier|public
specifier|static
name|String
name|getFileExtension
parameter_list|(
name|JobConf
name|jc
parameter_list|,
name|boolean
name|isCompressed
parameter_list|,
name|HiveOutputFormat
argument_list|<
name|?
argument_list|,
name|?
argument_list|>
name|hiveOutputFormat
parameter_list|)
block|{
name|String
name|extension
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|jc
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|OUTPUT_FILE_EXTENSION
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|StringUtils
operator|.
name|isEmpty
argument_list|(
name|extension
argument_list|)
condition|)
block|{
return|return
name|extension
return|;
block|}
if|if
condition|(
operator|(
name|hiveOutputFormat
operator|instanceof
name|HiveIgnoreKeyTextOutputFormat
operator|)
operator|&&
name|isCompressed
condition|)
block|{
name|Class
argument_list|<
name|?
extends|extends
name|CompressionCodec
argument_list|>
name|codecClass
init|=
name|FileOutputFormat
operator|.
name|getOutputCompressorClass
argument_list|(
name|jc
argument_list|,
name|DefaultCodec
operator|.
name|class
argument_list|)
decl_stmt|;
name|CompressionCodec
name|codec
init|=
operator|(
name|CompressionCodec
operator|)
name|ReflectionUtils
operator|.
name|newInstance
argument_list|(
name|codecClass
argument_list|,
name|jc
argument_list|)
decl_stmt|;
return|return
name|codec
operator|.
name|getDefaultExtension
argument_list|()
return|;
block|}
return|return
literal|""
return|;
block|}
comment|/**    * Create a sequencefile output stream based on job configuration.    *    * @param jc    *          Job configuration    * @param fs    *          File System to create file in    * @param file    *          Path to be created    * @param keyClass    *          Java Class for key    * @param valClass    *          Java Class for value    * @return output stream over the created sequencefile    */
specifier|public
specifier|static
name|SequenceFile
operator|.
name|Writer
name|createSequenceWriter
parameter_list|(
name|JobConf
name|jc
parameter_list|,
name|FileSystem
name|fs
parameter_list|,
name|Path
name|file
parameter_list|,
name|Class
argument_list|<
name|?
argument_list|>
name|keyClass
parameter_list|,
name|Class
argument_list|<
name|?
argument_list|>
name|valClass
parameter_list|)
throws|throws
name|IOException
block|{
name|boolean
name|isCompressed
init|=
name|FileOutputFormat
operator|.
name|getCompressOutput
argument_list|(
name|jc
argument_list|)
decl_stmt|;
return|return
name|createSequenceWriter
argument_list|(
name|jc
argument_list|,
name|fs
argument_list|,
name|file
argument_list|,
name|keyClass
argument_list|,
name|valClass
argument_list|,
name|isCompressed
argument_list|)
return|;
block|}
comment|/**    * Create a sequencefile output stream based on job configuration Uses user supplied compression    * flag (rather than obtaining it from the Job Configuration).    *    * @param jc    *          Job configuration    * @param fs    *          File System to create file in    * @param file    *          Path to be created    * @param keyClass    *          Java Class for key    * @param valClass    *          Java Class for value    * @return output stream over the created sequencefile    */
specifier|public
specifier|static
name|SequenceFile
operator|.
name|Writer
name|createSequenceWriter
parameter_list|(
name|JobConf
name|jc
parameter_list|,
name|FileSystem
name|fs
parameter_list|,
name|Path
name|file
parameter_list|,
name|Class
argument_list|<
name|?
argument_list|>
name|keyClass
parameter_list|,
name|Class
argument_list|<
name|?
argument_list|>
name|valClass
parameter_list|,
name|boolean
name|isCompressed
parameter_list|)
throws|throws
name|IOException
block|{
name|CompressionCodec
name|codec
init|=
literal|null
decl_stmt|;
name|CompressionType
name|compressionType
init|=
name|CompressionType
operator|.
name|NONE
decl_stmt|;
name|Class
name|codecClass
init|=
literal|null
decl_stmt|;
if|if
condition|(
name|isCompressed
condition|)
block|{
name|compressionType
operator|=
name|SequenceFileOutputFormat
operator|.
name|getOutputCompressionType
argument_list|(
name|jc
argument_list|)
expr_stmt|;
name|codecClass
operator|=
name|FileOutputFormat
operator|.
name|getOutputCompressorClass
argument_list|(
name|jc
argument_list|,
name|DefaultCodec
operator|.
name|class
argument_list|)
expr_stmt|;
name|codec
operator|=
operator|(
name|CompressionCodec
operator|)
name|ReflectionUtils
operator|.
name|newInstance
argument_list|(
name|codecClass
argument_list|,
name|jc
argument_list|)
expr_stmt|;
block|}
return|return
operator|(
name|SequenceFile
operator|.
name|createWriter
argument_list|(
name|fs
argument_list|,
name|jc
argument_list|,
name|file
argument_list|,
name|keyClass
argument_list|,
name|valClass
argument_list|,
name|compressionType
argument_list|,
name|codec
argument_list|)
operator|)
return|;
block|}
comment|/**    * Create a RCFile output stream based on job configuration Uses user supplied compression flag    * (rather than obtaining it from the Job Configuration).    *    * @param jc    *          Job configuration    * @param fs    *          File System to create file in    * @param file    *          Path to be created    * @return output stream over the created rcfile    */
specifier|public
specifier|static
name|RCFile
operator|.
name|Writer
name|createRCFileWriter
parameter_list|(
name|JobConf
name|jc
parameter_list|,
name|FileSystem
name|fs
parameter_list|,
name|Path
name|file
parameter_list|,
name|boolean
name|isCompressed
parameter_list|)
throws|throws
name|IOException
block|{
name|CompressionCodec
name|codec
init|=
literal|null
decl_stmt|;
name|Class
argument_list|<
name|?
argument_list|>
name|codecClass
init|=
literal|null
decl_stmt|;
if|if
condition|(
name|isCompressed
condition|)
block|{
name|codecClass
operator|=
name|FileOutputFormat
operator|.
name|getOutputCompressorClass
argument_list|(
name|jc
argument_list|,
name|DefaultCodec
operator|.
name|class
argument_list|)
expr_stmt|;
name|codec
operator|=
operator|(
name|CompressionCodec
operator|)
name|ReflectionUtils
operator|.
name|newInstance
argument_list|(
name|codecClass
argument_list|,
name|jc
argument_list|)
expr_stmt|;
block|}
return|return
operator|new
name|RCFile
operator|.
name|Writer
argument_list|(
name|fs
argument_list|,
name|jc
argument_list|,
name|file
argument_list|,
literal|null
argument_list|,
name|codec
argument_list|)
return|;
block|}
comment|/**    * Shamelessly cloned from GenericOptionsParser.    */
specifier|public
specifier|static
name|String
name|realFile
parameter_list|(
name|String
name|newFile
parameter_list|,
name|Configuration
name|conf
parameter_list|)
throws|throws
name|IOException
block|{
name|Path
name|path
init|=
operator|new
name|Path
argument_list|(
name|newFile
argument_list|)
decl_stmt|;
name|URI
name|pathURI
init|=
name|path
operator|.
name|toUri
argument_list|()
decl_stmt|;
name|FileSystem
name|fs
decl_stmt|;
if|if
condition|(
name|pathURI
operator|.
name|getScheme
argument_list|()
operator|==
literal|null
condition|)
block|{
name|fs
operator|=
name|FileSystem
operator|.
name|getLocal
argument_list|(
name|conf
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|fs
operator|=
name|path
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
operator|!
name|fs
operator|.
name|exists
argument_list|(
name|path
argument_list|)
condition|)
block|{
return|return
literal|null
return|;
block|}
name|String
name|file
init|=
name|path
operator|.
name|makeQualified
argument_list|(
name|fs
argument_list|)
operator|.
name|toString
argument_list|()
decl_stmt|;
comment|// For compatibility with hadoop 0.17, change file:/a/b/c to file:///a/b/c
if|if
condition|(
name|StringUtils
operator|.
name|startsWith
argument_list|(
name|file
argument_list|,
literal|"file:/"
argument_list|)
operator|&&
operator|!
name|StringUtils
operator|.
name|startsWith
argument_list|(
name|file
argument_list|,
literal|"file:///"
argument_list|)
condition|)
block|{
name|file
operator|=
literal|"file:///"
operator|+
name|file
operator|.
name|substring
argument_list|(
literal|"file:/"
operator|.
name|length
argument_list|()
argument_list|)
expr_stmt|;
block|}
return|return
name|file
return|;
block|}
specifier|public
specifier|static
name|List
argument_list|<
name|String
argument_list|>
name|mergeUniqElems
parameter_list|(
name|List
argument_list|<
name|String
argument_list|>
name|src
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|dest
parameter_list|)
block|{
if|if
condition|(
name|dest
operator|==
literal|null
condition|)
block|{
return|return
name|src
return|;
block|}
if|if
condition|(
name|src
operator|==
literal|null
condition|)
block|{
return|return
name|dest
return|;
block|}
name|int
name|pos
init|=
literal|0
decl_stmt|;
while|while
condition|(
name|pos
operator|<
name|dest
operator|.
name|size
argument_list|()
condition|)
block|{
if|if
condition|(
operator|!
name|src
operator|.
name|contains
argument_list|(
name|dest
operator|.
name|get
argument_list|(
name|pos
argument_list|)
argument_list|)
condition|)
block|{
name|src
operator|.
name|add
argument_list|(
name|dest
operator|.
name|get
argument_list|(
name|pos
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|pos
operator|++
expr_stmt|;
block|}
return|return
name|src
return|;
block|}
specifier|private
specifier|static
specifier|final
name|String
name|tmpPrefix
init|=
literal|"_tmp."
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|String
name|taskTmpPrefix
init|=
literal|"_task_tmp."
decl_stmt|;
specifier|public
specifier|static
name|Path
name|toTaskTempPath
parameter_list|(
name|Path
name|orig
parameter_list|)
block|{
if|if
condition|(
name|orig
operator|.
name|getName
argument_list|()
operator|.
name|indexOf
argument_list|(
name|taskTmpPrefix
argument_list|)
operator|==
literal|0
condition|)
block|{
return|return
name|orig
return|;
block|}
return|return
operator|new
name|Path
argument_list|(
name|orig
operator|.
name|getParent
argument_list|()
argument_list|,
name|taskTmpPrefix
operator|+
name|orig
operator|.
name|getName
argument_list|()
argument_list|)
return|;
block|}
specifier|public
specifier|static
name|Path
name|toTaskTempPath
parameter_list|(
name|String
name|orig
parameter_list|)
block|{
return|return
name|toTaskTempPath
argument_list|(
operator|new
name|Path
argument_list|(
name|orig
argument_list|)
argument_list|)
return|;
block|}
specifier|public
specifier|static
name|Path
name|toTempPath
parameter_list|(
name|Path
name|orig
parameter_list|)
block|{
if|if
condition|(
name|orig
operator|.
name|getName
argument_list|()
operator|.
name|indexOf
argument_list|(
name|tmpPrefix
argument_list|)
operator|==
literal|0
condition|)
block|{
return|return
name|orig
return|;
block|}
return|return
operator|new
name|Path
argument_list|(
name|orig
operator|.
name|getParent
argument_list|()
argument_list|,
name|tmpPrefix
operator|+
name|orig
operator|.
name|getName
argument_list|()
argument_list|)
return|;
block|}
comment|/**    * Given a path, convert to a temporary path.    */
specifier|public
specifier|static
name|Path
name|toTempPath
parameter_list|(
name|String
name|orig
parameter_list|)
block|{
return|return
name|toTempPath
argument_list|(
operator|new
name|Path
argument_list|(
name|orig
argument_list|)
argument_list|)
return|;
block|}
comment|/**    * Detect if the supplied file is a temporary path.    */
specifier|public
specifier|static
name|boolean
name|isTempPath
parameter_list|(
name|FileStatus
name|file
parameter_list|)
block|{
name|String
name|name
init|=
name|file
operator|.
name|getPath
argument_list|()
operator|.
name|getName
argument_list|()
decl_stmt|;
comment|// in addition to detecting hive temporary files, we also check hadoop
comment|// temporary folders that used to show up in older releases
return|return
operator|(
name|name
operator|.
name|startsWith
argument_list|(
literal|"_task"
argument_list|)
operator|||
name|name
operator|.
name|startsWith
argument_list|(
name|tmpPrefix
argument_list|)
operator|)
return|;
block|}
comment|/**    * Rename src to dst, or in the case dst already exists, move files in src to dst. If there is an    * existing file with the same name, the new file's name will be appended with "_1", "_2", etc.    *    * @param fs    *          the FileSystem where src and dst are on.    * @param src    *          the src directory    * @param dst    *          the target directory    * @throws IOException    */
specifier|public
specifier|static
name|void
name|rename
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|Path
name|src
parameter_list|,
name|Path
name|dst
parameter_list|)
throws|throws
name|IOException
throws|,
name|HiveException
block|{
if|if
condition|(
operator|!
name|fs
operator|.
name|rename
argument_list|(
name|src
argument_list|,
name|dst
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Unable to move: "
operator|+
name|src
operator|+
literal|" to: "
operator|+
name|dst
argument_list|)
throw|;
block|}
block|}
comment|/**    * Rename src to dst, or in the case dst already exists, move files in src to dst. If there is an    * existing file with the same name, the new file's name will be appended with "_1", "_2", etc.    *    * @param fs    *          the FileSystem where src and dst are on.    * @param src    *          the src directory    * @param dst    *          the target directory    * @throws IOException    */
specifier|public
specifier|static
name|void
name|renameOrMoveFiles
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|Path
name|src
parameter_list|,
name|Path
name|dst
parameter_list|)
throws|throws
name|IOException
throws|,
name|HiveException
block|{
if|if
condition|(
operator|!
name|fs
operator|.
name|exists
argument_list|(
name|dst
argument_list|)
condition|)
block|{
if|if
condition|(
operator|!
name|fs
operator|.
name|rename
argument_list|(
name|src
argument_list|,
name|dst
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Unable to move: "
operator|+
name|src
operator|+
literal|" to: "
operator|+
name|dst
argument_list|)
throw|;
block|}
block|}
else|else
block|{
comment|// move file by file
name|FileStatus
index|[]
name|files
init|=
name|fs
operator|.
name|listStatus
argument_list|(
name|src
argument_list|)
decl_stmt|;
for|for
control|(
name|FileStatus
name|file
range|:
name|files
control|)
block|{
name|Path
name|srcFilePath
init|=
name|file
operator|.
name|getPath
argument_list|()
decl_stmt|;
name|String
name|fileName
init|=
name|srcFilePath
operator|.
name|getName
argument_list|()
decl_stmt|;
name|Path
name|dstFilePath
init|=
operator|new
name|Path
argument_list|(
name|dst
argument_list|,
name|fileName
argument_list|)
decl_stmt|;
if|if
condition|(
name|file
operator|.
name|isDir
argument_list|()
condition|)
block|{
name|renameOrMoveFiles
argument_list|(
name|fs
argument_list|,
name|srcFilePath
argument_list|,
name|dstFilePath
argument_list|)
expr_stmt|;
block|}
else|else
block|{
if|if
condition|(
name|fs
operator|.
name|exists
argument_list|(
name|dstFilePath
argument_list|)
condition|)
block|{
name|int
name|suffix
init|=
literal|0
decl_stmt|;
do|do
block|{
name|suffix
operator|++
expr_stmt|;
name|dstFilePath
operator|=
operator|new
name|Path
argument_list|(
name|dst
argument_list|,
name|fileName
operator|+
literal|"_"
operator|+
name|suffix
argument_list|)
expr_stmt|;
block|}
do|while
condition|(
name|fs
operator|.
name|exists
argument_list|(
name|dstFilePath
argument_list|)
condition|)
do|;
block|}
if|if
condition|(
operator|!
name|fs
operator|.
name|rename
argument_list|(
name|srcFilePath
argument_list|,
name|dstFilePath
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Unable to move: "
operator|+
name|src
operator|+
literal|" to: "
operator|+
name|dst
argument_list|)
throw|;
block|}
block|}
block|}
block|}
block|}
comment|/**    * The first group will contain the task id. The second group is the optional extension. The file    * name looks like: "0_0" or "0_0.gz". There may be a leading prefix (tmp_). Since getTaskId() can    * return an integer only - this should match a pure integer as well. {1,3} is used to limit    * matching for attempts #'s 0-999.    */
specifier|private
specifier|static
specifier|final
name|Pattern
name|FILE_NAME_TO_TASK_ID_REGEX
init|=
name|Pattern
operator|.
name|compile
argument_list|(
literal|"^.*?([0-9]+)(_[0-9]{1,3})?(\\..*)?$"
argument_list|)
decl_stmt|;
comment|/**    * This retruns prefix part + taskID for bucket join for partitioned table    */
specifier|private
specifier|static
specifier|final
name|Pattern
name|FILE_NAME_PREFIXED_TASK_ID_REGEX
init|=
name|Pattern
operator|.
name|compile
argument_list|(
literal|"^.*?((\\(.*\\))?[0-9]+)(_[0-9]{1,3})?(\\..*)?$"
argument_list|)
decl_stmt|;
comment|/**    * This breaks a prefixed bucket number into the prefix and the taskID    */
specifier|private
specifier|static
specifier|final
name|Pattern
name|PREFIXED_TASK_ID_REGEX
init|=
name|Pattern
operator|.
name|compile
argument_list|(
literal|"^(.*?\\(.*\\))?([0-9]+)$"
argument_list|)
decl_stmt|;
comment|/**    * Get the task id from the filename. It is assumed that the filename is derived from the output    * of getTaskId    *    * @param filename    *          filename to extract taskid from    */
specifier|public
specifier|static
name|String
name|getTaskIdFromFilename
parameter_list|(
name|String
name|filename
parameter_list|)
block|{
return|return
name|getIdFromFilename
argument_list|(
name|filename
argument_list|,
name|FILE_NAME_TO_TASK_ID_REGEX
argument_list|)
return|;
block|}
comment|/**    * Get the part-spec + task id from the filename. It is assumed that the filename is derived    * from the output of getTaskId    *    * @param filename    *          filename to extract taskid from    */
specifier|public
specifier|static
name|String
name|getPrefixedTaskIdFromFilename
parameter_list|(
name|String
name|filename
parameter_list|)
block|{
return|return
name|getIdFromFilename
argument_list|(
name|filename
argument_list|,
name|FILE_NAME_PREFIXED_TASK_ID_REGEX
argument_list|)
return|;
block|}
specifier|private
specifier|static
name|String
name|getIdFromFilename
parameter_list|(
name|String
name|filename
parameter_list|,
name|Pattern
name|pattern
parameter_list|)
block|{
name|String
name|taskId
init|=
name|filename
decl_stmt|;
name|int
name|dirEnd
init|=
name|filename
operator|.
name|lastIndexOf
argument_list|(
name|Path
operator|.
name|SEPARATOR
argument_list|)
decl_stmt|;
if|if
condition|(
name|dirEnd
operator|!=
operator|-
literal|1
condition|)
block|{
name|taskId
operator|=
name|filename
operator|.
name|substring
argument_list|(
name|dirEnd
operator|+
literal|1
argument_list|)
expr_stmt|;
block|}
name|Matcher
name|m
init|=
name|pattern
operator|.
name|matcher
argument_list|(
name|taskId
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|m
operator|.
name|matches
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Unable to get task id from file name: "
operator|+
name|filename
operator|+
literal|". Using last component"
operator|+
name|taskId
operator|+
literal|" as task id."
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|taskId
operator|=
name|m
operator|.
name|group
argument_list|(
literal|1
argument_list|)
expr_stmt|;
block|}
name|LOG
operator|.
name|debug
argument_list|(
literal|"TaskId for "
operator|+
name|filename
operator|+
literal|" = "
operator|+
name|taskId
argument_list|)
expr_stmt|;
return|return
name|taskId
return|;
block|}
specifier|public
specifier|static
name|String
name|getFileNameFromDirName
parameter_list|(
name|String
name|dirName
parameter_list|)
block|{
name|int
name|dirEnd
init|=
name|dirName
operator|.
name|lastIndexOf
argument_list|(
name|Path
operator|.
name|SEPARATOR
argument_list|)
decl_stmt|;
if|if
condition|(
name|dirEnd
operator|!=
operator|-
literal|1
condition|)
block|{
return|return
name|dirName
operator|.
name|substring
argument_list|(
name|dirEnd
operator|+
literal|1
argument_list|)
return|;
block|}
return|return
name|dirName
return|;
block|}
comment|/**    * Replace the task id from the filename. It is assumed that the filename is derived from the    * output of getTaskId    *    * @param filename    *          filename to replace taskid "0_0" or "0_0.gz" by 33 to "33_0" or "33_0.gz"    */
specifier|public
specifier|static
name|String
name|replaceTaskIdFromFilename
parameter_list|(
name|String
name|filename
parameter_list|,
name|int
name|bucketNum
parameter_list|)
block|{
return|return
name|replaceTaskIdFromFilename
argument_list|(
name|filename
argument_list|,
name|String
operator|.
name|valueOf
argument_list|(
name|bucketNum
argument_list|)
argument_list|)
return|;
block|}
specifier|public
specifier|static
name|String
name|replaceTaskIdFromFilename
parameter_list|(
name|String
name|filename
parameter_list|,
name|String
name|fileId
parameter_list|)
block|{
name|String
name|taskId
init|=
name|getTaskIdFromFilename
argument_list|(
name|filename
argument_list|)
decl_stmt|;
name|String
name|newTaskId
init|=
name|replaceTaskId
argument_list|(
name|taskId
argument_list|,
name|fileId
argument_list|)
decl_stmt|;
name|String
name|ret
init|=
name|replaceTaskIdFromFilename
argument_list|(
name|filename
argument_list|,
name|taskId
argument_list|,
name|newTaskId
argument_list|)
decl_stmt|;
return|return
operator|(
name|ret
operator|)
return|;
block|}
specifier|private
specifier|static
name|String
name|replaceTaskId
parameter_list|(
name|String
name|taskId
parameter_list|,
name|int
name|bucketNum
parameter_list|)
block|{
return|return
name|replaceTaskId
argument_list|(
name|taskId
argument_list|,
name|String
operator|.
name|valueOf
argument_list|(
name|bucketNum
argument_list|)
argument_list|)
return|;
block|}
comment|/**    * Returns strBucketNum with enough 0's prefixing the task ID portion of the String to make it    * equal in length to taskId    *    * @param taskId - the taskId used as a template for length    * @param strBucketNum - the bucket number of the output, may or may not be prefixed    * @return    */
specifier|private
specifier|static
name|String
name|replaceTaskId
parameter_list|(
name|String
name|taskId
parameter_list|,
name|String
name|strBucketNum
parameter_list|)
block|{
name|Matcher
name|m
init|=
name|PREFIXED_TASK_ID_REGEX
operator|.
name|matcher
argument_list|(
name|strBucketNum
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|m
operator|.
name|matches
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Unable to determine bucket number from file ID: "
operator|+
name|strBucketNum
operator|+
literal|". Using "
operator|+
literal|"file ID as bucket number."
argument_list|)
expr_stmt|;
return|return
name|adjustBucketNumLen
argument_list|(
name|strBucketNum
argument_list|,
name|taskId
argument_list|)
return|;
block|}
else|else
block|{
name|String
name|adjustedBucketNum
init|=
name|adjustBucketNumLen
argument_list|(
name|m
operator|.
name|group
argument_list|(
literal|2
argument_list|)
argument_list|,
name|taskId
argument_list|)
decl_stmt|;
return|return
operator|(
name|m
operator|.
name|group
argument_list|(
literal|1
argument_list|)
operator|==
literal|null
condition|?
literal|""
else|:
name|m
operator|.
name|group
argument_list|(
literal|1
argument_list|)
operator|)
operator|+
name|adjustedBucketNum
return|;
block|}
block|}
comment|/**    * Adds 0's to the beginning of bucketNum until bucketNum and taskId are the same length.    *    * @param bucketNum - the bucket number, should not be prefixed    * @param taskId - the taskId used as a template for length    * @return    */
specifier|private
specifier|static
name|String
name|adjustBucketNumLen
parameter_list|(
name|String
name|bucketNum
parameter_list|,
name|String
name|taskId
parameter_list|)
block|{
name|int
name|bucketNumLen
init|=
name|bucketNum
operator|.
name|length
argument_list|()
decl_stmt|;
name|int
name|taskIdLen
init|=
name|taskId
operator|.
name|length
argument_list|()
decl_stmt|;
name|StringBuffer
name|s
init|=
operator|new
name|StringBuffer
argument_list|()
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|taskIdLen
operator|-
name|bucketNumLen
condition|;
name|i
operator|++
control|)
block|{
name|s
operator|.
name|append
argument_list|(
literal|"0"
argument_list|)
expr_stmt|;
block|}
return|return
name|s
operator|.
name|toString
argument_list|()
operator|+
name|bucketNum
return|;
block|}
comment|/**    * Replace the oldTaskId appearing in the filename by the newTaskId. The string oldTaskId could    * appear multiple times, we should only replace the last one.    *    * @param filename    * @param oldTaskId    * @param newTaskId    * @return    */
specifier|private
specifier|static
name|String
name|replaceTaskIdFromFilename
parameter_list|(
name|String
name|filename
parameter_list|,
name|String
name|oldTaskId
parameter_list|,
name|String
name|newTaskId
parameter_list|)
block|{
name|String
index|[]
name|spl
init|=
name|filename
operator|.
name|split
argument_list|(
name|oldTaskId
argument_list|)
decl_stmt|;
if|if
condition|(
operator|(
name|spl
operator|.
name|length
operator|==
literal|0
operator|)
operator|||
operator|(
name|spl
operator|.
name|length
operator|==
literal|1
operator|)
condition|)
block|{
return|return
name|filename
operator|.
name|replaceAll
argument_list|(
name|oldTaskId
argument_list|,
name|newTaskId
argument_list|)
return|;
block|}
name|StringBuffer
name|snew
init|=
operator|new
name|StringBuffer
argument_list|()
decl_stmt|;
for|for
control|(
name|int
name|idx
init|=
literal|0
init|;
name|idx
operator|<
name|spl
operator|.
name|length
operator|-
literal|1
condition|;
name|idx
operator|++
control|)
block|{
if|if
condition|(
name|idx
operator|>
literal|0
condition|)
block|{
name|snew
operator|.
name|append
argument_list|(
name|oldTaskId
argument_list|)
expr_stmt|;
block|}
name|snew
operator|.
name|append
argument_list|(
name|spl
index|[
name|idx
index|]
argument_list|)
expr_stmt|;
block|}
name|snew
operator|.
name|append
argument_list|(
name|newTaskId
argument_list|)
expr_stmt|;
name|snew
operator|.
name|append
argument_list|(
name|spl
index|[
name|spl
operator|.
name|length
operator|-
literal|1
index|]
argument_list|)
expr_stmt|;
return|return
name|snew
operator|.
name|toString
argument_list|()
return|;
block|}
comment|/**    * returns null if path is not exist    */
specifier|public
specifier|static
name|FileStatus
index|[]
name|listStatusIfExists
parameter_list|(
name|Path
name|path
parameter_list|,
name|FileSystem
name|fs
parameter_list|)
throws|throws
name|IOException
block|{
try|try
block|{
return|return
name|fs
operator|.
name|listStatus
argument_list|(
name|path
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|FileNotFoundException
name|e
parameter_list|)
block|{
comment|// FS in hadoop 2.0 throws FNF instead of returning null
return|return
literal|null
return|;
block|}
block|}
specifier|public
specifier|static
name|void
name|mvFileToFinalPath
parameter_list|(
name|String
name|specPath
parameter_list|,
name|Configuration
name|hconf
parameter_list|,
name|boolean
name|success
parameter_list|,
name|Log
name|log
parameter_list|,
name|DynamicPartitionCtx
name|dpCtx
parameter_list|,
name|FileSinkDesc
name|conf
parameter_list|,
name|Reporter
name|reporter
parameter_list|)
throws|throws
name|IOException
throws|,
name|HiveException
block|{
name|FileSystem
name|fs
init|=
operator|(
operator|new
name|Path
argument_list|(
name|specPath
argument_list|)
operator|)
operator|.
name|getFileSystem
argument_list|(
name|hconf
argument_list|)
decl_stmt|;
name|Path
name|tmpPath
init|=
name|Utilities
operator|.
name|toTempPath
argument_list|(
name|specPath
argument_list|)
decl_stmt|;
name|Path
name|taskTmpPath
init|=
name|Utilities
operator|.
name|toTaskTempPath
argument_list|(
name|specPath
argument_list|)
decl_stmt|;
name|Path
name|finalPath
init|=
operator|new
name|Path
argument_list|(
name|specPath
argument_list|)
decl_stmt|;
if|if
condition|(
name|success
condition|)
block|{
if|if
condition|(
name|fs
operator|.
name|exists
argument_list|(
name|tmpPath
argument_list|)
condition|)
block|{
comment|// remove any tmp file or double-committed output files
name|ArrayList
argument_list|<
name|String
argument_list|>
name|emptyBuckets
init|=
name|Utilities
operator|.
name|removeTempOrDuplicateFiles
argument_list|(
name|fs
argument_list|,
name|tmpPath
argument_list|,
name|dpCtx
argument_list|)
decl_stmt|;
comment|// create empty buckets if necessary
if|if
condition|(
name|emptyBuckets
operator|.
name|size
argument_list|()
operator|>
literal|0
condition|)
block|{
name|createEmptyBuckets
argument_list|(
name|hconf
argument_list|,
name|emptyBuckets
argument_list|,
name|conf
argument_list|,
name|reporter
argument_list|)
expr_stmt|;
block|}
comment|// move to the file destination
name|log
operator|.
name|info
argument_list|(
literal|"Moving tmp dir: "
operator|+
name|tmpPath
operator|+
literal|" to: "
operator|+
name|finalPath
argument_list|)
expr_stmt|;
name|Utilities
operator|.
name|renameOrMoveFiles
argument_list|(
name|fs
argument_list|,
name|tmpPath
argument_list|,
name|finalPath
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
name|fs
operator|.
name|delete
argument_list|(
name|tmpPath
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
name|fs
operator|.
name|delete
argument_list|(
name|taskTmpPath
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
comment|/**    * Check the existence of buckets according to bucket specification. Create empty buckets if    * needed.    *    * @param hconf    * @param paths A list of empty buckets to create    * @param conf The definition of the FileSink.    * @param reporter The mapreduce reporter object    * @throws HiveException    * @throws IOException    */
specifier|private
specifier|static
name|void
name|createEmptyBuckets
parameter_list|(
name|Configuration
name|hconf
parameter_list|,
name|ArrayList
argument_list|<
name|String
argument_list|>
name|paths
parameter_list|,
name|FileSinkDesc
name|conf
parameter_list|,
name|Reporter
name|reporter
parameter_list|)
throws|throws
name|HiveException
throws|,
name|IOException
block|{
name|JobConf
name|jc
decl_stmt|;
if|if
condition|(
name|hconf
operator|instanceof
name|JobConf
condition|)
block|{
name|jc
operator|=
operator|new
name|JobConf
argument_list|(
name|hconf
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// test code path
name|jc
operator|=
operator|new
name|JobConf
argument_list|(
name|hconf
argument_list|)
expr_stmt|;
block|}
name|HiveOutputFormat
argument_list|<
name|?
argument_list|,
name|?
argument_list|>
name|hiveOutputFormat
init|=
literal|null
decl_stmt|;
name|Class
argument_list|<
name|?
extends|extends
name|Writable
argument_list|>
name|outputClass
init|=
literal|null
decl_stmt|;
name|boolean
name|isCompressed
init|=
name|conf
operator|.
name|getCompressed
argument_list|()
decl_stmt|;
name|TableDesc
name|tableInfo
init|=
name|conf
operator|.
name|getTableInfo
argument_list|()
decl_stmt|;
try|try
block|{
name|Serializer
name|serializer
init|=
operator|(
name|Serializer
operator|)
name|tableInfo
operator|.
name|getDeserializerClass
argument_list|()
operator|.
name|newInstance
argument_list|()
decl_stmt|;
name|serializer
operator|.
name|initialize
argument_list|(
literal|null
argument_list|,
name|tableInfo
operator|.
name|getProperties
argument_list|()
argument_list|)
expr_stmt|;
name|outputClass
operator|=
name|serializer
operator|.
name|getSerializedClass
argument_list|()
expr_stmt|;
name|hiveOutputFormat
operator|=
name|conf
operator|.
name|getTableInfo
argument_list|()
operator|.
name|getOutputFileFormatClass
argument_list|()
operator|.
name|newInstance
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|SerDeException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
catch|catch
parameter_list|(
name|InstantiationException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
catch|catch
parameter_list|(
name|IllegalAccessException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
for|for
control|(
name|String
name|p
range|:
name|paths
control|)
block|{
name|Path
name|path
init|=
operator|new
name|Path
argument_list|(
name|p
argument_list|)
decl_stmt|;
name|FSRecordWriter
name|writer
init|=
name|HiveFileFormatUtils
operator|.
name|getRecordWriter
argument_list|(
name|jc
argument_list|,
name|hiveOutputFormat
argument_list|,
name|outputClass
argument_list|,
name|isCompressed
argument_list|,
name|tableInfo
operator|.
name|getProperties
argument_list|()
argument_list|,
name|path
argument_list|,
name|reporter
argument_list|)
decl_stmt|;
name|writer
operator|.
name|close
argument_list|(
literal|false
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"created empty bucket for enforcing bucketing at "
operator|+
name|path
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * Remove all temporary files and duplicate (double-committed) files from a given directory.    */
specifier|public
specifier|static
name|void
name|removeTempOrDuplicateFiles
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|Path
name|path
parameter_list|)
throws|throws
name|IOException
block|{
name|removeTempOrDuplicateFiles
argument_list|(
name|fs
argument_list|,
name|path
argument_list|,
literal|null
argument_list|)
expr_stmt|;
block|}
comment|/**    * Remove all temporary files and duplicate (double-committed) files from a given directory.    *    * @return a list of path names corresponding to should-be-created empty buckets.    */
specifier|public
specifier|static
name|ArrayList
argument_list|<
name|String
argument_list|>
name|removeTempOrDuplicateFiles
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|Path
name|path
parameter_list|,
name|DynamicPartitionCtx
name|dpCtx
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|path
operator|==
literal|null
condition|)
block|{
return|return
literal|null
return|;
block|}
name|ArrayList
argument_list|<
name|String
argument_list|>
name|result
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
if|if
condition|(
name|dpCtx
operator|!=
literal|null
condition|)
block|{
name|FileStatus
name|parts
index|[]
init|=
name|HiveStatsUtils
operator|.
name|getFileStatusRecurse
argument_list|(
name|path
argument_list|,
name|dpCtx
operator|.
name|getNumDPCols
argument_list|()
argument_list|,
name|fs
argument_list|)
decl_stmt|;
name|HashMap
argument_list|<
name|String
argument_list|,
name|FileStatus
argument_list|>
name|taskIDToFile
init|=
literal|null
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|parts
operator|.
name|length
condition|;
operator|++
name|i
control|)
block|{
assert|assert
name|parts
index|[
name|i
index|]
operator|.
name|isDir
argument_list|()
operator|:
literal|"dynamic partition "
operator|+
name|parts
index|[
name|i
index|]
operator|.
name|getPath
argument_list|()
operator|+
literal|" is not a direcgtory"
assert|;
name|FileStatus
index|[]
name|items
init|=
name|fs
operator|.
name|listStatus
argument_list|(
name|parts
index|[
name|i
index|]
operator|.
name|getPath
argument_list|()
argument_list|)
decl_stmt|;
comment|// remove empty directory since DP insert should not generate empty partitions.
comment|// empty directories could be generated by crashed Task/ScriptOperator
if|if
condition|(
name|items
operator|.
name|length
operator|==
literal|0
condition|)
block|{
if|if
condition|(
operator|!
name|fs
operator|.
name|delete
argument_list|(
name|parts
index|[
name|i
index|]
operator|.
name|getPath
argument_list|()
argument_list|,
literal|true
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Cannot delete empty directory "
operator|+
name|parts
index|[
name|i
index|]
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Cannot delete empty directory "
operator|+
name|parts
index|[
name|i
index|]
operator|.
name|getPath
argument_list|()
argument_list|)
throw|;
block|}
block|}
name|taskIDToFile
operator|=
name|removeTempOrDuplicateFiles
argument_list|(
name|items
argument_list|,
name|fs
argument_list|)
expr_stmt|;
comment|// if the table is bucketed and enforce bucketing, we should check and generate all buckets
if|if
condition|(
name|dpCtx
operator|.
name|getNumBuckets
argument_list|()
operator|>
literal|0
operator|&&
name|taskIDToFile
operator|!=
literal|null
condition|)
block|{
comment|// refresh the file list
name|items
operator|=
name|fs
operator|.
name|listStatus
argument_list|(
name|parts
index|[
name|i
index|]
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
comment|// get the missing buckets and generate empty buckets
name|String
name|taskID1
init|=
name|taskIDToFile
operator|.
name|keySet
argument_list|()
operator|.
name|iterator
argument_list|()
operator|.
name|next
argument_list|()
decl_stmt|;
name|Path
name|bucketPath
init|=
name|taskIDToFile
operator|.
name|values
argument_list|()
operator|.
name|iterator
argument_list|()
operator|.
name|next
argument_list|()
operator|.
name|getPath
argument_list|()
decl_stmt|;
for|for
control|(
name|int
name|j
init|=
literal|0
init|;
name|j
operator|<
name|dpCtx
operator|.
name|getNumBuckets
argument_list|()
condition|;
operator|++
name|j
control|)
block|{
name|String
name|taskID2
init|=
name|replaceTaskId
argument_list|(
name|taskID1
argument_list|,
name|j
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|taskIDToFile
operator|.
name|containsKey
argument_list|(
name|taskID2
argument_list|)
condition|)
block|{
comment|// create empty bucket, file name should be derived from taskID2
name|String
name|path2
init|=
name|replaceTaskIdFromFilename
argument_list|(
name|bucketPath
operator|.
name|toUri
argument_list|()
operator|.
name|getPath
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|,
name|j
argument_list|)
decl_stmt|;
name|result
operator|.
name|add
argument_list|(
name|path2
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
block|}
else|else
block|{
name|FileStatus
index|[]
name|items
init|=
name|fs
operator|.
name|listStatus
argument_list|(
name|path
argument_list|)
decl_stmt|;
name|removeTempOrDuplicateFiles
argument_list|(
name|items
argument_list|,
name|fs
argument_list|)
expr_stmt|;
block|}
return|return
name|result
return|;
block|}
specifier|public
specifier|static
name|HashMap
argument_list|<
name|String
argument_list|,
name|FileStatus
argument_list|>
name|removeTempOrDuplicateFiles
parameter_list|(
name|FileStatus
index|[]
name|items
parameter_list|,
name|FileSystem
name|fs
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|items
operator|==
literal|null
operator|||
name|fs
operator|==
literal|null
condition|)
block|{
return|return
literal|null
return|;
block|}
name|HashMap
argument_list|<
name|String
argument_list|,
name|FileStatus
argument_list|>
name|taskIdToFile
init|=
operator|new
name|HashMap
argument_list|<
name|String
argument_list|,
name|FileStatus
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|FileStatus
name|one
range|:
name|items
control|)
block|{
if|if
condition|(
name|isTempPath
argument_list|(
name|one
argument_list|)
condition|)
block|{
if|if
condition|(
operator|!
name|fs
operator|.
name|delete
argument_list|(
name|one
operator|.
name|getPath
argument_list|()
argument_list|,
literal|true
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Unable to delete tmp file: "
operator|+
name|one
operator|.
name|getPath
argument_list|()
argument_list|)
throw|;
block|}
block|}
else|else
block|{
name|String
name|taskId
init|=
name|getPrefixedTaskIdFromFilename
argument_list|(
name|one
operator|.
name|getPath
argument_list|()
operator|.
name|getName
argument_list|()
argument_list|)
decl_stmt|;
name|FileStatus
name|otherFile
init|=
name|taskIdToFile
operator|.
name|get
argument_list|(
name|taskId
argument_list|)
decl_stmt|;
if|if
condition|(
name|otherFile
operator|==
literal|null
condition|)
block|{
name|taskIdToFile
operator|.
name|put
argument_list|(
name|taskId
argument_list|,
name|one
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// Compare the file sizes of all the attempt files for the same task, the largest win
comment|// any attempt files could contain partial results (due to task failures or
comment|// speculative runs), but the largest should be the correct one since the result
comment|// of a successful run should never be smaller than a failed/speculative run.
name|FileStatus
name|toDelete
init|=
literal|null
decl_stmt|;
if|if
condition|(
name|otherFile
operator|.
name|getLen
argument_list|()
operator|>=
name|one
operator|.
name|getLen
argument_list|()
condition|)
block|{
name|toDelete
operator|=
name|one
expr_stmt|;
block|}
else|else
block|{
name|toDelete
operator|=
name|otherFile
expr_stmt|;
name|taskIdToFile
operator|.
name|put
argument_list|(
name|taskId
argument_list|,
name|one
argument_list|)
expr_stmt|;
block|}
name|long
name|len1
init|=
name|toDelete
operator|.
name|getLen
argument_list|()
decl_stmt|;
name|long
name|len2
init|=
name|taskIdToFile
operator|.
name|get
argument_list|(
name|taskId
argument_list|)
operator|.
name|getLen
argument_list|()
decl_stmt|;
if|if
condition|(
operator|!
name|fs
operator|.
name|delete
argument_list|(
name|toDelete
operator|.
name|getPath
argument_list|()
argument_list|,
literal|true
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Unable to delete duplicate file: "
operator|+
name|toDelete
operator|.
name|getPath
argument_list|()
operator|+
literal|". Existing file: "
operator|+
name|taskIdToFile
operator|.
name|get
argument_list|(
name|taskId
argument_list|)
operator|.
name|getPath
argument_list|()
argument_list|)
throw|;
block|}
else|else
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Duplicate taskid file removed: "
operator|+
name|toDelete
operator|.
name|getPath
argument_list|()
operator|+
literal|" with length "
operator|+
name|len1
operator|+
literal|". Existing file: "
operator|+
name|taskIdToFile
operator|.
name|get
argument_list|(
name|taskId
argument_list|)
operator|.
name|getPath
argument_list|()
operator|+
literal|" with length "
operator|+
name|len2
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
return|return
name|taskIdToFile
return|;
block|}
specifier|public
specifier|static
name|String
name|getNameMessage
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
return|return
name|e
operator|.
name|getClass
argument_list|()
operator|.
name|getName
argument_list|()
operator|+
literal|"("
operator|+
name|e
operator|.
name|getMessage
argument_list|()
operator|+
literal|")"
return|;
block|}
specifier|public
specifier|static
name|String
name|getResourceFiles
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|SessionState
operator|.
name|ResourceType
name|t
parameter_list|)
block|{
comment|// fill in local files to be added to the task environment
name|SessionState
name|ss
init|=
name|SessionState
operator|.
name|get
argument_list|()
decl_stmt|;
name|Set
argument_list|<
name|String
argument_list|>
name|files
init|=
operator|(
name|ss
operator|==
literal|null
operator|)
condition|?
literal|null
else|:
name|ss
operator|.
name|list_resource
argument_list|(
name|t
argument_list|,
literal|null
argument_list|)
decl_stmt|;
if|if
condition|(
name|files
operator|!=
literal|null
condition|)
block|{
name|List
argument_list|<
name|String
argument_list|>
name|realFiles
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|(
name|files
operator|.
name|size
argument_list|()
argument_list|)
decl_stmt|;
for|for
control|(
name|String
name|one
range|:
name|files
control|)
block|{
try|try
block|{
name|realFiles
operator|.
name|add
argument_list|(
name|realFile
argument_list|(
name|one
argument_list|,
name|conf
argument_list|)
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
literal|"Cannot validate file "
operator|+
name|one
operator|+
literal|"due to exception: "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
return|return
name|StringUtils
operator|.
name|join
argument_list|(
name|realFiles
argument_list|,
literal|","
argument_list|)
return|;
block|}
else|else
block|{
return|return
literal|""
return|;
block|}
block|}
comment|/**    * Add new elements to the classpath.    *    * @param newPaths    *          Array of classpath elements    */
specifier|public
specifier|static
name|ClassLoader
name|addToClassPath
parameter_list|(
name|ClassLoader
name|cloader
parameter_list|,
name|String
index|[]
name|newPaths
parameter_list|)
throws|throws
name|Exception
block|{
name|URLClassLoader
name|loader
init|=
operator|(
name|URLClassLoader
operator|)
name|cloader
decl_stmt|;
name|List
argument_list|<
name|URL
argument_list|>
name|curPath
init|=
name|Arrays
operator|.
name|asList
argument_list|(
name|loader
operator|.
name|getURLs
argument_list|()
argument_list|)
decl_stmt|;
name|ArrayList
argument_list|<
name|URL
argument_list|>
name|newPath
init|=
operator|new
name|ArrayList
argument_list|<
name|URL
argument_list|>
argument_list|()
decl_stmt|;
comment|// get a list with the current classpath components
for|for
control|(
name|URL
name|onePath
range|:
name|curPath
control|)
block|{
name|newPath
operator|.
name|add
argument_list|(
name|onePath
argument_list|)
expr_stmt|;
block|}
name|curPath
operator|=
name|newPath
expr_stmt|;
for|for
control|(
name|String
name|onestr
range|:
name|newPaths
control|)
block|{
comment|// special processing for hadoop-17. file:// needs to be removed
if|if
condition|(
name|StringUtils
operator|.
name|indexOf
argument_list|(
name|onestr
argument_list|,
literal|"file://"
argument_list|)
operator|==
literal|0
condition|)
block|{
name|onestr
operator|=
name|StringUtils
operator|.
name|substring
argument_list|(
name|onestr
argument_list|,
literal|7
argument_list|)
expr_stmt|;
block|}
name|URL
name|oneurl
init|=
operator|(
operator|new
name|File
argument_list|(
name|onestr
argument_list|)
operator|)
operator|.
name|toURL
argument_list|()
decl_stmt|;
if|if
condition|(
operator|!
name|curPath
operator|.
name|contains
argument_list|(
name|oneurl
argument_list|)
condition|)
block|{
name|curPath
operator|.
name|add
argument_list|(
name|oneurl
argument_list|)
expr_stmt|;
block|}
block|}
return|return
operator|new
name|URLClassLoader
argument_list|(
name|curPath
operator|.
name|toArray
argument_list|(
operator|new
name|URL
index|[
literal|0
index|]
argument_list|)
argument_list|,
name|loader
argument_list|)
return|;
block|}
comment|/**    * remove elements from the classpath.    *    * @param pathsToRemove    *          Array of classpath elements    */
specifier|public
specifier|static
name|void
name|removeFromClassPath
parameter_list|(
name|String
index|[]
name|pathsToRemove
parameter_list|)
throws|throws
name|Exception
block|{
name|Thread
name|curThread
init|=
name|Thread
operator|.
name|currentThread
argument_list|()
decl_stmt|;
name|URLClassLoader
name|loader
init|=
operator|(
name|URLClassLoader
operator|)
name|curThread
operator|.
name|getContextClassLoader
argument_list|()
decl_stmt|;
name|Set
argument_list|<
name|URL
argument_list|>
name|newPath
init|=
operator|new
name|HashSet
argument_list|<
name|URL
argument_list|>
argument_list|(
name|Arrays
operator|.
name|asList
argument_list|(
name|loader
operator|.
name|getURLs
argument_list|()
argument_list|)
argument_list|)
decl_stmt|;
for|for
control|(
name|String
name|onestr
range|:
name|pathsToRemove
control|)
block|{
comment|// special processing for hadoop-17. file:// needs to be removed
if|if
condition|(
name|StringUtils
operator|.
name|indexOf
argument_list|(
name|onestr
argument_list|,
literal|"file://"
argument_list|)
operator|==
literal|0
condition|)
block|{
name|onestr
operator|=
name|StringUtils
operator|.
name|substring
argument_list|(
name|onestr
argument_list|,
literal|7
argument_list|)
expr_stmt|;
block|}
name|URL
name|oneurl
init|=
operator|(
operator|new
name|File
argument_list|(
name|onestr
argument_list|)
operator|)
operator|.
name|toURL
argument_list|()
decl_stmt|;
name|newPath
operator|.
name|remove
argument_list|(
name|oneurl
argument_list|)
expr_stmt|;
block|}
name|loader
operator|=
operator|new
name|URLClassLoader
argument_list|(
name|newPath
operator|.
name|toArray
argument_list|(
operator|new
name|URL
index|[
literal|0
index|]
argument_list|)
argument_list|)
expr_stmt|;
name|curThread
operator|.
name|setContextClassLoader
argument_list|(
name|loader
argument_list|)
expr_stmt|;
name|SessionState
operator|.
name|get
argument_list|()
operator|.
name|getConf
argument_list|()
operator|.
name|setClassLoader
argument_list|(
name|loader
argument_list|)
expr_stmt|;
block|}
specifier|public
specifier|static
name|String
name|formatBinaryString
parameter_list|(
name|byte
index|[]
name|array
parameter_list|,
name|int
name|start
parameter_list|,
name|int
name|length
parameter_list|)
block|{
name|StringBuilder
name|sb
init|=
operator|new
name|StringBuilder
argument_list|()
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
name|start
init|;
name|i
operator|<
name|start
operator|+
name|length
condition|;
name|i
operator|++
control|)
block|{
name|sb
operator|.
name|append
argument_list|(
literal|"x"
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
name|array
index|[
name|i
index|]
operator|<
literal|0
condition|?
name|array
index|[
name|i
index|]
operator|+
literal|256
else|:
name|array
index|[
name|i
index|]
operator|+
literal|0
argument_list|)
expr_stmt|;
block|}
return|return
name|sb
operator|.
name|toString
argument_list|()
return|;
block|}
specifier|public
specifier|static
name|List
argument_list|<
name|String
argument_list|>
name|getColumnNamesFromSortCols
parameter_list|(
name|List
argument_list|<
name|Order
argument_list|>
name|sortCols
parameter_list|)
block|{
name|List
argument_list|<
name|String
argument_list|>
name|names
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|Order
name|o
range|:
name|sortCols
control|)
block|{
name|names
operator|.
name|add
argument_list|(
name|o
operator|.
name|getCol
argument_list|()
argument_list|)
expr_stmt|;
block|}
return|return
name|names
return|;
block|}
specifier|public
specifier|static
name|List
argument_list|<
name|String
argument_list|>
name|getColumnNamesFromFieldSchema
parameter_list|(
name|List
argument_list|<
name|FieldSchema
argument_list|>
name|partCols
parameter_list|)
block|{
name|List
argument_list|<
name|String
argument_list|>
name|names
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|FieldSchema
name|o
range|:
name|partCols
control|)
block|{
name|names
operator|.
name|add
argument_list|(
name|o
operator|.
name|getName
argument_list|()
argument_list|)
expr_stmt|;
block|}
return|return
name|names
return|;
block|}
specifier|public
specifier|static
name|List
argument_list|<
name|String
argument_list|>
name|getColumnNames
parameter_list|(
name|Properties
name|props
parameter_list|)
block|{
name|List
argument_list|<
name|String
argument_list|>
name|names
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
name|String
name|colNames
init|=
name|props
operator|.
name|getProperty
argument_list|(
name|serdeConstants
operator|.
name|LIST_COLUMNS
argument_list|)
decl_stmt|;
name|String
index|[]
name|cols
init|=
name|colNames
operator|.
name|trim
argument_list|()
operator|.
name|split
argument_list|(
literal|","
argument_list|)
decl_stmt|;
if|if
condition|(
name|cols
operator|!=
literal|null
condition|)
block|{
for|for
control|(
name|String
name|col
range|:
name|cols
control|)
block|{
if|if
condition|(
name|col
operator|!=
literal|null
operator|&&
operator|!
name|col
operator|.
name|trim
argument_list|()
operator|.
name|equals
argument_list|(
literal|""
argument_list|)
condition|)
block|{
name|names
operator|.
name|add
argument_list|(
name|col
argument_list|)
expr_stmt|;
block|}
block|}
block|}
return|return
name|names
return|;
block|}
specifier|public
specifier|static
name|List
argument_list|<
name|String
argument_list|>
name|getColumnTypes
parameter_list|(
name|Properties
name|props
parameter_list|)
block|{
name|List
argument_list|<
name|String
argument_list|>
name|names
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
name|String
name|colNames
init|=
name|props
operator|.
name|getProperty
argument_list|(
name|serdeConstants
operator|.
name|LIST_COLUMN_TYPES
argument_list|)
decl_stmt|;
name|String
index|[]
name|cols
init|=
name|colNames
operator|.
name|trim
argument_list|()
operator|.
name|split
argument_list|(
literal|","
argument_list|)
decl_stmt|;
if|if
condition|(
name|cols
operator|!=
literal|null
condition|)
block|{
for|for
control|(
name|String
name|col
range|:
name|cols
control|)
block|{
if|if
condition|(
name|col
operator|!=
literal|null
operator|&&
operator|!
name|col
operator|.
name|trim
argument_list|()
operator|.
name|equals
argument_list|(
literal|""
argument_list|)
condition|)
block|{
name|names
operator|.
name|add
argument_list|(
name|col
argument_list|)
expr_stmt|;
block|}
block|}
block|}
return|return
name|names
return|;
block|}
specifier|public
specifier|static
name|void
name|validateColumnNames
parameter_list|(
name|List
argument_list|<
name|String
argument_list|>
name|colNames
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|checkCols
parameter_list|)
throws|throws
name|SemanticException
block|{
name|Iterator
argument_list|<
name|String
argument_list|>
name|checkColsIter
init|=
name|checkCols
operator|.
name|iterator
argument_list|()
decl_stmt|;
while|while
condition|(
name|checkColsIter
operator|.
name|hasNext
argument_list|()
condition|)
block|{
name|String
name|toCheck
init|=
name|checkColsIter
operator|.
name|next
argument_list|()
decl_stmt|;
name|boolean
name|found
init|=
literal|false
decl_stmt|;
name|Iterator
argument_list|<
name|String
argument_list|>
name|colNamesIter
init|=
name|colNames
operator|.
name|iterator
argument_list|()
decl_stmt|;
while|while
condition|(
name|colNamesIter
operator|.
name|hasNext
argument_list|()
condition|)
block|{
name|String
name|colName
init|=
name|colNamesIter
operator|.
name|next
argument_list|()
decl_stmt|;
if|if
condition|(
name|toCheck
operator|.
name|equalsIgnoreCase
argument_list|(
name|colName
argument_list|)
condition|)
block|{
name|found
operator|=
literal|true
expr_stmt|;
break|break;
block|}
block|}
if|if
condition|(
operator|!
name|found
condition|)
block|{
throw|throw
operator|new
name|SemanticException
argument_list|(
name|ErrorMsg
operator|.
name|INVALID_COLUMN
operator|.
name|getMsg
argument_list|()
argument_list|)
throw|;
block|}
block|}
block|}
comment|/**    * Gets the default notification interval to send progress updates to the tracker. Useful for    * operators that may not output data for a while.    *    * @param hconf    * @return the interval in milliseconds    */
specifier|public
specifier|static
name|int
name|getDefaultNotificationInterval
parameter_list|(
name|Configuration
name|hconf
parameter_list|)
block|{
name|int
name|notificationInterval
decl_stmt|;
name|Integer
name|expInterval
init|=
name|Integer
operator|.
name|decode
argument_list|(
name|hconf
operator|.
name|get
argument_list|(
literal|"mapred.tasktracker.expiry.interval"
argument_list|)
argument_list|)
decl_stmt|;
if|if
condition|(
name|expInterval
operator|!=
literal|null
condition|)
block|{
name|notificationInterval
operator|=
name|expInterval
operator|.
name|intValue
argument_list|()
operator|/
literal|2
expr_stmt|;
block|}
else|else
block|{
comment|// 5 minutes
name|notificationInterval
operator|=
literal|5
operator|*
literal|60
operator|*
literal|1000
expr_stmt|;
block|}
return|return
name|notificationInterval
return|;
block|}
comment|/**    * Copies the storage handler properties configured for a table descriptor to a runtime job    * configuration.    *    * @param tbl    *          table descriptor from which to read    *    * @param job    *          configuration which receives configured properties    */
specifier|public
specifier|static
name|void
name|copyTableJobPropertiesToConf
parameter_list|(
name|TableDesc
name|tbl
parameter_list|,
name|JobConf
name|job
parameter_list|)
block|{
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|jobProperties
init|=
name|tbl
operator|.
name|getJobProperties
argument_list|()
decl_stmt|;
if|if
condition|(
name|jobProperties
operator|==
literal|null
condition|)
block|{
return|return;
block|}
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|entry
range|:
name|jobProperties
operator|.
name|entrySet
argument_list|()
control|)
block|{
name|job
operator|.
name|set
argument_list|(
name|entry
operator|.
name|getKey
argument_list|()
argument_list|,
name|entry
operator|.
name|getValue
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
specifier|private
specifier|static
specifier|final
name|Object
name|INPUT_SUMMARY_LOCK
init|=
operator|new
name|Object
argument_list|()
decl_stmt|;
comment|/**    * Calculate the total size of input files.    *    * @param ctx    *          the hadoop job context    * @param work    *          map reduce job plan    * @param filter    *          filter to apply to the input paths before calculating size    * @return the summary of all the input paths.    * @throws IOException    */
specifier|public
specifier|static
name|ContentSummary
name|getInputSummary
parameter_list|(
name|Context
name|ctx
parameter_list|,
name|MapWork
name|work
parameter_list|,
name|PathFilter
name|filter
parameter_list|)
throws|throws
name|IOException
block|{
name|PerfLogger
name|perfLogger
init|=
name|PerfLogger
operator|.
name|getPerfLogger
argument_list|()
decl_stmt|;
name|perfLogger
operator|.
name|PerfLogBegin
argument_list|(
name|CLASS_NAME
argument_list|,
name|PerfLogger
operator|.
name|INPUT_SUMMARY
argument_list|)
expr_stmt|;
name|long
index|[]
name|summary
init|=
block|{
literal|0
block|,
literal|0
block|,
literal|0
block|}
decl_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|pathNeedProcess
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
comment|// Since multiple threads could call this method concurrently, locking
comment|// this method will avoid number of threads out of control.
synchronized|synchronized
init|(
name|INPUT_SUMMARY_LOCK
init|)
block|{
comment|// For each input path, calculate the total size.
for|for
control|(
name|String
name|path
range|:
name|work
operator|.
name|getPathToAliases
argument_list|()
operator|.
name|keySet
argument_list|()
control|)
block|{
name|Path
name|p
init|=
operator|new
name|Path
argument_list|(
name|path
argument_list|)
decl_stmt|;
if|if
condition|(
name|filter
operator|!=
literal|null
operator|&&
operator|!
name|filter
operator|.
name|accept
argument_list|(
name|p
argument_list|)
condition|)
block|{
continue|continue;
block|}
name|ContentSummary
name|cs
init|=
name|ctx
operator|.
name|getCS
argument_list|(
name|path
argument_list|)
decl_stmt|;
if|if
condition|(
name|cs
operator|==
literal|null
condition|)
block|{
if|if
condition|(
name|path
operator|==
literal|null
condition|)
block|{
continue|continue;
block|}
name|pathNeedProcess
operator|.
name|add
argument_list|(
name|path
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|summary
index|[
literal|0
index|]
operator|+=
name|cs
operator|.
name|getLength
argument_list|()
expr_stmt|;
name|summary
index|[
literal|1
index|]
operator|+=
name|cs
operator|.
name|getFileCount
argument_list|()
expr_stmt|;
name|summary
index|[
literal|2
index|]
operator|+=
name|cs
operator|.
name|getDirectoryCount
argument_list|()
expr_stmt|;
block|}
block|}
comment|// Process the case when name node call is needed
specifier|final
name|Map
argument_list|<
name|String
argument_list|,
name|ContentSummary
argument_list|>
name|resultMap
init|=
operator|new
name|ConcurrentHashMap
argument_list|<
name|String
argument_list|,
name|ContentSummary
argument_list|>
argument_list|()
decl_stmt|;
name|ArrayList
argument_list|<
name|Future
argument_list|<
name|?
argument_list|>
argument_list|>
name|results
init|=
operator|new
name|ArrayList
argument_list|<
name|Future
argument_list|<
name|?
argument_list|>
argument_list|>
argument_list|()
decl_stmt|;
specifier|final
name|ThreadPoolExecutor
name|executor
decl_stmt|;
name|int
name|maxThreads
init|=
name|ctx
operator|.
name|getConf
argument_list|()
operator|.
name|getInt
argument_list|(
literal|"mapred.dfsclient.parallelism.max"
argument_list|,
literal|0
argument_list|)
decl_stmt|;
if|if
condition|(
name|pathNeedProcess
operator|.
name|size
argument_list|()
operator|>
literal|1
operator|&&
name|maxThreads
operator|>
literal|1
condition|)
block|{
name|int
name|numExecutors
init|=
name|Math
operator|.
name|min
argument_list|(
name|pathNeedProcess
operator|.
name|size
argument_list|()
argument_list|,
name|maxThreads
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Using "
operator|+
name|numExecutors
operator|+
literal|" threads for getContentSummary"
argument_list|)
expr_stmt|;
name|executor
operator|=
operator|new
name|ThreadPoolExecutor
argument_list|(
name|numExecutors
argument_list|,
name|numExecutors
argument_list|,
literal|60
argument_list|,
name|TimeUnit
operator|.
name|SECONDS
argument_list|,
operator|new
name|LinkedBlockingQueue
argument_list|<
name|Runnable
argument_list|>
argument_list|()
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|executor
operator|=
literal|null
expr_stmt|;
block|}
name|HiveInterruptCallback
name|interrup
init|=
name|HiveInterruptUtils
operator|.
name|add
argument_list|(
operator|new
name|HiveInterruptCallback
argument_list|()
block|{
annotation|@
name|Override
specifier|public
name|void
name|interrupt
parameter_list|()
block|{
if|if
condition|(
name|executor
operator|!=
literal|null
condition|)
block|{
name|executor
operator|.
name|shutdownNow
argument_list|()
expr_stmt|;
block|}
block|}
block|}
argument_list|)
decl_stmt|;
try|try
block|{
name|Configuration
name|conf
init|=
name|ctx
operator|.
name|getConf
argument_list|()
decl_stmt|;
name|JobConf
name|jobConf
init|=
operator|new
name|JobConf
argument_list|(
name|conf
argument_list|)
decl_stmt|;
for|for
control|(
name|String
name|path
range|:
name|pathNeedProcess
control|)
block|{
specifier|final
name|Path
name|p
init|=
operator|new
name|Path
argument_list|(
name|path
argument_list|)
decl_stmt|;
specifier|final
name|String
name|pathStr
init|=
name|path
decl_stmt|;
comment|// All threads share the same Configuration and JobConf based on the
comment|// assumption that they are thread safe if only read operations are
comment|// executed. It is not stated in Hadoop's javadoc, the sourcce codes
comment|// clearly showed that they made efforts for it and we believe it is
comment|// thread safe. Will revisit this piece of codes if we find the assumption
comment|// is not correct.
specifier|final
name|Configuration
name|myConf
init|=
name|conf
decl_stmt|;
specifier|final
name|JobConf
name|myJobConf
init|=
name|jobConf
decl_stmt|;
specifier|final
name|Map
argument_list|<
name|String
argument_list|,
name|Operator
argument_list|<
name|?
argument_list|>
argument_list|>
name|aliasToWork
init|=
name|work
operator|.
name|getAliasToWork
argument_list|()
decl_stmt|;
specifier|final
name|Map
argument_list|<
name|String
argument_list|,
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|>
name|pathToAlias
init|=
name|work
operator|.
name|getPathToAliases
argument_list|()
decl_stmt|;
specifier|final
name|PartitionDesc
name|partDesc
init|=
name|work
operator|.
name|getPathToPartitionInfo
argument_list|()
operator|.
name|get
argument_list|(
name|p
operator|.
name|toString
argument_list|()
argument_list|)
decl_stmt|;
name|Runnable
name|r
init|=
operator|new
name|Runnable
argument_list|()
block|{
specifier|public
name|void
name|run
parameter_list|()
block|{
try|try
block|{
name|Class
argument_list|<
name|?
extends|extends
name|InputFormat
argument_list|>
name|inputFormatCls
init|=
name|partDesc
operator|.
name|getInputFileFormatClass
argument_list|()
decl_stmt|;
name|InputFormat
name|inputFormatObj
init|=
name|HiveInputFormat
operator|.
name|getInputFormatFromCache
argument_list|(
name|inputFormatCls
argument_list|,
name|myJobConf
argument_list|)
decl_stmt|;
if|if
condition|(
name|inputFormatObj
operator|instanceof
name|ContentSummaryInputFormat
condition|)
block|{
name|ContentSummaryInputFormat
name|cs
init|=
operator|(
name|ContentSummaryInputFormat
operator|)
name|inputFormatObj
decl_stmt|;
name|resultMap
operator|.
name|put
argument_list|(
name|pathStr
argument_list|,
name|cs
operator|.
name|getContentSummary
argument_list|(
name|p
argument_list|,
name|myJobConf
argument_list|)
argument_list|)
expr_stmt|;
return|return;
block|}
name|HiveStorageHandler
name|handler
init|=
name|HiveUtils
operator|.
name|getStorageHandler
argument_list|(
name|myConf
argument_list|,
name|partDesc
operator|.
name|getOverlayedProperties
argument_list|()
operator|.
name|getProperty
argument_list|(
name|hive_metastoreConstants
operator|.
name|META_TABLE_STORAGE
argument_list|)
argument_list|)
decl_stmt|;
if|if
condition|(
name|handler
operator|instanceof
name|InputEstimator
condition|)
block|{
name|long
name|total
init|=
literal|0
decl_stmt|;
name|TableDesc
name|tableDesc
init|=
name|partDesc
operator|.
name|getTableDesc
argument_list|()
decl_stmt|;
name|InputEstimator
name|estimator
init|=
operator|(
name|InputEstimator
operator|)
name|handler
decl_stmt|;
for|for
control|(
name|String
name|alias
range|:
name|HiveFileFormatUtils
operator|.
name|doGetAliasesFromPath
argument_list|(
name|pathToAlias
argument_list|,
name|p
argument_list|)
control|)
block|{
name|JobConf
name|jobConf
init|=
operator|new
name|JobConf
argument_list|(
name|myJobConf
argument_list|)
decl_stmt|;
name|TableScanOperator
name|scanOp
init|=
operator|(
name|TableScanOperator
operator|)
name|aliasToWork
operator|.
name|get
argument_list|(
name|alias
argument_list|)
decl_stmt|;
name|Utilities
operator|.
name|setColumnNameList
argument_list|(
name|jobConf
argument_list|,
name|scanOp
argument_list|,
literal|true
argument_list|)
expr_stmt|;
name|Utilities
operator|.
name|setColumnTypeList
argument_list|(
name|jobConf
argument_list|,
name|scanOp
argument_list|,
literal|true
argument_list|)
expr_stmt|;
name|PlanUtils
operator|.
name|configureInputJobPropertiesForStorageHandler
argument_list|(
name|tableDesc
argument_list|)
expr_stmt|;
name|Utilities
operator|.
name|copyTableJobPropertiesToConf
argument_list|(
name|tableDesc
argument_list|,
name|jobConf
argument_list|)
expr_stmt|;
name|total
operator|+=
name|estimator
operator|.
name|estimate
argument_list|(
name|myJobConf
argument_list|,
name|scanOp
argument_list|,
operator|-
literal|1
argument_list|)
operator|.
name|getTotalLength
argument_list|()
expr_stmt|;
block|}
name|resultMap
operator|.
name|put
argument_list|(
name|pathStr
argument_list|,
operator|new
name|ContentSummary
argument_list|(
name|total
argument_list|,
operator|-
literal|1
argument_list|,
operator|-
literal|1
argument_list|)
argument_list|)
expr_stmt|;
block|}
comment|// todo: should nullify summary for non-native tables,
comment|// not to be selected as a mapjoin target
name|FileSystem
name|fs
init|=
name|p
operator|.
name|getFileSystem
argument_list|(
name|myConf
argument_list|)
decl_stmt|;
name|resultMap
operator|.
name|put
argument_list|(
name|pathStr
argument_list|,
name|fs
operator|.
name|getContentSummary
argument_list|(
name|p
argument_list|)
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
comment|// We safely ignore this exception for summary data.
comment|// We don't update the cache to protect it from polluting other
comment|// usages. The worst case is that IOException will always be
comment|// retried for another getInputSummary(), which is fine as
comment|// IOException is not considered as a common case.
name|LOG
operator|.
name|info
argument_list|(
literal|"Cannot get size of "
operator|+
name|pathStr
operator|+
literal|". Safely ignored."
argument_list|)
expr_stmt|;
block|}
block|}
block|}
decl_stmt|;
if|if
condition|(
name|executor
operator|==
literal|null
condition|)
block|{
name|r
operator|.
name|run
argument_list|()
expr_stmt|;
block|}
else|else
block|{
name|Future
argument_list|<
name|?
argument_list|>
name|result
init|=
name|executor
operator|.
name|submit
argument_list|(
name|r
argument_list|)
decl_stmt|;
name|results
operator|.
name|add
argument_list|(
name|result
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
name|executor
operator|!=
literal|null
condition|)
block|{
for|for
control|(
name|Future
argument_list|<
name|?
argument_list|>
name|result
range|:
name|results
control|)
block|{
name|boolean
name|executorDone
init|=
literal|false
decl_stmt|;
do|do
block|{
try|try
block|{
name|result
operator|.
name|get
argument_list|()
expr_stmt|;
name|executorDone
operator|=
literal|true
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|InterruptedException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Interrupted when waiting threads: "
argument_list|,
name|e
argument_list|)
expr_stmt|;
name|Thread
operator|.
name|currentThread
argument_list|()
operator|.
name|interrupt
argument_list|()
expr_stmt|;
break|break;
block|}
catch|catch
parameter_list|(
name|ExecutionException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
do|while
condition|(
operator|!
name|executorDone
condition|)
do|;
block|}
name|executor
operator|.
name|shutdown
argument_list|()
expr_stmt|;
block|}
name|HiveInterruptUtils
operator|.
name|checkInterrupted
argument_list|()
expr_stmt|;
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|String
argument_list|,
name|ContentSummary
argument_list|>
name|entry
range|:
name|resultMap
operator|.
name|entrySet
argument_list|()
control|)
block|{
name|ContentSummary
name|cs
init|=
name|entry
operator|.
name|getValue
argument_list|()
decl_stmt|;
name|summary
index|[
literal|0
index|]
operator|+=
name|cs
operator|.
name|getLength
argument_list|()
expr_stmt|;
name|summary
index|[
literal|1
index|]
operator|+=
name|cs
operator|.
name|getFileCount
argument_list|()
expr_stmt|;
name|summary
index|[
literal|2
index|]
operator|+=
name|cs
operator|.
name|getDirectoryCount
argument_list|()
expr_stmt|;
name|ctx
operator|.
name|addCS
argument_list|(
name|entry
operator|.
name|getKey
argument_list|()
argument_list|,
name|cs
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Cache Content Summary for "
operator|+
name|entry
operator|.
name|getKey
argument_list|()
operator|+
literal|" length: "
operator|+
name|cs
operator|.
name|getLength
argument_list|()
operator|+
literal|" file count: "
operator|+
name|cs
operator|.
name|getFileCount
argument_list|()
operator|+
literal|" directory count: "
operator|+
name|cs
operator|.
name|getDirectoryCount
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|perfLogger
operator|.
name|PerfLogEnd
argument_list|(
name|CLASS_NAME
argument_list|,
name|PerfLogger
operator|.
name|INPUT_SUMMARY
argument_list|)
expr_stmt|;
return|return
operator|new
name|ContentSummary
argument_list|(
name|summary
index|[
literal|0
index|]
argument_list|,
name|summary
index|[
literal|1
index|]
argument_list|,
name|summary
index|[
literal|2
index|]
argument_list|)
return|;
block|}
finally|finally
block|{
name|HiveInterruptUtils
operator|.
name|remove
argument_list|(
name|interrup
argument_list|)
expr_stmt|;
block|}
block|}
block|}
comment|// return sum of lengths except one alias. returns -1 if any of other alias is unknown
specifier|public
specifier|static
name|long
name|sumOfExcept
parameter_list|(
name|Map
argument_list|<
name|String
argument_list|,
name|Long
argument_list|>
name|aliasToSize
parameter_list|,
name|Set
argument_list|<
name|String
argument_list|>
name|aliases
parameter_list|,
name|String
name|except
parameter_list|)
block|{
name|long
name|total
init|=
literal|0
decl_stmt|;
for|for
control|(
name|String
name|alias
range|:
name|aliases
control|)
block|{
if|if
condition|(
name|alias
operator|.
name|equals
argument_list|(
name|except
argument_list|)
condition|)
block|{
continue|continue;
block|}
name|Long
name|size
init|=
name|aliasToSize
operator|.
name|get
argument_list|(
name|alias
argument_list|)
decl_stmt|;
if|if
condition|(
name|size
operator|==
literal|null
condition|)
block|{
return|return
operator|-
literal|1
return|;
block|}
name|total
operator|+=
name|size
expr_stmt|;
block|}
return|return
name|total
return|;
block|}
specifier|public
specifier|static
name|boolean
name|isEmptyPath
parameter_list|(
name|JobConf
name|job
parameter_list|,
name|Path
name|dirPath
parameter_list|,
name|Context
name|ctx
parameter_list|)
throws|throws
name|Exception
block|{
name|ContentSummary
name|cs
init|=
name|ctx
operator|.
name|getCS
argument_list|(
name|dirPath
argument_list|)
decl_stmt|;
if|if
condition|(
name|cs
operator|!=
literal|null
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Content Summary "
operator|+
name|dirPath
operator|+
literal|"length: "
operator|+
name|cs
operator|.
name|getLength
argument_list|()
operator|+
literal|" num files: "
operator|+
name|cs
operator|.
name|getFileCount
argument_list|()
operator|+
literal|" num directories: "
operator|+
name|cs
operator|.
name|getDirectoryCount
argument_list|()
argument_list|)
expr_stmt|;
return|return
operator|(
name|cs
operator|.
name|getLength
argument_list|()
operator|==
literal|0
operator|&&
name|cs
operator|.
name|getFileCount
argument_list|()
operator|==
literal|0
operator|&&
name|cs
operator|.
name|getDirectoryCount
argument_list|()
operator|<=
literal|1
operator|)
return|;
block|}
else|else
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Content Summary not cached for "
operator|+
name|dirPath
argument_list|)
expr_stmt|;
block|}
return|return
name|isEmptyPath
argument_list|(
name|job
argument_list|,
name|dirPath
argument_list|)
return|;
block|}
specifier|public
specifier|static
name|boolean
name|isEmptyPath
parameter_list|(
name|JobConf
name|job
parameter_list|,
name|Path
name|dirPath
parameter_list|)
throws|throws
name|Exception
block|{
name|FileSystem
name|inpFs
init|=
name|dirPath
operator|.
name|getFileSystem
argument_list|(
name|job
argument_list|)
decl_stmt|;
if|if
condition|(
name|inpFs
operator|.
name|exists
argument_list|(
name|dirPath
argument_list|)
condition|)
block|{
name|FileStatus
index|[]
name|fStats
init|=
name|inpFs
operator|.
name|listStatus
argument_list|(
name|dirPath
argument_list|)
decl_stmt|;
if|if
condition|(
name|fStats
operator|.
name|length
operator|>
literal|0
condition|)
block|{
return|return
literal|false
return|;
block|}
block|}
return|return
literal|true
return|;
block|}
specifier|public
specifier|static
name|List
argument_list|<
name|TezTask
argument_list|>
name|getTezTasks
parameter_list|(
name|List
argument_list|<
name|Task
argument_list|<
name|?
extends|extends
name|Serializable
argument_list|>
argument_list|>
name|tasks
parameter_list|)
block|{
name|List
argument_list|<
name|TezTask
argument_list|>
name|tezTasks
init|=
operator|new
name|ArrayList
argument_list|<
name|TezTask
argument_list|>
argument_list|()
decl_stmt|;
if|if
condition|(
name|tasks
operator|!=
literal|null
condition|)
block|{
name|getTezTasks
argument_list|(
name|tasks
argument_list|,
name|tezTasks
argument_list|)
expr_stmt|;
block|}
return|return
name|tezTasks
return|;
block|}
specifier|private
specifier|static
name|void
name|getTezTasks
parameter_list|(
name|List
argument_list|<
name|Task
argument_list|<
name|?
extends|extends
name|Serializable
argument_list|>
argument_list|>
name|tasks
parameter_list|,
name|List
argument_list|<
name|TezTask
argument_list|>
name|tezTasks
parameter_list|)
block|{
for|for
control|(
name|Task
argument_list|<
name|?
extends|extends
name|Serializable
argument_list|>
name|task
range|:
name|tasks
control|)
block|{
if|if
condition|(
name|task
operator|instanceof
name|TezTask
operator|&&
operator|!
name|tezTasks
operator|.
name|contains
argument_list|(
operator|(
name|TezTask
operator|)
name|task
argument_list|)
condition|)
block|{
name|tezTasks
operator|.
name|add
argument_list|(
operator|(
name|TezTask
operator|)
name|task
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|task
operator|.
name|getDependentTasks
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|getTezTasks
argument_list|(
name|task
operator|.
name|getDependentTasks
argument_list|()
argument_list|,
name|tezTasks
argument_list|)
expr_stmt|;
block|}
block|}
block|}
specifier|public
specifier|static
name|List
argument_list|<
name|ExecDriver
argument_list|>
name|getMRTasks
parameter_list|(
name|List
argument_list|<
name|Task
argument_list|<
name|?
extends|extends
name|Serializable
argument_list|>
argument_list|>
name|tasks
parameter_list|)
block|{
name|List
argument_list|<
name|ExecDriver
argument_list|>
name|mrTasks
init|=
operator|new
name|ArrayList
argument_list|<
name|ExecDriver
argument_list|>
argument_list|()
decl_stmt|;
if|if
condition|(
name|tasks
operator|!=
literal|null
condition|)
block|{
name|getMRTasks
argument_list|(
name|tasks
argument_list|,
name|mrTasks
argument_list|)
expr_stmt|;
block|}
return|return
name|mrTasks
return|;
block|}
specifier|private
specifier|static
name|void
name|getMRTasks
parameter_list|(
name|List
argument_list|<
name|Task
argument_list|<
name|?
extends|extends
name|Serializable
argument_list|>
argument_list|>
name|tasks
parameter_list|,
name|List
argument_list|<
name|ExecDriver
argument_list|>
name|mrTasks
parameter_list|)
block|{
for|for
control|(
name|Task
argument_list|<
name|?
extends|extends
name|Serializable
argument_list|>
name|task
range|:
name|tasks
control|)
block|{
if|if
condition|(
name|task
operator|instanceof
name|ExecDriver
operator|&&
operator|!
name|mrTasks
operator|.
name|contains
argument_list|(
operator|(
name|ExecDriver
operator|)
name|task
argument_list|)
condition|)
block|{
name|mrTasks
operator|.
name|add
argument_list|(
operator|(
name|ExecDriver
operator|)
name|task
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|task
operator|.
name|getDependentTasks
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|getMRTasks
argument_list|(
name|task
operator|.
name|getDependentTasks
argument_list|()
argument_list|,
name|mrTasks
argument_list|)
expr_stmt|;
block|}
block|}
block|}
comment|/**    * Construct a list of full partition spec from Dynamic Partition Context and the directory names    * corresponding to these dynamic partitions.    */
specifier|public
specifier|static
name|List
argument_list|<
name|LinkedHashMap
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
argument_list|>
name|getFullDPSpecs
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|DynamicPartitionCtx
name|dpCtx
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|Path
name|loadPath
init|=
name|dpCtx
operator|.
name|getRootPath
argument_list|()
decl_stmt|;
name|FileSystem
name|fs
init|=
name|loadPath
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|int
name|numDPCols
init|=
name|dpCtx
operator|.
name|getNumDPCols
argument_list|()
decl_stmt|;
name|FileStatus
index|[]
name|status
init|=
name|HiveStatsUtils
operator|.
name|getFileStatusRecurse
argument_list|(
name|loadPath
argument_list|,
name|numDPCols
argument_list|,
name|fs
argument_list|)
decl_stmt|;
if|if
condition|(
name|status
operator|.
name|length
operator|==
literal|0
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"No partition is generated by dynamic partitioning"
argument_list|)
expr_stmt|;
return|return
literal|null
return|;
block|}
comment|// partial partition specification
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|partSpec
init|=
name|dpCtx
operator|.
name|getPartSpec
argument_list|()
decl_stmt|;
comment|// list of full partition specification
name|List
argument_list|<
name|LinkedHashMap
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
argument_list|>
name|fullPartSpecs
init|=
operator|new
name|ArrayList
argument_list|<
name|LinkedHashMap
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
argument_list|>
argument_list|()
decl_stmt|;
comment|// for each dynamically created DP directory, construct a full partition spec
comment|// and load the partition based on that
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|status
operator|.
name|length
condition|;
operator|++
name|i
control|)
block|{
comment|// get the dynamically created directory
name|Path
name|partPath
init|=
name|status
index|[
name|i
index|]
operator|.
name|getPath
argument_list|()
decl_stmt|;
assert|assert
name|fs
operator|.
name|getFileStatus
argument_list|(
name|partPath
argument_list|)
operator|.
name|isDir
argument_list|()
operator|:
literal|"partitions "
operator|+
name|partPath
operator|+
literal|" is not a directory !"
assert|;
comment|// generate a full partition specification
name|LinkedHashMap
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|fullPartSpec
init|=
operator|new
name|LinkedHashMap
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
argument_list|(
name|partSpec
argument_list|)
decl_stmt|;
name|Warehouse
operator|.
name|makeSpecFromName
argument_list|(
name|fullPartSpec
argument_list|,
name|partPath
argument_list|)
expr_stmt|;
name|fullPartSpecs
operator|.
name|add
argument_list|(
name|fullPartSpec
argument_list|)
expr_stmt|;
block|}
return|return
name|fullPartSpecs
return|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
specifier|static
name|StatsPublisher
name|getStatsPublisher
parameter_list|(
name|JobConf
name|jc
parameter_list|)
block|{
name|StatsFactory
name|factory
init|=
name|StatsFactory
operator|.
name|newFactory
argument_list|(
name|jc
argument_list|)
decl_stmt|;
return|return
name|factory
operator|==
literal|null
condition|?
literal|null
else|:
name|factory
operator|.
name|getStatsPublisher
argument_list|()
return|;
block|}
comment|/**    * If statsPrefix's length is greater than maxPrefixLength and maxPrefixLength> 0,    * then it returns an MD5 hash of statsPrefix followed by path separator, otherwise    * it returns statsPrefix    *    * @param statsPrefix    * @param maxPrefixLength    * @return    */
specifier|public
specifier|static
name|String
name|getHashedStatsPrefix
parameter_list|(
name|String
name|statsPrefix
parameter_list|,
name|int
name|maxPrefixLength
parameter_list|,
name|int
name|postfixLength
parameter_list|)
block|{
comment|// todo: this might return possibly longer prefix than
comment|// maxPrefixLength (if set) when maxPrefixLength - postfixLength< 17,
comment|// which would make stat values invalid (especially for 'counter' type)
if|if
condition|(
name|maxPrefixLength
operator|>=
literal|0
operator|&&
name|statsPrefix
operator|.
name|length
argument_list|()
operator|>
name|maxPrefixLength
operator|-
name|postfixLength
condition|)
block|{
try|try
block|{
name|MessageDigest
name|digester
init|=
name|MessageDigest
operator|.
name|getInstance
argument_list|(
literal|"MD5"
argument_list|)
decl_stmt|;
name|digester
operator|.
name|update
argument_list|(
name|statsPrefix
operator|.
name|getBytes
argument_list|()
argument_list|)
expr_stmt|;
return|return
operator|new
name|String
argument_list|(
name|digester
operator|.
name|digest
argument_list|()
argument_list|)
operator|+
name|Path
operator|.
name|SEPARATOR
return|;
comment|// 17 byte
block|}
catch|catch
parameter_list|(
name|NoSuchAlgorithmException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
return|return
name|statsPrefix
operator|.
name|endsWith
argument_list|(
name|Path
operator|.
name|SEPARATOR
argument_list|)
condition|?
name|statsPrefix
else|:
name|statsPrefix
operator|+
name|Path
operator|.
name|SEPARATOR
return|;
block|}
specifier|public
specifier|static
name|String
name|join
parameter_list|(
name|String
modifier|...
name|elements
parameter_list|)
block|{
name|StringBuilder
name|builder
init|=
operator|new
name|StringBuilder
argument_list|()
decl_stmt|;
for|for
control|(
name|String
name|element
range|:
name|elements
control|)
block|{
if|if
condition|(
name|element
operator|==
literal|null
operator|||
name|element
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
continue|continue;
block|}
name|builder
operator|.
name|append
argument_list|(
name|element
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|element
operator|.
name|endsWith
argument_list|(
name|Path
operator|.
name|SEPARATOR
argument_list|)
condition|)
block|{
name|builder
operator|.
name|append
argument_list|(
name|Path
operator|.
name|SEPARATOR
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|builder
operator|.
name|toString
argument_list|()
return|;
block|}
specifier|public
specifier|static
name|void
name|setColumnNameList
parameter_list|(
name|JobConf
name|jobConf
parameter_list|,
name|Operator
name|op
parameter_list|)
block|{
name|setColumnNameList
argument_list|(
name|jobConf
argument_list|,
name|op
argument_list|,
literal|false
argument_list|)
expr_stmt|;
block|}
specifier|public
specifier|static
name|void
name|setColumnNameList
parameter_list|(
name|JobConf
name|jobConf
parameter_list|,
name|Operator
name|op
parameter_list|,
name|boolean
name|excludeVCs
parameter_list|)
block|{
name|RowSchema
name|rowSchema
init|=
name|op
operator|.
name|getSchema
argument_list|()
decl_stmt|;
if|if
condition|(
name|rowSchema
operator|==
literal|null
condition|)
block|{
return|return;
block|}
name|StringBuilder
name|columnNames
init|=
operator|new
name|StringBuilder
argument_list|()
decl_stmt|;
for|for
control|(
name|ColumnInfo
name|colInfo
range|:
name|rowSchema
operator|.
name|getSignature
argument_list|()
control|)
block|{
if|if
condition|(
name|excludeVCs
operator|&&
name|colInfo
operator|.
name|getIsVirtualCol
argument_list|()
condition|)
block|{
continue|continue;
block|}
if|if
condition|(
name|columnNames
operator|.
name|length
argument_list|()
operator|>
literal|0
condition|)
block|{
name|columnNames
operator|.
name|append
argument_list|(
literal|","
argument_list|)
expr_stmt|;
block|}
name|columnNames
operator|.
name|append
argument_list|(
name|colInfo
operator|.
name|getInternalName
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|String
name|columnNamesString
init|=
name|columnNames
operator|.
name|toString
argument_list|()
decl_stmt|;
name|jobConf
operator|.
name|set
argument_list|(
name|serdeConstants
operator|.
name|LIST_COLUMNS
argument_list|,
name|columnNamesString
argument_list|)
expr_stmt|;
block|}
specifier|public
specifier|static
name|void
name|setColumnTypeList
parameter_list|(
name|JobConf
name|jobConf
parameter_list|,
name|Operator
name|op
parameter_list|)
block|{
name|setColumnTypeList
argument_list|(
name|jobConf
argument_list|,
name|op
argument_list|,
literal|false
argument_list|)
expr_stmt|;
block|}
specifier|public
specifier|static
name|void
name|setColumnTypeList
parameter_list|(
name|JobConf
name|jobConf
parameter_list|,
name|Operator
name|op
parameter_list|,
name|boolean
name|excludeVCs
parameter_list|)
block|{
name|RowSchema
name|rowSchema
init|=
name|op
operator|.
name|getSchema
argument_list|()
decl_stmt|;
if|if
condition|(
name|rowSchema
operator|==
literal|null
condition|)
block|{
return|return;
block|}
name|StringBuilder
name|columnTypes
init|=
operator|new
name|StringBuilder
argument_list|()
decl_stmt|;
for|for
control|(
name|ColumnInfo
name|colInfo
range|:
name|rowSchema
operator|.
name|getSignature
argument_list|()
control|)
block|{
if|if
condition|(
name|excludeVCs
operator|&&
name|colInfo
operator|.
name|getIsVirtualCol
argument_list|()
condition|)
block|{
continue|continue;
block|}
if|if
condition|(
name|columnTypes
operator|.
name|length
argument_list|()
operator|>
literal|0
condition|)
block|{
name|columnTypes
operator|.
name|append
argument_list|(
literal|","
argument_list|)
expr_stmt|;
block|}
name|columnTypes
operator|.
name|append
argument_list|(
name|colInfo
operator|.
name|getTypeName
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|String
name|columnTypesString
init|=
name|columnTypes
operator|.
name|toString
argument_list|()
decl_stmt|;
name|jobConf
operator|.
name|set
argument_list|(
name|serdeConstants
operator|.
name|LIST_COLUMN_TYPES
argument_list|,
name|columnTypesString
argument_list|)
expr_stmt|;
block|}
specifier|public
specifier|static
name|String
name|suffix
init|=
literal|".hashtable"
decl_stmt|;
specifier|public
specifier|static
name|String
name|generatePath
parameter_list|(
name|String
name|baseURI
parameter_list|,
name|String
name|dumpFilePrefix
parameter_list|,
name|Byte
name|tag
parameter_list|,
name|String
name|bigBucketFileName
parameter_list|)
block|{
name|String
name|path
init|=
operator|new
name|String
argument_list|(
name|baseURI
operator|+
name|Path
operator|.
name|SEPARATOR
operator|+
literal|"MapJoin-"
operator|+
name|dumpFilePrefix
operator|+
name|tag
operator|+
literal|"-"
operator|+
name|bigBucketFileName
operator|+
name|suffix
argument_list|)
decl_stmt|;
return|return
name|path
return|;
block|}
specifier|public
specifier|static
name|String
name|generateFileName
parameter_list|(
name|Byte
name|tag
parameter_list|,
name|String
name|bigBucketFileName
parameter_list|)
block|{
name|String
name|fileName
init|=
operator|new
name|String
argument_list|(
literal|"MapJoin-"
operator|+
name|tag
operator|+
literal|"-"
operator|+
name|bigBucketFileName
operator|+
name|suffix
argument_list|)
decl_stmt|;
return|return
name|fileName
return|;
block|}
specifier|public
specifier|static
name|String
name|generateTmpURI
parameter_list|(
name|String
name|baseURI
parameter_list|,
name|String
name|id
parameter_list|)
block|{
name|String
name|tmpFileURI
init|=
operator|new
name|String
argument_list|(
name|baseURI
operator|+
name|Path
operator|.
name|SEPARATOR
operator|+
literal|"HashTable-"
operator|+
name|id
argument_list|)
decl_stmt|;
return|return
name|tmpFileURI
return|;
block|}
specifier|public
specifier|static
name|String
name|generateTarURI
parameter_list|(
name|String
name|baseURI
parameter_list|,
name|String
name|filename
parameter_list|)
block|{
name|String
name|tmpFileURI
init|=
operator|new
name|String
argument_list|(
name|baseURI
operator|+
name|Path
operator|.
name|SEPARATOR
operator|+
name|filename
operator|+
literal|".tar.gz"
argument_list|)
decl_stmt|;
return|return
name|tmpFileURI
return|;
block|}
specifier|public
specifier|static
name|String
name|generateTarURI
parameter_list|(
name|Path
name|baseURI
parameter_list|,
name|String
name|filename
parameter_list|)
block|{
name|String
name|tmpFileURI
init|=
operator|new
name|String
argument_list|(
name|baseURI
operator|+
name|Path
operator|.
name|SEPARATOR
operator|+
name|filename
operator|+
literal|".tar.gz"
argument_list|)
decl_stmt|;
return|return
name|tmpFileURI
return|;
block|}
specifier|public
specifier|static
name|String
name|generateTarFileName
parameter_list|(
name|String
name|name
parameter_list|)
block|{
name|String
name|tmpFileURI
init|=
operator|new
name|String
argument_list|(
name|name
operator|+
literal|".tar.gz"
argument_list|)
decl_stmt|;
return|return
name|tmpFileURI
return|;
block|}
specifier|public
specifier|static
name|String
name|generatePath
parameter_list|(
name|Path
name|baseURI
parameter_list|,
name|String
name|filename
parameter_list|)
block|{
name|String
name|path
init|=
operator|new
name|String
argument_list|(
name|baseURI
operator|+
name|Path
operator|.
name|SEPARATOR
operator|+
name|filename
argument_list|)
decl_stmt|;
return|return
name|path
return|;
block|}
specifier|public
specifier|static
name|String
name|now
parameter_list|()
block|{
name|Calendar
name|cal
init|=
name|Calendar
operator|.
name|getInstance
argument_list|()
decl_stmt|;
name|SimpleDateFormat
name|sdf
init|=
operator|new
name|SimpleDateFormat
argument_list|(
literal|"yyyy-MM-dd hh:mm:ss"
argument_list|)
decl_stmt|;
return|return
name|sdf
operator|.
name|format
argument_list|(
name|cal
operator|.
name|getTime
argument_list|()
argument_list|)
return|;
block|}
specifier|public
specifier|static
name|double
name|showTime
parameter_list|(
name|long
name|time
parameter_list|)
block|{
name|double
name|result
init|=
operator|(
name|double
operator|)
name|time
operator|/
operator|(
name|double
operator|)
literal|1000
decl_stmt|;
return|return
name|result
return|;
block|}
comment|/**    * The check here is kind of not clean. It first use a for loop to go through    * all input formats, and choose the ones that extend ReworkMapredInputFormat    * to a set. And finally go through the ReworkMapredInputFormat set, and call    * rework for each one.    *    * Technically all these can be avoided if all Hive's input formats can share    * a same interface. As in today's hive and Hadoop, it is not possible because    * a lot of Hive's input formats are in Hadoop's code. And most of Hadoop's    * input formats just extend InputFormat interface.    *    * @param task    * @param reworkMapredWork    * @param conf    * @throws SemanticException    */
specifier|public
specifier|static
name|void
name|reworkMapRedWork
parameter_list|(
name|Task
argument_list|<
name|?
extends|extends
name|Serializable
argument_list|>
name|task
parameter_list|,
name|boolean
name|reworkMapredWork
parameter_list|,
name|HiveConf
name|conf
parameter_list|)
throws|throws
name|SemanticException
block|{
if|if
condition|(
name|reworkMapredWork
operator|&&
operator|(
name|task
operator|instanceof
name|MapRedTask
operator|)
condition|)
block|{
try|try
block|{
name|MapredWork
name|mapredWork
init|=
operator|(
operator|(
name|MapRedTask
operator|)
name|task
operator|)
operator|.
name|getWork
argument_list|()
decl_stmt|;
name|Set
argument_list|<
name|Class
argument_list|<
name|?
extends|extends
name|InputFormat
argument_list|>
argument_list|>
name|reworkInputFormats
init|=
operator|new
name|HashSet
argument_list|<
name|Class
argument_list|<
name|?
extends|extends
name|InputFormat
argument_list|>
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|PartitionDesc
name|part
range|:
name|mapredWork
operator|.
name|getMapWork
argument_list|()
operator|.
name|getPathToPartitionInfo
argument_list|()
operator|.
name|values
argument_list|()
control|)
block|{
name|Class
argument_list|<
name|?
extends|extends
name|InputFormat
argument_list|>
name|inputFormatCls
init|=
name|part
operator|.
name|getInputFileFormatClass
argument_list|()
decl_stmt|;
if|if
condition|(
name|ReworkMapredInputFormat
operator|.
name|class
operator|.
name|isAssignableFrom
argument_list|(
name|inputFormatCls
argument_list|)
condition|)
block|{
name|reworkInputFormats
operator|.
name|add
argument_list|(
name|inputFormatCls
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
name|reworkInputFormats
operator|.
name|size
argument_list|()
operator|>
literal|0
condition|)
block|{
for|for
control|(
name|Class
argument_list|<
name|?
extends|extends
name|InputFormat
argument_list|>
name|inputFormatCls
range|:
name|reworkInputFormats
control|)
block|{
name|ReworkMapredInputFormat
name|inst
init|=
operator|(
name|ReworkMapredInputFormat
operator|)
name|ReflectionUtils
operator|.
name|newInstance
argument_list|(
name|inputFormatCls
argument_list|,
literal|null
argument_list|)
decl_stmt|;
name|inst
operator|.
name|rework
argument_list|(
name|conf
argument_list|,
name|mapredWork
argument_list|)
expr_stmt|;
block|}
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|SemanticException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
block|}
specifier|public
specifier|static
class|class
name|SQLCommand
parameter_list|<
name|T
parameter_list|>
block|{
specifier|public
name|T
name|run
parameter_list|(
name|PreparedStatement
name|stmt
parameter_list|)
throws|throws
name|SQLException
block|{
return|return
literal|null
return|;
block|}
block|}
comment|/**    * Retry SQL execution with random backoff (same as the one implemented in HDFS-767).    * This function only retries when the SQL query throws a SQLTransientException (which    * might be able to succeed with a simple retry). It doesn't retry when the exception    * is a SQLRecoverableException or SQLNonTransientException. For SQLRecoverableException    * the caller needs to reconnect to the database and restart the whole transaction.    *    * @param cmd the SQL command    * @param stmt the prepared statement of SQL.    * @param baseWindow  The base time window (in milliseconds) before the next retry.    * see {@link #getRandomWaitTime} for details.    * @param maxRetries the maximum # of retries when getting a SQLTransientException.    * @throws SQLException throws SQLRecoverableException or SQLNonTransientException the    * first time it is caught, or SQLTransientException when the maxRetries has reached.    */
specifier|public
specifier|static
parameter_list|<
name|T
parameter_list|>
name|T
name|executeWithRetry
parameter_list|(
name|SQLCommand
argument_list|<
name|T
argument_list|>
name|cmd
parameter_list|,
name|PreparedStatement
name|stmt
parameter_list|,
name|int
name|baseWindow
parameter_list|,
name|int
name|maxRetries
parameter_list|)
throws|throws
name|SQLException
block|{
name|Random
name|r
init|=
operator|new
name|Random
argument_list|()
decl_stmt|;
name|T
name|result
init|=
literal|null
decl_stmt|;
comment|// retry with # of maxRetries before throwing exception
for|for
control|(
name|int
name|failures
init|=
literal|0
init|;
condition|;
name|failures
operator|++
control|)
block|{
try|try
block|{
name|result
operator|=
name|cmd
operator|.
name|run
argument_list|(
name|stmt
argument_list|)
expr_stmt|;
return|return
name|result
return|;
block|}
catch|catch
parameter_list|(
name|SQLTransientException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Failure and retry #"
operator|+
name|failures
operator|+
literal|" with exception "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|failures
operator|>=
name|maxRetries
condition|)
block|{
throw|throw
name|e
throw|;
block|}
name|long
name|waitTime
init|=
name|getRandomWaitTime
argument_list|(
name|baseWindow
argument_list|,
name|failures
argument_list|,
name|r
argument_list|)
decl_stmt|;
try|try
block|{
name|Thread
operator|.
name|sleep
argument_list|(
name|waitTime
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|InterruptedException
name|iex
parameter_list|)
block|{          }
block|}
catch|catch
parameter_list|(
name|SQLException
name|e
parameter_list|)
block|{
comment|// throw other types of SQLExceptions (SQLNonTransientException / SQLRecoverableException)
throw|throw
name|e
throw|;
block|}
block|}
block|}
comment|/**    * Retry connecting to a database with random backoff (same as the one implemented in HDFS-767).    * This function only retries when the SQL query throws a SQLTransientException (which    * might be able to succeed with a simple retry). It doesn't retry when the exception    * is a SQLRecoverableException or SQLNonTransientException. For SQLRecoverableException    * the caller needs to reconnect to the database and restart the whole transaction.    *    * @param connectionString the JDBC connection string.    * @param waitWindow  The base time window (in milliseconds) before the next retry.    * see {@link #getRandomWaitTime} for details.    * @param maxRetries the maximum # of retries when getting a SQLTransientException.    * @throws SQLException throws SQLRecoverableException or SQLNonTransientException the    * first time it is caught, or SQLTransientException when the maxRetries has reached.    */
specifier|public
specifier|static
name|Connection
name|connectWithRetry
parameter_list|(
name|String
name|connectionString
parameter_list|,
name|int
name|waitWindow
parameter_list|,
name|int
name|maxRetries
parameter_list|)
throws|throws
name|SQLException
block|{
name|Random
name|r
init|=
operator|new
name|Random
argument_list|()
decl_stmt|;
comment|// retry with # of maxRetries before throwing exception
for|for
control|(
name|int
name|failures
init|=
literal|0
init|;
condition|;
name|failures
operator|++
control|)
block|{
try|try
block|{
name|Connection
name|conn
init|=
name|DriverManager
operator|.
name|getConnection
argument_list|(
name|connectionString
argument_list|)
decl_stmt|;
return|return
name|conn
return|;
block|}
catch|catch
parameter_list|(
name|SQLTransientException
name|e
parameter_list|)
block|{
if|if
condition|(
name|failures
operator|>=
name|maxRetries
condition|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Error during JDBC connection. "
operator|+
name|e
argument_list|)
expr_stmt|;
throw|throw
name|e
throw|;
block|}
name|long
name|waitTime
init|=
name|Utilities
operator|.
name|getRandomWaitTime
argument_list|(
name|waitWindow
argument_list|,
name|failures
argument_list|,
name|r
argument_list|)
decl_stmt|;
try|try
block|{
name|Thread
operator|.
name|sleep
argument_list|(
name|waitTime
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|InterruptedException
name|e1
parameter_list|)
block|{         }
block|}
catch|catch
parameter_list|(
name|SQLException
name|e
parameter_list|)
block|{
comment|// just throw other types (SQLNonTransientException / SQLRecoverableException)
throw|throw
name|e
throw|;
block|}
block|}
block|}
comment|/**    * Retry preparing a SQL statement with random backoff (same as the one implemented in HDFS-767).    * This function only retries when the SQL query throws a SQLTransientException (which    * might be able to succeed with a simple retry). It doesn't retry when the exception    * is a SQLRecoverableException or SQLNonTransientException. For SQLRecoverableException    * the caller needs to reconnect to the database and restart the whole transaction.    *    * @param conn a JDBC connection.    * @param stmt the SQL statement to be prepared.    * @param waitWindow  The base time window (in milliseconds) before the next retry.    * see {@link #getRandomWaitTime} for details.    * @param maxRetries the maximum # of retries when getting a SQLTransientException.    * @throws SQLException throws SQLRecoverableException or SQLNonTransientException the    * first time it is caught, or SQLTransientException when the maxRetries has reached.    */
specifier|public
specifier|static
name|PreparedStatement
name|prepareWithRetry
parameter_list|(
name|Connection
name|conn
parameter_list|,
name|String
name|stmt
parameter_list|,
name|int
name|waitWindow
parameter_list|,
name|int
name|maxRetries
parameter_list|)
throws|throws
name|SQLException
block|{
name|Random
name|r
init|=
operator|new
name|Random
argument_list|()
decl_stmt|;
comment|// retry with # of maxRetries before throwing exception
for|for
control|(
name|int
name|failures
init|=
literal|0
init|;
condition|;
name|failures
operator|++
control|)
block|{
try|try
block|{
return|return
name|conn
operator|.
name|prepareStatement
argument_list|(
name|stmt
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|SQLTransientException
name|e
parameter_list|)
block|{
if|if
condition|(
name|failures
operator|>=
name|maxRetries
condition|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Error preparing JDBC Statement "
operator|+
name|stmt
operator|+
literal|" :"
operator|+
name|e
argument_list|)
expr_stmt|;
throw|throw
name|e
throw|;
block|}
name|long
name|waitTime
init|=
name|Utilities
operator|.
name|getRandomWaitTime
argument_list|(
name|waitWindow
argument_list|,
name|failures
argument_list|,
name|r
argument_list|)
decl_stmt|;
try|try
block|{
name|Thread
operator|.
name|sleep
argument_list|(
name|waitTime
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|InterruptedException
name|e1
parameter_list|)
block|{         }
block|}
catch|catch
parameter_list|(
name|SQLException
name|e
parameter_list|)
block|{
comment|// just throw other types (SQLNonTransientException / SQLRecoverableException)
throw|throw
name|e
throw|;
block|}
block|}
block|}
comment|/**    * Introducing a random factor to the wait time before another retry.    * The wait time is dependent on # of failures and a random factor.    * At the first time of getting an exception , the wait time    * is a random number between 0..baseWindow msec. If the first retry    * still fails, we will wait baseWindow msec grace period before the 2nd retry.    * Also at the second retry, the waiting window is expanded to 2*baseWindow msec    * alleviating the request rate from the server. Similarly the 3rd retry    * will wait 2*baseWindow msec. grace period before retry and the waiting window is    * expanded to 3*baseWindow msec and so on.    * @param baseWindow the base waiting window.    * @param failures number of failures so far.    * @param r a random generator.    * @return number of milliseconds for the next wait time.    */
specifier|public
specifier|static
name|long
name|getRandomWaitTime
parameter_list|(
name|int
name|baseWindow
parameter_list|,
name|int
name|failures
parameter_list|,
name|Random
name|r
parameter_list|)
block|{
return|return
call|(
name|long
call|)
argument_list|(
name|baseWindow
operator|*
name|failures
operator|+
comment|// grace period for the last round of attempt
name|baseWindow
operator|*
operator|(
name|failures
operator|+
literal|1
operator|)
operator|*
name|r
operator|.
name|nextDouble
argument_list|()
argument_list|)
return|;
comment|// expanding time window for each failure
block|}
specifier|public
specifier|static
specifier|final
name|char
name|sqlEscapeChar
init|=
literal|'\\'
decl_stmt|;
comment|/**    * Escape the '_', '%', as well as the escape characters inside the string key.    * @param key the string that will be used for the SQL LIKE operator.    * @return a string with escaped '_' and '%'.    */
specifier|public
specifier|static
name|String
name|escapeSqlLike
parameter_list|(
name|String
name|key
parameter_list|)
block|{
name|StringBuffer
name|sb
init|=
operator|new
name|StringBuffer
argument_list|(
name|key
operator|.
name|length
argument_list|()
argument_list|)
decl_stmt|;
for|for
control|(
name|char
name|c
range|:
name|key
operator|.
name|toCharArray
argument_list|()
control|)
block|{
switch|switch
condition|(
name|c
condition|)
block|{
case|case
literal|'_'
case|:
case|case
literal|'%'
case|:
case|case
name|sqlEscapeChar
case|:
name|sb
operator|.
name|append
argument_list|(
name|sqlEscapeChar
argument_list|)
expr_stmt|;
comment|// fall through
default|default:
name|sb
operator|.
name|append
argument_list|(
name|c
argument_list|)
expr_stmt|;
break|break;
block|}
block|}
return|return
name|sb
operator|.
name|toString
argument_list|()
return|;
block|}
comment|/**    * Format number of milliseconds to strings    *    * @param msec milliseconds    * @return a formatted string like "x days y hours z minutes a seconds b msec"    */
specifier|public
specifier|static
name|String
name|formatMsecToStr
parameter_list|(
name|long
name|msec
parameter_list|)
block|{
name|long
name|day
init|=
operator|-
literal|1
decl_stmt|,
name|hour
init|=
operator|-
literal|1
decl_stmt|,
name|minute
init|=
operator|-
literal|1
decl_stmt|,
name|second
init|=
operator|-
literal|1
decl_stmt|;
name|long
name|ms
init|=
name|msec
operator|%
literal|1000
decl_stmt|;
name|long
name|timeLeft
init|=
name|msec
operator|/
literal|1000
decl_stmt|;
if|if
condition|(
name|timeLeft
operator|>
literal|0
condition|)
block|{
name|second
operator|=
name|timeLeft
operator|%
literal|60
expr_stmt|;
name|timeLeft
operator|/=
literal|60
expr_stmt|;
if|if
condition|(
name|timeLeft
operator|>
literal|0
condition|)
block|{
name|minute
operator|=
name|timeLeft
operator|%
literal|60
expr_stmt|;
name|timeLeft
operator|/=
literal|60
expr_stmt|;
if|if
condition|(
name|timeLeft
operator|>
literal|0
condition|)
block|{
name|hour
operator|=
name|timeLeft
operator|%
literal|24
expr_stmt|;
name|day
operator|=
name|timeLeft
operator|/
literal|24
expr_stmt|;
block|}
block|}
block|}
name|StringBuilder
name|sb
init|=
operator|new
name|StringBuilder
argument_list|()
decl_stmt|;
if|if
condition|(
name|day
operator|!=
operator|-
literal|1
condition|)
block|{
name|sb
operator|.
name|append
argument_list|(
name|day
operator|+
literal|" days "
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|hour
operator|!=
operator|-
literal|1
condition|)
block|{
name|sb
operator|.
name|append
argument_list|(
name|hour
operator|+
literal|" hours "
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|minute
operator|!=
operator|-
literal|1
condition|)
block|{
name|sb
operator|.
name|append
argument_list|(
name|minute
operator|+
literal|" minutes "
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|second
operator|!=
operator|-
literal|1
condition|)
block|{
name|sb
operator|.
name|append
argument_list|(
name|second
operator|+
literal|" seconds "
argument_list|)
expr_stmt|;
block|}
name|sb
operator|.
name|append
argument_list|(
name|ms
operator|+
literal|" msec"
argument_list|)
expr_stmt|;
return|return
name|sb
operator|.
name|toString
argument_list|()
return|;
block|}
comment|/**    * Estimate the number of reducers needed for this job, based on job input,    * and configuration parameters.    *    * The output of this method should only be used if the output of this    * MapRedTask is not being used to populate a bucketed table and the user    * has not specified the number of reducers to use.    *    * @return the number of reducers.    */
specifier|public
specifier|static
name|int
name|estimateNumberOfReducers
parameter_list|(
name|HiveConf
name|conf
parameter_list|,
name|ContentSummary
name|inputSummary
parameter_list|,
name|MapWork
name|work
parameter_list|,
name|boolean
name|finalMapRed
parameter_list|)
throws|throws
name|IOException
block|{
name|long
name|bytesPerReducer
init|=
name|conf
operator|.
name|getLongVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|BYTESPERREDUCER
argument_list|)
decl_stmt|;
name|int
name|maxReducers
init|=
name|conf
operator|.
name|getIntVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|MAXREDUCERS
argument_list|)
decl_stmt|;
name|double
name|samplePercentage
init|=
name|getHighestSamplePercentage
argument_list|(
name|work
argument_list|)
decl_stmt|;
name|long
name|totalInputFileSize
init|=
name|getTotalInputFileSize
argument_list|(
name|inputSummary
argument_list|,
name|work
argument_list|,
name|samplePercentage
argument_list|)
decl_stmt|;
comment|// if all inputs are sampled, we should shrink the size of reducers accordingly.
if|if
condition|(
name|totalInputFileSize
operator|!=
name|inputSummary
operator|.
name|getLength
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"BytesPerReducer="
operator|+
name|bytesPerReducer
operator|+
literal|" maxReducers="
operator|+
name|maxReducers
operator|+
literal|" estimated totalInputFileSize="
operator|+
name|totalInputFileSize
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"BytesPerReducer="
operator|+
name|bytesPerReducer
operator|+
literal|" maxReducers="
operator|+
name|maxReducers
operator|+
literal|" totalInputFileSize="
operator|+
name|totalInputFileSize
argument_list|)
expr_stmt|;
block|}
comment|// If this map reduce job writes final data to a table and bucketing is being inferred,
comment|// and the user has configured Hive to do this, make sure the number of reducers is a
comment|// power of two
name|boolean
name|powersOfTwo
init|=
name|conf
operator|.
name|getBoolVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_INFER_BUCKET_SORT_NUM_BUCKETS_POWER_TWO
argument_list|)
operator|&&
name|finalMapRed
operator|&&
operator|!
name|work
operator|.
name|getBucketedColsByDirectory
argument_list|()
operator|.
name|isEmpty
argument_list|()
decl_stmt|;
return|return
name|estimateReducers
argument_list|(
name|totalInputFileSize
argument_list|,
name|bytesPerReducer
argument_list|,
name|maxReducers
argument_list|,
name|powersOfTwo
argument_list|)
return|;
block|}
specifier|public
specifier|static
name|int
name|estimateReducers
parameter_list|(
name|long
name|totalInputFileSize
parameter_list|,
name|long
name|bytesPerReducer
parameter_list|,
name|int
name|maxReducers
parameter_list|,
name|boolean
name|powersOfTwo
parameter_list|)
block|{
name|int
name|reducers
init|=
call|(
name|int
call|)
argument_list|(
operator|(
name|totalInputFileSize
operator|+
name|bytesPerReducer
operator|-
literal|1
operator|)
operator|/
name|bytesPerReducer
argument_list|)
decl_stmt|;
name|reducers
operator|=
name|Math
operator|.
name|max
argument_list|(
literal|1
argument_list|,
name|reducers
argument_list|)
expr_stmt|;
name|reducers
operator|=
name|Math
operator|.
name|min
argument_list|(
name|maxReducers
argument_list|,
name|reducers
argument_list|)
expr_stmt|;
name|int
name|reducersLog
init|=
call|(
name|int
call|)
argument_list|(
name|Math
operator|.
name|log
argument_list|(
name|reducers
argument_list|)
operator|/
name|Math
operator|.
name|log
argument_list|(
literal|2
argument_list|)
argument_list|)
operator|+
literal|1
decl_stmt|;
name|int
name|reducersPowerTwo
init|=
operator|(
name|int
operator|)
name|Math
operator|.
name|pow
argument_list|(
literal|2
argument_list|,
name|reducersLog
argument_list|)
decl_stmt|;
if|if
condition|(
name|powersOfTwo
condition|)
block|{
comment|// If the original number of reducers was a power of two, use that
if|if
condition|(
name|reducersPowerTwo
operator|/
literal|2
operator|==
name|reducers
condition|)
block|{
comment|// nothing to do
block|}
elseif|else
if|if
condition|(
name|reducersPowerTwo
operator|>
name|maxReducers
condition|)
block|{
comment|// If the next power of two greater than the original number of reducers is greater
comment|// than the max number of reducers, use the preceding power of two, which is strictly
comment|// less than the original number of reducers and hence the max
name|reducers
operator|=
name|reducersPowerTwo
operator|/
literal|2
expr_stmt|;
block|}
else|else
block|{
comment|// Otherwise use the smallest power of two greater than the original number of reducers
name|reducers
operator|=
name|reducersPowerTwo
expr_stmt|;
block|}
block|}
return|return
name|reducers
return|;
block|}
comment|/**    * Computes the total input file size. If block sampling was used it will scale this    * value by the highest sample percentage (as an estimate for input).    *    * @param inputSummary    * @param work    * @param highestSamplePercentage    * @return estimated total input size for job    */
specifier|public
specifier|static
name|long
name|getTotalInputFileSize
parameter_list|(
name|ContentSummary
name|inputSummary
parameter_list|,
name|MapWork
name|work
parameter_list|,
name|double
name|highestSamplePercentage
parameter_list|)
block|{
name|long
name|totalInputFileSize
init|=
name|inputSummary
operator|.
name|getLength
argument_list|()
decl_stmt|;
if|if
condition|(
name|work
operator|.
name|getNameToSplitSample
argument_list|()
operator|==
literal|null
operator|||
name|work
operator|.
name|getNameToSplitSample
argument_list|()
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
comment|// If percentage block sampling wasn't used, we don't need to do any estimation
return|return
name|totalInputFileSize
return|;
block|}
if|if
condition|(
name|highestSamplePercentage
operator|>=
literal|0
condition|)
block|{
name|totalInputFileSize
operator|=
name|Math
operator|.
name|min
argument_list|(
call|(
name|long
call|)
argument_list|(
name|totalInputFileSize
operator|*
name|highestSamplePercentage
operator|/
literal|100D
argument_list|)
argument_list|,
name|totalInputFileSize
argument_list|)
expr_stmt|;
block|}
return|return
name|totalInputFileSize
return|;
block|}
comment|/**    * Computes the total number of input files. If block sampling was used it will scale this    * value by the highest sample percentage (as an estimate for # input files).    *    * @param inputSummary    * @param work    * @param highestSamplePercentage    * @return    */
specifier|public
specifier|static
name|long
name|getTotalInputNumFiles
parameter_list|(
name|ContentSummary
name|inputSummary
parameter_list|,
name|MapWork
name|work
parameter_list|,
name|double
name|highestSamplePercentage
parameter_list|)
block|{
name|long
name|totalInputNumFiles
init|=
name|inputSummary
operator|.
name|getFileCount
argument_list|()
decl_stmt|;
if|if
condition|(
name|work
operator|.
name|getNameToSplitSample
argument_list|()
operator|==
literal|null
operator|||
name|work
operator|.
name|getNameToSplitSample
argument_list|()
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
comment|// If percentage block sampling wasn't used, we don't need to do any estimation
return|return
name|totalInputNumFiles
return|;
block|}
if|if
condition|(
name|highestSamplePercentage
operator|>=
literal|0
condition|)
block|{
name|totalInputNumFiles
operator|=
name|Math
operator|.
name|min
argument_list|(
call|(
name|long
call|)
argument_list|(
name|totalInputNumFiles
operator|*
name|highestSamplePercentage
operator|/
literal|100D
argument_list|)
argument_list|,
name|totalInputNumFiles
argument_list|)
expr_stmt|;
block|}
return|return
name|totalInputNumFiles
return|;
block|}
comment|/**    * Returns the highest sample percentage of any alias in the given MapWork    */
specifier|public
specifier|static
name|double
name|getHighestSamplePercentage
parameter_list|(
name|MapWork
name|work
parameter_list|)
block|{
name|double
name|highestSamplePercentage
init|=
literal|0
decl_stmt|;
for|for
control|(
name|String
name|alias
range|:
name|work
operator|.
name|getAliasToWork
argument_list|()
operator|.
name|keySet
argument_list|()
control|)
block|{
if|if
condition|(
name|work
operator|.
name|getNameToSplitSample
argument_list|()
operator|.
name|containsKey
argument_list|(
name|alias
argument_list|)
condition|)
block|{
name|Double
name|rate
init|=
name|work
operator|.
name|getNameToSplitSample
argument_list|()
operator|.
name|get
argument_list|(
name|alias
argument_list|)
operator|.
name|getPercent
argument_list|()
decl_stmt|;
if|if
condition|(
name|rate
operator|!=
literal|null
operator|&&
name|rate
operator|>
name|highestSamplePercentage
condition|)
block|{
name|highestSamplePercentage
operator|=
name|rate
expr_stmt|;
block|}
block|}
else|else
block|{
name|highestSamplePercentage
operator|=
operator|-
literal|1
expr_stmt|;
break|break;
block|}
block|}
return|return
name|highestSamplePercentage
return|;
block|}
comment|/**    * Computes a list of all input paths needed to compute the given MapWork. All aliases    * are considered and a merged list of input paths is returned. If any input path points    * to an empty table or partition a dummy file in the scratch dir is instead created and    * added to the list. This is needed to avoid special casing the operator pipeline for    * these cases.    *    * @param job JobConf used to run the job    * @param work MapWork encapsulating the info about the task    * @param hiveScratchDir The tmp dir used to create dummy files if needed    * @param ctx Context object    * @return List of paths to process for the given MapWork    * @throws Exception    */
specifier|public
specifier|static
name|List
argument_list|<
name|Path
argument_list|>
name|getInputPaths
parameter_list|(
name|JobConf
name|job
parameter_list|,
name|MapWork
name|work
parameter_list|,
name|String
name|hiveScratchDir
parameter_list|,
name|Context
name|ctx
parameter_list|)
throws|throws
name|Exception
block|{
name|int
name|sequenceNumber
init|=
literal|0
decl_stmt|;
name|Set
argument_list|<
name|Path
argument_list|>
name|pathsProcessed
init|=
operator|new
name|HashSet
argument_list|<
name|Path
argument_list|>
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|Path
argument_list|>
name|pathsToAdd
init|=
operator|new
name|LinkedList
argument_list|<
name|Path
argument_list|>
argument_list|()
decl_stmt|;
comment|// AliasToWork contains all the aliases
for|for
control|(
name|String
name|alias
range|:
name|work
operator|.
name|getAliasToWork
argument_list|()
operator|.
name|keySet
argument_list|()
control|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Processing alias "
operator|+
name|alias
argument_list|)
expr_stmt|;
comment|// The alias may not have any path
name|Path
name|path
init|=
literal|null
decl_stmt|;
for|for
control|(
name|String
name|file
range|:
operator|new
name|LinkedList
argument_list|<
name|String
argument_list|>
argument_list|(
name|work
operator|.
name|getPathToAliases
argument_list|()
operator|.
name|keySet
argument_list|()
argument_list|)
control|)
block|{
name|List
argument_list|<
name|String
argument_list|>
name|aliases
init|=
name|work
operator|.
name|getPathToAliases
argument_list|()
operator|.
name|get
argument_list|(
name|file
argument_list|)
decl_stmt|;
if|if
condition|(
name|aliases
operator|.
name|contains
argument_list|(
name|alias
argument_list|)
condition|)
block|{
name|path
operator|=
operator|new
name|Path
argument_list|(
name|file
argument_list|)
expr_stmt|;
comment|// Multiple aliases can point to the same path - it should be
comment|// processed only once
if|if
condition|(
name|pathsProcessed
operator|.
name|contains
argument_list|(
name|path
argument_list|)
condition|)
block|{
continue|continue;
block|}
name|pathsProcessed
operator|.
name|add
argument_list|(
name|path
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Adding input file "
operator|+
name|path
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|HiveConf
operator|.
name|getVar
argument_list|(
name|job
argument_list|,
name|ConfVars
operator|.
name|HIVE_EXECUTION_ENGINE
argument_list|)
operator|.
name|equals
argument_list|(
literal|"tez"
argument_list|)
operator|&&
name|isEmptyPath
argument_list|(
name|job
argument_list|,
name|path
argument_list|,
name|ctx
argument_list|)
condition|)
block|{
name|path
operator|=
name|createDummyFileForEmptyPartition
argument_list|(
name|path
argument_list|,
name|job
argument_list|,
name|work
argument_list|,
name|hiveScratchDir
argument_list|,
name|alias
argument_list|,
name|sequenceNumber
operator|++
argument_list|)
expr_stmt|;
block|}
name|pathsToAdd
operator|.
name|add
argument_list|(
name|path
argument_list|)
expr_stmt|;
block|}
block|}
comment|// If the query references non-existent partitions
comment|// We need to add a empty file, it is not acceptable to change the
comment|// operator tree
comment|// Consider the query:
comment|// select * from (select count(1) from T union all select count(1) from
comment|// T2) x;
comment|// If T is empty and T2 contains 100 rows, the user expects: 0, 100 (2
comment|// rows)
if|if
condition|(
name|path
operator|==
literal|null
operator|&&
operator|!
name|HiveConf
operator|.
name|getVar
argument_list|(
name|job
argument_list|,
name|ConfVars
operator|.
name|HIVE_EXECUTION_ENGINE
argument_list|)
operator|.
name|equals
argument_list|(
literal|"tez"
argument_list|)
condition|)
block|{
name|path
operator|=
name|createDummyFileForEmptyTable
argument_list|(
name|job
argument_list|,
name|work
argument_list|,
name|hiveScratchDir
argument_list|,
name|alias
argument_list|,
name|sequenceNumber
operator|++
argument_list|)
expr_stmt|;
name|pathsToAdd
operator|.
name|add
argument_list|(
name|path
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|pathsToAdd
return|;
block|}
annotation|@
name|SuppressWarnings
argument_list|(
block|{
literal|"rawtypes"
block|,
literal|"unchecked"
block|}
argument_list|)
specifier|private
specifier|static
name|Path
name|createEmptyFile
parameter_list|(
name|String
name|hiveScratchDir
parameter_list|,
name|Class
argument_list|<
name|?
extends|extends
name|HiveOutputFormat
argument_list|>
name|outFileFormat
parameter_list|,
name|JobConf
name|job
parameter_list|,
name|int
name|sequenceNumber
parameter_list|,
name|Properties
name|props
parameter_list|,
name|boolean
name|dummyRow
parameter_list|)
throws|throws
name|IOException
throws|,
name|InstantiationException
throws|,
name|IllegalAccessException
block|{
comment|// create a dummy empty file in a new directory
name|String
name|newDir
init|=
name|hiveScratchDir
operator|+
name|File
operator|.
name|separator
operator|+
name|sequenceNumber
decl_stmt|;
name|Path
name|newPath
init|=
operator|new
name|Path
argument_list|(
name|newDir
argument_list|)
decl_stmt|;
name|FileSystem
name|fs
init|=
name|newPath
operator|.
name|getFileSystem
argument_list|(
name|job
argument_list|)
decl_stmt|;
name|fs
operator|.
name|mkdirs
argument_list|(
name|newPath
argument_list|)
expr_stmt|;
comment|//Qualify the path against the file system. The user configured path might contain default port which is skipped
comment|//in the file status. This makes sure that all paths which goes into PathToPartitionInfo are always listed status
comment|//file path.
name|newPath
operator|=
name|fs
operator|.
name|makeQualified
argument_list|(
name|newPath
argument_list|)
expr_stmt|;
name|String
name|newFile
init|=
name|newDir
operator|+
name|File
operator|.
name|separator
operator|+
literal|"emptyFile"
decl_stmt|;
name|Path
name|newFilePath
init|=
operator|new
name|Path
argument_list|(
name|newFile
argument_list|)
decl_stmt|;
name|String
name|onefile
init|=
name|newPath
operator|.
name|toString
argument_list|()
decl_stmt|;
name|FSRecordWriter
name|recWriter
init|=
name|outFileFormat
operator|.
name|newInstance
argument_list|()
operator|.
name|getHiveRecordWriter
argument_list|(
name|job
argument_list|,
name|newFilePath
argument_list|,
name|Text
operator|.
name|class
argument_list|,
literal|false
argument_list|,
name|props
argument_list|,
literal|null
argument_list|)
decl_stmt|;
if|if
condition|(
name|dummyRow
condition|)
block|{
comment|// empty files are omitted at CombineHiveInputFormat.
comment|// for meta-data only query, it effectively makes partition columns disappear..
comment|// this could be fixed by other methods, but this seemed to be the most easy (HIVEV-2955)
name|recWriter
operator|.
name|write
argument_list|(
operator|new
name|Text
argument_list|(
literal|"empty"
argument_list|)
argument_list|)
expr_stmt|;
comment|// written via HiveIgnoreKeyTextOutputFormat
block|}
name|recWriter
operator|.
name|close
argument_list|(
literal|false
argument_list|)
expr_stmt|;
return|return
name|newPath
return|;
block|}
annotation|@
name|SuppressWarnings
argument_list|(
literal|"rawtypes"
argument_list|)
specifier|private
specifier|static
name|Path
name|createDummyFileForEmptyPartition
parameter_list|(
name|Path
name|path
parameter_list|,
name|JobConf
name|job
parameter_list|,
name|MapWork
name|work
parameter_list|,
name|String
name|hiveScratchDir
parameter_list|,
name|String
name|alias
parameter_list|,
name|int
name|sequenceNumber
parameter_list|)
throws|throws
name|IOException
throws|,
name|InstantiationException
throws|,
name|IllegalAccessException
block|{
name|String
name|strPath
init|=
name|path
operator|.
name|toString
argument_list|()
decl_stmt|;
comment|// The input file does not exist, replace it by a empty file
name|PartitionDesc
name|partDesc
init|=
name|work
operator|.
name|getPathToPartitionInfo
argument_list|()
operator|.
name|get
argument_list|(
name|strPath
argument_list|)
decl_stmt|;
name|boolean
name|nonNative
init|=
name|partDesc
operator|.
name|getTableDesc
argument_list|()
operator|.
name|isNonNative
argument_list|()
decl_stmt|;
name|boolean
name|oneRow
init|=
name|partDesc
operator|.
name|getInputFileFormatClass
argument_list|()
operator|==
name|OneNullRowInputFormat
operator|.
name|class
decl_stmt|;
name|Properties
name|props
init|=
name|partDesc
operator|.
name|getProperties
argument_list|()
decl_stmt|;
name|Class
argument_list|<
name|?
extends|extends
name|HiveOutputFormat
argument_list|>
name|outFileFormat
init|=
name|partDesc
operator|.
name|getOutputFileFormatClass
argument_list|()
decl_stmt|;
if|if
condition|(
name|nonNative
condition|)
block|{
comment|// if this isn't a hive table we can't create an empty file for it.
return|return
name|path
return|;
block|}
name|Path
name|newPath
init|=
name|createEmptyFile
argument_list|(
name|hiveScratchDir
argument_list|,
name|outFileFormat
argument_list|,
name|job
argument_list|,
name|sequenceNumber
argument_list|,
name|props
argument_list|,
name|oneRow
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Changed input file to "
operator|+
name|newPath
argument_list|)
expr_stmt|;
comment|// update the work
name|String
name|strNewPath
init|=
name|newPath
operator|.
name|toString
argument_list|()
decl_stmt|;
name|LinkedHashMap
argument_list|<
name|String
argument_list|,
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|>
name|pathToAliases
init|=
name|work
operator|.
name|getPathToAliases
argument_list|()
decl_stmt|;
name|pathToAliases
operator|.
name|put
argument_list|(
name|strNewPath
argument_list|,
name|pathToAliases
operator|.
name|get
argument_list|(
name|strPath
argument_list|)
argument_list|)
expr_stmt|;
name|pathToAliases
operator|.
name|remove
argument_list|(
name|strPath
argument_list|)
expr_stmt|;
name|work
operator|.
name|setPathToAliases
argument_list|(
name|pathToAliases
argument_list|)
expr_stmt|;
name|LinkedHashMap
argument_list|<
name|String
argument_list|,
name|PartitionDesc
argument_list|>
name|pathToPartitionInfo
init|=
name|work
operator|.
name|getPathToPartitionInfo
argument_list|()
decl_stmt|;
name|pathToPartitionInfo
operator|.
name|put
argument_list|(
name|strNewPath
argument_list|,
name|pathToPartitionInfo
operator|.
name|get
argument_list|(
name|strPath
argument_list|)
argument_list|)
expr_stmt|;
name|pathToPartitionInfo
operator|.
name|remove
argument_list|(
name|strPath
argument_list|)
expr_stmt|;
name|work
operator|.
name|setPathToPartitionInfo
argument_list|(
name|pathToPartitionInfo
argument_list|)
expr_stmt|;
return|return
name|newPath
return|;
block|}
annotation|@
name|SuppressWarnings
argument_list|(
literal|"rawtypes"
argument_list|)
specifier|private
specifier|static
name|Path
name|createDummyFileForEmptyTable
parameter_list|(
name|JobConf
name|job
parameter_list|,
name|MapWork
name|work
parameter_list|,
name|String
name|hiveScratchDir
parameter_list|,
name|String
name|alias
parameter_list|,
name|int
name|sequenceNumber
parameter_list|)
throws|throws
name|IOException
throws|,
name|InstantiationException
throws|,
name|IllegalAccessException
block|{
name|TableDesc
name|tableDesc
init|=
name|work
operator|.
name|getAliasToPartnInfo
argument_list|()
operator|.
name|get
argument_list|(
name|alias
argument_list|)
operator|.
name|getTableDesc
argument_list|()
decl_stmt|;
name|Properties
name|props
init|=
name|tableDesc
operator|.
name|getProperties
argument_list|()
decl_stmt|;
name|boolean
name|nonNative
init|=
name|tableDesc
operator|.
name|isNonNative
argument_list|()
decl_stmt|;
name|Class
argument_list|<
name|?
extends|extends
name|HiveOutputFormat
argument_list|>
name|outFileFormat
init|=
name|tableDesc
operator|.
name|getOutputFileFormatClass
argument_list|()
decl_stmt|;
if|if
condition|(
name|nonNative
condition|)
block|{
comment|// if this isn't a hive table we can't create an empty file for it.
return|return
literal|null
return|;
block|}
name|Path
name|newPath
init|=
name|createEmptyFile
argument_list|(
name|hiveScratchDir
argument_list|,
name|outFileFormat
argument_list|,
name|job
argument_list|,
name|sequenceNumber
argument_list|,
name|props
argument_list|,
literal|false
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Changed input file to "
operator|+
name|newPath
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
comment|// update the work
name|LinkedHashMap
argument_list|<
name|String
argument_list|,
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|>
name|pathToAliases
init|=
name|work
operator|.
name|getPathToAliases
argument_list|()
decl_stmt|;
name|ArrayList
argument_list|<
name|String
argument_list|>
name|newList
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
name|newList
operator|.
name|add
argument_list|(
name|alias
argument_list|)
expr_stmt|;
name|pathToAliases
operator|.
name|put
argument_list|(
name|newPath
operator|.
name|toUri
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|,
name|newList
argument_list|)
expr_stmt|;
name|work
operator|.
name|setPathToAliases
argument_list|(
name|pathToAliases
argument_list|)
expr_stmt|;
name|LinkedHashMap
argument_list|<
name|String
argument_list|,
name|PartitionDesc
argument_list|>
name|pathToPartitionInfo
init|=
name|work
operator|.
name|getPathToPartitionInfo
argument_list|()
decl_stmt|;
name|PartitionDesc
name|pDesc
init|=
name|work
operator|.
name|getAliasToPartnInfo
argument_list|()
operator|.
name|get
argument_list|(
name|alias
argument_list|)
operator|.
name|clone
argument_list|()
decl_stmt|;
name|pathToPartitionInfo
operator|.
name|put
argument_list|(
name|newPath
operator|.
name|toUri
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|,
name|pDesc
argument_list|)
expr_stmt|;
name|work
operator|.
name|setPathToPartitionInfo
argument_list|(
name|pathToPartitionInfo
argument_list|)
expr_stmt|;
return|return
name|newPath
return|;
block|}
comment|/**    * setInputPaths add all the paths in the provided list to the Job conf object    * as input paths for the job.    *    * @param job    * @param pathsToAdd    */
specifier|public
specifier|static
name|void
name|setInputPaths
parameter_list|(
name|JobConf
name|job
parameter_list|,
name|List
argument_list|<
name|Path
argument_list|>
name|pathsToAdd
parameter_list|)
block|{
name|Path
index|[]
name|addedPaths
init|=
name|FileInputFormat
operator|.
name|getInputPaths
argument_list|(
name|job
argument_list|)
decl_stmt|;
if|if
condition|(
name|addedPaths
operator|==
literal|null
condition|)
block|{
name|addedPaths
operator|=
operator|new
name|Path
index|[
literal|0
index|]
expr_stmt|;
block|}
name|Path
index|[]
name|combined
init|=
operator|new
name|Path
index|[
name|addedPaths
operator|.
name|length
operator|+
name|pathsToAdd
operator|.
name|size
argument_list|()
index|]
decl_stmt|;
name|System
operator|.
name|arraycopy
argument_list|(
name|addedPaths
argument_list|,
literal|0
argument_list|,
name|combined
argument_list|,
literal|0
argument_list|,
name|addedPaths
operator|.
name|length
argument_list|)
expr_stmt|;
name|int
name|i
init|=
literal|0
decl_stmt|;
for|for
control|(
name|Path
name|p
range|:
name|pathsToAdd
control|)
block|{
name|combined
index|[
name|addedPaths
operator|.
name|length
operator|+
operator|(
name|i
operator|++
operator|)
index|]
operator|=
name|p
expr_stmt|;
block|}
name|FileInputFormat
operator|.
name|setInputPaths
argument_list|(
name|job
argument_list|,
name|combined
argument_list|)
expr_stmt|;
block|}
comment|/**    * Set hive input format, and input format file if necessary.    */
specifier|public
specifier|static
name|void
name|setInputAttributes
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|MapWork
name|mWork
parameter_list|)
block|{
if|if
condition|(
name|mWork
operator|.
name|getInputformat
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|HiveConf
operator|.
name|setVar
argument_list|(
name|conf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEINPUTFORMAT
argument_list|,
name|mWork
operator|.
name|getInputformat
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|mWork
operator|.
name|getIndexIntermediateFile
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|conf
operator|.
name|set
argument_list|(
literal|"hive.index.compact.file"
argument_list|,
name|mWork
operator|.
name|getIndexIntermediateFile
argument_list|()
argument_list|)
expr_stmt|;
name|conf
operator|.
name|set
argument_list|(
literal|"hive.index.blockfilter.file"
argument_list|,
name|mWork
operator|.
name|getIndexIntermediateFile
argument_list|()
argument_list|)
expr_stmt|;
block|}
comment|// Intentionally overwrites anything the user may have put here
name|conf
operator|.
name|setBoolean
argument_list|(
literal|"hive.input.format.sorted"
argument_list|,
name|mWork
operator|.
name|isInputFormatSorted
argument_list|()
argument_list|)
expr_stmt|;
block|}
comment|/**    * Hive uses tmp directories to capture the output of each FileSinkOperator.    * This method creates all necessary tmp directories for FileSinks in the Mapwork.    *    * @param conf Used to get the right FileSystem    * @param mWork Used to find FileSinkOperators    * @throws IOException    */
specifier|public
specifier|static
name|void
name|createTmpDirs
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|MapWork
name|mWork
parameter_list|)
throws|throws
name|IOException
block|{
name|Map
argument_list|<
name|String
argument_list|,
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|>
name|pa
init|=
name|mWork
operator|.
name|getPathToAliases
argument_list|()
decl_stmt|;
if|if
condition|(
name|pa
operator|!=
literal|null
condition|)
block|{
name|List
argument_list|<
name|Operator
argument_list|<
name|?
extends|extends
name|OperatorDesc
argument_list|>
argument_list|>
name|ops
init|=
operator|new
name|ArrayList
argument_list|<
name|Operator
argument_list|<
name|?
extends|extends
name|OperatorDesc
argument_list|>
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|List
argument_list|<
name|String
argument_list|>
name|ls
range|:
name|pa
operator|.
name|values
argument_list|()
control|)
block|{
for|for
control|(
name|String
name|a
range|:
name|ls
control|)
block|{
name|ops
operator|.
name|add
argument_list|(
name|mWork
operator|.
name|getAliasToWork
argument_list|()
operator|.
name|get
argument_list|(
name|a
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
name|createTmpDirs
argument_list|(
name|conf
argument_list|,
name|ops
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * Hive uses tmp directories to capture the output of each FileSinkOperator.    * This method creates all necessary tmp directories for FileSinks in the ReduceWork.    *    * @param conf Used to get the right FileSystem    * @param rWork Used to find FileSinkOperators    * @throws IOException    */
annotation|@
name|SuppressWarnings
argument_list|(
literal|"unchecked"
argument_list|)
specifier|public
specifier|static
name|void
name|createTmpDirs
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|ReduceWork
name|rWork
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|rWork
operator|==
literal|null
condition|)
block|{
return|return;
block|}
name|List
argument_list|<
name|Operator
argument_list|<
name|?
extends|extends
name|OperatorDesc
argument_list|>
argument_list|>
name|ops
init|=
operator|new
name|LinkedList
argument_list|<
name|Operator
argument_list|<
name|?
extends|extends
name|OperatorDesc
argument_list|>
argument_list|>
argument_list|()
decl_stmt|;
name|ops
operator|.
name|add
argument_list|(
name|rWork
operator|.
name|getReducer
argument_list|()
argument_list|)
expr_stmt|;
name|createTmpDirs
argument_list|(
name|conf
argument_list|,
name|ops
argument_list|)
expr_stmt|;
block|}
specifier|private
specifier|static
name|void
name|createTmpDirs
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|List
argument_list|<
name|Operator
argument_list|<
name|?
extends|extends
name|OperatorDesc
argument_list|>
argument_list|>
name|ops
parameter_list|)
throws|throws
name|IOException
block|{
while|while
condition|(
operator|!
name|ops
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|Operator
argument_list|<
name|?
extends|extends
name|OperatorDesc
argument_list|>
name|op
init|=
name|ops
operator|.
name|remove
argument_list|(
literal|0
argument_list|)
decl_stmt|;
if|if
condition|(
name|op
operator|instanceof
name|FileSinkOperator
condition|)
block|{
name|FileSinkDesc
name|fdesc
init|=
operator|(
operator|(
name|FileSinkOperator
operator|)
name|op
operator|)
operator|.
name|getConf
argument_list|()
decl_stmt|;
name|Path
name|tempDir
init|=
name|fdesc
operator|.
name|getDirName
argument_list|()
decl_stmt|;
if|if
condition|(
name|tempDir
operator|!=
literal|null
condition|)
block|{
name|Path
name|tempPath
init|=
name|Utilities
operator|.
name|toTempPath
argument_list|(
name|tempDir
argument_list|)
decl_stmt|;
name|FileSystem
name|fs
init|=
name|tempPath
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|fs
operator|.
name|mkdirs
argument_list|(
name|tempPath
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
name|op
operator|.
name|getChildOperators
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|ops
operator|.
name|addAll
argument_list|(
name|op
operator|.
name|getChildOperators
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
block|}
specifier|public
specifier|static
name|void
name|clearWorkMap
parameter_list|()
block|{
name|gWorkMap
operator|.
name|clear
argument_list|()
expr_stmt|;
block|}
comment|/**    * Create a temp dir in specified baseDir    * This can go away once hive moves to support only JDK 7    *  and can use Files.createTempDirectory    *  Guava Files.createTempDir() does not take a base dir    * @param baseDir - directory under which new temp dir will be created    * @return File object for new temp dir    */
specifier|public
specifier|static
name|File
name|createTempDir
parameter_list|(
name|String
name|baseDir
parameter_list|)
block|{
comment|//try creating the temp dir MAX_ATTEMPTS times
specifier|final
name|int
name|MAX_ATTEMPS
init|=
literal|30
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|MAX_ATTEMPS
condition|;
name|i
operator|++
control|)
block|{
comment|//pick a random file name
name|String
name|tempDirName
init|=
literal|"tmp_"
operator|+
operator|(
call|(
name|int
call|)
argument_list|(
literal|100000
operator|*
name|Math
operator|.
name|random
argument_list|()
argument_list|)
operator|)
decl_stmt|;
comment|//return if dir could successfully be created with that file name
name|File
name|tempDir
init|=
operator|new
name|File
argument_list|(
name|baseDir
argument_list|,
name|tempDirName
argument_list|)
decl_stmt|;
if|if
condition|(
name|tempDir
operator|.
name|mkdir
argument_list|()
condition|)
block|{
return|return
name|tempDir
return|;
block|}
block|}
throw|throw
operator|new
name|IllegalStateException
argument_list|(
literal|"Failed to create a temp dir under "
operator|+
name|baseDir
operator|+
literal|" Giving up after "
operator|+
name|MAX_ATTEMPS
operator|+
literal|" attemps"
argument_list|)
throw|;
block|}
comment|/**    * Skip header lines in the table file when reading the record.    *    * @param currRecReader    *          Record reader.    *    * @param headerCount    *          Header line number of the table files.    *    * @param key    *          Key of current reading record.    *    * @param value    *          Value of current reading record.    *    * @return Return true if there are 0 or more records left in the file    *         after skipping all headers, otherwise return false.    */
specifier|public
specifier|static
name|boolean
name|skipHeader
parameter_list|(
name|RecordReader
argument_list|<
name|WritableComparable
argument_list|,
name|Writable
argument_list|>
name|currRecReader
parameter_list|,
name|int
name|headerCount
parameter_list|,
name|WritableComparable
name|key
parameter_list|,
name|Writable
name|value
parameter_list|)
throws|throws
name|IOException
block|{
while|while
condition|(
name|headerCount
operator|>
literal|0
condition|)
block|{
if|if
condition|(
operator|!
name|currRecReader
operator|.
name|next
argument_list|(
name|key
argument_list|,
name|value
argument_list|)
condition|)
return|return
literal|false
return|;
name|headerCount
operator|--
expr_stmt|;
block|}
return|return
literal|true
return|;
block|}
comment|/**    * Get header line count for a table.    *    * @param table    *          Table description for target table.    *    */
specifier|public
specifier|static
name|int
name|getHeaderCount
parameter_list|(
name|TableDesc
name|table
parameter_list|)
throws|throws
name|IOException
block|{
name|int
name|headerCount
decl_stmt|;
try|try
block|{
name|headerCount
operator|=
name|Integer
operator|.
name|parseInt
argument_list|(
name|table
operator|.
name|getProperties
argument_list|()
operator|.
name|getProperty
argument_list|(
name|serdeConstants
operator|.
name|HEADER_COUNT
argument_list|,
literal|"0"
argument_list|)
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|NumberFormatException
name|nfe
parameter_list|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
name|nfe
argument_list|)
throw|;
block|}
return|return
name|headerCount
return|;
block|}
comment|/**    * Get footer line count for a table.    *    * @param table    *          Table description for target table.    *    * @param job    *          Job configuration for current job.    */
specifier|public
specifier|static
name|int
name|getFooterCount
parameter_list|(
name|TableDesc
name|table
parameter_list|,
name|JobConf
name|job
parameter_list|)
throws|throws
name|IOException
block|{
name|int
name|footerCount
decl_stmt|;
try|try
block|{
name|footerCount
operator|=
name|Integer
operator|.
name|parseInt
argument_list|(
name|table
operator|.
name|getProperties
argument_list|()
operator|.
name|getProperty
argument_list|(
name|serdeConstants
operator|.
name|FOOTER_COUNT
argument_list|,
literal|"0"
argument_list|)
argument_list|)
expr_stmt|;
if|if
condition|(
name|footerCount
operator|>
name|HiveConf
operator|.
name|getIntVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_FILE_MAX_FOOTER
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"footer number exceeds the limit defined in hive.file.max.footer"
argument_list|)
throw|;
block|}
block|}
catch|catch
parameter_list|(
name|NumberFormatException
name|nfe
parameter_list|)
block|{
comment|// Footer line number must be set as an integer.
throw|throw
operator|new
name|IOException
argument_list|(
name|nfe
argument_list|)
throw|;
block|}
return|return
name|footerCount
return|;
block|}
block|}
end_class

end_unit

