begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/*  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
package|;
end_package

begin_import
import|import
name|com
operator|.
name|esotericsoftware
operator|.
name|kryo
operator|.
name|Kryo
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|annotations
operator|.
name|VisibleForTesting
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|base
operator|.
name|Preconditions
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|collect
operator|.
name|Lists
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|collect
operator|.
name|Sets
import|;
end_import

begin_import
import|import
name|java
operator|.
name|beans
operator|.
name|DefaultPersistenceDelegate
import|;
end_import

begin_import
import|import
name|java
operator|.
name|beans
operator|.
name|Encoder
import|;
end_import

begin_import
import|import
name|java
operator|.
name|beans
operator|.
name|Expression
import|;
end_import

begin_import
import|import
name|java
operator|.
name|beans
operator|.
name|Statement
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|ByteArrayInputStream
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|ByteArrayOutputStream
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|DataInput
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|EOFException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|File
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|FileNotFoundException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|InputStream
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|OutputStream
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|Serializable
import|;
end_import

begin_import
import|import
name|java
operator|.
name|net
operator|.
name|URI
import|;
end_import

begin_import
import|import
name|java
operator|.
name|net
operator|.
name|URL
import|;
end_import

begin_import
import|import
name|java
operator|.
name|net
operator|.
name|URLClassLoader
import|;
end_import

begin_import
import|import
name|java
operator|.
name|net
operator|.
name|URLDecoder
import|;
end_import

begin_import
import|import
name|java
operator|.
name|sql
operator|.
name|Connection
import|;
end_import

begin_import
import|import
name|java
operator|.
name|sql
operator|.
name|DriverManager
import|;
end_import

begin_import
import|import
name|java
operator|.
name|sql
operator|.
name|PreparedStatement
import|;
end_import

begin_import
import|import
name|java
operator|.
name|sql
operator|.
name|SQLException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|sql
operator|.
name|SQLFeatureNotSupportedException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|sql
operator|.
name|SQLTransientException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|text
operator|.
name|SimpleDateFormat
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Arrays
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Calendar
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Collection
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Collections
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Enumeration
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashSet
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Iterator
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|LinkedHashMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|LinkedList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Properties
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Random
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Set
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|UUID
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|ConcurrentHashMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|ExecutionException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|Future
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|regex
operator|.
name|Matcher
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|regex
operator|.
name|Pattern
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|zip
operator|.
name|Deflater
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|zip
operator|.
name|DeflaterOutputStream
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|zip
operator|.
name|InflaterInputStream
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|util
operator|.
name|concurrent
operator|.
name|ThreadFactoryBuilder
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|codec
operator|.
name|binary
operator|.
name|Base64
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|collections
operator|.
name|CollectionUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|collections
operator|.
name|MapUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|lang
operator|.
name|StringUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|lang
operator|.
name|WordUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|lang3
operator|.
name|StringEscapeUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configuration
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|filecache
operator|.
name|DistributedCache
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|ContentSummary
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FSDataInputStream
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FSDataOutputStream
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileStatus
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|LocatedFileStatus
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|PathFilter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|RemoteIterator
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|permission
operator|.
name|FsPermission
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|BlobStorageUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|FileUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|HiveInterruptCallback
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|HiveInterruptUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|HiveStatsUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|JavaUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|StatsSetupConst
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|StringInternUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|ValidTxnList
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
operator|.
name|ConfVars
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|Warehouse
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|FieldSchema
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Order
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|hive_metastoreConstants
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|utils
operator|.
name|MetaStoreUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|Context
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|Driver
operator|.
name|LockedDriverState
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|ErrorMsg
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|QueryPlan
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|FileSinkOperator
operator|.
name|RecordWriter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|mr
operator|.
name|ExecDriver
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|mr
operator|.
name|ExecMapper
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|mr
operator|.
name|ExecReducer
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|mr
operator|.
name|MapRedTask
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|spark
operator|.
name|SparkTask
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|tez
operator|.
name|DagUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|tez
operator|.
name|TezTask
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|util
operator|.
name|DAGTraversal
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|vector
operator|.
name|VectorizedInputFormatInterface
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|vector
operator|.
name|VectorizedRowBatchCtx
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|AcidUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|ContentSummaryInputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|HiveFileFormatUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|HiveIgnoreKeyTextOutputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|HiveInputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|HiveOutputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|HiveSequenceFileOutputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|IOConstants
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|IgnoreKeyTextOutputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|OneNullRowInputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|RCFile
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|ReworkMapredInputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|SelfDescribingInputFormatInterface
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|merge
operator|.
name|MergeFileMapper
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|merge
operator|.
name|MergeFileWork
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|rcfile
operator|.
name|truncate
operator|.
name|ColumnTruncateMapper
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|rcfile
operator|.
name|truncate
operator|.
name|ColumnTruncateWork
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|log
operator|.
name|PerfLogger
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|HiveException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|HiveStorageHandler
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|HiveUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|InputEstimator
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|Partition
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|Table
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|parse
operator|.
name|SemanticException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|BaseWork
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|DynamicPartitionCtx
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|FileSinkDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|IStatsGatherDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|MapWork
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|MapredWork
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|MergeJoinWork
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|OperatorDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|PartitionDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|PlanUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|ReduceWork
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|TableDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|api
operator|.
name|Adjacency
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|api
operator|.
name|Graph
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|session
operator|.
name|SessionState
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|stats
operator|.
name|StatsFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|stats
operator|.
name|StatsPublisher
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde
operator|.
name|serdeConstants
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|MetadataTypedColumnsetSerDe
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|SerDeException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|SerDeUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|Serializer
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|lazy
operator|.
name|LazySimpleSerDe
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|objectinspector
operator|.
name|ObjectInspector
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|objectinspector
operator|.
name|ObjectInspectorFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|objectinspector
operator|.
name|ObjectInspectorUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|objectinspector
operator|.
name|StandardStructObjectInspector
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|objectinspector
operator|.
name|StructField
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|objectinspector
operator|.
name|StructObjectInspector
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|typeinfo
operator|.
name|TypeInfo
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|typeinfo
operator|.
name|TypeInfoUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|shims
operator|.
name|ShimLoader
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|IOUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|SequenceFile
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|SequenceFile
operator|.
name|CompressionType
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|Text
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|Writable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|WritableComparable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|compress
operator|.
name|CompressionCodec
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|compress
operator|.
name|DefaultCodec
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|FileInputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|FileOutputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|FileSplit
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|InputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|InputSplit
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|JobConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|RecordReader
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|Reporter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|SequenceFileInputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|SequenceFileOutputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|TextInputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|security
operator|.
name|UserGroupInformation
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|Progressable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hive
operator|.
name|common
operator|.
name|util
operator|.
name|ACLConfigurationParser
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hive
operator|.
name|common
operator|.
name|util
operator|.
name|ReflectionUtil
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|Logger
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|LoggerFactory
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|Callable
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|ExecutorService
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|Executors
import|;
end_import

begin_comment
comment|/**  * Utilities.  *  */
end_comment

begin_class
annotation|@
name|SuppressWarnings
argument_list|(
block|{
literal|"nls"
block|,
literal|"deprecation"
block|}
argument_list|)
specifier|public
specifier|final
class|class
name|Utilities
block|{
comment|/**    * A logger mostly used to trace-log the details of Hive table file operations. Filtering the    * logs for FileOperations (with trace logs present) allows one to debug what Hive has done with    * various files and directories while committing writes, as well as reading.    */
specifier|public
specifier|static
specifier|final
name|Logger
name|FILE_OP_LOGGER
init|=
name|LoggerFactory
operator|.
name|getLogger
argument_list|(
literal|"FileOperations"
argument_list|)
decl_stmt|;
comment|/**    * The object in the reducer are composed of these top level fields.    */
specifier|public
specifier|static
specifier|final
name|String
name|HADOOP_LOCAL_FS
init|=
literal|"file:///"
decl_stmt|;
specifier|public
specifier|static
specifier|final
name|String
name|HADOOP_LOCAL_FS_SCHEME
init|=
literal|"file"
decl_stmt|;
specifier|public
specifier|static
specifier|final
name|String
name|MAP_PLAN_NAME
init|=
literal|"map.xml"
decl_stmt|;
specifier|public
specifier|static
specifier|final
name|String
name|REDUCE_PLAN_NAME
init|=
literal|"reduce.xml"
decl_stmt|;
specifier|public
specifier|static
specifier|final
name|String
name|MERGE_PLAN_NAME
init|=
literal|"merge.xml"
decl_stmt|;
specifier|public
specifier|static
specifier|final
name|String
name|INPUT_NAME
init|=
literal|"iocontext.input.name"
decl_stmt|;
specifier|public
specifier|static
specifier|final
name|String
name|HAS_MAP_WORK
init|=
literal|"has.map.work"
decl_stmt|;
specifier|public
specifier|static
specifier|final
name|String
name|HAS_REDUCE_WORK
init|=
literal|"has.reduce.work"
decl_stmt|;
specifier|public
specifier|static
specifier|final
name|String
name|MAPRED_MAPPER_CLASS
init|=
literal|"mapred.mapper.class"
decl_stmt|;
specifier|public
specifier|static
specifier|final
name|String
name|MAPRED_REDUCER_CLASS
init|=
literal|"mapred.reducer.class"
decl_stmt|;
specifier|public
specifier|static
specifier|final
name|String
name|HIVE_ADDED_JARS
init|=
literal|"hive.added.jars"
decl_stmt|;
specifier|public
specifier|static
specifier|final
name|String
name|VECTOR_MODE
init|=
literal|"VECTOR_MODE"
decl_stmt|;
specifier|public
specifier|static
specifier|final
name|String
name|USE_VECTORIZED_INPUT_FILE_FORMAT
init|=
literal|"USE_VECTORIZED_INPUT_FILE_FORMAT"
decl_stmt|;
specifier|public
specifier|static
specifier|final
name|String
name|MAPNAME
init|=
literal|"Map "
decl_stmt|;
specifier|public
specifier|static
specifier|final
name|String
name|REDUCENAME
init|=
literal|"Reducer "
decl_stmt|;
annotation|@
name|Deprecated
specifier|protected
specifier|static
specifier|final
name|String
name|DEPRECATED_MAPRED_DFSCLIENT_PARALLELISM_MAX
init|=
literal|"mapred.dfsclient.parallelism.max"
decl_stmt|;
specifier|public
specifier|static
name|Random
name|randGen
init|=
operator|new
name|Random
argument_list|()
decl_stmt|;
comment|/**    * ReduceField:    * KEY: record key    * VALUE: record value    */
specifier|public
specifier|static
enum|enum
name|ReduceField
block|{
name|KEY
block|,
name|VALUE
block|}
empty_stmt|;
specifier|public
specifier|static
name|List
argument_list|<
name|String
argument_list|>
name|reduceFieldNameList
decl_stmt|;
static|static
block|{
name|reduceFieldNameList
operator|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
expr_stmt|;
for|for
control|(
name|ReduceField
name|r
range|:
name|ReduceField
operator|.
name|values
argument_list|()
control|)
block|{
name|reduceFieldNameList
operator|.
name|add
argument_list|(
name|r
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
specifier|public
specifier|static
name|String
name|removeValueTag
parameter_list|(
name|String
name|column
parameter_list|)
block|{
if|if
condition|(
name|column
operator|.
name|startsWith
argument_list|(
name|ReduceField
operator|.
name|VALUE
operator|+
literal|"."
argument_list|)
condition|)
block|{
return|return
name|column
operator|.
name|substring
argument_list|(
literal|6
argument_list|)
return|;
block|}
return|return
name|column
return|;
block|}
specifier|private
name|Utilities
parameter_list|()
block|{
comment|// prevent instantiation
block|}
specifier|private
specifier|static
name|GlobalWorkMapFactory
name|gWorkMap
init|=
operator|new
name|GlobalWorkMapFactory
argument_list|()
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|String
name|CLASS_NAME
init|=
name|Utilities
operator|.
name|class
operator|.
name|getName
argument_list|()
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|Logger
name|LOG
init|=
name|LoggerFactory
operator|.
name|getLogger
argument_list|(
name|CLASS_NAME
argument_list|)
decl_stmt|;
specifier|public
specifier|static
name|void
name|clearWork
parameter_list|(
name|Configuration
name|conf
parameter_list|)
block|{
name|Path
name|mapPath
init|=
name|getPlanPath
argument_list|(
name|conf
argument_list|,
name|MAP_PLAN_NAME
argument_list|)
decl_stmt|;
name|Path
name|reducePath
init|=
name|getPlanPath
argument_list|(
name|conf
argument_list|,
name|REDUCE_PLAN_NAME
argument_list|)
decl_stmt|;
comment|// if the plan path hasn't been initialized just return, nothing to clean.
if|if
condition|(
name|mapPath
operator|==
literal|null
operator|&&
name|reducePath
operator|==
literal|null
condition|)
block|{
return|return;
block|}
try|try
block|{
name|FileSystem
name|fs
init|=
name|mapPath
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
if|if
condition|(
name|fs
operator|.
name|exists
argument_list|(
name|mapPath
argument_list|)
condition|)
block|{
name|fs
operator|.
name|delete
argument_list|(
name|mapPath
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|fs
operator|.
name|exists
argument_list|(
name|reducePath
argument_list|)
condition|)
block|{
name|fs
operator|.
name|delete
argument_list|(
name|reducePath
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Failed to clean-up tmp directories."
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
comment|// where a single process works with multiple plans - we must clear
comment|// the cache before working with the next plan.
name|clearWorkMapForConf
argument_list|(
name|conf
argument_list|)
expr_stmt|;
block|}
block|}
specifier|public
specifier|static
name|MapredWork
name|getMapRedWork
parameter_list|(
name|Configuration
name|conf
parameter_list|)
block|{
name|MapredWork
name|w
init|=
operator|new
name|MapredWork
argument_list|()
decl_stmt|;
name|w
operator|.
name|setMapWork
argument_list|(
name|getMapWork
argument_list|(
name|conf
argument_list|)
argument_list|)
expr_stmt|;
name|w
operator|.
name|setReduceWork
argument_list|(
name|getReduceWork
argument_list|(
name|conf
argument_list|)
argument_list|)
expr_stmt|;
return|return
name|w
return|;
block|}
specifier|public
specifier|static
name|void
name|cacheMapWork
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|MapWork
name|work
parameter_list|,
name|Path
name|hiveScratchDir
parameter_list|)
block|{
name|cacheBaseWork
argument_list|(
name|conf
argument_list|,
name|MAP_PLAN_NAME
argument_list|,
name|work
argument_list|,
name|hiveScratchDir
argument_list|)
expr_stmt|;
block|}
specifier|public
specifier|static
name|void
name|setMapWork
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|MapWork
name|work
parameter_list|)
block|{
name|setBaseWork
argument_list|(
name|conf
argument_list|,
name|MAP_PLAN_NAME
argument_list|,
name|work
argument_list|)
expr_stmt|;
block|}
specifier|public
specifier|static
name|MapWork
name|getMapWork
parameter_list|(
name|Configuration
name|conf
parameter_list|)
block|{
if|if
condition|(
operator|!
name|conf
operator|.
name|getBoolean
argument_list|(
name|HAS_MAP_WORK
argument_list|,
literal|false
argument_list|)
condition|)
block|{
return|return
literal|null
return|;
block|}
return|return
operator|(
name|MapWork
operator|)
name|getBaseWork
argument_list|(
name|conf
argument_list|,
name|MAP_PLAN_NAME
argument_list|)
return|;
block|}
specifier|public
specifier|static
name|void
name|setReduceWork
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|ReduceWork
name|work
parameter_list|)
block|{
name|setBaseWork
argument_list|(
name|conf
argument_list|,
name|REDUCE_PLAN_NAME
argument_list|,
name|work
argument_list|)
expr_stmt|;
block|}
specifier|public
specifier|static
name|ReduceWork
name|getReduceWork
parameter_list|(
name|Configuration
name|conf
parameter_list|)
block|{
if|if
condition|(
operator|!
name|conf
operator|.
name|getBoolean
argument_list|(
name|HAS_REDUCE_WORK
argument_list|,
literal|false
argument_list|)
condition|)
block|{
return|return
literal|null
return|;
block|}
return|return
operator|(
name|ReduceWork
operator|)
name|getBaseWork
argument_list|(
name|conf
argument_list|,
name|REDUCE_PLAN_NAME
argument_list|)
return|;
block|}
specifier|public
specifier|static
name|Path
name|setMergeWork
parameter_list|(
name|JobConf
name|conf
parameter_list|,
name|MergeJoinWork
name|mergeJoinWork
parameter_list|,
name|Path
name|mrScratchDir
parameter_list|,
name|boolean
name|useCache
parameter_list|)
block|{
for|for
control|(
name|BaseWork
name|baseWork
range|:
name|mergeJoinWork
operator|.
name|getBaseWorkList
argument_list|()
control|)
block|{
name|setBaseWork
argument_list|(
name|conf
argument_list|,
name|baseWork
argument_list|,
name|mrScratchDir
argument_list|,
name|baseWork
operator|.
name|getName
argument_list|()
operator|+
name|MERGE_PLAN_NAME
argument_list|,
name|useCache
argument_list|)
expr_stmt|;
name|String
name|prefixes
init|=
name|conf
operator|.
name|get
argument_list|(
name|DagUtils
operator|.
name|TEZ_MERGE_WORK_FILE_PREFIXES
argument_list|)
decl_stmt|;
if|if
condition|(
name|prefixes
operator|==
literal|null
condition|)
block|{
name|prefixes
operator|=
name|baseWork
operator|.
name|getName
argument_list|()
expr_stmt|;
block|}
else|else
block|{
name|prefixes
operator|=
name|prefixes
operator|+
literal|","
operator|+
name|baseWork
operator|.
name|getName
argument_list|()
expr_stmt|;
block|}
name|conf
operator|.
name|set
argument_list|(
name|DagUtils
operator|.
name|TEZ_MERGE_WORK_FILE_PREFIXES
argument_list|,
name|prefixes
argument_list|)
expr_stmt|;
block|}
comment|// nothing to return
return|return
literal|null
return|;
block|}
specifier|public
specifier|static
name|BaseWork
name|getMergeWork
parameter_list|(
name|Configuration
name|jconf
parameter_list|)
block|{
name|String
name|currentMergePrefix
init|=
name|jconf
operator|.
name|get
argument_list|(
name|DagUtils
operator|.
name|TEZ_MERGE_CURRENT_MERGE_FILE_PREFIX
argument_list|)
decl_stmt|;
if|if
condition|(
name|StringUtils
operator|.
name|isEmpty
argument_list|(
name|currentMergePrefix
argument_list|)
condition|)
block|{
return|return
literal|null
return|;
block|}
return|return
name|getMergeWork
argument_list|(
name|jconf
argument_list|,
name|jconf
operator|.
name|get
argument_list|(
name|DagUtils
operator|.
name|TEZ_MERGE_CURRENT_MERGE_FILE_PREFIX
argument_list|)
argument_list|)
return|;
block|}
specifier|public
specifier|static
name|BaseWork
name|getMergeWork
parameter_list|(
name|Configuration
name|jconf
parameter_list|,
name|String
name|prefix
parameter_list|)
block|{
if|if
condition|(
name|StringUtils
operator|.
name|isEmpty
argument_list|(
name|prefix
argument_list|)
condition|)
block|{
return|return
literal|null
return|;
block|}
return|return
name|getBaseWork
argument_list|(
name|jconf
argument_list|,
name|prefix
operator|+
name|MERGE_PLAN_NAME
argument_list|)
return|;
block|}
specifier|public
specifier|static
name|void
name|cacheBaseWork
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|String
name|name
parameter_list|,
name|BaseWork
name|work
parameter_list|,
name|Path
name|hiveScratchDir
parameter_list|)
block|{
try|try
block|{
name|setPlanPath
argument_list|(
name|conf
argument_list|,
name|hiveScratchDir
argument_list|)
expr_stmt|;
name|setBaseWork
argument_list|(
name|conf
argument_list|,
name|name
argument_list|,
name|work
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Failed to cache plan"
argument_list|,
name|e
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|RuntimeException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    * Pushes work into the global work map    */
specifier|public
specifier|static
name|void
name|setBaseWork
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|String
name|name
parameter_list|,
name|BaseWork
name|work
parameter_list|)
block|{
name|Path
name|path
init|=
name|getPlanPath
argument_list|(
name|conf
argument_list|,
name|name
argument_list|)
decl_stmt|;
name|setHasWork
argument_list|(
name|conf
argument_list|,
name|name
argument_list|)
expr_stmt|;
name|gWorkMap
operator|.
name|get
argument_list|(
name|conf
argument_list|)
operator|.
name|put
argument_list|(
name|path
argument_list|,
name|work
argument_list|)
expr_stmt|;
block|}
comment|/**    * Returns the Map or Reduce plan    * Side effect: the BaseWork returned is also placed in the gWorkMap    * @param conf    * @param name    * @return BaseWork based on the name supplied will return null if name is null    * @throws RuntimeException if the configuration files are not proper or if plan can not be loaded    */
specifier|private
specifier|static
name|BaseWork
name|getBaseWork
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|String
name|name
parameter_list|)
block|{
name|Path
name|path
init|=
literal|null
decl_stmt|;
name|InputStream
name|in
init|=
literal|null
decl_stmt|;
name|Kryo
name|kryo
init|=
name|SerializationUtilities
operator|.
name|borrowKryo
argument_list|()
decl_stmt|;
try|try
block|{
name|String
name|engine
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|conf
argument_list|,
name|ConfVars
operator|.
name|HIVE_EXECUTION_ENGINE
argument_list|)
decl_stmt|;
if|if
condition|(
name|engine
operator|.
name|equals
argument_list|(
literal|"spark"
argument_list|)
condition|)
block|{
comment|// TODO Add jar into current thread context classloader as it may be invoked by Spark driver inside
comment|// threads, should be unnecessary while SPARK-5377 is resolved.
name|String
name|addedJars
init|=
name|conf
operator|.
name|get
argument_list|(
name|HIVE_ADDED_JARS
argument_list|)
decl_stmt|;
if|if
condition|(
name|StringUtils
operator|.
name|isNotEmpty
argument_list|(
name|addedJars
argument_list|)
condition|)
block|{
name|ClassLoader
name|loader
init|=
name|Thread
operator|.
name|currentThread
argument_list|()
operator|.
name|getContextClassLoader
argument_list|()
decl_stmt|;
name|ClassLoader
name|newLoader
init|=
name|addToClassPath
argument_list|(
name|loader
argument_list|,
name|addedJars
operator|.
name|split
argument_list|(
literal|";"
argument_list|)
argument_list|)
decl_stmt|;
name|Thread
operator|.
name|currentThread
argument_list|()
operator|.
name|setContextClassLoader
argument_list|(
name|newLoader
argument_list|)
expr_stmt|;
name|kryo
operator|.
name|setClassLoader
argument_list|(
name|newLoader
argument_list|)
expr_stmt|;
block|}
block|}
name|path
operator|=
name|getPlanPath
argument_list|(
name|conf
argument_list|,
name|name
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"PLAN PATH = {}"
argument_list|,
name|path
argument_list|)
expr_stmt|;
if|if
condition|(
name|path
operator|==
literal|null
condition|)
block|{
comment|// Map/reduce plan may not be generated
return|return
literal|null
return|;
block|}
name|BaseWork
name|gWork
init|=
name|gWorkMap
operator|.
name|get
argument_list|(
name|conf
argument_list|)
operator|.
name|get
argument_list|(
name|path
argument_list|)
decl_stmt|;
if|if
condition|(
name|gWork
operator|==
literal|null
condition|)
block|{
name|Path
name|localPath
init|=
name|path
decl_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"local path = {}"
argument_list|,
name|localPath
argument_list|)
expr_stmt|;
specifier|final
name|long
name|serializedSize
decl_stmt|;
specifier|final
name|String
name|planMode
decl_stmt|;
if|if
condition|(
name|HiveConf
operator|.
name|getBoolVar
argument_list|(
name|conf
argument_list|,
name|ConfVars
operator|.
name|HIVE_RPC_QUERY_PLAN
argument_list|)
condition|)
block|{
name|String
name|planStringPath
init|=
name|path
operator|.
name|toUri
argument_list|()
operator|.
name|getPath
argument_list|()
decl_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Loading plan from string: {}"
argument_list|,
name|planStringPath
argument_list|)
expr_stmt|;
name|String
name|planString
init|=
name|conf
operator|.
name|getRaw
argument_list|(
name|planStringPath
argument_list|)
decl_stmt|;
if|if
condition|(
name|planString
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Could not find plan string in conf"
argument_list|)
expr_stmt|;
return|return
literal|null
return|;
block|}
name|serializedSize
operator|=
name|planString
operator|.
name|length
argument_list|()
expr_stmt|;
name|planMode
operator|=
literal|"RPC"
expr_stmt|;
name|byte
index|[]
name|planBytes
init|=
name|Base64
operator|.
name|decodeBase64
argument_list|(
name|planString
argument_list|)
decl_stmt|;
name|in
operator|=
operator|new
name|ByteArrayInputStream
argument_list|(
name|planBytes
argument_list|)
expr_stmt|;
name|in
operator|=
operator|new
name|InflaterInputStream
argument_list|(
name|in
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Open file to read in plan: {}"
argument_list|,
name|localPath
argument_list|)
expr_stmt|;
name|FileSystem
name|fs
init|=
name|localPath
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|in
operator|=
name|fs
operator|.
name|open
argument_list|(
name|localPath
argument_list|)
expr_stmt|;
name|serializedSize
operator|=
name|fs
operator|.
name|getFileStatus
argument_list|(
name|localPath
argument_list|)
operator|.
name|getLen
argument_list|()
expr_stmt|;
name|planMode
operator|=
literal|"FILE"
expr_stmt|;
block|}
if|if
condition|(
name|MAP_PLAN_NAME
operator|.
name|equals
argument_list|(
name|name
argument_list|)
condition|)
block|{
if|if
condition|(
name|ExecMapper
operator|.
name|class
operator|.
name|getName
argument_list|()
operator|.
name|equals
argument_list|(
name|conf
operator|.
name|get
argument_list|(
name|MAPRED_MAPPER_CLASS
argument_list|)
argument_list|)
condition|)
block|{
name|gWork
operator|=
name|SerializationUtilities
operator|.
name|deserializePlan
argument_list|(
name|kryo
argument_list|,
name|in
argument_list|,
name|MapWork
operator|.
name|class
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|MergeFileMapper
operator|.
name|class
operator|.
name|getName
argument_list|()
operator|.
name|equals
argument_list|(
name|conf
operator|.
name|get
argument_list|(
name|MAPRED_MAPPER_CLASS
argument_list|)
argument_list|)
condition|)
block|{
name|gWork
operator|=
name|SerializationUtilities
operator|.
name|deserializePlan
argument_list|(
name|kryo
argument_list|,
name|in
argument_list|,
name|MergeFileWork
operator|.
name|class
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|ColumnTruncateMapper
operator|.
name|class
operator|.
name|getName
argument_list|()
operator|.
name|equals
argument_list|(
name|conf
operator|.
name|get
argument_list|(
name|MAPRED_MAPPER_CLASS
argument_list|)
argument_list|)
condition|)
block|{
name|gWork
operator|=
name|SerializationUtilities
operator|.
name|deserializePlan
argument_list|(
name|kryo
argument_list|,
name|in
argument_list|,
name|ColumnTruncateWork
operator|.
name|class
argument_list|)
expr_stmt|;
block|}
else|else
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
literal|"unable to determine work from configuration ."
operator|+
name|MAPRED_MAPPER_CLASS
operator|+
literal|" was "
operator|+
name|conf
operator|.
name|get
argument_list|(
name|MAPRED_MAPPER_CLASS
argument_list|)
argument_list|)
throw|;
block|}
block|}
elseif|else
if|if
condition|(
name|REDUCE_PLAN_NAME
operator|.
name|equals
argument_list|(
name|name
argument_list|)
condition|)
block|{
if|if
condition|(
name|ExecReducer
operator|.
name|class
operator|.
name|getName
argument_list|()
operator|.
name|equals
argument_list|(
name|conf
operator|.
name|get
argument_list|(
name|MAPRED_REDUCER_CLASS
argument_list|)
argument_list|)
condition|)
block|{
name|gWork
operator|=
name|SerializationUtilities
operator|.
name|deserializePlan
argument_list|(
name|kryo
argument_list|,
name|in
argument_list|,
name|ReduceWork
operator|.
name|class
argument_list|)
expr_stmt|;
block|}
else|else
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
literal|"unable to determine work from configuration ."
operator|+
name|MAPRED_REDUCER_CLASS
operator|+
literal|" was "
operator|+
name|conf
operator|.
name|get
argument_list|(
name|MAPRED_REDUCER_CLASS
argument_list|)
argument_list|)
throw|;
block|}
block|}
elseif|else
if|if
condition|(
name|name
operator|.
name|contains
argument_list|(
name|MERGE_PLAN_NAME
argument_list|)
condition|)
block|{
if|if
condition|(
name|name
operator|.
name|startsWith
argument_list|(
name|MAPNAME
argument_list|)
condition|)
block|{
name|gWork
operator|=
name|SerializationUtilities
operator|.
name|deserializePlan
argument_list|(
name|kryo
argument_list|,
name|in
argument_list|,
name|MapWork
operator|.
name|class
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|name
operator|.
name|startsWith
argument_list|(
name|REDUCENAME
argument_list|)
condition|)
block|{
name|gWork
operator|=
name|SerializationUtilities
operator|.
name|deserializePlan
argument_list|(
name|kryo
argument_list|,
name|in
argument_list|,
name|ReduceWork
operator|.
name|class
argument_list|)
expr_stmt|;
block|}
else|else
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
literal|"Unknown work type: "
operator|+
name|name
argument_list|)
throw|;
block|}
block|}
name|LOG
operator|.
name|info
argument_list|(
literal|"Deserialized plan (via {}) - name: {} size: {}"
argument_list|,
name|planMode
argument_list|,
name|gWork
operator|.
name|getName
argument_list|()
argument_list|,
name|humanReadableByteCount
argument_list|(
name|serializedSize
argument_list|)
argument_list|)
expr_stmt|;
name|gWorkMap
operator|.
name|get
argument_list|(
name|conf
argument_list|)
operator|.
name|put
argument_list|(
name|path
argument_list|,
name|gWork
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Found plan in cache for name: {}"
argument_list|,
name|name
argument_list|)
expr_stmt|;
block|}
return|return
name|gWork
return|;
block|}
catch|catch
parameter_list|(
name|FileNotFoundException
name|fnf
parameter_list|)
block|{
comment|// happens. e.g.: no reduce work.
name|LOG
operator|.
name|debug
argument_list|(
literal|"No plan file found: {}"
argument_list|,
name|path
argument_list|,
name|fnf
argument_list|)
expr_stmt|;
return|return
literal|null
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|String
name|msg
init|=
literal|"Failed to load plan: "
operator|+
name|path
decl_stmt|;
name|LOG
operator|.
name|error
argument_list|(
name|msg
argument_list|,
name|e
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|RuntimeException
argument_list|(
name|msg
argument_list|,
name|e
argument_list|)
throw|;
block|}
finally|finally
block|{
name|SerializationUtilities
operator|.
name|releaseKryo
argument_list|(
name|kryo
argument_list|)
expr_stmt|;
name|IOUtils
operator|.
name|closeStream
argument_list|(
name|in
argument_list|)
expr_stmt|;
block|}
block|}
specifier|private
specifier|static
name|void
name|setHasWork
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|String
name|name
parameter_list|)
block|{
if|if
condition|(
name|MAP_PLAN_NAME
operator|.
name|equals
argument_list|(
name|name
argument_list|)
condition|)
block|{
name|conf
operator|.
name|setBoolean
argument_list|(
name|HAS_MAP_WORK
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|REDUCE_PLAN_NAME
operator|.
name|equals
argument_list|(
name|name
argument_list|)
condition|)
block|{
name|conf
operator|.
name|setBoolean
argument_list|(
name|HAS_REDUCE_WORK
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
block|}
specifier|public
specifier|static
name|void
name|setWorkflowAdjacencies
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|QueryPlan
name|plan
parameter_list|)
block|{
try|try
block|{
name|Graph
name|stageGraph
init|=
name|plan
operator|.
name|getQueryPlan
argument_list|()
operator|.
name|getStageGraph
argument_list|()
decl_stmt|;
if|if
condition|(
name|stageGraph
operator|==
literal|null
condition|)
block|{
return|return;
block|}
name|List
argument_list|<
name|Adjacency
argument_list|>
name|adjList
init|=
name|stageGraph
operator|.
name|getAdjacencyList
argument_list|()
decl_stmt|;
if|if
condition|(
name|adjList
operator|==
literal|null
condition|)
block|{
return|return;
block|}
for|for
control|(
name|Adjacency
name|adj
range|:
name|adjList
control|)
block|{
name|List
argument_list|<
name|String
argument_list|>
name|children
init|=
name|adj
operator|.
name|getChildren
argument_list|()
decl_stmt|;
if|if
condition|(
name|CollectionUtils
operator|.
name|isEmpty
argument_list|(
name|children
argument_list|)
condition|)
block|{
return|return;
block|}
name|conf
operator|.
name|setStrings
argument_list|(
literal|"mapreduce.workflow.adjacency."
operator|+
name|adj
operator|.
name|getNode
argument_list|()
argument_list|,
name|children
operator|.
name|toArray
argument_list|(
operator|new
name|String
index|[
literal|0
index|]
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{     }
block|}
specifier|public
specifier|static
name|List
argument_list|<
name|String
argument_list|>
name|getFieldSchemaString
parameter_list|(
name|List
argument_list|<
name|FieldSchema
argument_list|>
name|fl
parameter_list|)
block|{
if|if
condition|(
name|fl
operator|==
literal|null
condition|)
block|{
return|return
literal|null
return|;
block|}
name|ArrayList
argument_list|<
name|String
argument_list|>
name|ret
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|FieldSchema
name|f
range|:
name|fl
control|)
block|{
name|ret
operator|.
name|add
argument_list|(
name|f
operator|.
name|getName
argument_list|()
operator|+
literal|" "
operator|+
name|f
operator|.
name|getType
argument_list|()
operator|+
operator|(
name|f
operator|.
name|getComment
argument_list|()
operator|!=
literal|null
condition|?
operator|(
literal|" "
operator|+
name|f
operator|.
name|getComment
argument_list|()
operator|)
else|:
literal|""
operator|)
argument_list|)
expr_stmt|;
block|}
return|return
name|ret
return|;
block|}
specifier|public
specifier|static
name|void
name|setMapRedWork
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|MapredWork
name|w
parameter_list|,
name|Path
name|hiveScratchDir
parameter_list|)
block|{
name|String
name|useName
init|=
name|conf
operator|.
name|get
argument_list|(
name|INPUT_NAME
argument_list|)
decl_stmt|;
if|if
condition|(
name|useName
operator|==
literal|null
condition|)
block|{
name|useName
operator|=
literal|"mapreduce:"
operator|+
name|hiveScratchDir
expr_stmt|;
block|}
name|conf
operator|.
name|set
argument_list|(
name|INPUT_NAME
argument_list|,
name|useName
argument_list|)
expr_stmt|;
name|setMapWork
argument_list|(
name|conf
argument_list|,
name|w
operator|.
name|getMapWork
argument_list|()
argument_list|,
name|hiveScratchDir
argument_list|,
literal|true
argument_list|)
expr_stmt|;
if|if
condition|(
name|w
operator|.
name|getReduceWork
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|conf
operator|.
name|set
argument_list|(
name|INPUT_NAME
argument_list|,
name|useName
argument_list|)
expr_stmt|;
name|setReduceWork
argument_list|(
name|conf
argument_list|,
name|w
operator|.
name|getReduceWork
argument_list|()
argument_list|,
name|hiveScratchDir
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
block|}
specifier|public
specifier|static
name|Path
name|setMapWork
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|MapWork
name|w
parameter_list|,
name|Path
name|hiveScratchDir
parameter_list|,
name|boolean
name|useCache
parameter_list|)
block|{
return|return
name|setBaseWork
argument_list|(
name|conf
argument_list|,
name|w
argument_list|,
name|hiveScratchDir
argument_list|,
name|MAP_PLAN_NAME
argument_list|,
name|useCache
argument_list|)
return|;
block|}
specifier|public
specifier|static
name|Path
name|setReduceWork
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|ReduceWork
name|w
parameter_list|,
name|Path
name|hiveScratchDir
parameter_list|,
name|boolean
name|useCache
parameter_list|)
block|{
return|return
name|setBaseWork
argument_list|(
name|conf
argument_list|,
name|w
argument_list|,
name|hiveScratchDir
argument_list|,
name|REDUCE_PLAN_NAME
argument_list|,
name|useCache
argument_list|)
return|;
block|}
specifier|private
specifier|static
name|Path
name|setBaseWork
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|BaseWork
name|w
parameter_list|,
name|Path
name|hiveScratchDir
parameter_list|,
name|String
name|name
parameter_list|,
name|boolean
name|useCache
parameter_list|)
block|{
name|Kryo
name|kryo
init|=
name|SerializationUtilities
operator|.
name|borrowKryo
argument_list|()
decl_stmt|;
try|try
block|{
name|setPlanPath
argument_list|(
name|conf
argument_list|,
name|hiveScratchDir
argument_list|)
expr_stmt|;
name|Path
name|planPath
init|=
name|getPlanPath
argument_list|(
name|conf
argument_list|,
name|name
argument_list|)
decl_stmt|;
name|setHasWork
argument_list|(
name|conf
argument_list|,
name|name
argument_list|)
expr_stmt|;
name|OutputStream
name|out
init|=
literal|null
decl_stmt|;
specifier|final
name|long
name|serializedSize
decl_stmt|;
specifier|final
name|String
name|planMode
decl_stmt|;
if|if
condition|(
name|HiveConf
operator|.
name|getBoolVar
argument_list|(
name|conf
argument_list|,
name|ConfVars
operator|.
name|HIVE_RPC_QUERY_PLAN
argument_list|)
condition|)
block|{
comment|// add it to the conf
name|ByteArrayOutputStream
name|byteOut
init|=
operator|new
name|ByteArrayOutputStream
argument_list|()
decl_stmt|;
try|try
block|{
name|out
operator|=
operator|new
name|DeflaterOutputStream
argument_list|(
name|byteOut
argument_list|,
operator|new
name|Deflater
argument_list|(
name|Deflater
operator|.
name|BEST_SPEED
argument_list|)
argument_list|)
expr_stmt|;
name|SerializationUtilities
operator|.
name|serializePlan
argument_list|(
name|kryo
argument_list|,
name|w
argument_list|,
name|out
argument_list|)
expr_stmt|;
name|out
operator|.
name|close
argument_list|()
expr_stmt|;
name|out
operator|=
literal|null
expr_stmt|;
block|}
finally|finally
block|{
name|IOUtils
operator|.
name|closeStream
argument_list|(
name|out
argument_list|)
expr_stmt|;
block|}
specifier|final
name|String
name|serializedPlan
init|=
name|Base64
operator|.
name|encodeBase64String
argument_list|(
name|byteOut
operator|.
name|toByteArray
argument_list|()
argument_list|)
decl_stmt|;
name|serializedSize
operator|=
name|serializedPlan
operator|.
name|length
argument_list|()
expr_stmt|;
name|planMode
operator|=
literal|"RPC"
expr_stmt|;
name|conf
operator|.
name|set
argument_list|(
name|planPath
operator|.
name|toUri
argument_list|()
operator|.
name|getPath
argument_list|()
argument_list|,
name|serializedPlan
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// use the default file system of the conf
name|FileSystem
name|fs
init|=
name|planPath
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
try|try
block|{
name|out
operator|=
name|fs
operator|.
name|create
argument_list|(
name|planPath
argument_list|)
expr_stmt|;
name|SerializationUtilities
operator|.
name|serializePlan
argument_list|(
name|kryo
argument_list|,
name|w
argument_list|,
name|out
argument_list|)
expr_stmt|;
name|out
operator|.
name|close
argument_list|()
expr_stmt|;
name|out
operator|=
literal|null
expr_stmt|;
name|long
name|fileLen
init|=
name|fs
operator|.
name|getFileStatus
argument_list|(
name|planPath
argument_list|)
operator|.
name|getLen
argument_list|()
decl_stmt|;
name|serializedSize
operator|=
name|fileLen
expr_stmt|;
name|planMode
operator|=
literal|"FILE"
expr_stmt|;
block|}
finally|finally
block|{
name|IOUtils
operator|.
name|closeStream
argument_list|(
name|out
argument_list|)
expr_stmt|;
block|}
comment|// Serialize the plan to the default hdfs instance
comment|// Except for hadoop local mode execution where we should be
comment|// able to get the plan directly from the cache
if|if
condition|(
name|useCache
operator|&&
operator|!
name|ShimLoader
operator|.
name|getHadoopShims
argument_list|()
operator|.
name|isLocalMode
argument_list|(
name|conf
argument_list|)
condition|)
block|{
comment|// Set up distributed cache
if|if
condition|(
operator|!
name|DistributedCache
operator|.
name|getSymlink
argument_list|(
name|conf
argument_list|)
condition|)
block|{
name|DistributedCache
operator|.
name|createSymlink
argument_list|(
name|conf
argument_list|)
expr_stmt|;
block|}
name|String
name|uriWithLink
init|=
name|planPath
operator|.
name|toUri
argument_list|()
operator|.
name|toString
argument_list|()
operator|+
literal|"#"
operator|+
name|name
decl_stmt|;
name|DistributedCache
operator|.
name|addCacheFile
argument_list|(
operator|new
name|URI
argument_list|(
name|uriWithLink
argument_list|)
argument_list|,
name|conf
argument_list|)
expr_stmt|;
comment|// set replication of the plan file to a high number. we use the same
comment|// replication factor as used by the hadoop jobclient for job.xml etc.
name|short
name|replication
init|=
operator|(
name|short
operator|)
name|conf
operator|.
name|getInt
argument_list|(
literal|"mapred.submit.replication"
argument_list|,
literal|10
argument_list|)
decl_stmt|;
name|fs
operator|.
name|setReplication
argument_list|(
name|planPath
argument_list|,
name|replication
argument_list|)
expr_stmt|;
block|}
block|}
name|LOG
operator|.
name|info
argument_list|(
literal|"Serialized plan (via {}) - name: {} size: {}"
argument_list|,
name|planMode
argument_list|,
name|w
operator|.
name|getName
argument_list|()
argument_list|,
name|humanReadableByteCount
argument_list|(
name|serializedSize
argument_list|)
argument_list|)
expr_stmt|;
comment|// Cache the plan in this process
name|gWorkMap
operator|.
name|get
argument_list|(
name|conf
argument_list|)
operator|.
name|put
argument_list|(
name|planPath
argument_list|,
name|w
argument_list|)
expr_stmt|;
return|return
name|planPath
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|String
name|msg
init|=
literal|"Error caching "
operator|+
name|name
decl_stmt|;
name|LOG
operator|.
name|error
argument_list|(
name|msg
argument_list|,
name|e
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|RuntimeException
argument_list|(
name|msg
argument_list|,
name|e
argument_list|)
throw|;
block|}
finally|finally
block|{
name|SerializationUtilities
operator|.
name|releaseKryo
argument_list|(
name|kryo
argument_list|)
expr_stmt|;
block|}
block|}
specifier|private
specifier|static
name|Path
name|getPlanPath
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|String
name|name
parameter_list|)
block|{
name|Path
name|planPath
init|=
name|getPlanPath
argument_list|(
name|conf
argument_list|)
decl_stmt|;
if|if
condition|(
name|planPath
operator|==
literal|null
condition|)
block|{
return|return
literal|null
return|;
block|}
return|return
operator|new
name|Path
argument_list|(
name|planPath
argument_list|,
name|name
argument_list|)
return|;
block|}
specifier|private
specifier|static
name|void
name|setPlanPath
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|Path
name|hiveScratchDir
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|getPlanPath
argument_list|(
name|conf
argument_list|)
operator|==
literal|null
condition|)
block|{
comment|// this is the unique conf ID, which is kept in JobConf as part of the plan file name
name|String
name|jobID
init|=
name|UUID
operator|.
name|randomUUID
argument_list|()
operator|.
name|toString
argument_list|()
decl_stmt|;
name|Path
name|planPath
init|=
operator|new
name|Path
argument_list|(
name|hiveScratchDir
argument_list|,
name|jobID
argument_list|)
decl_stmt|;
name|FileSystem
name|fs
init|=
name|planPath
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|fs
operator|.
name|mkdirs
argument_list|(
name|planPath
argument_list|)
expr_stmt|;
name|HiveConf
operator|.
name|setVar
argument_list|(
name|conf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|PLAN
argument_list|,
name|planPath
operator|.
name|toUri
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
specifier|public
specifier|static
name|Path
name|getPlanPath
parameter_list|(
name|Configuration
name|conf
parameter_list|)
block|{
name|String
name|plan
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|conf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|PLAN
argument_list|)
decl_stmt|;
if|if
condition|(
name|plan
operator|!=
literal|null
operator|&&
operator|!
name|plan
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
return|return
operator|new
name|Path
argument_list|(
name|plan
argument_list|)
return|;
block|}
return|return
literal|null
return|;
block|}
specifier|public
specifier|static
class|class
name|CollectionPersistenceDelegate
extends|extends
name|DefaultPersistenceDelegate
block|{
annotation|@
name|Override
specifier|protected
name|Expression
name|instantiate
parameter_list|(
name|Object
name|oldInstance
parameter_list|,
name|Encoder
name|out
parameter_list|)
block|{
return|return
operator|new
name|Expression
argument_list|(
name|oldInstance
argument_list|,
name|oldInstance
operator|.
name|getClass
argument_list|()
argument_list|,
literal|"new"
argument_list|,
literal|null
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|protected
name|void
name|initialize
parameter_list|(
name|Class
argument_list|<
name|?
argument_list|>
name|type
parameter_list|,
name|Object
name|oldInstance
parameter_list|,
name|Object
name|newInstance
parameter_list|,
name|Encoder
name|out
parameter_list|)
block|{
name|Iterator
argument_list|<
name|?
argument_list|>
name|ite
init|=
operator|(
operator|(
name|Collection
argument_list|<
name|?
argument_list|>
operator|)
name|oldInstance
operator|)
operator|.
name|iterator
argument_list|()
decl_stmt|;
while|while
condition|(
name|ite
operator|.
name|hasNext
argument_list|()
condition|)
block|{
name|out
operator|.
name|writeStatement
argument_list|(
operator|new
name|Statement
argument_list|(
name|oldInstance
argument_list|,
literal|"add"
argument_list|,
operator|new
name|Object
index|[]
block|{
name|ite
operator|.
name|next
argument_list|()
block|}
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
block|}
annotation|@
name|VisibleForTesting
specifier|public
specifier|static
name|TableDesc
name|defaultTd
decl_stmt|;
static|static
block|{
comment|// by default we expect ^A separated strings
comment|// This tableDesc does not provide column names. We should always use
comment|// PlanUtils.getDefaultTableDesc(String separatorCode, String columns)
comment|// or getBinarySortableTableDesc(List<FieldSchema> fieldSchemas) when
comment|// we know the column names.
comment|/**      * Generate the table descriptor of MetadataTypedColumnsetSerDe with the      * separatorCode. MetaDataTypedColumnsetSerDe is used because LazySimpleSerDe      * does not support a table with a single column "col" with type      * "array<string>".      */
name|defaultTd
operator|=
operator|new
name|TableDesc
argument_list|(
name|TextInputFormat
operator|.
name|class
argument_list|,
name|IgnoreKeyTextOutputFormat
operator|.
name|class
argument_list|,
name|Utilities
operator|.
name|makeProperties
argument_list|(
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde
operator|.
name|serdeConstants
operator|.
name|SERIALIZATION_FORMAT
argument_list|,
literal|""
operator|+
name|Utilities
operator|.
name|ctrlaCode
argument_list|,
name|serdeConstants
operator|.
name|SERIALIZATION_LIB
argument_list|,
name|MetadataTypedColumnsetSerDe
operator|.
name|class
operator|.
name|getName
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
specifier|public
specifier|static
specifier|final
name|int
name|carriageReturnCode
init|=
literal|13
decl_stmt|;
specifier|public
specifier|static
specifier|final
name|int
name|newLineCode
init|=
literal|10
decl_stmt|;
specifier|public
specifier|static
specifier|final
name|int
name|tabCode
init|=
literal|9
decl_stmt|;
specifier|public
specifier|static
specifier|final
name|int
name|ctrlaCode
init|=
literal|1
decl_stmt|;
specifier|public
specifier|static
specifier|final
name|String
name|INDENT
init|=
literal|"  "
decl_stmt|;
comment|// Note: When DDL supports specifying what string to represent null,
comment|// we should specify "NULL" to represent null in the temp table, and then
comment|// we can make the following translation deprecated.
specifier|public
specifier|static
specifier|final
name|String
name|nullStringStorage
init|=
literal|"\\N"
decl_stmt|;
specifier|public
specifier|static
specifier|final
name|String
name|nullStringOutput
init|=
literal|"NULL"
decl_stmt|;
comment|/**    * Gets the task id if we are running as a Hadoop job. Gets a random number otherwise.    */
specifier|public
specifier|static
name|String
name|getTaskId
parameter_list|(
name|Configuration
name|hconf
parameter_list|)
block|{
name|String
name|taskid
init|=
operator|(
name|hconf
operator|==
literal|null
operator|)
condition|?
literal|null
else|:
name|hconf
operator|.
name|get
argument_list|(
literal|"mapred.task.id"
argument_list|)
decl_stmt|;
if|if
condition|(
name|StringUtils
operator|.
name|isEmpty
argument_list|(
name|taskid
argument_list|)
condition|)
block|{
return|return
operator|(
name|Integer
operator|.
name|toString
argument_list|(
name|randGen
operator|.
name|nextInt
argument_list|(
name|Integer
operator|.
name|MAX_VALUE
argument_list|)
argument_list|)
operator|)
return|;
block|}
else|else
block|{
comment|/*        * extract the task and attempt id from the hadoop taskid. in version 17 the leading component        * was 'task_'. thereafter the leading component is 'attempt_'. in 17 - hadoop also seems to        * have used _map_ and _reduce_ to denote map/reduce task types        */
name|String
name|ret
init|=
name|taskid
operator|.
name|replaceAll
argument_list|(
literal|".*_[mr]_"
argument_list|,
literal|""
argument_list|)
operator|.
name|replaceAll
argument_list|(
literal|".*_(map|reduce)_"
argument_list|,
literal|""
argument_list|)
decl_stmt|;
return|return
operator|(
name|ret
operator|)
return|;
block|}
block|}
specifier|public
specifier|static
name|HashMap
name|makeMap
parameter_list|(
name|Object
modifier|...
name|olist
parameter_list|)
block|{
name|HashMap
name|ret
init|=
operator|new
name|HashMap
argument_list|()
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|olist
operator|.
name|length
condition|;
name|i
operator|+=
literal|2
control|)
block|{
name|ret
operator|.
name|put
argument_list|(
name|olist
index|[
name|i
index|]
argument_list|,
name|olist
index|[
name|i
operator|+
literal|1
index|]
argument_list|)
expr_stmt|;
block|}
return|return
operator|(
name|ret
operator|)
return|;
block|}
specifier|public
specifier|static
name|Properties
name|makeProperties
parameter_list|(
name|String
modifier|...
name|olist
parameter_list|)
block|{
name|Properties
name|ret
init|=
operator|new
name|Properties
argument_list|()
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|olist
operator|.
name|length
condition|;
name|i
operator|+=
literal|2
control|)
block|{
name|ret
operator|.
name|setProperty
argument_list|(
name|olist
index|[
name|i
index|]
argument_list|,
name|olist
index|[
name|i
operator|+
literal|1
index|]
argument_list|)
expr_stmt|;
block|}
return|return
operator|(
name|ret
operator|)
return|;
block|}
specifier|public
specifier|static
name|ArrayList
name|makeList
parameter_list|(
name|Object
modifier|...
name|olist
parameter_list|)
block|{
name|ArrayList
name|ret
init|=
operator|new
name|ArrayList
argument_list|()
decl_stmt|;
for|for
control|(
name|Object
name|element
range|:
name|olist
control|)
block|{
name|ret
operator|.
name|add
argument_list|(
name|element
argument_list|)
expr_stmt|;
block|}
return|return
operator|(
name|ret
operator|)
return|;
block|}
specifier|public
specifier|static
name|TableDesc
name|getTableDesc
parameter_list|(
name|Table
name|tbl
parameter_list|)
block|{
name|Properties
name|props
init|=
name|tbl
operator|.
name|getMetadata
argument_list|()
decl_stmt|;
name|props
operator|.
name|put
argument_list|(
name|serdeConstants
operator|.
name|SERIALIZATION_LIB
argument_list|,
name|tbl
operator|.
name|getDeserializer
argument_list|()
operator|.
name|getClass
argument_list|()
operator|.
name|getName
argument_list|()
argument_list|)
expr_stmt|;
return|return
operator|(
operator|new
name|TableDesc
argument_list|(
name|tbl
operator|.
name|getInputFormatClass
argument_list|()
argument_list|,
name|tbl
operator|.
name|getOutputFormatClass
argument_list|()
argument_list|,
name|props
argument_list|)
operator|)
return|;
block|}
comment|// column names and column types are all delimited by comma
specifier|public
specifier|static
name|TableDesc
name|getTableDesc
parameter_list|(
name|String
name|cols
parameter_list|,
name|String
name|colTypes
parameter_list|)
block|{
return|return
operator|(
operator|new
name|TableDesc
argument_list|(
name|SequenceFileInputFormat
operator|.
name|class
argument_list|,
name|HiveSequenceFileOutputFormat
operator|.
name|class
argument_list|,
name|Utilities
operator|.
name|makeProperties
argument_list|(
name|serdeConstants
operator|.
name|SERIALIZATION_FORMAT
argument_list|,
literal|""
operator|+
name|Utilities
operator|.
name|ctrlaCode
argument_list|,
name|serdeConstants
operator|.
name|LIST_COLUMNS
argument_list|,
name|cols
argument_list|,
name|serdeConstants
operator|.
name|LIST_COLUMN_TYPES
argument_list|,
name|colTypes
argument_list|,
name|serdeConstants
operator|.
name|SERIALIZATION_LIB
argument_list|,
name|LazySimpleSerDe
operator|.
name|class
operator|.
name|getName
argument_list|()
argument_list|)
argument_list|)
operator|)
return|;
block|}
specifier|public
specifier|static
name|PartitionDesc
name|getPartitionDesc
parameter_list|(
name|Partition
name|part
parameter_list|,
name|TableDesc
name|tableDesc
parameter_list|)
throws|throws
name|HiveException
block|{
return|return
operator|new
name|PartitionDesc
argument_list|(
name|part
argument_list|,
name|tableDesc
argument_list|)
return|;
block|}
specifier|public
specifier|static
name|PartitionDesc
name|getPartitionDesc
parameter_list|(
name|Partition
name|part
parameter_list|)
throws|throws
name|HiveException
block|{
return|return
operator|new
name|PartitionDesc
argument_list|(
name|part
argument_list|)
return|;
block|}
specifier|public
specifier|static
name|PartitionDesc
name|getPartitionDescFromTableDesc
parameter_list|(
name|TableDesc
name|tblDesc
parameter_list|,
name|Partition
name|part
parameter_list|,
name|boolean
name|usePartSchemaProperties
parameter_list|)
throws|throws
name|HiveException
block|{
return|return
operator|new
name|PartitionDesc
argument_list|(
name|part
argument_list|,
name|tblDesc
argument_list|,
name|usePartSchemaProperties
argument_list|)
return|;
block|}
specifier|private
specifier|static
name|String
name|getOpTreeSkel_helper
parameter_list|(
name|Operator
argument_list|<
name|?
argument_list|>
name|op
parameter_list|,
name|String
name|indent
parameter_list|)
block|{
if|if
condition|(
name|op
operator|==
literal|null
condition|)
block|{
return|return
name|StringUtils
operator|.
name|EMPTY
return|;
block|}
name|StringBuilder
name|sb
init|=
operator|new
name|StringBuilder
argument_list|()
decl_stmt|;
name|sb
operator|.
name|append
argument_list|(
name|indent
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
name|op
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
literal|"\n"
argument_list|)
expr_stmt|;
if|if
condition|(
name|op
operator|.
name|getChildOperators
argument_list|()
operator|!=
literal|null
condition|)
block|{
for|for
control|(
name|Object
name|child
range|:
name|op
operator|.
name|getChildOperators
argument_list|()
control|)
block|{
name|sb
operator|.
name|append
argument_list|(
name|getOpTreeSkel_helper
argument_list|(
operator|(
name|Operator
argument_list|<
name|?
argument_list|>
operator|)
name|child
argument_list|,
name|indent
operator|+
literal|"  "
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|sb
operator|.
name|toString
argument_list|()
return|;
block|}
specifier|public
specifier|static
name|String
name|getOpTreeSkel
parameter_list|(
name|Operator
argument_list|<
name|?
argument_list|>
name|op
parameter_list|)
block|{
return|return
name|getOpTreeSkel_helper
argument_list|(
name|op
argument_list|,
name|StringUtils
operator|.
name|EMPTY
argument_list|)
return|;
block|}
specifier|private
specifier|static
name|boolean
name|isWhitespace
parameter_list|(
name|int
name|c
parameter_list|)
block|{
if|if
condition|(
name|c
operator|==
operator|-
literal|1
condition|)
block|{
return|return
literal|false
return|;
block|}
return|return
name|Character
operator|.
name|isWhitespace
argument_list|(
operator|(
name|char
operator|)
name|c
argument_list|)
return|;
block|}
specifier|public
specifier|static
name|boolean
name|contentsEqual
parameter_list|(
name|InputStream
name|is1
parameter_list|,
name|InputStream
name|is2
parameter_list|,
name|boolean
name|ignoreWhitespace
parameter_list|)
throws|throws
name|IOException
block|{
try|try
block|{
if|if
condition|(
operator|(
name|is1
operator|==
name|is2
operator|)
operator|||
operator|(
name|is1
operator|==
literal|null
operator|&&
name|is2
operator|==
literal|null
operator|)
condition|)
block|{
return|return
literal|true
return|;
block|}
if|if
condition|(
name|is1
operator|==
literal|null
operator|||
name|is2
operator|==
literal|null
condition|)
block|{
return|return
literal|false
return|;
block|}
while|while
condition|(
literal|true
condition|)
block|{
name|int
name|c1
init|=
name|is1
operator|.
name|read
argument_list|()
decl_stmt|;
while|while
condition|(
name|ignoreWhitespace
operator|&&
name|isWhitespace
argument_list|(
name|c1
argument_list|)
condition|)
block|{
name|c1
operator|=
name|is1
operator|.
name|read
argument_list|()
expr_stmt|;
block|}
name|int
name|c2
init|=
name|is2
operator|.
name|read
argument_list|()
decl_stmt|;
while|while
condition|(
name|ignoreWhitespace
operator|&&
name|isWhitespace
argument_list|(
name|c2
argument_list|)
condition|)
block|{
name|c2
operator|=
name|is2
operator|.
name|read
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
name|c1
operator|==
operator|-
literal|1
operator|&&
name|c2
operator|==
operator|-
literal|1
condition|)
block|{
return|return
literal|true
return|;
block|}
if|if
condition|(
name|c1
operator|!=
name|c2
condition|)
block|{
break|break;
block|}
block|}
block|}
catch|catch
parameter_list|(
name|FileNotFoundException
name|e
parameter_list|)
block|{
name|e
operator|.
name|printStackTrace
argument_list|()
expr_stmt|;
block|}
return|return
literal|false
return|;
block|}
comment|/**    * convert "From src insert blah blah" to "From src insert ... blah"    */
specifier|public
specifier|static
name|String
name|abbreviate
parameter_list|(
name|String
name|str
parameter_list|,
name|int
name|max
parameter_list|)
block|{
name|str
operator|=
name|str
operator|.
name|trim
argument_list|()
expr_stmt|;
name|int
name|len
init|=
name|str
operator|.
name|length
argument_list|()
decl_stmt|;
name|int
name|suffixlength
init|=
literal|20
decl_stmt|;
if|if
condition|(
name|len
operator|<=
name|max
condition|)
block|{
return|return
name|str
return|;
block|}
name|suffixlength
operator|=
name|Math
operator|.
name|min
argument_list|(
name|suffixlength
argument_list|,
operator|(
name|max
operator|-
literal|3
operator|)
operator|/
literal|2
argument_list|)
expr_stmt|;
name|String
name|rev
init|=
name|StringUtils
operator|.
name|reverse
argument_list|(
name|str
argument_list|)
decl_stmt|;
comment|// get the last few words
name|String
name|suffix
init|=
name|WordUtils
operator|.
name|abbreviate
argument_list|(
name|rev
argument_list|,
literal|0
argument_list|,
name|suffixlength
argument_list|,
name|StringUtils
operator|.
name|EMPTY
argument_list|)
decl_stmt|;
name|suffix
operator|=
name|StringUtils
operator|.
name|reverse
argument_list|(
name|suffix
argument_list|)
expr_stmt|;
comment|// first few ..
name|String
name|prefix
init|=
name|StringUtils
operator|.
name|abbreviate
argument_list|(
name|str
argument_list|,
name|max
operator|-
name|suffix
operator|.
name|length
argument_list|()
argument_list|)
decl_stmt|;
return|return
name|prefix
operator|+
name|suffix
return|;
block|}
specifier|public
specifier|static
specifier|final
name|String
name|NSTR
init|=
literal|""
decl_stmt|;
comment|/**    * StreamStatus.    *    */
specifier|public
specifier|static
enum|enum
name|StreamStatus
block|{
name|EOF
block|,
name|TERMINATED
block|}
specifier|public
specifier|static
name|StreamStatus
name|readColumn
parameter_list|(
name|DataInput
name|in
parameter_list|,
name|OutputStream
name|out
parameter_list|)
throws|throws
name|IOException
block|{
while|while
condition|(
literal|true
condition|)
block|{
name|int
name|b
decl_stmt|;
try|try
block|{
name|b
operator|=
name|in
operator|.
name|readByte
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|EOFException
name|e
parameter_list|)
block|{
return|return
name|StreamStatus
operator|.
name|EOF
return|;
block|}
if|if
condition|(
name|b
operator|==
name|Utilities
operator|.
name|newLineCode
condition|)
block|{
return|return
name|StreamStatus
operator|.
name|TERMINATED
return|;
block|}
name|out
operator|.
name|write
argument_list|(
name|b
argument_list|)
expr_stmt|;
block|}
comment|// Unreachable
block|}
comment|/**    * Convert an output stream to a compressed output stream based on codecs and compression options    * specified in the Job Configuration.    *    * @param jc    *          Job Configuration    * @param out    *          Output Stream to be converted into compressed output stream    * @return compressed output stream    */
specifier|public
specifier|static
name|OutputStream
name|createCompressedStream
parameter_list|(
name|JobConf
name|jc
parameter_list|,
name|OutputStream
name|out
parameter_list|)
throws|throws
name|IOException
block|{
name|boolean
name|isCompressed
init|=
name|FileOutputFormat
operator|.
name|getCompressOutput
argument_list|(
name|jc
argument_list|)
decl_stmt|;
return|return
name|createCompressedStream
argument_list|(
name|jc
argument_list|,
name|out
argument_list|,
name|isCompressed
argument_list|)
return|;
block|}
comment|/**    * Convert an output stream to a compressed output stream based on codecs codecs in the Job    * Configuration. Caller specifies directly whether file is compressed or not    *    * @param jc    *          Job Configuration    * @param out    *          Output Stream to be converted into compressed output stream    * @param isCompressed    *          whether the output stream needs to be compressed or not    * @return compressed output stream    */
specifier|public
specifier|static
name|OutputStream
name|createCompressedStream
parameter_list|(
name|JobConf
name|jc
parameter_list|,
name|OutputStream
name|out
parameter_list|,
name|boolean
name|isCompressed
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|isCompressed
condition|)
block|{
name|Class
argument_list|<
name|?
extends|extends
name|CompressionCodec
argument_list|>
name|codecClass
init|=
name|FileOutputFormat
operator|.
name|getOutputCompressorClass
argument_list|(
name|jc
argument_list|,
name|DefaultCodec
operator|.
name|class
argument_list|)
decl_stmt|;
name|CompressionCodec
name|codec
init|=
name|ReflectionUtil
operator|.
name|newInstance
argument_list|(
name|codecClass
argument_list|,
name|jc
argument_list|)
decl_stmt|;
return|return
name|codec
operator|.
name|createOutputStream
argument_list|(
name|out
argument_list|)
return|;
block|}
else|else
block|{
return|return
operator|(
name|out
operator|)
return|;
block|}
block|}
comment|/**    * Based on compression option and configured output codec - get extension for output file. This    * is only required for text files - not sequencefiles    *    * @param jc    *          Job Configuration    * @param isCompressed    *          Whether the output file is compressed or not    * @return the required file extension (example: .gz)    * @deprecated Use {@link #getFileExtension(JobConf, boolean, HiveOutputFormat)}    */
annotation|@
name|Deprecated
specifier|public
specifier|static
name|String
name|getFileExtension
parameter_list|(
name|JobConf
name|jc
parameter_list|,
name|boolean
name|isCompressed
parameter_list|)
block|{
return|return
name|getFileExtension
argument_list|(
name|jc
argument_list|,
name|isCompressed
argument_list|,
operator|new
name|HiveIgnoreKeyTextOutputFormat
argument_list|()
argument_list|)
return|;
block|}
comment|/**    * Based on compression option, output format, and configured output codec -    * get extension for output file. Text files require an extension, whereas    * others, like sequence files, do not.    *<p>    * The property<code>hive.output.file.extension</code> is used to determine    * the extension - if set, it will override other logic for choosing an    * extension.    *    * @param jc    *          Job Configuration    * @param isCompressed    *          Whether the output file is compressed or not    * @param hiveOutputFormat    *          The output format, used to detect if the format is text    * @return the required file extension (example: .gz)    */
specifier|public
specifier|static
name|String
name|getFileExtension
parameter_list|(
name|JobConf
name|jc
parameter_list|,
name|boolean
name|isCompressed
parameter_list|,
name|HiveOutputFormat
argument_list|<
name|?
argument_list|,
name|?
argument_list|>
name|hiveOutputFormat
parameter_list|)
block|{
name|String
name|extension
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|jc
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|OUTPUT_FILE_EXTENSION
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|StringUtils
operator|.
name|isEmpty
argument_list|(
name|extension
argument_list|)
condition|)
block|{
return|return
name|extension
return|;
block|}
if|if
condition|(
operator|(
name|hiveOutputFormat
operator|instanceof
name|HiveIgnoreKeyTextOutputFormat
operator|)
operator|&&
name|isCompressed
condition|)
block|{
name|Class
argument_list|<
name|?
extends|extends
name|CompressionCodec
argument_list|>
name|codecClass
init|=
name|FileOutputFormat
operator|.
name|getOutputCompressorClass
argument_list|(
name|jc
argument_list|,
name|DefaultCodec
operator|.
name|class
argument_list|)
decl_stmt|;
name|CompressionCodec
name|codec
init|=
name|ReflectionUtil
operator|.
name|newInstance
argument_list|(
name|codecClass
argument_list|,
name|jc
argument_list|)
decl_stmt|;
return|return
name|codec
operator|.
name|getDefaultExtension
argument_list|()
return|;
block|}
return|return
name|StringUtils
operator|.
name|EMPTY
return|;
block|}
comment|/**    * Create a sequencefile output stream based on job configuration.    *    * @param jc    *          Job configuration    * @param fs    *          File System to create file in    * @param file    *          Path to be created    * @param keyClass    *          Java Class for key    * @param valClass    *          Java Class for value    * @return output stream over the created sequencefile    */
specifier|public
specifier|static
name|SequenceFile
operator|.
name|Writer
name|createSequenceWriter
parameter_list|(
name|JobConf
name|jc
parameter_list|,
name|FileSystem
name|fs
parameter_list|,
name|Path
name|file
parameter_list|,
name|Class
argument_list|<
name|?
argument_list|>
name|keyClass
parameter_list|,
name|Class
argument_list|<
name|?
argument_list|>
name|valClass
parameter_list|,
name|Progressable
name|progressable
parameter_list|)
throws|throws
name|IOException
block|{
name|boolean
name|isCompressed
init|=
name|FileOutputFormat
operator|.
name|getCompressOutput
argument_list|(
name|jc
argument_list|)
decl_stmt|;
return|return
name|createSequenceWriter
argument_list|(
name|jc
argument_list|,
name|fs
argument_list|,
name|file
argument_list|,
name|keyClass
argument_list|,
name|valClass
argument_list|,
name|isCompressed
argument_list|,
name|progressable
argument_list|)
return|;
block|}
comment|/**    * Create a sequencefile output stream based on job configuration Uses user supplied compression    * flag (rather than obtaining it from the Job Configuration).    *    * @param jc    *          Job configuration    * @param fs    *          File System to create file in    * @param file    *          Path to be created    * @param keyClass    *          Java Class for key    * @param valClass    *          Java Class for value    * @return output stream over the created sequencefile    */
specifier|public
specifier|static
name|SequenceFile
operator|.
name|Writer
name|createSequenceWriter
parameter_list|(
name|JobConf
name|jc
parameter_list|,
name|FileSystem
name|fs
parameter_list|,
name|Path
name|file
parameter_list|,
name|Class
argument_list|<
name|?
argument_list|>
name|keyClass
parameter_list|,
name|Class
argument_list|<
name|?
argument_list|>
name|valClass
parameter_list|,
name|boolean
name|isCompressed
parameter_list|,
name|Progressable
name|progressable
parameter_list|)
throws|throws
name|IOException
block|{
name|CompressionCodec
name|codec
init|=
literal|null
decl_stmt|;
name|CompressionType
name|compressionType
init|=
name|CompressionType
operator|.
name|NONE
decl_stmt|;
name|Class
name|codecClass
init|=
literal|null
decl_stmt|;
if|if
condition|(
name|isCompressed
condition|)
block|{
name|compressionType
operator|=
name|SequenceFileOutputFormat
operator|.
name|getOutputCompressionType
argument_list|(
name|jc
argument_list|)
expr_stmt|;
name|codecClass
operator|=
name|FileOutputFormat
operator|.
name|getOutputCompressorClass
argument_list|(
name|jc
argument_list|,
name|DefaultCodec
operator|.
name|class
argument_list|)
expr_stmt|;
name|codec
operator|=
operator|(
name|CompressionCodec
operator|)
name|ReflectionUtil
operator|.
name|newInstance
argument_list|(
name|codecClass
argument_list|,
name|jc
argument_list|)
expr_stmt|;
block|}
return|return
name|SequenceFile
operator|.
name|createWriter
argument_list|(
name|fs
argument_list|,
name|jc
argument_list|,
name|file
argument_list|,
name|keyClass
argument_list|,
name|valClass
argument_list|,
name|compressionType
argument_list|,
name|codec
argument_list|,
name|progressable
argument_list|)
return|;
block|}
comment|/**    * Create a RCFile output stream based on job configuration Uses user supplied compression flag    * (rather than obtaining it from the Job Configuration).    *    * @param jc    *          Job configuration    * @param fs    *          File System to create file in    * @param file    *          Path to be created    * @return output stream over the created rcfile    */
specifier|public
specifier|static
name|RCFile
operator|.
name|Writer
name|createRCFileWriter
parameter_list|(
name|JobConf
name|jc
parameter_list|,
name|FileSystem
name|fs
parameter_list|,
name|Path
name|file
parameter_list|,
name|boolean
name|isCompressed
parameter_list|,
name|Progressable
name|progressable
parameter_list|)
throws|throws
name|IOException
block|{
name|CompressionCodec
name|codec
init|=
literal|null
decl_stmt|;
if|if
condition|(
name|isCompressed
condition|)
block|{
name|Class
argument_list|<
name|?
argument_list|>
name|codecClass
init|=
name|FileOutputFormat
operator|.
name|getOutputCompressorClass
argument_list|(
name|jc
argument_list|,
name|DefaultCodec
operator|.
name|class
argument_list|)
decl_stmt|;
name|codec
operator|=
operator|(
name|CompressionCodec
operator|)
name|ReflectionUtil
operator|.
name|newInstance
argument_list|(
name|codecClass
argument_list|,
name|jc
argument_list|)
expr_stmt|;
block|}
return|return
operator|new
name|RCFile
operator|.
name|Writer
argument_list|(
name|fs
argument_list|,
name|jc
argument_list|,
name|file
argument_list|,
name|progressable
argument_list|,
name|codec
argument_list|)
return|;
block|}
comment|/**    * Shamelessly cloned from GenericOptionsParser.    */
specifier|public
specifier|static
name|String
name|realFile
parameter_list|(
name|String
name|newFile
parameter_list|,
name|Configuration
name|conf
parameter_list|)
throws|throws
name|IOException
block|{
name|Path
name|path
init|=
operator|new
name|Path
argument_list|(
name|newFile
argument_list|)
decl_stmt|;
name|URI
name|pathURI
init|=
name|path
operator|.
name|toUri
argument_list|()
decl_stmt|;
name|FileSystem
name|fs
decl_stmt|;
if|if
condition|(
name|pathURI
operator|.
name|getScheme
argument_list|()
operator|==
literal|null
condition|)
block|{
name|fs
operator|=
name|FileSystem
operator|.
name|getLocal
argument_list|(
name|conf
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|fs
operator|=
name|path
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
operator|!
name|fs
operator|.
name|exists
argument_list|(
name|path
argument_list|)
condition|)
block|{
return|return
literal|null
return|;
block|}
name|String
name|file
init|=
name|path
operator|.
name|makeQualified
argument_list|(
name|fs
argument_list|)
operator|.
name|toString
argument_list|()
decl_stmt|;
return|return
name|file
return|;
block|}
specifier|public
specifier|static
name|List
argument_list|<
name|String
argument_list|>
name|mergeUniqElems
parameter_list|(
name|List
argument_list|<
name|String
argument_list|>
name|src
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|dest
parameter_list|)
block|{
if|if
condition|(
name|dest
operator|==
literal|null
condition|)
block|{
return|return
name|src
return|;
block|}
if|if
condition|(
name|src
operator|==
literal|null
condition|)
block|{
return|return
name|dest
return|;
block|}
name|int
name|pos
init|=
literal|0
decl_stmt|;
while|while
condition|(
name|pos
operator|<
name|dest
operator|.
name|size
argument_list|()
condition|)
block|{
if|if
condition|(
operator|!
name|src
operator|.
name|contains
argument_list|(
name|dest
operator|.
name|get
argument_list|(
name|pos
argument_list|)
argument_list|)
condition|)
block|{
name|src
operator|.
name|add
argument_list|(
name|dest
operator|.
name|get
argument_list|(
name|pos
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|pos
operator|++
expr_stmt|;
block|}
return|return
name|src
return|;
block|}
specifier|private
specifier|static
specifier|final
name|String
name|tmpPrefix
init|=
literal|"_tmp."
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|String
name|taskTmpPrefix
init|=
literal|"_task_tmp."
decl_stmt|;
specifier|public
specifier|static
name|Path
name|toTaskTempPath
parameter_list|(
name|Path
name|orig
parameter_list|)
block|{
if|if
condition|(
name|orig
operator|.
name|getName
argument_list|()
operator|.
name|indexOf
argument_list|(
name|taskTmpPrefix
argument_list|)
operator|==
literal|0
condition|)
block|{
return|return
name|orig
return|;
block|}
return|return
operator|new
name|Path
argument_list|(
name|orig
operator|.
name|getParent
argument_list|()
argument_list|,
name|taskTmpPrefix
operator|+
name|orig
operator|.
name|getName
argument_list|()
argument_list|)
return|;
block|}
specifier|public
specifier|static
name|Path
name|toTempPath
parameter_list|(
name|Path
name|orig
parameter_list|)
block|{
if|if
condition|(
name|orig
operator|.
name|getName
argument_list|()
operator|.
name|indexOf
argument_list|(
name|tmpPrefix
argument_list|)
operator|==
literal|0
condition|)
block|{
return|return
name|orig
return|;
block|}
return|return
operator|new
name|Path
argument_list|(
name|orig
operator|.
name|getParent
argument_list|()
argument_list|,
name|tmpPrefix
operator|+
name|orig
operator|.
name|getName
argument_list|()
argument_list|)
return|;
block|}
comment|/**    * Given a path, convert to a temporary path.    */
specifier|public
specifier|static
name|Path
name|toTempPath
parameter_list|(
name|String
name|orig
parameter_list|)
block|{
return|return
name|toTempPath
argument_list|(
operator|new
name|Path
argument_list|(
name|orig
argument_list|)
argument_list|)
return|;
block|}
comment|/**    * Detect if the supplied file is a temporary path.    */
specifier|public
specifier|static
name|boolean
name|isTempPath
parameter_list|(
name|FileStatus
name|file
parameter_list|)
block|{
name|String
name|name
init|=
name|file
operator|.
name|getPath
argument_list|()
operator|.
name|getName
argument_list|()
decl_stmt|;
comment|// in addition to detecting hive temporary files, we also check hadoop
comment|// temporary folders that used to show up in older releases
return|return
operator|(
name|name
operator|.
name|startsWith
argument_list|(
literal|"_task"
argument_list|)
operator|||
name|name
operator|.
name|startsWith
argument_list|(
name|tmpPrefix
argument_list|)
operator|)
return|;
block|}
comment|/**    * Rename src to dst, or in the case dst already exists, move files in src to dst. If there is an    * existing file with the same name, the new file's name will be appended with "_1", "_2", etc.    *    * @param fs    *          the FileSystem where src and dst are on.    * @param src    *          the src directory    * @param dst    *          the target directory    * @throws IOException    */
specifier|public
specifier|static
name|void
name|rename
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|Path
name|src
parameter_list|,
name|Path
name|dst
parameter_list|)
throws|throws
name|IOException
throws|,
name|HiveException
block|{
if|if
condition|(
operator|!
name|fs
operator|.
name|rename
argument_list|(
name|src
argument_list|,
name|dst
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Unable to move: "
operator|+
name|src
operator|+
literal|" to: "
operator|+
name|dst
argument_list|)
throw|;
block|}
block|}
comment|/**    * Moves files from src to dst if it is within the specified set of paths    * @param fs    * @param src    * @param dst    * @param filesToMove    * @throws IOException    * @throws HiveException    */
specifier|private
specifier|static
name|void
name|moveSpecifiedFiles
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|Path
name|src
parameter_list|,
name|Path
name|dst
parameter_list|,
name|Set
argument_list|<
name|Path
argument_list|>
name|filesToMove
parameter_list|)
throws|throws
name|IOException
throws|,
name|HiveException
block|{
if|if
condition|(
operator|!
name|fs
operator|.
name|exists
argument_list|(
name|dst
argument_list|)
condition|)
block|{
name|fs
operator|.
name|mkdirs
argument_list|(
name|dst
argument_list|)
expr_stmt|;
block|}
name|FileStatus
index|[]
name|files
init|=
name|fs
operator|.
name|listStatus
argument_list|(
name|src
argument_list|)
decl_stmt|;
for|for
control|(
name|FileStatus
name|file
range|:
name|files
control|)
block|{
if|if
condition|(
name|filesToMove
operator|.
name|contains
argument_list|(
name|file
operator|.
name|getPath
argument_list|()
argument_list|)
condition|)
block|{
name|Utilities
operator|.
name|moveFile
argument_list|(
name|fs
argument_list|,
name|file
argument_list|,
name|dst
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|file
operator|.
name|isDir
argument_list|()
condition|)
block|{
comment|// Traverse directory contents.
comment|// Directory nesting for dst needs to match src.
name|Path
name|nestedDstPath
init|=
operator|new
name|Path
argument_list|(
name|dst
argument_list|,
name|file
operator|.
name|getPath
argument_list|()
operator|.
name|getName
argument_list|()
argument_list|)
decl_stmt|;
name|Utilities
operator|.
name|moveSpecifiedFiles
argument_list|(
name|fs
argument_list|,
name|file
operator|.
name|getPath
argument_list|()
argument_list|,
name|nestedDstPath
argument_list|,
name|filesToMove
argument_list|)
expr_stmt|;
block|}
block|}
block|}
specifier|private
specifier|static
name|void
name|moveFile
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|FileStatus
name|file
parameter_list|,
name|Path
name|dst
parameter_list|)
throws|throws
name|IOException
throws|,
name|HiveException
block|{
name|Path
name|srcFilePath
init|=
name|file
operator|.
name|getPath
argument_list|()
decl_stmt|;
name|String
name|fileName
init|=
name|srcFilePath
operator|.
name|getName
argument_list|()
decl_stmt|;
name|Path
name|dstFilePath
init|=
operator|new
name|Path
argument_list|(
name|dst
argument_list|,
name|fileName
argument_list|)
decl_stmt|;
if|if
condition|(
name|file
operator|.
name|isDir
argument_list|()
condition|)
block|{
name|renameOrMoveFiles
argument_list|(
name|fs
argument_list|,
name|srcFilePath
argument_list|,
name|dstFilePath
argument_list|)
expr_stmt|;
block|}
else|else
block|{
if|if
condition|(
name|fs
operator|.
name|exists
argument_list|(
name|dstFilePath
argument_list|)
condition|)
block|{
name|int
name|suffix
init|=
literal|0
decl_stmt|;
do|do
block|{
name|suffix
operator|++
expr_stmt|;
name|dstFilePath
operator|=
operator|new
name|Path
argument_list|(
name|dst
argument_list|,
name|fileName
operator|+
literal|"_"
operator|+
name|suffix
argument_list|)
expr_stmt|;
block|}
do|while
condition|(
name|fs
operator|.
name|exists
argument_list|(
name|dstFilePath
argument_list|)
condition|)
do|;
block|}
if|if
condition|(
operator|!
name|fs
operator|.
name|rename
argument_list|(
name|srcFilePath
argument_list|,
name|dstFilePath
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Unable to move: "
operator|+
name|srcFilePath
operator|+
literal|" to: "
operator|+
name|dst
argument_list|)
throw|;
block|}
block|}
block|}
comment|/**    * Rename src to dst, or in the case dst already exists, move files in src to dst. If there is an    * existing file with the same name, the new file's name will be appended with "_1", "_2", etc.    *    * @param fs    *          the FileSystem where src and dst are on.    * @param src    *          the src directory    * @param dst    *          the target directory    * @throws IOException    */
specifier|public
specifier|static
name|void
name|renameOrMoveFiles
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|Path
name|src
parameter_list|,
name|Path
name|dst
parameter_list|)
throws|throws
name|IOException
throws|,
name|HiveException
block|{
if|if
condition|(
operator|!
name|fs
operator|.
name|exists
argument_list|(
name|dst
argument_list|)
condition|)
block|{
if|if
condition|(
operator|!
name|fs
operator|.
name|rename
argument_list|(
name|src
argument_list|,
name|dst
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Unable to move: "
operator|+
name|src
operator|+
literal|" to: "
operator|+
name|dst
argument_list|)
throw|;
block|}
block|}
else|else
block|{
comment|// move file by file
name|FileStatus
index|[]
name|files
init|=
name|fs
operator|.
name|listStatus
argument_list|(
name|src
argument_list|)
decl_stmt|;
for|for
control|(
name|FileStatus
name|file
range|:
name|files
control|)
block|{
name|Utilities
operator|.
name|moveFile
argument_list|(
name|fs
argument_list|,
name|file
argument_list|,
name|dst
argument_list|)
expr_stmt|;
block|}
block|}
block|}
comment|/**    * The first group will contain the task id. The second group is the optional extension. The file    * name looks like: "0_0" or "0_0.gz". There may be a leading prefix (tmp_). Since getTaskId() can    * return an integer only - this should match a pure integer as well. {1,6} is used to limit    * matching for attempts #'s 0-999999.    */
specifier|private
specifier|static
specifier|final
name|Pattern
name|FILE_NAME_TO_TASK_ID_REGEX
init|=
name|Pattern
operator|.
name|compile
argument_list|(
literal|"^.*?([0-9]+)(_[0-9]{1,6})?(\\..*)?$"
argument_list|)
decl_stmt|;
comment|/**    * Some jobs like "INSERT INTO" jobs create copies of files like 0000001_0_copy_2.    * For such files,    * Group 1: 00000001 [taskId]    * Group 3: 0        [task attempId]    * Group 4: _copy_2  [copy suffix]    * Group 6: copy     [copy keyword]    * Group 8: 2        [copy file index]    */
specifier|public
specifier|static
specifier|final
name|String
name|COPY_KEYWORD
init|=
literal|"_copy_"
decl_stmt|;
comment|// copy keyword
specifier|private
specifier|static
specifier|final
name|Pattern
name|COPY_FILE_NAME_TO_TASK_ID_REGEX
init|=
name|Pattern
operator|.
name|compile
argument_list|(
literal|"^.*?"
operator|+
comment|// any prefix
literal|"([0-9]+)"
operator|+
comment|// taskId
literal|"(_)"
operator|+
comment|// separator
literal|"([0-9]{1,6})?"
operator|+
comment|// attemptId (limited to 6 digits)
literal|"((_)(\\Bcopy\\B)(_)"
operator|+
literal|"([0-9]{1,6})$)?"
operator|+
comment|// copy file index
literal|"(\\..*)?$"
argument_list|)
decl_stmt|;
comment|// any suffix/file extension
comment|/**    * This retruns prefix part + taskID for bucket join for partitioned table    */
specifier|private
specifier|static
specifier|final
name|Pattern
name|FILE_NAME_PREFIXED_TASK_ID_REGEX
init|=
name|Pattern
operator|.
name|compile
argument_list|(
literal|"^.*?((\\(.*\\))?[0-9]+)(_[0-9]{1,6})?(\\..*)?$"
argument_list|)
decl_stmt|;
comment|/**    * This breaks a prefixed bucket number into the prefix and the taskID    */
specifier|private
specifier|static
specifier|final
name|Pattern
name|PREFIXED_TASK_ID_REGEX
init|=
name|Pattern
operator|.
name|compile
argument_list|(
literal|"^(.*?\\(.*\\))?([0-9]+)$"
argument_list|)
decl_stmt|;
comment|/**    * This breaks a prefixed bucket number out into a single integer    */
specifier|private
specifier|static
specifier|final
name|Pattern
name|PREFIXED_BUCKET_ID_REGEX
init|=
name|Pattern
operator|.
name|compile
argument_list|(
literal|"^(0*([0-9]+))_([0-9]+).*"
argument_list|)
decl_stmt|;
comment|/**    * Get the task id from the filename. It is assumed that the filename is derived from the output    * of getTaskId    *    * @param filename    *          filename to extract taskid from    */
specifier|public
specifier|static
name|String
name|getTaskIdFromFilename
parameter_list|(
name|String
name|filename
parameter_list|)
block|{
return|return
name|getIdFromFilename
argument_list|(
name|filename
argument_list|,
name|FILE_NAME_TO_TASK_ID_REGEX
argument_list|)
return|;
block|}
comment|/**    * Get the part-spec + task id from the filename. It is assumed that the filename is derived    * from the output of getTaskId    *    * @param filename    *          filename to extract taskid from    */
specifier|public
specifier|static
name|String
name|getPrefixedTaskIdFromFilename
parameter_list|(
name|String
name|filename
parameter_list|)
block|{
return|return
name|getIdFromFilename
argument_list|(
name|filename
argument_list|,
name|FILE_NAME_PREFIXED_TASK_ID_REGEX
argument_list|)
return|;
block|}
specifier|private
specifier|static
name|String
name|getIdFromFilename
parameter_list|(
name|String
name|filename
parameter_list|,
name|Pattern
name|pattern
parameter_list|)
block|{
name|String
name|taskId
init|=
name|filename
decl_stmt|;
name|int
name|dirEnd
init|=
name|filename
operator|.
name|lastIndexOf
argument_list|(
name|Path
operator|.
name|SEPARATOR
argument_list|)
decl_stmt|;
if|if
condition|(
name|dirEnd
operator|!=
operator|-
literal|1
condition|)
block|{
name|taskId
operator|=
name|filename
operator|.
name|substring
argument_list|(
name|dirEnd
operator|+
literal|1
argument_list|)
expr_stmt|;
block|}
name|Matcher
name|m
init|=
name|pattern
operator|.
name|matcher
argument_list|(
name|taskId
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|m
operator|.
name|matches
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Unable to get task id from file name: {}. Using last component {}"
operator|+
literal|" as task id."
argument_list|,
name|filename
argument_list|,
name|taskId
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|taskId
operator|=
name|m
operator|.
name|group
argument_list|(
literal|1
argument_list|)
expr_stmt|;
block|}
name|LOG
operator|.
name|debug
argument_list|(
literal|"TaskId for {} = {}"
argument_list|,
name|filename
argument_list|,
name|taskId
argument_list|)
expr_stmt|;
return|return
name|taskId
return|;
block|}
specifier|public
specifier|static
name|String
name|getFileNameFromDirName
parameter_list|(
name|String
name|dirName
parameter_list|)
block|{
name|int
name|dirEnd
init|=
name|dirName
operator|.
name|lastIndexOf
argument_list|(
name|Path
operator|.
name|SEPARATOR
argument_list|)
decl_stmt|;
if|if
condition|(
name|dirEnd
operator|!=
operator|-
literal|1
condition|)
block|{
return|return
name|dirName
operator|.
name|substring
argument_list|(
name|dirEnd
operator|+
literal|1
argument_list|)
return|;
block|}
return|return
name|dirName
return|;
block|}
comment|/**    * Replace the task id from the filename. It is assumed that the filename is derived from the    * output of getTaskId    *    * @param filename    *          filename to replace taskid "0_0" or "0_0.gz" by 33 to "33_0" or "33_0.gz"    */
specifier|public
specifier|static
name|String
name|replaceTaskIdFromFilename
parameter_list|(
name|String
name|filename
parameter_list|,
name|int
name|bucketNum
parameter_list|)
block|{
return|return
name|replaceTaskIdFromFilename
argument_list|(
name|filename
argument_list|,
name|String
operator|.
name|valueOf
argument_list|(
name|bucketNum
argument_list|)
argument_list|)
return|;
block|}
specifier|public
specifier|static
name|String
name|replaceTaskIdFromFilename
parameter_list|(
name|String
name|filename
parameter_list|,
name|String
name|fileId
parameter_list|)
block|{
name|String
name|taskId
init|=
name|getTaskIdFromFilename
argument_list|(
name|filename
argument_list|)
decl_stmt|;
name|String
name|newTaskId
init|=
name|replaceTaskId
argument_list|(
name|taskId
argument_list|,
name|fileId
argument_list|)
decl_stmt|;
name|String
name|ret
init|=
name|replaceTaskIdFromFilename
argument_list|(
name|filename
argument_list|,
name|taskId
argument_list|,
name|newTaskId
argument_list|)
decl_stmt|;
return|return
operator|(
name|ret
operator|)
return|;
block|}
comment|/**    * Replace taskId with input bucketNum. For example, if taskId is 000000 and bucketNum is 1,    * return should be 000001; if taskId is (ds%3D1)000000 and bucketNum is 1, return should be    * (ds%3D1)000001. This method is different from the replaceTaskId(String, String) method.    * In this method, the pattern is in taskId.    * @param taskId    * @param bucketNum    * @return    */
specifier|public
specifier|static
name|String
name|replaceTaskId
parameter_list|(
name|String
name|taskId
parameter_list|,
name|int
name|bucketNum
parameter_list|)
block|{
name|String
name|bucketNumStr
init|=
name|String
operator|.
name|valueOf
argument_list|(
name|bucketNum
argument_list|)
decl_stmt|;
name|Matcher
name|m
init|=
name|PREFIXED_TASK_ID_REGEX
operator|.
name|matcher
argument_list|(
name|taskId
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|m
operator|.
name|matches
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Unable to determine bucket number from task id: {}. Using "
operator|+
literal|"task ID as bucket number."
argument_list|,
name|taskId
argument_list|)
expr_stmt|;
return|return
name|adjustBucketNumLen
argument_list|(
name|bucketNumStr
argument_list|,
name|taskId
argument_list|)
return|;
block|}
else|else
block|{
name|String
name|adjustedBucketNum
init|=
name|adjustBucketNumLen
argument_list|(
name|bucketNumStr
argument_list|,
name|m
operator|.
name|group
argument_list|(
literal|2
argument_list|)
argument_list|)
decl_stmt|;
return|return
operator|(
name|m
operator|.
name|group
argument_list|(
literal|1
argument_list|)
operator|==
literal|null
condition|?
name|StringUtils
operator|.
name|EMPTY
else|:
name|m
operator|.
name|group
argument_list|(
literal|1
argument_list|)
operator|)
operator|+
name|adjustedBucketNum
return|;
block|}
block|}
comment|/**    * Returns strBucketNum with enough 0's prefixing the task ID portion of the String to make it    * equal in length to taskId    *    * @param taskId - the taskId used as a template for length    * @param strBucketNum - the bucket number of the output, may or may not be prefixed    * @return    */
specifier|private
specifier|static
name|String
name|replaceTaskId
parameter_list|(
name|String
name|taskId
parameter_list|,
name|String
name|strBucketNum
parameter_list|)
block|{
name|Matcher
name|m
init|=
name|PREFIXED_TASK_ID_REGEX
operator|.
name|matcher
argument_list|(
name|strBucketNum
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|m
operator|.
name|matches
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Unable to determine bucket number from file ID: {}. Using "
operator|+
literal|"file ID as bucket number."
argument_list|,
name|strBucketNum
argument_list|)
expr_stmt|;
return|return
name|adjustBucketNumLen
argument_list|(
name|strBucketNum
argument_list|,
name|taskId
argument_list|)
return|;
block|}
else|else
block|{
name|String
name|adjustedBucketNum
init|=
name|adjustBucketNumLen
argument_list|(
name|m
operator|.
name|group
argument_list|(
literal|2
argument_list|)
argument_list|,
name|taskId
argument_list|)
decl_stmt|;
return|return
operator|(
name|m
operator|.
name|group
argument_list|(
literal|1
argument_list|)
operator|==
literal|null
condition|?
name|StringUtils
operator|.
name|EMPTY
else|:
name|m
operator|.
name|group
argument_list|(
literal|1
argument_list|)
operator|)
operator|+
name|adjustedBucketNum
return|;
block|}
block|}
comment|/**    * Adds 0's to the beginning of bucketNum until bucketNum and taskId are the same length.    *    * @param bucketNum - the bucket number, should not be prefixed    * @param taskId - the taskId used as a template for length    * @return    */
specifier|private
specifier|static
name|String
name|adjustBucketNumLen
parameter_list|(
name|String
name|bucketNum
parameter_list|,
name|String
name|taskId
parameter_list|)
block|{
name|int
name|bucketNumLen
init|=
name|bucketNum
operator|.
name|length
argument_list|()
decl_stmt|;
name|int
name|taskIdLen
init|=
name|taskId
operator|.
name|length
argument_list|()
decl_stmt|;
name|StringBuilder
name|s
init|=
operator|new
name|StringBuilder
argument_list|()
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|taskIdLen
operator|-
name|bucketNumLen
condition|;
name|i
operator|++
control|)
block|{
name|s
operator|.
name|append
argument_list|(
literal|'0'
argument_list|)
expr_stmt|;
block|}
name|s
operator|.
name|append
argument_list|(
name|bucketNum
argument_list|)
expr_stmt|;
return|return
name|s
operator|.
name|toString
argument_list|()
return|;
block|}
comment|/**    * Replace the oldTaskId appearing in the filename by the newTaskId. The string oldTaskId could    * appear multiple times, we should only replace the last one.    *    * @param filename    * @param oldTaskId    * @param newTaskId    * @return    */
specifier|private
specifier|static
name|String
name|replaceTaskIdFromFilename
parameter_list|(
name|String
name|filename
parameter_list|,
name|String
name|oldTaskId
parameter_list|,
name|String
name|newTaskId
parameter_list|)
block|{
name|String
index|[]
name|spl
init|=
name|filename
operator|.
name|split
argument_list|(
name|oldTaskId
argument_list|)
decl_stmt|;
if|if
condition|(
operator|(
name|spl
operator|.
name|length
operator|==
literal|0
operator|)
operator|||
operator|(
name|spl
operator|.
name|length
operator|==
literal|1
operator|)
condition|)
block|{
return|return
name|filename
operator|.
name|replaceAll
argument_list|(
name|oldTaskId
argument_list|,
name|newTaskId
argument_list|)
return|;
block|}
name|StringBuilder
name|snew
init|=
operator|new
name|StringBuilder
argument_list|()
decl_stmt|;
for|for
control|(
name|int
name|idx
init|=
literal|0
init|;
name|idx
operator|<
name|spl
operator|.
name|length
operator|-
literal|1
condition|;
name|idx
operator|++
control|)
block|{
if|if
condition|(
name|idx
operator|>
literal|0
condition|)
block|{
name|snew
operator|.
name|append
argument_list|(
name|oldTaskId
argument_list|)
expr_stmt|;
block|}
name|snew
operator|.
name|append
argument_list|(
name|spl
index|[
name|idx
index|]
argument_list|)
expr_stmt|;
block|}
name|snew
operator|.
name|append
argument_list|(
name|newTaskId
argument_list|)
expr_stmt|;
name|snew
operator|.
name|append
argument_list|(
name|spl
index|[
name|spl
operator|.
name|length
operator|-
literal|1
index|]
argument_list|)
expr_stmt|;
return|return
name|snew
operator|.
name|toString
argument_list|()
return|;
block|}
comment|/**    * returns null if path is not exist    */
specifier|public
specifier|static
name|FileStatus
index|[]
name|listStatusIfExists
parameter_list|(
name|Path
name|path
parameter_list|,
name|FileSystem
name|fs
parameter_list|)
throws|throws
name|IOException
block|{
try|try
block|{
return|return
name|fs
operator|.
name|listStatus
argument_list|(
name|path
argument_list|,
name|FileUtils
operator|.
name|HIDDEN_FILES_PATH_FILTER
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|FileNotFoundException
name|e
parameter_list|)
block|{
comment|// FS in hadoop 2.0 throws FNF instead of returning null
return|return
literal|null
return|;
block|}
block|}
specifier|public
specifier|static
name|void
name|mvFileToFinalPath
parameter_list|(
name|Path
name|specPath
parameter_list|,
name|Configuration
name|hconf
parameter_list|,
name|boolean
name|success
parameter_list|,
name|Logger
name|log
parameter_list|,
name|DynamicPartitionCtx
name|dpCtx
parameter_list|,
name|FileSinkDesc
name|conf
parameter_list|,
name|Reporter
name|reporter
parameter_list|)
throws|throws
name|IOException
throws|,
name|HiveException
block|{
comment|//
comment|// Runaway task attempts (which are unable to be killed by MR/YARN) can cause HIVE-17113,
comment|// where they can write duplicate output files to tmpPath after de-duplicating the files,
comment|// but before tmpPath is moved to specPath.
comment|// Fixing this issue will be done differently for blobstore (e.g. S3)
comment|// vs non-blobstore (local filesystem, HDFS) filesystems due to differences in
comment|// implementation - a directory move in a blobstore effectively results in file-by-file
comment|// moves for every file in a directory, while in HDFS/localFS a directory move is just a
comment|// single filesystem operation.
comment|// - For non-blobstore FS, do the following:
comment|//   1) Rename tmpPath to a new directory name to prevent additional files
comment|//      from being added by runaway processes.
comment|//   2) Remove duplicates from the temp directory
comment|//   3) Rename/move the temp directory to specPath
comment|//
comment|// - For blobstore FS, do the following:
comment|//   1) Remove duplicates from tmpPath
comment|//   2) Use moveSpecifiedFiles() to perform a file-by-file move of the de-duped files
comment|//      to specPath. On blobstore FS, assuming n files in the directory, this results
comment|//      in n file moves, compared to 2*n file moves with the previous solution
comment|//      (each directory move would result in a file-by-file move of the files in the directory)
comment|//
name|FileSystem
name|fs
init|=
name|specPath
operator|.
name|getFileSystem
argument_list|(
name|hconf
argument_list|)
decl_stmt|;
name|boolean
name|isBlobStorage
init|=
name|BlobStorageUtils
operator|.
name|isBlobStorageFileSystem
argument_list|(
name|hconf
argument_list|,
name|fs
argument_list|)
decl_stmt|;
name|Path
name|tmpPath
init|=
name|Utilities
operator|.
name|toTempPath
argument_list|(
name|specPath
argument_list|)
decl_stmt|;
name|Path
name|taskTmpPath
init|=
name|Utilities
operator|.
name|toTaskTempPath
argument_list|(
name|specPath
argument_list|)
decl_stmt|;
if|if
condition|(
name|success
condition|)
block|{
if|if
condition|(
operator|!
name|isBlobStorage
operator|&&
name|fs
operator|.
name|exists
argument_list|(
name|tmpPath
argument_list|)
condition|)
block|{
comment|//   1) Rename tmpPath to a new directory name to prevent additional files
comment|//      from being added by runaway processes.
name|Path
name|tmpPathOriginal
init|=
name|tmpPath
decl_stmt|;
name|tmpPath
operator|=
operator|new
name|Path
argument_list|(
name|tmpPath
operator|.
name|getParent
argument_list|()
argument_list|,
name|tmpPath
operator|.
name|getName
argument_list|()
operator|+
literal|".moved"
argument_list|)
expr_stmt|;
name|Utilities
operator|.
name|rename
argument_list|(
name|fs
argument_list|,
name|tmpPathOriginal
argument_list|,
name|tmpPath
argument_list|)
expr_stmt|;
block|}
comment|// Remove duplicates from tmpPath
name|FileStatus
index|[]
name|statuses
init|=
name|HiveStatsUtils
operator|.
name|getFileStatusRecurse
argument_list|(
name|tmpPath
argument_list|,
operator|(
operator|(
name|dpCtx
operator|==
literal|null
operator|)
condition|?
literal|1
else|:
name|dpCtx
operator|.
name|getNumDPCols
argument_list|()
operator|)
argument_list|,
name|fs
argument_list|)
decl_stmt|;
if|if
condition|(
name|statuses
operator|!=
literal|null
operator|&&
name|statuses
operator|.
name|length
operator|>
literal|0
condition|)
block|{
name|PerfLogger
name|perfLogger
init|=
name|SessionState
operator|.
name|getPerfLogger
argument_list|()
decl_stmt|;
name|Set
argument_list|<
name|Path
argument_list|>
name|filesKept
init|=
operator|new
name|HashSet
argument_list|<
name|Path
argument_list|>
argument_list|()
decl_stmt|;
name|perfLogger
operator|.
name|PerfLogBegin
argument_list|(
literal|"FileSinkOperator"
argument_list|,
literal|"RemoveTempOrDuplicateFiles"
argument_list|)
expr_stmt|;
comment|// remove any tmp file or double-committed output files
name|List
argument_list|<
name|Path
argument_list|>
name|emptyBuckets
init|=
name|Utilities
operator|.
name|removeTempOrDuplicateFiles
argument_list|(
name|fs
argument_list|,
name|statuses
argument_list|,
name|dpCtx
argument_list|,
name|conf
argument_list|,
name|hconf
argument_list|,
name|filesKept
argument_list|,
literal|false
argument_list|)
decl_stmt|;
name|perfLogger
operator|.
name|PerfLogEnd
argument_list|(
literal|"FileSinkOperator"
argument_list|,
literal|"RemoveTempOrDuplicateFiles"
argument_list|)
expr_stmt|;
comment|// create empty buckets if necessary
if|if
condition|(
operator|!
name|emptyBuckets
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|perfLogger
operator|.
name|PerfLogBegin
argument_list|(
literal|"FileSinkOperator"
argument_list|,
literal|"CreateEmptyBuckets"
argument_list|)
expr_stmt|;
name|createEmptyBuckets
argument_list|(
name|hconf
argument_list|,
name|emptyBuckets
argument_list|,
name|conf
operator|.
name|getCompressed
argument_list|()
argument_list|,
name|conf
operator|.
name|getTableInfo
argument_list|()
argument_list|,
name|reporter
argument_list|)
expr_stmt|;
name|filesKept
operator|.
name|addAll
argument_list|(
name|emptyBuckets
argument_list|)
expr_stmt|;
name|perfLogger
operator|.
name|PerfLogEnd
argument_list|(
literal|"FileSinkOperator"
argument_list|,
literal|"CreateEmptyBuckets"
argument_list|)
expr_stmt|;
block|}
comment|// move to the file destination
name|Utilities
operator|.
name|FILE_OP_LOGGER
operator|.
name|trace
argument_list|(
literal|"Moving tmp dir: {} to: {}"
argument_list|,
name|tmpPath
argument_list|,
name|specPath
argument_list|)
expr_stmt|;
name|perfLogger
operator|.
name|PerfLogBegin
argument_list|(
literal|"FileSinkOperator"
argument_list|,
literal|"RenameOrMoveFiles"
argument_list|)
expr_stmt|;
if|if
condition|(
name|isBlobStorage
condition|)
block|{
comment|// HIVE-17113 - avoid copying files that may have been written to the temp dir by runaway tasks,
comment|// by moving just the files we've tracked from removeTempOrDuplicateFiles().
name|Utilities
operator|.
name|moveSpecifiedFiles
argument_list|(
name|fs
argument_list|,
name|tmpPath
argument_list|,
name|specPath
argument_list|,
name|filesKept
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// For non-blobstore case, can just move the directory - the initial directory rename
comment|// at the start of this method should prevent files written by runaway tasks.
name|Utilities
operator|.
name|renameOrMoveFiles
argument_list|(
name|fs
argument_list|,
name|tmpPath
argument_list|,
name|specPath
argument_list|)
expr_stmt|;
block|}
name|perfLogger
operator|.
name|PerfLogEnd
argument_list|(
literal|"FileSinkOperator"
argument_list|,
literal|"RenameOrMoveFiles"
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
name|Utilities
operator|.
name|FILE_OP_LOGGER
operator|.
name|trace
argument_list|(
literal|"deleting tmpPath {}"
argument_list|,
name|tmpPath
argument_list|)
expr_stmt|;
name|fs
operator|.
name|delete
argument_list|(
name|tmpPath
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
name|Utilities
operator|.
name|FILE_OP_LOGGER
operator|.
name|trace
argument_list|(
literal|"deleting taskTmpPath {}"
argument_list|,
name|taskTmpPath
argument_list|)
expr_stmt|;
name|fs
operator|.
name|delete
argument_list|(
name|taskTmpPath
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
comment|/**    * Check the existence of buckets according to bucket specification. Create empty buckets if    * needed.    *    * @param hconf    * @param paths A list of empty buckets to create    * @param conf The definition of the FileSink.    * @param reporter The mapreduce reporter object    * @throws HiveException    * @throws IOException    */
specifier|static
name|void
name|createEmptyBuckets
parameter_list|(
name|Configuration
name|hconf
parameter_list|,
name|List
argument_list|<
name|Path
argument_list|>
name|paths
parameter_list|,
name|boolean
name|isCompressed
parameter_list|,
name|TableDesc
name|tableInfo
parameter_list|,
name|Reporter
name|reporter
parameter_list|)
throws|throws
name|HiveException
throws|,
name|IOException
block|{
name|JobConf
name|jc
decl_stmt|;
if|if
condition|(
name|hconf
operator|instanceof
name|JobConf
condition|)
block|{
name|jc
operator|=
operator|new
name|JobConf
argument_list|(
name|hconf
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// test code path
name|jc
operator|=
operator|new
name|JobConf
argument_list|(
name|hconf
argument_list|)
expr_stmt|;
block|}
name|HiveOutputFormat
argument_list|<
name|?
argument_list|,
name|?
argument_list|>
name|hiveOutputFormat
init|=
literal|null
decl_stmt|;
name|Class
argument_list|<
name|?
extends|extends
name|Writable
argument_list|>
name|outputClass
init|=
literal|null
decl_stmt|;
try|try
block|{
name|Serializer
name|serializer
init|=
operator|(
name|Serializer
operator|)
name|tableInfo
operator|.
name|getDeserializerClass
argument_list|()
operator|.
name|newInstance
argument_list|()
decl_stmt|;
name|serializer
operator|.
name|initialize
argument_list|(
literal|null
argument_list|,
name|tableInfo
operator|.
name|getProperties
argument_list|()
argument_list|)
expr_stmt|;
name|outputClass
operator|=
name|serializer
operator|.
name|getSerializedClass
argument_list|()
expr_stmt|;
name|hiveOutputFormat
operator|=
name|HiveFileFormatUtils
operator|.
name|getHiveOutputFormat
argument_list|(
name|hconf
argument_list|,
name|tableInfo
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|SerDeException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
catch|catch
parameter_list|(
name|InstantiationException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
catch|catch
parameter_list|(
name|IllegalAccessException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
for|for
control|(
name|Path
name|path
range|:
name|paths
control|)
block|{
name|Utilities
operator|.
name|FILE_OP_LOGGER
operator|.
name|trace
argument_list|(
literal|"creating empty bucket for {}"
argument_list|,
name|path
argument_list|)
expr_stmt|;
name|RecordWriter
name|writer
init|=
name|HiveFileFormatUtils
operator|.
name|getRecordWriter
argument_list|(
name|jc
argument_list|,
name|hiveOutputFormat
argument_list|,
name|outputClass
argument_list|,
name|isCompressed
argument_list|,
name|tableInfo
operator|.
name|getProperties
argument_list|()
argument_list|,
name|path
argument_list|,
name|reporter
argument_list|)
decl_stmt|;
name|writer
operator|.
name|close
argument_list|(
literal|false
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"created empty bucket for enforcing bucketing at {}"
argument_list|,
name|path
argument_list|)
expr_stmt|;
block|}
block|}
specifier|private
specifier|static
name|void
name|addFilesToPathSet
parameter_list|(
name|Collection
argument_list|<
name|FileStatus
argument_list|>
name|files
parameter_list|,
name|Set
argument_list|<
name|Path
argument_list|>
name|fileSet
parameter_list|)
block|{
for|for
control|(
name|FileStatus
name|file
range|:
name|files
control|)
block|{
name|fileSet
operator|.
name|add
argument_list|(
name|file
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * Remove all temporary files and duplicate (double-committed) files from a given directory.    */
specifier|public
specifier|static
name|void
name|removeTempOrDuplicateFiles
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|Path
name|path
parameter_list|,
name|boolean
name|isBaseDir
parameter_list|)
throws|throws
name|IOException
block|{
name|removeTempOrDuplicateFiles
argument_list|(
name|fs
argument_list|,
name|path
argument_list|,
literal|null
argument_list|,
literal|null
argument_list|,
literal|null
argument_list|,
name|isBaseDir
argument_list|)
expr_stmt|;
block|}
specifier|public
specifier|static
name|List
argument_list|<
name|Path
argument_list|>
name|removeTempOrDuplicateFiles
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|Path
name|path
parameter_list|,
name|DynamicPartitionCtx
name|dpCtx
parameter_list|,
name|FileSinkDesc
name|conf
parameter_list|,
name|Configuration
name|hconf
parameter_list|,
name|boolean
name|isBaseDir
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|path
operator|==
literal|null
condition|)
block|{
return|return
literal|null
return|;
block|}
name|FileStatus
index|[]
name|stats
init|=
name|HiveStatsUtils
operator|.
name|getFileStatusRecurse
argument_list|(
name|path
argument_list|,
operator|(
operator|(
name|dpCtx
operator|==
literal|null
operator|)
condition|?
literal|1
else|:
name|dpCtx
operator|.
name|getNumDPCols
argument_list|()
operator|)
argument_list|,
name|fs
argument_list|)
decl_stmt|;
return|return
name|removeTempOrDuplicateFiles
argument_list|(
name|fs
argument_list|,
name|stats
argument_list|,
name|dpCtx
argument_list|,
name|conf
argument_list|,
name|hconf
argument_list|,
name|isBaseDir
argument_list|)
return|;
block|}
specifier|public
specifier|static
name|List
argument_list|<
name|Path
argument_list|>
name|removeTempOrDuplicateFiles
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|FileStatus
index|[]
name|fileStats
parameter_list|,
name|DynamicPartitionCtx
name|dpCtx
parameter_list|,
name|FileSinkDesc
name|conf
parameter_list|,
name|Configuration
name|hconf
parameter_list|,
name|boolean
name|isBaseDir
parameter_list|)
throws|throws
name|IOException
block|{
return|return
name|removeTempOrDuplicateFiles
argument_list|(
name|fs
argument_list|,
name|fileStats
argument_list|,
name|dpCtx
argument_list|,
name|conf
argument_list|,
name|hconf
argument_list|,
literal|null
argument_list|,
name|isBaseDir
argument_list|)
return|;
block|}
comment|/**    * Remove all temporary files and duplicate (double-committed) files from a given directory.    *    * @return a list of path names corresponding to should-be-created empty buckets.    */
specifier|public
specifier|static
name|List
argument_list|<
name|Path
argument_list|>
name|removeTempOrDuplicateFiles
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|FileStatus
index|[]
name|fileStats
parameter_list|,
name|DynamicPartitionCtx
name|dpCtx
parameter_list|,
name|FileSinkDesc
name|conf
parameter_list|,
name|Configuration
name|hconf
parameter_list|,
name|Set
argument_list|<
name|Path
argument_list|>
name|filesKept
parameter_list|,
name|boolean
name|isBaseDir
parameter_list|)
throws|throws
name|IOException
block|{
name|int
name|dpLevels
init|=
name|dpCtx
operator|==
literal|null
condition|?
literal|0
else|:
name|dpCtx
operator|.
name|getNumDPCols
argument_list|()
decl_stmt|,
name|numBuckets
init|=
operator|(
name|conf
operator|!=
literal|null
operator|&&
name|conf
operator|.
name|getTable
argument_list|()
operator|!=
literal|null
operator|)
condition|?
name|conf
operator|.
name|getTable
argument_list|()
operator|.
name|getNumBuckets
argument_list|()
else|:
literal|0
decl_stmt|;
return|return
name|removeTempOrDuplicateFiles
argument_list|(
name|fs
argument_list|,
name|fileStats
argument_list|,
literal|null
argument_list|,
name|dpLevels
argument_list|,
name|numBuckets
argument_list|,
name|hconf
argument_list|,
literal|null
argument_list|,
literal|0
argument_list|,
literal|false
argument_list|,
name|filesKept
argument_list|,
name|isBaseDir
argument_list|)
return|;
block|}
specifier|private
specifier|static
name|boolean
name|removeEmptyDpDirectory
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|Path
name|path
parameter_list|)
throws|throws
name|IOException
block|{
name|FileStatus
index|[]
name|items
init|=
name|fs
operator|.
name|listStatus
argument_list|(
name|path
argument_list|)
decl_stmt|;
comment|// remove empty directory since DP insert should not generate empty partitions.
comment|// empty directories could be generated by crashed Task/ScriptOperator
if|if
condition|(
name|items
operator|.
name|length
operator|!=
literal|0
condition|)
block|{
return|return
literal|false
return|;
block|}
if|if
condition|(
operator|!
name|fs
operator|.
name|delete
argument_list|(
name|path
argument_list|,
literal|true
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Cannot delete empty directory {}"
argument_list|,
name|path
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Cannot delete empty directory "
operator|+
name|path
argument_list|)
throw|;
block|}
return|return
literal|true
return|;
block|}
specifier|public
specifier|static
name|List
argument_list|<
name|Path
argument_list|>
name|removeTempOrDuplicateFiles
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|FileStatus
index|[]
name|fileStats
parameter_list|,
name|String
name|unionSuffix
parameter_list|,
name|int
name|dpLevels
parameter_list|,
name|int
name|numBuckets
parameter_list|,
name|Configuration
name|hconf
parameter_list|,
name|Long
name|txnId
parameter_list|,
name|int
name|stmtId
parameter_list|,
name|boolean
name|isMmTable
parameter_list|,
name|Set
argument_list|<
name|Path
argument_list|>
name|filesKept
parameter_list|,
name|boolean
name|isBaseDir
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|fileStats
operator|==
literal|null
condition|)
block|{
return|return
literal|null
return|;
block|}
name|List
argument_list|<
name|Path
argument_list|>
name|result
init|=
operator|new
name|ArrayList
argument_list|<
name|Path
argument_list|>
argument_list|()
decl_stmt|;
name|HashMap
argument_list|<
name|String
argument_list|,
name|FileStatus
argument_list|>
name|taskIDToFile
init|=
literal|null
decl_stmt|;
if|if
condition|(
name|dpLevels
operator|>
literal|0
condition|)
block|{
name|FileStatus
index|[]
name|parts
init|=
name|fileStats
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|parts
operator|.
name|length
condition|;
operator|++
name|i
control|)
block|{
assert|assert
name|parts
index|[
name|i
index|]
operator|.
name|isDirectory
argument_list|()
operator|:
literal|"dynamic partition "
operator|+
name|parts
index|[
name|i
index|]
operator|.
name|getPath
argument_list|()
operator|+
literal|" is not a directory"
assert|;
name|Path
name|path
init|=
name|parts
index|[
name|i
index|]
operator|.
name|getPath
argument_list|()
decl_stmt|;
if|if
condition|(
name|removeEmptyDpDirectory
argument_list|(
name|fs
argument_list|,
name|path
argument_list|)
condition|)
block|{
name|parts
index|[
name|i
index|]
operator|=
literal|null
expr_stmt|;
continue|continue;
block|}
if|if
condition|(
name|isMmTable
condition|)
block|{
name|Path
name|mmDir
init|=
name|parts
index|[
name|i
index|]
operator|.
name|getPath
argument_list|()
decl_stmt|;
if|if
condition|(
operator|!
name|mmDir
operator|.
name|getName
argument_list|()
operator|.
name|equals
argument_list|(
name|AcidUtils
operator|.
name|baseOrDeltaSubdir
argument_list|(
name|isBaseDir
argument_list|,
name|txnId
argument_list|,
name|txnId
argument_list|,
name|stmtId
argument_list|)
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Unexpected non-MM directory name "
operator|+
name|mmDir
argument_list|)
throw|;
block|}
name|Utilities
operator|.
name|FILE_OP_LOGGER
operator|.
name|trace
argument_list|(
literal|"removeTempOrDuplicateFiles processing files in MM directory {}"
argument_list|,
name|mmDir
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|StringUtils
operator|.
name|isEmpty
argument_list|(
name|unionSuffix
argument_list|)
condition|)
block|{
name|path
operator|=
operator|new
name|Path
argument_list|(
name|path
argument_list|,
name|unionSuffix
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|fs
operator|.
name|exists
argument_list|(
name|path
argument_list|)
condition|)
block|{
continue|continue;
block|}
block|}
block|}
name|FileStatus
index|[]
name|items
init|=
name|fs
operator|.
name|listStatus
argument_list|(
name|path
argument_list|)
decl_stmt|;
name|taskIDToFile
operator|=
name|removeTempOrDuplicateFilesNonMm
argument_list|(
name|items
argument_list|,
name|fs
argument_list|)
expr_stmt|;
if|if
condition|(
name|filesKept
operator|!=
literal|null
operator|&&
name|taskIDToFile
operator|!=
literal|null
condition|)
block|{
name|addFilesToPathSet
argument_list|(
name|taskIDToFile
operator|.
name|values
argument_list|()
argument_list|,
name|filesKept
argument_list|)
expr_stmt|;
block|}
name|addBucketFileToResults
argument_list|(
name|taskIDToFile
argument_list|,
name|numBuckets
argument_list|,
name|hconf
argument_list|,
name|result
argument_list|)
expr_stmt|;
block|}
block|}
elseif|else
if|if
condition|(
name|isMmTable
operator|&&
operator|!
name|StringUtils
operator|.
name|isEmpty
argument_list|(
name|unionSuffix
argument_list|)
condition|)
block|{
name|FileStatus
index|[]
name|items
init|=
name|fileStats
decl_stmt|;
if|if
condition|(
name|fileStats
operator|.
name|length
operator|==
literal|0
condition|)
block|{
return|return
name|result
return|;
block|}
name|Path
name|mmDir
init|=
name|extractNonDpMmDir
argument_list|(
name|txnId
argument_list|,
name|stmtId
argument_list|,
name|items
argument_list|,
name|isBaseDir
argument_list|)
decl_stmt|;
name|taskIDToFile
operator|=
name|removeTempOrDuplicateFilesNonMm
argument_list|(
name|fs
operator|.
name|listStatus
argument_list|(
operator|new
name|Path
argument_list|(
name|mmDir
argument_list|,
name|unionSuffix
argument_list|)
argument_list|)
argument_list|,
name|fs
argument_list|)
expr_stmt|;
if|if
condition|(
name|filesKept
operator|!=
literal|null
operator|&&
name|taskIDToFile
operator|!=
literal|null
condition|)
block|{
name|addFilesToPathSet
argument_list|(
name|taskIDToFile
operator|.
name|values
argument_list|()
argument_list|,
name|filesKept
argument_list|)
expr_stmt|;
block|}
name|addBucketFileToResults2
argument_list|(
name|taskIDToFile
argument_list|,
name|numBuckets
argument_list|,
name|hconf
argument_list|,
name|result
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|FileStatus
index|[]
name|items
init|=
name|fileStats
decl_stmt|;
if|if
condition|(
name|items
operator|.
name|length
operator|==
literal|0
condition|)
block|{
return|return
name|result
return|;
block|}
if|if
condition|(
operator|!
name|isMmTable
condition|)
block|{
name|taskIDToFile
operator|=
name|removeTempOrDuplicateFilesNonMm
argument_list|(
name|items
argument_list|,
name|fs
argument_list|)
expr_stmt|;
if|if
condition|(
name|filesKept
operator|!=
literal|null
operator|&&
name|taskIDToFile
operator|!=
literal|null
condition|)
block|{
name|addFilesToPathSet
argument_list|(
name|taskIDToFile
operator|.
name|values
argument_list|()
argument_list|,
name|filesKept
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
name|Path
name|mmDir
init|=
name|extractNonDpMmDir
argument_list|(
name|txnId
argument_list|,
name|stmtId
argument_list|,
name|items
argument_list|,
name|isBaseDir
argument_list|)
decl_stmt|;
name|taskIDToFile
operator|=
name|removeTempOrDuplicateFilesNonMm
argument_list|(
name|fs
operator|.
name|listStatus
argument_list|(
name|mmDir
argument_list|)
argument_list|,
name|fs
argument_list|)
expr_stmt|;
if|if
condition|(
name|filesKept
operator|!=
literal|null
operator|&&
name|taskIDToFile
operator|!=
literal|null
condition|)
block|{
name|addFilesToPathSet
argument_list|(
name|taskIDToFile
operator|.
name|values
argument_list|()
argument_list|,
name|filesKept
argument_list|)
expr_stmt|;
block|}
block|}
name|addBucketFileToResults2
argument_list|(
name|taskIDToFile
argument_list|,
name|numBuckets
argument_list|,
name|hconf
argument_list|,
name|result
argument_list|)
expr_stmt|;
block|}
return|return
name|result
return|;
block|}
specifier|private
specifier|static
name|Path
name|extractNonDpMmDir
parameter_list|(
name|Long
name|txnId
parameter_list|,
name|int
name|stmtId
parameter_list|,
name|FileStatus
index|[]
name|items
parameter_list|,
name|boolean
name|isBaseDir
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|items
operator|.
name|length
operator|>
literal|1
condition|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Unexpected directories for non-DP MM: "
operator|+
name|Arrays
operator|.
name|toString
argument_list|(
name|items
argument_list|)
argument_list|)
throw|;
block|}
name|Path
name|mmDir
init|=
name|items
index|[
literal|0
index|]
operator|.
name|getPath
argument_list|()
decl_stmt|;
if|if
condition|(
operator|!
name|mmDir
operator|.
name|getName
argument_list|()
operator|.
name|equals
argument_list|(
name|AcidUtils
operator|.
name|baseOrDeltaSubdir
argument_list|(
name|isBaseDir
argument_list|,
name|txnId
argument_list|,
name|txnId
argument_list|,
name|stmtId
argument_list|)
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Unexpected non-MM directory "
operator|+
name|mmDir
argument_list|)
throw|;
block|}
name|Utilities
operator|.
name|FILE_OP_LOGGER
operator|.
name|trace
argument_list|(
literal|"removeTempOrDuplicateFiles processing files in MM directory {}"
argument_list|,
name|mmDir
argument_list|)
expr_stmt|;
return|return
name|mmDir
return|;
block|}
comment|// TODO: not clear why two if conditions are different. Preserve the existing logic for now.
specifier|private
specifier|static
name|void
name|addBucketFileToResults2
parameter_list|(
name|HashMap
argument_list|<
name|String
argument_list|,
name|FileStatus
argument_list|>
name|taskIDToFile
parameter_list|,
name|int
name|numBuckets
parameter_list|,
name|Configuration
name|hconf
parameter_list|,
name|List
argument_list|<
name|Path
argument_list|>
name|result
parameter_list|)
block|{
if|if
condition|(
name|MapUtils
operator|.
name|isNotEmpty
argument_list|(
name|taskIDToFile
argument_list|)
operator|&&
operator|(
name|numBuckets
operator|>
name|taskIDToFile
operator|.
name|size
argument_list|()
operator|)
operator|&&
operator|!
literal|"tez"
operator|.
name|equalsIgnoreCase
argument_list|(
name|hconf
operator|.
name|get
argument_list|(
name|ConfVars
operator|.
name|HIVE_EXECUTION_ENGINE
operator|.
name|varname
argument_list|)
argument_list|)
condition|)
block|{
name|addBucketsToResultsCommon
argument_list|(
name|taskIDToFile
argument_list|,
name|numBuckets
argument_list|,
name|result
argument_list|)
expr_stmt|;
block|}
block|}
comment|// TODO: not clear why two if conditions are different. Preserve the existing logic for now.
specifier|private
specifier|static
name|void
name|addBucketFileToResults
parameter_list|(
name|HashMap
argument_list|<
name|String
argument_list|,
name|FileStatus
argument_list|>
name|taskIDToFile
parameter_list|,
name|int
name|numBuckets
parameter_list|,
name|Configuration
name|hconf
parameter_list|,
name|List
argument_list|<
name|Path
argument_list|>
name|result
parameter_list|)
block|{
comment|// if the table is bucketed and enforce bucketing, we should check and generate all buckets
if|if
condition|(
name|numBuckets
operator|>
literal|0
operator|&&
name|taskIDToFile
operator|!=
literal|null
operator|&&
operator|!
literal|"tez"
operator|.
name|equalsIgnoreCase
argument_list|(
name|hconf
operator|.
name|get
argument_list|(
name|ConfVars
operator|.
name|HIVE_EXECUTION_ENGINE
operator|.
name|varname
argument_list|)
argument_list|)
condition|)
block|{
name|addBucketsToResultsCommon
argument_list|(
name|taskIDToFile
argument_list|,
name|numBuckets
argument_list|,
name|result
argument_list|)
expr_stmt|;
block|}
block|}
specifier|private
specifier|static
name|void
name|addBucketsToResultsCommon
parameter_list|(
name|HashMap
argument_list|<
name|String
argument_list|,
name|FileStatus
argument_list|>
name|taskIDToFile
parameter_list|,
name|int
name|numBuckets
parameter_list|,
name|List
argument_list|<
name|Path
argument_list|>
name|result
parameter_list|)
block|{
name|String
name|taskID1
init|=
name|taskIDToFile
operator|.
name|keySet
argument_list|()
operator|.
name|iterator
argument_list|()
operator|.
name|next
argument_list|()
decl_stmt|;
name|Path
name|bucketPath
init|=
name|taskIDToFile
operator|.
name|values
argument_list|()
operator|.
name|iterator
argument_list|()
operator|.
name|next
argument_list|()
operator|.
name|getPath
argument_list|()
decl_stmt|;
for|for
control|(
name|int
name|j
init|=
literal|0
init|;
name|j
operator|<
name|numBuckets
condition|;
operator|++
name|j
control|)
block|{
name|addBucketFileIfMissing
argument_list|(
name|result
argument_list|,
name|taskIDToFile
argument_list|,
name|taskID1
argument_list|,
name|bucketPath
argument_list|,
name|j
argument_list|)
expr_stmt|;
block|}
block|}
specifier|private
specifier|static
name|void
name|addBucketFileIfMissing
parameter_list|(
name|List
argument_list|<
name|Path
argument_list|>
name|result
parameter_list|,
name|HashMap
argument_list|<
name|String
argument_list|,
name|FileStatus
argument_list|>
name|taskIDToFile
parameter_list|,
name|String
name|taskID1
parameter_list|,
name|Path
name|bucketPath
parameter_list|,
name|int
name|j
parameter_list|)
block|{
name|String
name|taskID2
init|=
name|replaceTaskId
argument_list|(
name|taskID1
argument_list|,
name|j
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|taskIDToFile
operator|.
name|containsKey
argument_list|(
name|taskID2
argument_list|)
condition|)
block|{
comment|// create empty bucket, file name should be derived from taskID2
name|URI
name|bucketUri
init|=
name|bucketPath
operator|.
name|toUri
argument_list|()
decl_stmt|;
name|String
name|path2
init|=
name|replaceTaskIdFromFilename
argument_list|(
name|bucketUri
operator|.
name|getPath
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|,
name|j
argument_list|)
decl_stmt|;
name|Utilities
operator|.
name|FILE_OP_LOGGER
operator|.
name|trace
argument_list|(
literal|"Creating an empty bucket file {}"
argument_list|,
name|path2
argument_list|)
expr_stmt|;
name|result
operator|.
name|add
argument_list|(
operator|new
name|Path
argument_list|(
name|bucketUri
operator|.
name|getScheme
argument_list|()
argument_list|,
name|bucketUri
operator|.
name|getAuthority
argument_list|()
argument_list|,
name|path2
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
specifier|private
specifier|static
name|HashMap
argument_list|<
name|String
argument_list|,
name|FileStatus
argument_list|>
name|removeTempOrDuplicateFilesNonMm
parameter_list|(
name|FileStatus
index|[]
name|files
parameter_list|,
name|FileSystem
name|fs
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|files
operator|==
literal|null
operator|||
name|fs
operator|==
literal|null
condition|)
block|{
return|return
literal|null
return|;
block|}
name|HashMap
argument_list|<
name|String
argument_list|,
name|FileStatus
argument_list|>
name|taskIdToFile
init|=
operator|new
name|HashMap
argument_list|<
name|String
argument_list|,
name|FileStatus
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|FileStatus
name|one
range|:
name|files
control|)
block|{
if|if
condition|(
name|isTempPath
argument_list|(
name|one
argument_list|)
condition|)
block|{
name|Path
name|onePath
init|=
name|one
operator|.
name|getPath
argument_list|()
decl_stmt|;
name|Utilities
operator|.
name|FILE_OP_LOGGER
operator|.
name|trace
argument_list|(
literal|"removeTempOrDuplicateFiles deleting {}"
argument_list|,
name|onePath
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|fs
operator|.
name|delete
argument_list|(
name|onePath
argument_list|,
literal|true
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Unable to delete tmp file: "
operator|+
name|onePath
argument_list|)
throw|;
block|}
block|}
else|else
block|{
comment|// This would be a single file. See if we need to remove it.
name|ponderRemovingTempOrDuplicateFile
argument_list|(
name|fs
argument_list|,
name|one
argument_list|,
name|taskIdToFile
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|taskIdToFile
return|;
block|}
specifier|private
specifier|static
name|void
name|ponderRemovingTempOrDuplicateFile
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|FileStatus
name|file
parameter_list|,
name|HashMap
argument_list|<
name|String
argument_list|,
name|FileStatus
argument_list|>
name|taskIdToFile
parameter_list|)
throws|throws
name|IOException
block|{
name|Path
name|filePath
init|=
name|file
operator|.
name|getPath
argument_list|()
decl_stmt|;
name|String
name|taskId
init|=
name|getPrefixedTaskIdFromFilename
argument_list|(
name|filePath
operator|.
name|getName
argument_list|()
argument_list|)
decl_stmt|;
name|Utilities
operator|.
name|FILE_OP_LOGGER
operator|.
name|trace
argument_list|(
literal|"removeTempOrDuplicateFiles looking at {}"
operator|+
literal|", taskId {}"
argument_list|,
name|filePath
argument_list|,
name|taskId
argument_list|)
expr_stmt|;
name|FileStatus
name|otherFile
init|=
name|taskIdToFile
operator|.
name|get
argument_list|(
name|taskId
argument_list|)
decl_stmt|;
name|taskIdToFile
operator|.
name|put
argument_list|(
name|taskId
argument_list|,
operator|(
name|otherFile
operator|==
literal|null
operator|)
condition|?
name|file
else|:
name|compareTempOrDuplicateFiles
argument_list|(
name|fs
argument_list|,
name|file
argument_list|,
name|otherFile
argument_list|)
argument_list|)
expr_stmt|;
block|}
specifier|private
specifier|static
name|FileStatus
name|compareTempOrDuplicateFiles
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|FileStatus
name|file
parameter_list|,
name|FileStatus
name|existingFile
parameter_list|)
throws|throws
name|IOException
block|{
comment|// Compare the file sizes of all the attempt files for the same task, the largest win
comment|// any attempt files could contain partial results (due to task failures or
comment|// speculative runs), but the largest should be the correct one since the result
comment|// of a successful run should never be smaller than a failed/speculative run.
name|FileStatus
name|toDelete
init|=
literal|null
decl_stmt|,
name|toRetain
init|=
literal|null
decl_stmt|;
comment|// "LOAD .. INTO" and "INSERT INTO" commands will generate files with
comment|// "_copy_x" suffix. These files are usually read by map tasks and the
comment|// task output gets written to some tmp path. The output file names will
comment|// be of format taskId_attemptId. The usual path for all these tasks is
comment|// srcPath -> taskTmpPath -> tmpPath -> finalPath.
comment|// But, MergeFileTask can move files directly from src path to final path
comment|// without copying it to tmp path. In such cases, different files with
comment|// "_copy_x" suffix will be identified as duplicates (change in value
comment|// of x is wrongly identified as attempt id) and will be deleted.
comment|// To avoid that we will ignore files with "_copy_x" suffix from duplicate
comment|// elimination.
name|Path
name|filePath
init|=
name|file
operator|.
name|getPath
argument_list|()
decl_stmt|;
if|if
condition|(
name|isCopyFile
argument_list|(
name|filePath
operator|.
name|getName
argument_list|()
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"{} file identified as duplicate. This file is"
operator|+
literal|" not deleted as it has copySuffix."
argument_list|,
name|filePath
argument_list|)
expr_stmt|;
return|return
name|existingFile
return|;
block|}
if|if
condition|(
name|existingFile
operator|.
name|getLen
argument_list|()
operator|>=
name|file
operator|.
name|getLen
argument_list|()
condition|)
block|{
name|toDelete
operator|=
name|file
expr_stmt|;
name|toRetain
operator|=
name|existingFile
expr_stmt|;
block|}
else|else
block|{
name|toDelete
operator|=
name|existingFile
expr_stmt|;
name|toRetain
operator|=
name|file
expr_stmt|;
block|}
if|if
condition|(
operator|!
name|fs
operator|.
name|delete
argument_list|(
name|toDelete
operator|.
name|getPath
argument_list|()
argument_list|,
literal|true
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Unable to delete duplicate file: "
operator|+
name|toDelete
operator|.
name|getPath
argument_list|()
operator|+
literal|". Existing file: "
operator|+
name|toRetain
operator|.
name|getPath
argument_list|()
argument_list|)
throw|;
block|}
else|else
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Duplicate taskid file removed: "
operator|+
name|toDelete
operator|.
name|getPath
argument_list|()
operator|+
literal|" with length "
operator|+
name|toDelete
operator|.
name|getLen
argument_list|()
operator|+
literal|". Existing file: "
operator|+
name|toRetain
operator|.
name|getPath
argument_list|()
operator|+
literal|" with length "
operator|+
name|toRetain
operator|.
name|getLen
argument_list|()
argument_list|)
expr_stmt|;
block|}
return|return
name|toRetain
return|;
block|}
specifier|public
specifier|static
name|boolean
name|isCopyFile
parameter_list|(
name|String
name|filename
parameter_list|)
block|{
name|String
name|taskId
init|=
name|filename
decl_stmt|;
name|String
name|copyFileSuffix
init|=
literal|null
decl_stmt|;
name|int
name|dirEnd
init|=
name|filename
operator|.
name|lastIndexOf
argument_list|(
name|Path
operator|.
name|SEPARATOR
argument_list|)
decl_stmt|;
if|if
condition|(
name|dirEnd
operator|!=
operator|-
literal|1
condition|)
block|{
name|taskId
operator|=
name|filename
operator|.
name|substring
argument_list|(
name|dirEnd
operator|+
literal|1
argument_list|)
expr_stmt|;
block|}
name|Matcher
name|m
init|=
name|COPY_FILE_NAME_TO_TASK_ID_REGEX
operator|.
name|matcher
argument_list|(
name|taskId
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|m
operator|.
name|matches
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Unable to verify if file name {} has _copy_ suffix."
argument_list|,
name|filename
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|taskId
operator|=
name|m
operator|.
name|group
argument_list|(
literal|1
argument_list|)
expr_stmt|;
name|copyFileSuffix
operator|=
name|m
operator|.
name|group
argument_list|(
literal|4
argument_list|)
expr_stmt|;
block|}
name|LOG
operator|.
name|debug
argument_list|(
literal|"Filename: {} TaskId: {} CopySuffix: {}"
argument_list|,
name|filename
argument_list|,
name|taskId
argument_list|,
name|copyFileSuffix
argument_list|)
expr_stmt|;
if|if
condition|(
name|taskId
operator|!=
literal|null
operator|&&
name|copyFileSuffix
operator|!=
literal|null
condition|)
block|{
return|return
literal|true
return|;
block|}
return|return
literal|false
return|;
block|}
specifier|public
specifier|static
name|String
name|getBucketFileNameFromPathSubString
parameter_list|(
name|String
name|bucketName
parameter_list|)
block|{
try|try
block|{
return|return
name|bucketName
operator|.
name|split
argument_list|(
name|COPY_KEYWORD
argument_list|)
index|[
literal|0
index|]
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|e
operator|.
name|printStackTrace
argument_list|()
expr_stmt|;
return|return
name|bucketName
return|;
block|}
block|}
comment|/* compute bucket id from from Split */
specifier|public
specifier|static
name|int
name|parseSplitBucket
parameter_list|(
name|InputSplit
name|split
parameter_list|)
block|{
if|if
condition|(
name|split
operator|instanceof
name|FileSplit
condition|)
block|{
return|return
name|getBucketIdFromFile
argument_list|(
operator|(
operator|(
name|FileSplit
operator|)
name|split
operator|)
operator|.
name|getPath
argument_list|()
operator|.
name|getName
argument_list|()
argument_list|)
return|;
block|}
comment|// cannot get this for combined splits
return|return
operator|-
literal|1
return|;
block|}
specifier|public
specifier|static
name|int
name|getBucketIdFromFile
parameter_list|(
name|String
name|bucketName
parameter_list|)
block|{
name|Matcher
name|m
init|=
name|PREFIXED_BUCKET_ID_REGEX
operator|.
name|matcher
argument_list|(
name|bucketName
argument_list|)
decl_stmt|;
if|if
condition|(
name|m
operator|.
name|matches
argument_list|()
condition|)
block|{
if|if
condition|(
name|m
operator|.
name|group
argument_list|(
literal|2
argument_list|)
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
comment|// all zeros
return|return
name|m
operator|.
name|group
argument_list|(
literal|1
argument_list|)
operator|.
name|isEmpty
argument_list|()
condition|?
operator|-
literal|1
else|:
literal|0
return|;
block|}
return|return
name|Integer
operator|.
name|parseInt
argument_list|(
name|m
operator|.
name|group
argument_list|(
literal|2
argument_list|)
argument_list|)
return|;
block|}
comment|// Check to see if the bucketName matches the pattern "bucket_([0-9]+).*"
comment|// This can happen in ACID cases when we have splits on delta files, where the filenames
comment|// are of the form delta_x_y/bucket_a.
if|if
condition|(
name|bucketName
operator|.
name|startsWith
argument_list|(
name|AcidUtils
operator|.
name|BUCKET_PREFIX
argument_list|)
condition|)
block|{
name|m
operator|=
name|AcidUtils
operator|.
name|BUCKET_DIGIT_PATTERN
operator|.
name|matcher
argument_list|(
name|bucketName
argument_list|)
expr_stmt|;
if|if
condition|(
name|m
operator|.
name|find
argument_list|()
condition|)
block|{
return|return
name|Integer
operator|.
name|parseInt
argument_list|(
name|m
operator|.
name|group
argument_list|()
argument_list|)
return|;
block|}
comment|// Note that legacy bucket digit pattern are being ignored here.
block|}
return|return
operator|-
literal|1
return|;
block|}
specifier|public
specifier|static
name|String
name|getNameMessage
parameter_list|(
name|Throwable
name|e
parameter_list|)
block|{
return|return
name|e
operator|.
name|getClass
argument_list|()
operator|.
name|getName
argument_list|()
operator|+
literal|"("
operator|+
name|e
operator|.
name|getMessage
argument_list|()
operator|+
literal|")"
return|;
block|}
specifier|public
specifier|static
name|String
name|getResourceFiles
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|SessionState
operator|.
name|ResourceType
name|t
parameter_list|)
block|{
comment|// fill in local files (includes copy of HDFS files) to be added to the task environment
name|SessionState
name|ss
init|=
name|SessionState
operator|.
name|get
argument_list|()
decl_stmt|;
name|Set
argument_list|<
name|String
argument_list|>
name|files
init|=
operator|(
name|ss
operator|==
literal|null
operator|)
condition|?
literal|null
else|:
name|ss
operator|.
name|list_resource
argument_list|(
name|t
argument_list|,
literal|null
argument_list|)
decl_stmt|;
return|return
name|validateFiles
argument_list|(
name|conf
argument_list|,
name|files
argument_list|)
return|;
block|}
specifier|public
specifier|static
name|String
name|getHdfsResourceFiles
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|SessionState
operator|.
name|ResourceType
name|type
parameter_list|)
block|{
comment|// fill in HDFS files to be added to the task environment
name|SessionState
name|ss
init|=
name|SessionState
operator|.
name|get
argument_list|()
decl_stmt|;
name|Set
argument_list|<
name|String
argument_list|>
name|files
init|=
operator|(
name|ss
operator|==
literal|null
operator|)
condition|?
literal|null
else|:
name|ss
operator|.
name|list_hdfs_resource
argument_list|(
name|type
argument_list|)
decl_stmt|;
return|return
name|validateFiles
argument_list|(
name|conf
argument_list|,
name|files
argument_list|)
return|;
block|}
specifier|public
specifier|static
name|String
name|getLocalResourceFiles
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|SessionState
operator|.
name|ResourceType
name|type
parameter_list|)
block|{
comment|// fill in local only files (excludes copy of HDFS files) to be added to the task environment
name|SessionState
name|ss
init|=
name|SessionState
operator|.
name|get
argument_list|()
decl_stmt|;
name|Set
argument_list|<
name|String
argument_list|>
name|files
init|=
operator|(
name|ss
operator|==
literal|null
operator|)
condition|?
literal|null
else|:
name|ss
operator|.
name|list_local_resource
argument_list|(
name|type
argument_list|)
decl_stmt|;
return|return
name|validateFiles
argument_list|(
name|conf
argument_list|,
name|files
argument_list|)
return|;
block|}
specifier|private
specifier|static
name|String
name|validateFiles
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|Set
argument_list|<
name|String
argument_list|>
name|files
parameter_list|)
block|{
if|if
condition|(
name|files
operator|!=
literal|null
condition|)
block|{
name|List
argument_list|<
name|String
argument_list|>
name|realFiles
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|(
name|files
operator|.
name|size
argument_list|()
argument_list|)
decl_stmt|;
for|for
control|(
name|String
name|one
range|:
name|files
control|)
block|{
try|try
block|{
name|String
name|onefile
init|=
name|realFile
argument_list|(
name|one
argument_list|,
name|conf
argument_list|)
decl_stmt|;
if|if
condition|(
name|onefile
operator|!=
literal|null
condition|)
block|{
name|realFiles
operator|.
name|add
argument_list|(
name|realFile
argument_list|(
name|one
argument_list|,
name|conf
argument_list|)
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"The file {} does not exist."
argument_list|,
name|one
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
literal|"Cannot validate file "
operator|+
name|one
operator|+
literal|"due to exception: "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
return|return
name|StringUtils
operator|.
name|join
argument_list|(
name|realFiles
argument_list|,
literal|","
argument_list|)
return|;
block|}
else|else
block|{
return|return
name|StringUtils
operator|.
name|EMPTY
return|;
block|}
block|}
comment|/**    * get session specified class loader and get current class loader if fall    *    * @return    */
specifier|public
specifier|static
name|ClassLoader
name|getSessionSpecifiedClassLoader
parameter_list|()
block|{
name|SessionState
name|state
init|=
name|SessionState
operator|.
name|get
argument_list|()
decl_stmt|;
if|if
condition|(
name|state
operator|==
literal|null
operator|||
name|state
operator|.
name|getConf
argument_list|()
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Hive Conf not found or Session not initiated, use thread based class loader instead"
argument_list|)
expr_stmt|;
return|return
name|JavaUtils
operator|.
name|getClassLoader
argument_list|()
return|;
block|}
name|ClassLoader
name|sessionCL
init|=
name|state
operator|.
name|getConf
argument_list|()
operator|.
name|getClassLoader
argument_list|()
decl_stmt|;
if|if
condition|(
name|sessionCL
operator|!=
literal|null
condition|)
block|{
name|LOG
operator|.
name|trace
argument_list|(
literal|"Use session specified class loader"
argument_list|)
expr_stmt|;
comment|//it's normal case
return|return
name|sessionCL
return|;
block|}
name|LOG
operator|.
name|debug
argument_list|(
literal|"Session specified class loader not found, use thread based class loader"
argument_list|)
expr_stmt|;
return|return
name|JavaUtils
operator|.
name|getClassLoader
argument_list|()
return|;
block|}
specifier|public
specifier|static
name|void
name|restoreSessionSpecifiedClassLoader
parameter_list|(
name|ClassLoader
name|prev
parameter_list|)
block|{
name|SessionState
name|state
init|=
name|SessionState
operator|.
name|get
argument_list|()
decl_stmt|;
if|if
condition|(
name|state
operator|!=
literal|null
operator|&&
name|state
operator|.
name|getConf
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|ClassLoader
name|current
init|=
name|state
operator|.
name|getConf
argument_list|()
operator|.
name|getClassLoader
argument_list|()
decl_stmt|;
if|if
condition|(
name|current
operator|!=
name|prev
operator|&&
name|JavaUtils
operator|.
name|closeClassLoadersTo
argument_list|(
name|current
argument_list|,
name|prev
argument_list|)
condition|)
block|{
name|Thread
operator|.
name|currentThread
argument_list|()
operator|.
name|setContextClassLoader
argument_list|(
name|prev
argument_list|)
expr_stmt|;
name|state
operator|.
name|getConf
argument_list|()
operator|.
name|setClassLoader
argument_list|(
name|prev
argument_list|)
expr_stmt|;
block|}
block|}
block|}
comment|/**    * Create a URL from a string representing a path to a local file.    * The path string can be just a path, or can start with file:/, file:///    * @param onestr  path string    * @return    */
specifier|private
specifier|static
name|URL
name|urlFromPathString
parameter_list|(
name|String
name|onestr
parameter_list|)
block|{
name|URL
name|oneurl
init|=
literal|null
decl_stmt|;
try|try
block|{
if|if
condition|(
name|StringUtils
operator|.
name|indexOf
argument_list|(
name|onestr
argument_list|,
literal|"file:/"
argument_list|)
operator|==
literal|0
condition|)
block|{
name|oneurl
operator|=
operator|new
name|URL
argument_list|(
name|onestr
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|oneurl
operator|=
operator|new
name|File
argument_list|(
name|onestr
argument_list|)
operator|.
name|toURL
argument_list|()
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|err
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Bad URL {}, ignoring path"
argument_list|,
name|onestr
argument_list|)
expr_stmt|;
block|}
return|return
name|oneurl
return|;
block|}
specifier|private
specifier|static
name|boolean
name|useExistingClassLoader
parameter_list|(
name|ClassLoader
name|cl
parameter_list|)
block|{
if|if
condition|(
operator|!
operator|(
name|cl
operator|instanceof
name|UDFClassLoader
operator|)
condition|)
block|{
comment|// Cannot use the same classloader if it is not an instance of {@code UDFClassLoader}
return|return
literal|false
return|;
block|}
specifier|final
name|UDFClassLoader
name|udfClassLoader
init|=
operator|(
name|UDFClassLoader
operator|)
name|cl
decl_stmt|;
if|if
condition|(
name|udfClassLoader
operator|.
name|isClosed
argument_list|()
condition|)
block|{
comment|// The classloader may have been closed, Cannot add to the same instance
return|return
literal|false
return|;
block|}
return|return
literal|true
return|;
block|}
comment|/**    * Add new elements to the classpath.    *    * @param newPaths    *          Array of classpath elements    */
specifier|public
specifier|static
name|ClassLoader
name|addToClassPath
parameter_list|(
name|ClassLoader
name|cloader
parameter_list|,
name|String
index|[]
name|newPaths
parameter_list|)
block|{
specifier|final
name|URLClassLoader
name|loader
init|=
operator|(
name|URLClassLoader
operator|)
name|cloader
decl_stmt|;
if|if
condition|(
name|useExistingClassLoader
argument_list|(
name|cloader
argument_list|)
condition|)
block|{
specifier|final
name|UDFClassLoader
name|udfClassLoader
init|=
operator|(
name|UDFClassLoader
operator|)
name|loader
decl_stmt|;
for|for
control|(
name|String
name|path
range|:
name|newPaths
control|)
block|{
name|udfClassLoader
operator|.
name|addURL
argument_list|(
name|urlFromPathString
argument_list|(
name|path
argument_list|)
argument_list|)
expr_stmt|;
block|}
return|return
name|udfClassLoader
return|;
block|}
else|else
block|{
return|return
name|createUDFClassLoader
argument_list|(
name|loader
argument_list|,
name|newPaths
argument_list|)
return|;
block|}
block|}
specifier|public
specifier|static
name|ClassLoader
name|createUDFClassLoader
parameter_list|(
name|URLClassLoader
name|loader
parameter_list|,
name|String
index|[]
name|newPaths
parameter_list|)
block|{
specifier|final
name|Set
argument_list|<
name|URL
argument_list|>
name|curPathsSet
init|=
name|Sets
operator|.
name|newHashSet
argument_list|(
name|loader
operator|.
name|getURLs
argument_list|()
argument_list|)
decl_stmt|;
specifier|final
name|List
argument_list|<
name|URL
argument_list|>
name|curPaths
init|=
name|Lists
operator|.
name|newArrayList
argument_list|(
name|curPathsSet
argument_list|)
decl_stmt|;
for|for
control|(
name|String
name|onestr
range|:
name|newPaths
control|)
block|{
specifier|final
name|URL
name|oneurl
init|=
name|urlFromPathString
argument_list|(
name|onestr
argument_list|)
decl_stmt|;
if|if
condition|(
name|oneurl
operator|!=
literal|null
operator|&&
operator|!
name|curPathsSet
operator|.
name|contains
argument_list|(
name|oneurl
argument_list|)
condition|)
block|{
name|curPaths
operator|.
name|add
argument_list|(
name|oneurl
argument_list|)
expr_stmt|;
block|}
block|}
return|return
operator|new
name|UDFClassLoader
argument_list|(
name|curPaths
operator|.
name|toArray
argument_list|(
operator|new
name|URL
index|[
literal|0
index|]
argument_list|)
argument_list|,
name|loader
argument_list|)
return|;
block|}
comment|/**    * remove elements from the classpath.    *    * @param pathsToRemove    *          Array of classpath elements    */
specifier|public
specifier|static
name|void
name|removeFromClassPath
parameter_list|(
name|String
index|[]
name|pathsToRemove
parameter_list|)
throws|throws
name|IOException
block|{
name|Thread
name|curThread
init|=
name|Thread
operator|.
name|currentThread
argument_list|()
decl_stmt|;
name|URLClassLoader
name|loader
init|=
operator|(
name|URLClassLoader
operator|)
name|curThread
operator|.
name|getContextClassLoader
argument_list|()
decl_stmt|;
name|Set
argument_list|<
name|URL
argument_list|>
name|newPath
init|=
operator|new
name|HashSet
argument_list|<
name|URL
argument_list|>
argument_list|(
name|Arrays
operator|.
name|asList
argument_list|(
name|loader
operator|.
name|getURLs
argument_list|()
argument_list|)
argument_list|)
decl_stmt|;
for|for
control|(
name|String
name|onestr
range|:
name|pathsToRemove
control|)
block|{
name|URL
name|oneurl
init|=
name|urlFromPathString
argument_list|(
name|onestr
argument_list|)
decl_stmt|;
if|if
condition|(
name|oneurl
operator|!=
literal|null
condition|)
block|{
name|newPath
operator|.
name|remove
argument_list|(
name|oneurl
argument_list|)
expr_stmt|;
block|}
block|}
name|JavaUtils
operator|.
name|closeClassLoader
argument_list|(
name|loader
argument_list|)
expr_stmt|;
comment|// This loader is closed, remove it from cached registry loaders to avoid removing it again.
name|Registry
name|reg
init|=
name|SessionState
operator|.
name|getRegistry
argument_list|()
decl_stmt|;
if|if
condition|(
name|reg
operator|!=
literal|null
condition|)
block|{
name|reg
operator|.
name|removeFromUDFLoaders
argument_list|(
name|loader
argument_list|)
expr_stmt|;
block|}
name|loader
operator|=
operator|new
name|UDFClassLoader
argument_list|(
name|newPath
operator|.
name|toArray
argument_list|(
operator|new
name|URL
index|[
literal|0
index|]
argument_list|)
argument_list|)
expr_stmt|;
name|curThread
operator|.
name|setContextClassLoader
argument_list|(
name|loader
argument_list|)
expr_stmt|;
name|SessionState
operator|.
name|get
argument_list|()
operator|.
name|getConf
argument_list|()
operator|.
name|setClassLoader
argument_list|(
name|loader
argument_list|)
expr_stmt|;
block|}
specifier|public
specifier|static
name|String
name|formatBinaryString
parameter_list|(
name|byte
index|[]
name|array
parameter_list|,
name|int
name|start
parameter_list|,
name|int
name|length
parameter_list|)
block|{
name|StringBuilder
name|sb
init|=
operator|new
name|StringBuilder
argument_list|()
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
name|start
init|;
name|i
operator|<
name|start
operator|+
name|length
condition|;
name|i
operator|++
control|)
block|{
name|sb
operator|.
name|append
argument_list|(
literal|'x'
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
name|array
index|[
name|i
index|]
operator|<
literal|0
condition|?
name|array
index|[
name|i
index|]
operator|+
literal|256
else|:
name|array
index|[
name|i
index|]
operator|+
literal|0
argument_list|)
expr_stmt|;
block|}
return|return
name|sb
operator|.
name|toString
argument_list|()
return|;
block|}
specifier|public
specifier|static
name|List
argument_list|<
name|String
argument_list|>
name|getColumnNamesFromSortCols
parameter_list|(
name|List
argument_list|<
name|Order
argument_list|>
name|sortCols
parameter_list|)
block|{
name|List
argument_list|<
name|String
argument_list|>
name|names
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|Order
name|o
range|:
name|sortCols
control|)
block|{
name|names
operator|.
name|add
argument_list|(
name|o
operator|.
name|getCol
argument_list|()
argument_list|)
expr_stmt|;
block|}
return|return
name|names
return|;
block|}
specifier|public
specifier|static
name|List
argument_list|<
name|String
argument_list|>
name|getColumnNamesFromFieldSchema
parameter_list|(
name|List
argument_list|<
name|FieldSchema
argument_list|>
name|partCols
parameter_list|)
block|{
name|List
argument_list|<
name|String
argument_list|>
name|names
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|FieldSchema
name|o
range|:
name|partCols
control|)
block|{
name|names
operator|.
name|add
argument_list|(
name|o
operator|.
name|getName
argument_list|()
argument_list|)
expr_stmt|;
block|}
return|return
name|names
return|;
block|}
specifier|public
specifier|static
name|List
argument_list|<
name|String
argument_list|>
name|getInternalColumnNamesFromSignature
parameter_list|(
name|List
argument_list|<
name|ColumnInfo
argument_list|>
name|colInfos
parameter_list|)
block|{
name|List
argument_list|<
name|String
argument_list|>
name|names
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|ColumnInfo
name|ci
range|:
name|colInfos
control|)
block|{
name|names
operator|.
name|add
argument_list|(
name|ci
operator|.
name|getInternalName
argument_list|()
argument_list|)
expr_stmt|;
block|}
return|return
name|names
return|;
block|}
specifier|public
specifier|static
name|List
argument_list|<
name|String
argument_list|>
name|getColumnNames
parameter_list|(
name|Properties
name|props
parameter_list|)
block|{
name|List
argument_list|<
name|String
argument_list|>
name|names
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
name|String
name|colNames
init|=
name|props
operator|.
name|getProperty
argument_list|(
name|serdeConstants
operator|.
name|LIST_COLUMNS
argument_list|)
decl_stmt|;
name|String
index|[]
name|cols
init|=
name|colNames
operator|.
name|trim
argument_list|()
operator|.
name|split
argument_list|(
literal|","
argument_list|)
decl_stmt|;
for|for
control|(
name|String
name|col
range|:
name|cols
control|)
block|{
if|if
condition|(
name|StringUtils
operator|.
name|isNotBlank
argument_list|(
name|col
argument_list|)
condition|)
block|{
name|names
operator|.
name|add
argument_list|(
name|col
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|names
return|;
block|}
specifier|public
specifier|static
name|List
argument_list|<
name|String
argument_list|>
name|getColumnTypes
parameter_list|(
name|Properties
name|props
parameter_list|)
block|{
name|List
argument_list|<
name|String
argument_list|>
name|names
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
name|String
name|colNames
init|=
name|props
operator|.
name|getProperty
argument_list|(
name|serdeConstants
operator|.
name|LIST_COLUMN_TYPES
argument_list|)
decl_stmt|;
name|ArrayList
argument_list|<
name|TypeInfo
argument_list|>
name|cols
init|=
name|TypeInfoUtils
operator|.
name|getTypeInfosFromTypeString
argument_list|(
name|colNames
argument_list|)
decl_stmt|;
for|for
control|(
name|TypeInfo
name|col
range|:
name|cols
control|)
block|{
name|names
operator|.
name|add
argument_list|(
name|col
operator|.
name|getTypeName
argument_list|()
argument_list|)
expr_stmt|;
block|}
return|return
name|names
return|;
block|}
comment|/**    * Extract db and table name from dbtable string, where db and table are separated by "."    * If there is no db name part, set the current sessions default db    * @param dbtable    * @return String array with two elements, first is db name, second is table name    * @throws HiveException    */
specifier|public
specifier|static
name|String
index|[]
name|getDbTableName
parameter_list|(
name|String
name|dbtable
parameter_list|)
throws|throws
name|SemanticException
block|{
return|return
name|getDbTableName
argument_list|(
name|SessionState
operator|.
name|get
argument_list|()
operator|.
name|getCurrentDatabase
argument_list|()
argument_list|,
name|dbtable
argument_list|)
return|;
block|}
specifier|public
specifier|static
name|String
index|[]
name|getDbTableName
parameter_list|(
name|String
name|defaultDb
parameter_list|,
name|String
name|dbtable
parameter_list|)
throws|throws
name|SemanticException
block|{
if|if
condition|(
name|dbtable
operator|==
literal|null
condition|)
block|{
return|return
operator|new
name|String
index|[
literal|2
index|]
return|;
block|}
name|String
index|[]
name|names
init|=
name|dbtable
operator|.
name|split
argument_list|(
literal|"\\."
argument_list|)
decl_stmt|;
switch|switch
condition|(
name|names
operator|.
name|length
condition|)
block|{
case|case
literal|2
case|:
return|return
name|names
return|;
case|case
literal|1
case|:
return|return
operator|new
name|String
index|[]
block|{
name|defaultDb
block|,
name|dbtable
block|}
return|;
default|default:
throw|throw
operator|new
name|SemanticException
argument_list|(
name|ErrorMsg
operator|.
name|INVALID_TABLE_NAME
argument_list|,
name|dbtable
argument_list|)
throw|;
block|}
block|}
comment|/**    * Accepts qualified name which is in the form of dbname.tablename and returns dbname from it    *    * @param dbTableName    * @return dbname    * @throws SemanticException input string is not qualified name    */
specifier|public
specifier|static
name|String
name|getDatabaseName
parameter_list|(
name|String
name|dbTableName
parameter_list|)
throws|throws
name|SemanticException
block|{
name|String
index|[]
name|split
init|=
name|dbTableName
operator|.
name|split
argument_list|(
literal|"\\."
argument_list|)
decl_stmt|;
if|if
condition|(
name|split
operator|.
name|length
operator|!=
literal|2
condition|)
block|{
throw|throw
operator|new
name|SemanticException
argument_list|(
name|ErrorMsg
operator|.
name|INVALID_TABLE_NAME
argument_list|,
name|dbTableName
argument_list|)
throw|;
block|}
return|return
name|split
index|[
literal|0
index|]
return|;
block|}
comment|/**    * Accepts qualified name which is in the form of dbname.tablename and returns tablename from it    *    * @param dbTableName    * @return tablename    * @throws SemanticException input string is not qualified name    */
specifier|public
specifier|static
name|String
name|getTableName
parameter_list|(
name|String
name|dbTableName
parameter_list|)
throws|throws
name|SemanticException
block|{
name|String
index|[]
name|split
init|=
name|dbTableName
operator|.
name|split
argument_list|(
literal|"\\."
argument_list|)
decl_stmt|;
if|if
condition|(
name|split
operator|.
name|length
operator|!=
literal|2
condition|)
block|{
throw|throw
operator|new
name|SemanticException
argument_list|(
name|ErrorMsg
operator|.
name|INVALID_TABLE_NAME
argument_list|,
name|dbTableName
argument_list|)
throw|;
block|}
return|return
name|split
index|[
literal|1
index|]
return|;
block|}
specifier|public
specifier|static
name|void
name|validateColumnNames
parameter_list|(
name|List
argument_list|<
name|String
argument_list|>
name|colNames
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|checkCols
parameter_list|)
throws|throws
name|SemanticException
block|{
name|Iterator
argument_list|<
name|String
argument_list|>
name|checkColsIter
init|=
name|checkCols
operator|.
name|iterator
argument_list|()
decl_stmt|;
while|while
condition|(
name|checkColsIter
operator|.
name|hasNext
argument_list|()
condition|)
block|{
name|String
name|toCheck
init|=
name|checkColsIter
operator|.
name|next
argument_list|()
decl_stmt|;
name|boolean
name|found
init|=
literal|false
decl_stmt|;
name|Iterator
argument_list|<
name|String
argument_list|>
name|colNamesIter
init|=
name|colNames
operator|.
name|iterator
argument_list|()
decl_stmt|;
while|while
condition|(
name|colNamesIter
operator|.
name|hasNext
argument_list|()
condition|)
block|{
name|String
name|colName
init|=
name|colNamesIter
operator|.
name|next
argument_list|()
decl_stmt|;
if|if
condition|(
name|toCheck
operator|.
name|equalsIgnoreCase
argument_list|(
name|colName
argument_list|)
condition|)
block|{
name|found
operator|=
literal|true
expr_stmt|;
break|break;
block|}
block|}
if|if
condition|(
operator|!
name|found
condition|)
block|{
throw|throw
operator|new
name|SemanticException
argument_list|(
name|ErrorMsg
operator|.
name|INVALID_COLUMN
operator|.
name|getMsg
argument_list|()
argument_list|)
throw|;
block|}
block|}
block|}
comment|/**    * Gets the default notification interval to send progress updates to the tracker. Useful for    * operators that may not output data for a while.    *    * @param hconf    * @return the interval in milliseconds    */
specifier|public
specifier|static
name|int
name|getDefaultNotificationInterval
parameter_list|(
name|Configuration
name|hconf
parameter_list|)
block|{
name|int
name|notificationInterval
decl_stmt|;
name|Integer
name|expInterval
init|=
name|Integer
operator|.
name|decode
argument_list|(
name|hconf
operator|.
name|get
argument_list|(
literal|"mapred.tasktracker.expiry.interval"
argument_list|)
argument_list|)
decl_stmt|;
if|if
condition|(
name|expInterval
operator|!=
literal|null
condition|)
block|{
name|notificationInterval
operator|=
name|expInterval
operator|.
name|intValue
argument_list|()
operator|/
literal|2
expr_stmt|;
block|}
else|else
block|{
comment|// 5 minutes
name|notificationInterval
operator|=
literal|5
operator|*
literal|60
operator|*
literal|1000
expr_stmt|;
block|}
return|return
name|notificationInterval
return|;
block|}
comment|/**    * Copies the storage handler properties configured for a table descriptor to a runtime job    * configuration.    *    * @param tbl    *          table descriptor from which to read    *    * @param job    *          configuration which receives configured properties    */
specifier|public
specifier|static
name|void
name|copyTableJobPropertiesToConf
parameter_list|(
name|TableDesc
name|tbl
parameter_list|,
name|JobConf
name|job
parameter_list|)
throws|throws
name|HiveException
block|{
name|Properties
name|tblProperties
init|=
name|tbl
operator|.
name|getProperties
argument_list|()
decl_stmt|;
for|for
control|(
name|String
name|name
range|:
name|tblProperties
operator|.
name|stringPropertyNames
argument_list|()
control|)
block|{
if|if
condition|(
name|job
operator|.
name|get
argument_list|(
name|name
argument_list|)
operator|==
literal|null
condition|)
block|{
name|String
name|val
init|=
operator|(
name|String
operator|)
name|tblProperties
operator|.
name|get
argument_list|(
name|name
argument_list|)
decl_stmt|;
if|if
condition|(
name|val
operator|!=
literal|null
condition|)
block|{
name|job
operator|.
name|set
argument_list|(
name|name
argument_list|,
name|StringEscapeUtils
operator|.
name|escapeJava
argument_list|(
name|val
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
block|}
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|jobProperties
init|=
name|tbl
operator|.
name|getJobProperties
argument_list|()
decl_stmt|;
if|if
condition|(
name|jobProperties
operator|!=
literal|null
condition|)
block|{
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|entry
range|:
name|jobProperties
operator|.
name|entrySet
argument_list|()
control|)
block|{
name|job
operator|.
name|set
argument_list|(
name|entry
operator|.
name|getKey
argument_list|()
argument_list|,
name|entry
operator|.
name|getValue
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
try|try
block|{
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|jobSecrets
init|=
name|tbl
operator|.
name|getJobSecrets
argument_list|()
decl_stmt|;
if|if
condition|(
name|jobSecrets
operator|!=
literal|null
condition|)
block|{
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|entry
range|:
name|jobSecrets
operator|.
name|entrySet
argument_list|()
control|)
block|{
name|job
operator|.
name|getCredentials
argument_list|()
operator|.
name|addSecretKey
argument_list|(
operator|new
name|Text
argument_list|(
name|entry
operator|.
name|getKey
argument_list|()
argument_list|)
argument_list|,
name|entry
operator|.
name|getValue
argument_list|()
operator|.
name|getBytes
argument_list|()
argument_list|)
expr_stmt|;
name|UserGroupInformation
operator|.
name|getCurrentUser
argument_list|()
operator|.
name|getCredentials
argument_list|()
operator|.
name|addSecretKey
argument_list|(
operator|new
name|Text
argument_list|(
name|entry
operator|.
name|getKey
argument_list|()
argument_list|)
argument_list|,
name|entry
operator|.
name|getValue
argument_list|()
operator|.
name|getBytes
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    * Copies the storage handler proeprites configured for a table descriptor to a runtime job    * configuration.  This differs from {@link #copyTablePropertiesToConf(org.apache.hadoop.hive.ql.plan.TableDesc, org.apache.hadoop.mapred.JobConf)}    * in that it does not allow parameters already set in the job to override the values from the    * table.  This is important for setting the config up for reading,    * as the job may already have values in it from another table.    * @param tbl    * @param job    */
specifier|public
specifier|static
name|void
name|copyTablePropertiesToConf
parameter_list|(
name|TableDesc
name|tbl
parameter_list|,
name|JobConf
name|job
parameter_list|)
throws|throws
name|HiveException
block|{
name|Properties
name|tblProperties
init|=
name|tbl
operator|.
name|getProperties
argument_list|()
decl_stmt|;
for|for
control|(
name|String
name|name
range|:
name|tblProperties
operator|.
name|stringPropertyNames
argument_list|()
control|)
block|{
name|String
name|val
init|=
operator|(
name|String
operator|)
name|tblProperties
operator|.
name|get
argument_list|(
name|name
argument_list|)
decl_stmt|;
if|if
condition|(
name|val
operator|!=
literal|null
condition|)
block|{
name|job
operator|.
name|set
argument_list|(
name|name
argument_list|,
name|StringEscapeUtils
operator|.
name|escapeJava
argument_list|(
name|val
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|jobProperties
init|=
name|tbl
operator|.
name|getJobProperties
argument_list|()
decl_stmt|;
if|if
condition|(
name|jobProperties
operator|!=
literal|null
condition|)
block|{
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|entry
range|:
name|jobProperties
operator|.
name|entrySet
argument_list|()
control|)
block|{
name|job
operator|.
name|set
argument_list|(
name|entry
operator|.
name|getKey
argument_list|()
argument_list|,
name|entry
operator|.
name|getValue
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
try|try
block|{
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|jobSecrets
init|=
name|tbl
operator|.
name|getJobSecrets
argument_list|()
decl_stmt|;
if|if
condition|(
name|jobSecrets
operator|!=
literal|null
condition|)
block|{
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|entry
range|:
name|jobSecrets
operator|.
name|entrySet
argument_list|()
control|)
block|{
name|job
operator|.
name|getCredentials
argument_list|()
operator|.
name|addSecretKey
argument_list|(
operator|new
name|Text
argument_list|(
name|entry
operator|.
name|getKey
argument_list|()
argument_list|)
argument_list|,
name|entry
operator|.
name|getValue
argument_list|()
operator|.
name|getBytes
argument_list|()
argument_list|)
expr_stmt|;
name|UserGroupInformation
operator|.
name|getCurrentUser
argument_list|()
operator|.
name|getCredentials
argument_list|()
operator|.
name|addSecretKey
argument_list|(
operator|new
name|Text
argument_list|(
name|entry
operator|.
name|getKey
argument_list|()
argument_list|)
argument_list|,
name|entry
operator|.
name|getValue
argument_list|()
operator|.
name|getBytes
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|private
specifier|static
specifier|final
name|Object
name|INPUT_SUMMARY_LOCK
init|=
operator|new
name|Object
argument_list|()
decl_stmt|;
comment|/**    * Returns the maximum number of executors required to get file information from several input locations.    * It checks whether HIVE_EXEC_INPUT_LISTING_MAX_THREADS or DEPRECATED_MAPRED_DFSCLIENT_PARALLELISM_MAX are> 1    *    * @param conf Configuration object to get the maximum number of threads.    * @param inputLocationListSize Number of input locations required to process.    * @return The maximum number of executors to use.    */
annotation|@
name|VisibleForTesting
specifier|static
name|int
name|getMaxExecutorsForInputListing
parameter_list|(
specifier|final
name|Configuration
name|conf
parameter_list|,
name|int
name|inputLocationListSize
parameter_list|)
block|{
if|if
condition|(
name|inputLocationListSize
operator|<
literal|1
condition|)
block|{
return|return
literal|0
return|;
block|}
name|int
name|maxExecutors
init|=
literal|1
decl_stmt|;
if|if
condition|(
name|inputLocationListSize
operator|>
literal|1
condition|)
block|{
name|int
name|listingMaxThreads
init|=
name|HiveConf
operator|.
name|getIntVar
argument_list|(
name|conf
argument_list|,
name|ConfVars
operator|.
name|HIVE_EXEC_INPUT_LISTING_MAX_THREADS
argument_list|)
decl_stmt|;
comment|// DEPRECATED_MAPRED_DFSCLIENT_PARALLELISM_MAX must be removed on next Hive version (probably on 3.0).
comment|// If HIVE_EXEC_INPUT_LISTING_MAX_THREADS is not set, then we check of the deprecated configuration.
if|if
condition|(
name|listingMaxThreads
operator|<=
literal|0
condition|)
block|{
name|listingMaxThreads
operator|=
name|conf
operator|.
name|getInt
argument_list|(
name|DEPRECATED_MAPRED_DFSCLIENT_PARALLELISM_MAX
argument_list|,
literal|0
argument_list|)
expr_stmt|;
if|if
condition|(
name|listingMaxThreads
operator|>
literal|0
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Deprecated configuration is used: {}. Please use {}"
argument_list|,
name|DEPRECATED_MAPRED_DFSCLIENT_PARALLELISM_MAX
argument_list|,
name|ConfVars
operator|.
name|HIVE_EXEC_INPUT_LISTING_MAX_THREADS
operator|.
name|varname
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
name|listingMaxThreads
operator|>
literal|1
condition|)
block|{
name|maxExecutors
operator|=
name|Math
operator|.
name|min
argument_list|(
name|inputLocationListSize
argument_list|,
name|listingMaxThreads
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|maxExecutors
return|;
block|}
comment|/**    * Calculate the total size of input files.    *    * @param ctx    *          the hadoop job context    * @param work    *          map reduce job plan    * @param filter    *          filter to apply to the input paths before calculating size    * @return the summary of all the input paths.    * @throws IOException    */
specifier|public
specifier|static
name|ContentSummary
name|getInputSummary
parameter_list|(
specifier|final
name|Context
name|ctx
parameter_list|,
name|MapWork
name|work
parameter_list|,
name|PathFilter
name|filter
parameter_list|)
throws|throws
name|IOException
block|{
name|PerfLogger
name|perfLogger
init|=
name|SessionState
operator|.
name|getPerfLogger
argument_list|()
decl_stmt|;
name|perfLogger
operator|.
name|PerfLogBegin
argument_list|(
name|CLASS_NAME
argument_list|,
name|PerfLogger
operator|.
name|INPUT_SUMMARY
argument_list|)
expr_stmt|;
name|long
index|[]
name|summary
init|=
block|{
literal|0
block|,
literal|0
block|,
literal|0
block|}
decl_stmt|;
specifier|final
name|Set
argument_list|<
name|Path
argument_list|>
name|pathNeedProcess
init|=
operator|new
name|HashSet
argument_list|<>
argument_list|()
decl_stmt|;
comment|// Since multiple threads could call this method concurrently, locking
comment|// this method will avoid number of threads out of control.
synchronized|synchronized
init|(
name|INPUT_SUMMARY_LOCK
init|)
block|{
comment|// For each input path, calculate the total size.
for|for
control|(
name|Path
name|path
range|:
name|work
operator|.
name|getPathToAliases
argument_list|()
operator|.
name|keySet
argument_list|()
control|)
block|{
name|Path
name|p
init|=
name|path
decl_stmt|;
if|if
condition|(
name|filter
operator|!=
literal|null
operator|&&
operator|!
name|filter
operator|.
name|accept
argument_list|(
name|p
argument_list|)
condition|)
block|{
continue|continue;
block|}
name|ContentSummary
name|cs
init|=
name|ctx
operator|.
name|getCS
argument_list|(
name|path
argument_list|)
decl_stmt|;
if|if
condition|(
name|cs
operator|==
literal|null
condition|)
block|{
if|if
condition|(
name|path
operator|==
literal|null
condition|)
block|{
continue|continue;
block|}
name|pathNeedProcess
operator|.
name|add
argument_list|(
name|path
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|summary
index|[
literal|0
index|]
operator|+=
name|cs
operator|.
name|getLength
argument_list|()
expr_stmt|;
name|summary
index|[
literal|1
index|]
operator|+=
name|cs
operator|.
name|getFileCount
argument_list|()
expr_stmt|;
name|summary
index|[
literal|2
index|]
operator|+=
name|cs
operator|.
name|getDirectoryCount
argument_list|()
expr_stmt|;
block|}
block|}
comment|// Process the case when name node call is needed
specifier|final
name|Map
argument_list|<
name|String
argument_list|,
name|ContentSummary
argument_list|>
name|resultMap
init|=
operator|new
name|ConcurrentHashMap
argument_list|<
name|String
argument_list|,
name|ContentSummary
argument_list|>
argument_list|()
decl_stmt|;
specifier|final
name|ExecutorService
name|executor
decl_stmt|;
name|int
name|numExecutors
init|=
name|getMaxExecutorsForInputListing
argument_list|(
name|ctx
operator|.
name|getConf
argument_list|()
argument_list|,
name|pathNeedProcess
operator|.
name|size
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|numExecutors
operator|>
literal|1
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Using {} threads for getContentSummary"
argument_list|,
name|numExecutors
argument_list|)
expr_stmt|;
name|executor
operator|=
name|Executors
operator|.
name|newFixedThreadPool
argument_list|(
name|numExecutors
argument_list|,
operator|new
name|ThreadFactoryBuilder
argument_list|()
operator|.
name|setDaemon
argument_list|(
literal|true
argument_list|)
operator|.
name|setNameFormat
argument_list|(
literal|"Get-Input-Summary-%d"
argument_list|)
operator|.
name|build
argument_list|()
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|executor
operator|=
literal|null
expr_stmt|;
block|}
name|ContentSummary
name|cs
init|=
name|getInputSummaryWithPool
argument_list|(
name|ctx
argument_list|,
name|pathNeedProcess
argument_list|,
name|work
argument_list|,
name|summary
argument_list|,
name|executor
argument_list|)
decl_stmt|;
name|perfLogger
operator|.
name|PerfLogEnd
argument_list|(
name|CLASS_NAME
argument_list|,
name|PerfLogger
operator|.
name|INPUT_SUMMARY
argument_list|)
expr_stmt|;
return|return
name|cs
return|;
block|}
block|}
annotation|@
name|VisibleForTesting
specifier|static
name|ContentSummary
name|getInputSummaryWithPool
parameter_list|(
specifier|final
name|Context
name|ctx
parameter_list|,
name|Set
argument_list|<
name|Path
argument_list|>
name|pathNeedProcess
parameter_list|,
name|MapWork
name|work
parameter_list|,
name|long
index|[]
name|summary
parameter_list|,
name|ExecutorService
name|executor
parameter_list|)
throws|throws
name|IOException
block|{
name|List
argument_list|<
name|Future
argument_list|<
name|?
argument_list|>
argument_list|>
name|results
init|=
operator|new
name|ArrayList
argument_list|<
name|Future
argument_list|<
name|?
argument_list|>
argument_list|>
argument_list|()
decl_stmt|;
specifier|final
name|Map
argument_list|<
name|String
argument_list|,
name|ContentSummary
argument_list|>
name|resultMap
init|=
operator|new
name|ConcurrentHashMap
argument_list|<
name|String
argument_list|,
name|ContentSummary
argument_list|>
argument_list|()
decl_stmt|;
name|HiveInterruptCallback
name|interrup
init|=
name|HiveInterruptUtils
operator|.
name|add
argument_list|(
operator|new
name|HiveInterruptCallback
argument_list|()
block|{
annotation|@
name|Override
specifier|public
name|void
name|interrupt
parameter_list|()
block|{
for|for
control|(
name|Path
name|path
range|:
name|pathNeedProcess
control|)
block|{
try|try
block|{
name|path
operator|.
name|getFileSystem
argument_list|(
name|ctx
operator|.
name|getConf
argument_list|()
argument_list|)
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|ignore
parameter_list|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Failed to close filesystem"
argument_list|,
name|ignore
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
name|executor
operator|!=
literal|null
condition|)
block|{
name|executor
operator|.
name|shutdownNow
argument_list|()
expr_stmt|;
block|}
block|}
block|}
argument_list|)
decl_stmt|;
try|try
block|{
name|Configuration
name|conf
init|=
name|ctx
operator|.
name|getConf
argument_list|()
decl_stmt|;
name|JobConf
name|jobConf
init|=
operator|new
name|JobConf
argument_list|(
name|conf
argument_list|)
decl_stmt|;
for|for
control|(
name|Path
name|path
range|:
name|pathNeedProcess
control|)
block|{
specifier|final
name|Path
name|p
init|=
name|path
decl_stmt|;
specifier|final
name|String
name|pathStr
init|=
name|path
operator|.
name|toString
argument_list|()
decl_stmt|;
comment|// All threads share the same Configuration and JobConf based on the
comment|// assumption that they are thread safe if only read operations are
comment|// executed. It is not stated in Hadoop's javadoc, the sourcce codes
comment|// clearly showed that they made efforts for it and we believe it is
comment|// thread safe. Will revisit this piece of codes if we find the assumption
comment|// is not correct.
specifier|final
name|Configuration
name|myConf
init|=
name|conf
decl_stmt|;
specifier|final
name|JobConf
name|myJobConf
init|=
name|jobConf
decl_stmt|;
specifier|final
name|Map
argument_list|<
name|String
argument_list|,
name|Operator
argument_list|<
name|?
argument_list|>
argument_list|>
name|aliasToWork
init|=
name|work
operator|.
name|getAliasToWork
argument_list|()
decl_stmt|;
specifier|final
name|Map
argument_list|<
name|Path
argument_list|,
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|>
name|pathToAlias
init|=
name|work
operator|.
name|getPathToAliases
argument_list|()
decl_stmt|;
specifier|final
name|PartitionDesc
name|partDesc
init|=
name|work
operator|.
name|getPathToPartitionInfo
argument_list|()
operator|.
name|get
argument_list|(
name|p
argument_list|)
decl_stmt|;
name|Runnable
name|r
init|=
operator|new
name|Runnable
argument_list|()
block|{
annotation|@
name|Override
specifier|public
name|void
name|run
parameter_list|()
block|{
try|try
block|{
name|Class
argument_list|<
name|?
extends|extends
name|InputFormat
argument_list|>
name|inputFormatCls
init|=
name|partDesc
operator|.
name|getInputFileFormatClass
argument_list|()
decl_stmt|;
name|InputFormat
name|inputFormatObj
init|=
name|HiveInputFormat
operator|.
name|getInputFormatFromCache
argument_list|(
name|inputFormatCls
argument_list|,
name|myJobConf
argument_list|)
decl_stmt|;
if|if
condition|(
name|inputFormatObj
operator|instanceof
name|ContentSummaryInputFormat
condition|)
block|{
name|ContentSummaryInputFormat
name|cs
init|=
operator|(
name|ContentSummaryInputFormat
operator|)
name|inputFormatObj
decl_stmt|;
name|resultMap
operator|.
name|put
argument_list|(
name|pathStr
argument_list|,
name|cs
operator|.
name|getContentSummary
argument_list|(
name|p
argument_list|,
name|myJobConf
argument_list|)
argument_list|)
expr_stmt|;
return|return;
block|}
name|String
name|metaTableStorage
init|=
literal|null
decl_stmt|;
if|if
condition|(
name|partDesc
operator|.
name|getTableDesc
argument_list|()
operator|!=
literal|null
operator|&&
name|partDesc
operator|.
name|getTableDesc
argument_list|()
operator|.
name|getProperties
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|metaTableStorage
operator|=
name|partDesc
operator|.
name|getTableDesc
argument_list|()
operator|.
name|getProperties
argument_list|()
operator|.
name|getProperty
argument_list|(
name|hive_metastoreConstants
operator|.
name|META_TABLE_STORAGE
argument_list|,
literal|null
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|partDesc
operator|.
name|getProperties
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|metaTableStorage
operator|=
name|partDesc
operator|.
name|getProperties
argument_list|()
operator|.
name|getProperty
argument_list|(
name|hive_metastoreConstants
operator|.
name|META_TABLE_STORAGE
argument_list|,
name|metaTableStorage
argument_list|)
expr_stmt|;
block|}
name|HiveStorageHandler
name|handler
init|=
name|HiveUtils
operator|.
name|getStorageHandler
argument_list|(
name|myConf
argument_list|,
name|metaTableStorage
argument_list|)
decl_stmt|;
if|if
condition|(
name|handler
operator|instanceof
name|InputEstimator
condition|)
block|{
name|long
name|total
init|=
literal|0
decl_stmt|;
name|TableDesc
name|tableDesc
init|=
name|partDesc
operator|.
name|getTableDesc
argument_list|()
decl_stmt|;
name|InputEstimator
name|estimator
init|=
operator|(
name|InputEstimator
operator|)
name|handler
decl_stmt|;
for|for
control|(
name|String
name|alias
range|:
name|HiveFileFormatUtils
operator|.
name|doGetAliasesFromPath
argument_list|(
name|pathToAlias
argument_list|,
name|p
argument_list|)
control|)
block|{
name|JobConf
name|jobConf
init|=
operator|new
name|JobConf
argument_list|(
name|myJobConf
argument_list|)
decl_stmt|;
name|TableScanOperator
name|scanOp
init|=
operator|(
name|TableScanOperator
operator|)
name|aliasToWork
operator|.
name|get
argument_list|(
name|alias
argument_list|)
decl_stmt|;
name|Utilities
operator|.
name|setColumnNameList
argument_list|(
name|jobConf
argument_list|,
name|scanOp
argument_list|,
literal|true
argument_list|)
expr_stmt|;
name|Utilities
operator|.
name|setColumnTypeList
argument_list|(
name|jobConf
argument_list|,
name|scanOp
argument_list|,
literal|true
argument_list|)
expr_stmt|;
name|PlanUtils
operator|.
name|configureInputJobPropertiesForStorageHandler
argument_list|(
name|tableDesc
argument_list|)
expr_stmt|;
name|Utilities
operator|.
name|copyTableJobPropertiesToConf
argument_list|(
name|tableDesc
argument_list|,
name|jobConf
argument_list|)
expr_stmt|;
name|total
operator|+=
name|estimator
operator|.
name|estimate
argument_list|(
name|jobConf
argument_list|,
name|scanOp
argument_list|,
operator|-
literal|1
argument_list|)
operator|.
name|getTotalLength
argument_list|()
expr_stmt|;
block|}
name|resultMap
operator|.
name|put
argument_list|(
name|pathStr
argument_list|,
operator|new
name|ContentSummary
argument_list|(
name|total
argument_list|,
operator|-
literal|1
argument_list|,
operator|-
literal|1
argument_list|)
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// todo: should nullify summary for non-native tables,
comment|// not to be selected as a mapjoin target
name|FileSystem
name|fs
init|=
name|p
operator|.
name|getFileSystem
argument_list|(
name|myConf
argument_list|)
decl_stmt|;
name|resultMap
operator|.
name|put
argument_list|(
name|pathStr
argument_list|,
name|fs
operator|.
name|getContentSummary
argument_list|(
name|p
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
comment|// We safely ignore this exception for summary data.
comment|// We don't update the cache to protect it from polluting other
comment|// usages. The worst case is that IOException will always be
comment|// retried for another getInputSummary(), which is fine as
comment|// IOException is not considered as a common case.
name|LOG
operator|.
name|info
argument_list|(
literal|"Cannot get size of {}. Safely ignored."
argument_list|,
name|pathStr
argument_list|)
expr_stmt|;
block|}
block|}
block|}
decl_stmt|;
if|if
condition|(
name|executor
operator|==
literal|null
condition|)
block|{
name|r
operator|.
name|run
argument_list|()
expr_stmt|;
block|}
else|else
block|{
name|Future
argument_list|<
name|?
argument_list|>
name|result
init|=
name|executor
operator|.
name|submit
argument_list|(
name|r
argument_list|)
decl_stmt|;
name|results
operator|.
name|add
argument_list|(
name|result
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
name|executor
operator|!=
literal|null
condition|)
block|{
for|for
control|(
name|Future
argument_list|<
name|?
argument_list|>
name|result
range|:
name|results
control|)
block|{
name|boolean
name|executorDone
init|=
literal|false
decl_stmt|;
do|do
block|{
try|try
block|{
name|result
operator|.
name|get
argument_list|()
expr_stmt|;
name|executorDone
operator|=
literal|true
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|InterruptedException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Interrupted when waiting threads: "
argument_list|,
name|e
argument_list|)
expr_stmt|;
name|Thread
operator|.
name|currentThread
argument_list|()
operator|.
name|interrupt
argument_list|()
expr_stmt|;
break|break;
block|}
catch|catch
parameter_list|(
name|ExecutionException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
do|while
condition|(
operator|!
name|executorDone
condition|)
do|;
block|}
name|executor
operator|.
name|shutdown
argument_list|()
expr_stmt|;
block|}
name|HiveInterruptUtils
operator|.
name|checkInterrupted
argument_list|()
expr_stmt|;
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|String
argument_list|,
name|ContentSummary
argument_list|>
name|entry
range|:
name|resultMap
operator|.
name|entrySet
argument_list|()
control|)
block|{
name|ContentSummary
name|cs
init|=
name|entry
operator|.
name|getValue
argument_list|()
decl_stmt|;
name|summary
index|[
literal|0
index|]
operator|+=
name|cs
operator|.
name|getLength
argument_list|()
expr_stmt|;
name|summary
index|[
literal|1
index|]
operator|+=
name|cs
operator|.
name|getFileCount
argument_list|()
expr_stmt|;
name|summary
index|[
literal|2
index|]
operator|+=
name|cs
operator|.
name|getDirectoryCount
argument_list|()
expr_stmt|;
name|ctx
operator|.
name|addCS
argument_list|(
name|entry
operator|.
name|getKey
argument_list|()
argument_list|,
name|cs
argument_list|)
expr_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isInfoEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Cache Content Summary for {} length: {} file count: {} "
operator|+
literal|" directory count: {}"
argument_list|,
name|entry
operator|.
name|getKey
argument_list|()
argument_list|,
name|cs
operator|.
name|getLength
argument_list|()
argument_list|,
name|cs
operator|.
name|getFileCount
argument_list|()
argument_list|,
name|cs
operator|.
name|getDirectoryCount
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
return|return
operator|new
name|ContentSummary
argument_list|(
name|summary
index|[
literal|0
index|]
argument_list|,
name|summary
index|[
literal|1
index|]
argument_list|,
name|summary
index|[
literal|2
index|]
argument_list|)
return|;
block|}
finally|finally
block|{
if|if
condition|(
name|executor
operator|!=
literal|null
condition|)
block|{
name|executor
operator|.
name|shutdownNow
argument_list|()
expr_stmt|;
block|}
name|HiveInterruptUtils
operator|.
name|remove
argument_list|(
name|interrup
argument_list|)
expr_stmt|;
block|}
block|}
specifier|public
specifier|static
name|long
name|sumOf
parameter_list|(
name|Map
argument_list|<
name|String
argument_list|,
name|Long
argument_list|>
name|aliasToSize
parameter_list|,
name|Set
argument_list|<
name|String
argument_list|>
name|aliases
parameter_list|)
block|{
return|return
name|sumOfExcept
argument_list|(
name|aliasToSize
argument_list|,
name|aliases
argument_list|,
literal|null
argument_list|)
return|;
block|}
comment|// return sum of lengths except some aliases. returns -1 if any of other alias is unknown
specifier|public
specifier|static
name|long
name|sumOfExcept
parameter_list|(
name|Map
argument_list|<
name|String
argument_list|,
name|Long
argument_list|>
name|aliasToSize
parameter_list|,
name|Set
argument_list|<
name|String
argument_list|>
name|aliases
parameter_list|,
name|Set
argument_list|<
name|String
argument_list|>
name|excepts
parameter_list|)
block|{
name|long
name|total
init|=
literal|0
decl_stmt|;
for|for
control|(
name|String
name|alias
range|:
name|aliases
control|)
block|{
if|if
condition|(
name|excepts
operator|!=
literal|null
operator|&&
name|excepts
operator|.
name|contains
argument_list|(
name|alias
argument_list|)
condition|)
block|{
continue|continue;
block|}
name|Long
name|size
init|=
name|aliasToSize
operator|.
name|get
argument_list|(
name|alias
argument_list|)
decl_stmt|;
if|if
condition|(
name|size
operator|==
literal|null
condition|)
block|{
return|return
operator|-
literal|1
return|;
block|}
name|total
operator|+=
name|size
expr_stmt|;
block|}
return|return
name|total
return|;
block|}
specifier|public
specifier|static
name|boolean
name|isEmptyPath
parameter_list|(
name|JobConf
name|job
parameter_list|,
name|Path
name|dirPath
parameter_list|,
name|Context
name|ctx
parameter_list|)
throws|throws
name|Exception
block|{
if|if
condition|(
name|ctx
operator|!=
literal|null
condition|)
block|{
name|ContentSummary
name|cs
init|=
name|ctx
operator|.
name|getCS
argument_list|(
name|dirPath
argument_list|)
decl_stmt|;
if|if
condition|(
name|cs
operator|!=
literal|null
condition|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isInfoEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Content Summary {} length: {} num files: {}"
operator|+
literal|" num directories: {}"
argument_list|,
name|dirPath
argument_list|,
name|cs
operator|.
name|getLength
argument_list|()
argument_list|,
name|cs
operator|.
name|getFileCount
argument_list|()
argument_list|,
name|cs
operator|.
name|getDirectoryCount
argument_list|()
argument_list|)
expr_stmt|;
block|}
return|return
operator|(
name|cs
operator|.
name|getLength
argument_list|()
operator|==
literal|0
operator|&&
name|cs
operator|.
name|getFileCount
argument_list|()
operator|==
literal|0
operator|&&
name|cs
operator|.
name|getDirectoryCount
argument_list|()
operator|<=
literal|1
operator|)
return|;
block|}
else|else
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Content Summary not cached for {}"
argument_list|,
name|dirPath
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|isEmptyPath
argument_list|(
name|job
argument_list|,
name|dirPath
argument_list|)
return|;
block|}
specifier|public
specifier|static
name|boolean
name|isEmptyPath
parameter_list|(
name|Configuration
name|job
parameter_list|,
name|Path
name|dirPath
parameter_list|)
throws|throws
name|IOException
block|{
name|FileSystem
name|inpFs
init|=
name|dirPath
operator|.
name|getFileSystem
argument_list|(
name|job
argument_list|)
decl_stmt|;
try|try
block|{
name|FileStatus
index|[]
name|fStats
init|=
name|inpFs
operator|.
name|listStatus
argument_list|(
name|dirPath
argument_list|,
name|FileUtils
operator|.
name|HIDDEN_FILES_PATH_FILTER
argument_list|)
decl_stmt|;
if|if
condition|(
name|fStats
operator|.
name|length
operator|>
literal|0
condition|)
block|{
return|return
literal|false
return|;
block|}
block|}
catch|catch
parameter_list|(
name|FileNotFoundException
name|fnf
parameter_list|)
block|{
return|return
literal|true
return|;
block|}
return|return
literal|true
return|;
block|}
specifier|public
specifier|static
name|List
argument_list|<
name|TezTask
argument_list|>
name|getTezTasks
parameter_list|(
name|List
argument_list|<
name|Task
argument_list|<
name|?
extends|extends
name|Serializable
argument_list|>
argument_list|>
name|tasks
parameter_list|)
block|{
return|return
name|getTasks
argument_list|(
name|tasks
argument_list|,
operator|new
name|TaskFilterFunction
argument_list|<>
argument_list|(
name|TezTask
operator|.
name|class
argument_list|)
argument_list|)
return|;
block|}
specifier|public
specifier|static
name|List
argument_list|<
name|SparkTask
argument_list|>
name|getSparkTasks
parameter_list|(
name|List
argument_list|<
name|Task
argument_list|<
name|?
extends|extends
name|Serializable
argument_list|>
argument_list|>
name|tasks
parameter_list|)
block|{
return|return
name|getTasks
argument_list|(
name|tasks
argument_list|,
operator|new
name|TaskFilterFunction
argument_list|<>
argument_list|(
name|SparkTask
operator|.
name|class
argument_list|)
argument_list|)
return|;
block|}
specifier|public
specifier|static
name|List
argument_list|<
name|ExecDriver
argument_list|>
name|getMRTasks
parameter_list|(
name|List
argument_list|<
name|Task
argument_list|<
name|?
extends|extends
name|Serializable
argument_list|>
argument_list|>
name|tasks
parameter_list|)
block|{
return|return
name|getTasks
argument_list|(
name|tasks
argument_list|,
operator|new
name|TaskFilterFunction
argument_list|<>
argument_list|(
name|ExecDriver
operator|.
name|class
argument_list|)
argument_list|)
return|;
block|}
specifier|static
class|class
name|TaskFilterFunction
parameter_list|<
name|T
parameter_list|>
implements|implements
name|DAGTraversal
operator|.
name|Function
block|{
specifier|private
name|Set
argument_list|<
name|Task
argument_list|<
name|?
extends|extends
name|Serializable
argument_list|>
argument_list|>
name|visited
init|=
operator|new
name|HashSet
argument_list|<>
argument_list|()
decl_stmt|;
specifier|private
name|Class
argument_list|<
name|T
argument_list|>
name|requiredType
decl_stmt|;
specifier|private
name|List
argument_list|<
name|T
argument_list|>
name|typeSpecificTasks
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|()
decl_stmt|;
name|TaskFilterFunction
parameter_list|(
name|Class
argument_list|<
name|T
argument_list|>
name|requiredType
parameter_list|)
block|{
name|this
operator|.
name|requiredType
operator|=
name|requiredType
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|process
parameter_list|(
name|Task
argument_list|<
name|?
extends|extends
name|Serializable
argument_list|>
name|task
parameter_list|)
block|{
if|if
condition|(
name|requiredType
operator|.
name|isInstance
argument_list|(
name|task
argument_list|)
operator|&&
operator|!
name|typeSpecificTasks
operator|.
name|contains
argument_list|(
name|task
argument_list|)
condition|)
block|{
name|typeSpecificTasks
operator|.
name|add
argument_list|(
operator|(
name|T
operator|)
name|task
argument_list|)
expr_stmt|;
block|}
name|visited
operator|.
name|add
argument_list|(
name|task
argument_list|)
expr_stmt|;
block|}
name|List
argument_list|<
name|T
argument_list|>
name|getTasks
parameter_list|()
block|{
return|return
name|typeSpecificTasks
return|;
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|skipProcessing
parameter_list|(
name|Task
argument_list|<
name|?
extends|extends
name|Serializable
argument_list|>
name|task
parameter_list|)
block|{
return|return
name|visited
operator|.
name|contains
argument_list|(
name|task
argument_list|)
return|;
block|}
block|}
annotation|@
name|SuppressWarnings
argument_list|(
literal|"unchecked"
argument_list|)
specifier|private
specifier|static
parameter_list|<
name|T
parameter_list|>
name|List
argument_list|<
name|T
argument_list|>
name|getTasks
parameter_list|(
name|List
argument_list|<
name|Task
argument_list|<
name|?
extends|extends
name|Serializable
argument_list|>
argument_list|>
name|tasks
parameter_list|,
name|TaskFilterFunction
argument_list|<
name|T
argument_list|>
name|function
parameter_list|)
block|{
name|DAGTraversal
operator|.
name|traverse
argument_list|(
name|tasks
argument_list|,
name|function
argument_list|)
expr_stmt|;
return|return
name|function
operator|.
name|getTasks
argument_list|()
return|;
block|}
comment|/**    * Construct a list of full partition spec from Dynamic Partition Context and the directory names    * corresponding to these dynamic partitions.    */
specifier|public
specifier|static
name|List
argument_list|<
name|LinkedHashMap
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
argument_list|>
name|getFullDPSpecs
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|DynamicPartitionCtx
name|dpCtx
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|Path
name|loadPath
init|=
name|dpCtx
operator|.
name|getRootPath
argument_list|()
decl_stmt|;
name|FileSystem
name|fs
init|=
name|loadPath
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|int
name|numDPCols
init|=
name|dpCtx
operator|.
name|getNumDPCols
argument_list|()
decl_stmt|;
name|FileStatus
index|[]
name|status
init|=
name|HiveStatsUtils
operator|.
name|getFileStatusRecurse
argument_list|(
name|loadPath
argument_list|,
name|numDPCols
argument_list|,
name|fs
argument_list|)
decl_stmt|;
if|if
condition|(
name|status
operator|.
name|length
operator|==
literal|0
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"No partition is generated by dynamic partitioning"
argument_list|)
expr_stmt|;
return|return
literal|null
return|;
block|}
comment|// partial partition specification
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|partSpec
init|=
name|dpCtx
operator|.
name|getPartSpec
argument_list|()
decl_stmt|;
comment|// list of full partition specification
name|List
argument_list|<
name|LinkedHashMap
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
argument_list|>
name|fullPartSpecs
init|=
operator|new
name|ArrayList
argument_list|<
name|LinkedHashMap
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
argument_list|>
argument_list|()
decl_stmt|;
comment|// for each dynamically created DP directory, construct a full partition spec
comment|// and load the partition based on that
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|status
operator|.
name|length
condition|;
operator|++
name|i
control|)
block|{
comment|// get the dynamically created directory
name|Path
name|partPath
init|=
name|status
index|[
name|i
index|]
operator|.
name|getPath
argument_list|()
decl_stmt|;
assert|assert
name|fs
operator|.
name|getFileStatus
argument_list|(
name|partPath
argument_list|)
operator|.
name|isDir
argument_list|()
operator|:
literal|"partitions "
operator|+
name|partPath
operator|+
literal|" is not a directory !"
assert|;
comment|// generate a full partition specification
name|LinkedHashMap
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|fullPartSpec
init|=
operator|new
name|LinkedHashMap
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
argument_list|(
name|partSpec
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|Warehouse
operator|.
name|makeSpecFromName
argument_list|(
name|fullPartSpec
argument_list|,
name|partPath
argument_list|,
operator|new
name|HashSet
argument_list|<
name|String
argument_list|>
argument_list|(
name|partSpec
operator|.
name|keySet
argument_list|()
argument_list|)
argument_list|)
condition|)
block|{
name|Utilities
operator|.
name|FILE_OP_LOGGER
operator|.
name|warn
argument_list|(
literal|"Ignoring invalid DP directory {}"
argument_list|,
name|partPath
argument_list|)
expr_stmt|;
continue|continue;
block|}
name|Utilities
operator|.
name|FILE_OP_LOGGER
operator|.
name|trace
argument_list|(
literal|"Adding partition spec from {}: {}"
argument_list|,
name|partPath
argument_list|,
name|fullPartSpec
argument_list|)
expr_stmt|;
name|fullPartSpecs
operator|.
name|add
argument_list|(
name|fullPartSpec
argument_list|)
expr_stmt|;
block|}
return|return
name|fullPartSpecs
return|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
specifier|static
name|StatsPublisher
name|getStatsPublisher
parameter_list|(
name|JobConf
name|jc
parameter_list|)
block|{
name|StatsFactory
name|factory
init|=
name|StatsFactory
operator|.
name|newFactory
argument_list|(
name|jc
argument_list|)
decl_stmt|;
return|return
name|factory
operator|==
literal|null
condition|?
literal|null
else|:
name|factory
operator|.
name|getStatsPublisher
argument_list|()
return|;
block|}
specifier|public
specifier|static
name|String
name|join
parameter_list|(
name|String
modifier|...
name|elements
parameter_list|)
block|{
name|StringBuilder
name|builder
init|=
operator|new
name|StringBuilder
argument_list|()
decl_stmt|;
for|for
control|(
name|String
name|element
range|:
name|elements
control|)
block|{
if|if
condition|(
name|element
operator|==
literal|null
operator|||
name|element
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
continue|continue;
block|}
name|builder
operator|.
name|append
argument_list|(
name|element
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|element
operator|.
name|endsWith
argument_list|(
name|Path
operator|.
name|SEPARATOR
argument_list|)
condition|)
block|{
name|builder
operator|.
name|append
argument_list|(
name|Path
operator|.
name|SEPARATOR
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|builder
operator|.
name|toString
argument_list|()
return|;
block|}
specifier|public
specifier|static
name|void
name|setColumnNameList
parameter_list|(
name|JobConf
name|jobConf
parameter_list|,
name|RowSchema
name|rowSchema
parameter_list|)
block|{
name|setColumnNameList
argument_list|(
name|jobConf
argument_list|,
name|rowSchema
argument_list|,
literal|false
argument_list|)
expr_stmt|;
block|}
specifier|public
specifier|static
name|void
name|setColumnNameList
parameter_list|(
name|JobConf
name|jobConf
parameter_list|,
name|RowSchema
name|rowSchema
parameter_list|,
name|boolean
name|excludeVCs
parameter_list|)
block|{
if|if
condition|(
name|rowSchema
operator|==
literal|null
condition|)
block|{
return|return;
block|}
name|StringBuilder
name|columnNames
init|=
operator|new
name|StringBuilder
argument_list|()
decl_stmt|;
for|for
control|(
name|ColumnInfo
name|colInfo
range|:
name|rowSchema
operator|.
name|getSignature
argument_list|()
control|)
block|{
if|if
condition|(
name|excludeVCs
operator|&&
name|colInfo
operator|.
name|getIsVirtualCol
argument_list|()
condition|)
block|{
continue|continue;
block|}
if|if
condition|(
name|columnNames
operator|.
name|length
argument_list|()
operator|>
literal|0
condition|)
block|{
name|columnNames
operator|.
name|append
argument_list|(
literal|','
argument_list|)
expr_stmt|;
block|}
name|columnNames
operator|.
name|append
argument_list|(
name|colInfo
operator|.
name|getInternalName
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|String
name|columnNamesString
init|=
name|columnNames
operator|.
name|toString
argument_list|()
decl_stmt|;
name|jobConf
operator|.
name|set
argument_list|(
name|serdeConstants
operator|.
name|LIST_COLUMNS
argument_list|,
name|columnNamesString
argument_list|)
expr_stmt|;
block|}
specifier|public
specifier|static
name|void
name|setColumnNameList
parameter_list|(
name|JobConf
name|jobConf
parameter_list|,
name|Operator
name|op
parameter_list|)
block|{
name|setColumnNameList
argument_list|(
name|jobConf
argument_list|,
name|op
argument_list|,
literal|false
argument_list|)
expr_stmt|;
block|}
specifier|public
specifier|static
name|void
name|setColumnNameList
parameter_list|(
name|JobConf
name|jobConf
parameter_list|,
name|Operator
name|op
parameter_list|,
name|boolean
name|excludeVCs
parameter_list|)
block|{
name|RowSchema
name|rowSchema
init|=
name|op
operator|.
name|getSchema
argument_list|()
decl_stmt|;
name|setColumnNameList
argument_list|(
name|jobConf
argument_list|,
name|rowSchema
argument_list|,
name|excludeVCs
argument_list|)
expr_stmt|;
block|}
specifier|public
specifier|static
name|void
name|setColumnTypeList
parameter_list|(
name|JobConf
name|jobConf
parameter_list|,
name|RowSchema
name|rowSchema
parameter_list|)
block|{
name|setColumnTypeList
argument_list|(
name|jobConf
argument_list|,
name|rowSchema
argument_list|,
literal|false
argument_list|)
expr_stmt|;
block|}
specifier|public
specifier|static
name|void
name|setColumnTypeList
parameter_list|(
name|JobConf
name|jobConf
parameter_list|,
name|RowSchema
name|rowSchema
parameter_list|,
name|boolean
name|excludeVCs
parameter_list|)
block|{
if|if
condition|(
name|rowSchema
operator|==
literal|null
condition|)
block|{
return|return;
block|}
name|StringBuilder
name|columnTypes
init|=
operator|new
name|StringBuilder
argument_list|()
decl_stmt|;
for|for
control|(
name|ColumnInfo
name|colInfo
range|:
name|rowSchema
operator|.
name|getSignature
argument_list|()
control|)
block|{
if|if
condition|(
name|excludeVCs
operator|&&
name|colInfo
operator|.
name|getIsVirtualCol
argument_list|()
condition|)
block|{
continue|continue;
block|}
if|if
condition|(
name|columnTypes
operator|.
name|length
argument_list|()
operator|>
literal|0
condition|)
block|{
name|columnTypes
operator|.
name|append
argument_list|(
literal|','
argument_list|)
expr_stmt|;
block|}
name|columnTypes
operator|.
name|append
argument_list|(
name|colInfo
operator|.
name|getTypeName
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|String
name|columnTypesString
init|=
name|columnTypes
operator|.
name|toString
argument_list|()
decl_stmt|;
name|jobConf
operator|.
name|set
argument_list|(
name|serdeConstants
operator|.
name|LIST_COLUMN_TYPES
argument_list|,
name|columnTypesString
argument_list|)
expr_stmt|;
block|}
specifier|public
specifier|static
name|void
name|setColumnTypeList
parameter_list|(
name|JobConf
name|jobConf
parameter_list|,
name|Operator
name|op
parameter_list|)
block|{
name|setColumnTypeList
argument_list|(
name|jobConf
argument_list|,
name|op
argument_list|,
literal|false
argument_list|)
expr_stmt|;
block|}
specifier|public
specifier|static
name|void
name|setColumnTypeList
parameter_list|(
name|JobConf
name|jobConf
parameter_list|,
name|Operator
name|op
parameter_list|,
name|boolean
name|excludeVCs
parameter_list|)
block|{
name|RowSchema
name|rowSchema
init|=
name|op
operator|.
name|getSchema
argument_list|()
decl_stmt|;
name|setColumnTypeList
argument_list|(
name|jobConf
argument_list|,
name|rowSchema
argument_list|,
name|excludeVCs
argument_list|)
expr_stmt|;
block|}
specifier|public
specifier|static
specifier|final
name|String
name|suffix
init|=
literal|".hashtable"
decl_stmt|;
specifier|public
specifier|static
name|Path
name|generatePath
parameter_list|(
name|Path
name|basePath
parameter_list|,
name|String
name|dumpFilePrefix
parameter_list|,
name|Byte
name|tag
parameter_list|,
name|String
name|bigBucketFileName
parameter_list|)
block|{
return|return
operator|new
name|Path
argument_list|(
name|basePath
argument_list|,
literal|"MapJoin-"
operator|+
name|dumpFilePrefix
operator|+
name|tag
operator|+
literal|"-"
operator|+
name|bigBucketFileName
operator|+
name|suffix
argument_list|)
return|;
block|}
specifier|public
specifier|static
name|String
name|generateFileName
parameter_list|(
name|Byte
name|tag
parameter_list|,
name|String
name|bigBucketFileName
parameter_list|)
block|{
name|String
name|fileName
init|=
operator|new
name|String
argument_list|(
literal|"MapJoin-"
operator|+
name|tag
operator|+
literal|"-"
operator|+
name|bigBucketFileName
operator|+
name|suffix
argument_list|)
decl_stmt|;
return|return
name|fileName
return|;
block|}
specifier|public
specifier|static
name|Path
name|generateTmpPath
parameter_list|(
name|Path
name|basePath
parameter_list|,
name|String
name|id
parameter_list|)
block|{
return|return
operator|new
name|Path
argument_list|(
name|basePath
argument_list|,
literal|"HashTable-"
operator|+
name|id
argument_list|)
return|;
block|}
specifier|public
specifier|static
name|Path
name|generateTarPath
parameter_list|(
name|Path
name|basePath
parameter_list|,
name|String
name|filename
parameter_list|)
block|{
return|return
operator|new
name|Path
argument_list|(
name|basePath
argument_list|,
name|filename
operator|+
literal|".tar.gz"
argument_list|)
return|;
block|}
specifier|public
specifier|static
name|String
name|generateTarFileName
parameter_list|(
name|String
name|name
parameter_list|)
block|{
return|return
name|name
operator|+
literal|".tar.gz"
return|;
block|}
specifier|public
specifier|static
name|String
name|generatePath
parameter_list|(
name|Path
name|baseURI
parameter_list|,
name|String
name|filename
parameter_list|)
block|{
name|String
name|path
init|=
operator|new
name|String
argument_list|(
name|baseURI
operator|+
name|Path
operator|.
name|SEPARATOR
operator|+
name|filename
argument_list|)
decl_stmt|;
return|return
name|path
return|;
block|}
specifier|public
specifier|static
name|String
name|now
parameter_list|()
block|{
name|Calendar
name|cal
init|=
name|Calendar
operator|.
name|getInstance
argument_list|()
decl_stmt|;
name|SimpleDateFormat
name|sdf
init|=
operator|new
name|SimpleDateFormat
argument_list|(
literal|"yyyy-MM-dd HH:mm:ss"
argument_list|)
decl_stmt|;
return|return
name|sdf
operator|.
name|format
argument_list|(
name|cal
operator|.
name|getTime
argument_list|()
argument_list|)
return|;
block|}
specifier|public
specifier|static
name|double
name|showTime
parameter_list|(
name|long
name|time
parameter_list|)
block|{
name|double
name|result
init|=
operator|(
name|double
operator|)
name|time
operator|/
operator|(
name|double
operator|)
literal|1000
decl_stmt|;
return|return
name|result
return|;
block|}
comment|/**    * The check here is kind of not clean. It first use a for loop to go through    * all input formats, and choose the ones that extend ReworkMapredInputFormat    * to a set. And finally go through the ReworkMapredInputFormat set, and call    * rework for each one.    *    * Technically all these can be avoided if all Hive's input formats can share    * a same interface. As in today's hive and Hadoop, it is not possible because    * a lot of Hive's input formats are in Hadoop's code. And most of Hadoop's    * input formats just extend InputFormat interface.    *    * @param task    * @param reworkMapredWork    * @param conf    * @throws SemanticException    */
specifier|public
specifier|static
name|void
name|reworkMapRedWork
parameter_list|(
name|Task
argument_list|<
name|?
extends|extends
name|Serializable
argument_list|>
name|task
parameter_list|,
name|boolean
name|reworkMapredWork
parameter_list|,
name|HiveConf
name|conf
parameter_list|)
throws|throws
name|SemanticException
block|{
if|if
condition|(
name|reworkMapredWork
operator|&&
operator|(
name|task
operator|instanceof
name|MapRedTask
operator|)
condition|)
block|{
try|try
block|{
name|MapredWork
name|mapredWork
init|=
operator|(
operator|(
name|MapRedTask
operator|)
name|task
operator|)
operator|.
name|getWork
argument_list|()
decl_stmt|;
name|Set
argument_list|<
name|Class
argument_list|<
name|?
extends|extends
name|InputFormat
argument_list|>
argument_list|>
name|reworkInputFormats
init|=
operator|new
name|HashSet
argument_list|<
name|Class
argument_list|<
name|?
extends|extends
name|InputFormat
argument_list|>
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|PartitionDesc
name|part
range|:
name|mapredWork
operator|.
name|getMapWork
argument_list|()
operator|.
name|getPathToPartitionInfo
argument_list|()
operator|.
name|values
argument_list|()
control|)
block|{
name|Class
argument_list|<
name|?
extends|extends
name|InputFormat
argument_list|>
name|inputFormatCls
init|=
name|part
operator|.
name|getInputFileFormatClass
argument_list|()
decl_stmt|;
if|if
condition|(
name|ReworkMapredInputFormat
operator|.
name|class
operator|.
name|isAssignableFrom
argument_list|(
name|inputFormatCls
argument_list|)
condition|)
block|{
name|reworkInputFormats
operator|.
name|add
argument_list|(
name|inputFormatCls
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
name|reworkInputFormats
operator|.
name|size
argument_list|()
operator|>
literal|0
condition|)
block|{
for|for
control|(
name|Class
argument_list|<
name|?
extends|extends
name|InputFormat
argument_list|>
name|inputFormatCls
range|:
name|reworkInputFormats
control|)
block|{
name|ReworkMapredInputFormat
name|inst
init|=
operator|(
name|ReworkMapredInputFormat
operator|)
name|ReflectionUtil
operator|.
name|newInstance
argument_list|(
name|inputFormatCls
argument_list|,
literal|null
argument_list|)
decl_stmt|;
name|inst
operator|.
name|rework
argument_list|(
name|conf
argument_list|,
name|mapredWork
argument_list|)
expr_stmt|;
block|}
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|SemanticException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
block|}
specifier|public
specifier|static
class|class
name|SQLCommand
parameter_list|<
name|T
parameter_list|>
block|{
specifier|public
name|T
name|run
parameter_list|(
name|PreparedStatement
name|stmt
parameter_list|)
throws|throws
name|SQLException
block|{
return|return
literal|null
return|;
block|}
block|}
comment|/**    * Retry SQL execution with random backoff (same as the one implemented in HDFS-767).    * This function only retries when the SQL query throws a SQLTransientException (which    * might be able to succeed with a simple retry). It doesn't retry when the exception    * is a SQLRecoverableException or SQLNonTransientException. For SQLRecoverableException    * the caller needs to reconnect to the database and restart the whole transaction.    *    * @param cmd the SQL command    * @param stmt the prepared statement of SQL.    * @param baseWindow  The base time window (in milliseconds) before the next retry.    * see {@link #getRandomWaitTime} for details.    * @param maxRetries the maximum # of retries when getting a SQLTransientException.    * @throws SQLException throws SQLRecoverableException or SQLNonTransientException the    * first time it is caught, or SQLTransientException when the maxRetries has reached.    */
specifier|public
specifier|static
parameter_list|<
name|T
parameter_list|>
name|T
name|executeWithRetry
parameter_list|(
name|SQLCommand
argument_list|<
name|T
argument_list|>
name|cmd
parameter_list|,
name|PreparedStatement
name|stmt
parameter_list|,
name|long
name|baseWindow
parameter_list|,
name|int
name|maxRetries
parameter_list|)
throws|throws
name|SQLException
block|{
name|T
name|result
init|=
literal|null
decl_stmt|;
comment|// retry with # of maxRetries before throwing exception
for|for
control|(
name|int
name|failures
init|=
literal|0
init|;
condition|;
name|failures
operator|++
control|)
block|{
try|try
block|{
name|result
operator|=
name|cmd
operator|.
name|run
argument_list|(
name|stmt
argument_list|)
expr_stmt|;
return|return
name|result
return|;
block|}
catch|catch
parameter_list|(
name|SQLTransientException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Failure and retry # {}"
argument_list|,
name|failures
argument_list|,
name|e
argument_list|)
expr_stmt|;
if|if
condition|(
name|failures
operator|>=
name|maxRetries
condition|)
block|{
throw|throw
name|e
throw|;
block|}
name|long
name|waitTime
init|=
name|getRandomWaitTime
argument_list|(
name|baseWindow
argument_list|,
name|failures
argument_list|,
name|randGen
argument_list|)
decl_stmt|;
try|try
block|{
name|Thread
operator|.
name|sleep
argument_list|(
name|waitTime
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|InterruptedException
name|iex
parameter_list|)
block|{          }
block|}
catch|catch
parameter_list|(
name|SQLException
name|e
parameter_list|)
block|{
comment|// throw other types of SQLExceptions (SQLNonTransientException / SQLRecoverableException)
throw|throw
name|e
throw|;
block|}
block|}
block|}
comment|/**    * Retry connecting to a database with random backoff (same as the one implemented in HDFS-767).    * This function only retries when the SQL query throws a SQLTransientException (which    * might be able to succeed with a simple retry). It doesn't retry when the exception    * is a SQLRecoverableException or SQLNonTransientException. For SQLRecoverableException    * the caller needs to reconnect to the database and restart the whole transaction.    *    * @param connectionString the JDBC connection string.    * @param waitWindow  The base time window (in milliseconds) before the next retry.    * see {@link #getRandomWaitTime} for details.    * @param maxRetries the maximum # of retries when getting a SQLTransientException.    * @throws SQLException throws SQLRecoverableException or SQLNonTransientException the    * first time it is caught, or SQLTransientException when the maxRetries has reached.    */
specifier|public
specifier|static
name|Connection
name|connectWithRetry
parameter_list|(
name|String
name|connectionString
parameter_list|,
name|long
name|waitWindow
parameter_list|,
name|int
name|maxRetries
parameter_list|)
throws|throws
name|SQLException
block|{
comment|// retry with # of maxRetries before throwing exception
for|for
control|(
name|int
name|failures
init|=
literal|0
init|;
condition|;
name|failures
operator|++
control|)
block|{
try|try
block|{
name|Connection
name|conn
init|=
name|DriverManager
operator|.
name|getConnection
argument_list|(
name|connectionString
argument_list|)
decl_stmt|;
return|return
name|conn
return|;
block|}
catch|catch
parameter_list|(
name|SQLTransientException
name|e
parameter_list|)
block|{
if|if
condition|(
name|failures
operator|>=
name|maxRetries
condition|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Error during JDBC connection."
argument_list|,
name|e
argument_list|)
expr_stmt|;
throw|throw
name|e
throw|;
block|}
name|long
name|waitTime
init|=
name|Utilities
operator|.
name|getRandomWaitTime
argument_list|(
name|waitWindow
argument_list|,
name|failures
argument_list|,
name|randGen
argument_list|)
decl_stmt|;
try|try
block|{
name|Thread
operator|.
name|sleep
argument_list|(
name|waitTime
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|InterruptedException
name|e1
parameter_list|)
block|{         }
block|}
catch|catch
parameter_list|(
name|SQLException
name|e
parameter_list|)
block|{
comment|// just throw other types (SQLNonTransientException / SQLRecoverableException)
throw|throw
name|e
throw|;
block|}
block|}
block|}
comment|/**    * Retry preparing a SQL statement with random backoff (same as the one implemented in HDFS-767).    * This function only retries when the SQL query throws a SQLTransientException (which    * might be able to succeed with a simple retry). It doesn't retry when the exception    * is a SQLRecoverableException or SQLNonTransientException. For SQLRecoverableException    * the caller needs to reconnect to the database and restart the whole transaction.    *    * @param conn a JDBC connection.    * @param stmt the SQL statement to be prepared.    * @param waitWindow  The base time window (in milliseconds) before the next retry.    * see {@link #getRandomWaitTime} for details.    * @param maxRetries the maximum # of retries when getting a SQLTransientException.    * @throws SQLException throws SQLRecoverableException or SQLNonTransientException the    * first time it is caught, or SQLTransientException when the maxRetries has reached.    */
specifier|public
specifier|static
name|PreparedStatement
name|prepareWithRetry
parameter_list|(
name|Connection
name|conn
parameter_list|,
name|String
name|stmt
parameter_list|,
name|long
name|waitWindow
parameter_list|,
name|int
name|maxRetries
parameter_list|)
throws|throws
name|SQLException
block|{
comment|// retry with # of maxRetries before throwing exception
for|for
control|(
name|int
name|failures
init|=
literal|0
init|;
condition|;
name|failures
operator|++
control|)
block|{
try|try
block|{
return|return
name|conn
operator|.
name|prepareStatement
argument_list|(
name|stmt
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|SQLTransientException
name|e
parameter_list|)
block|{
if|if
condition|(
name|failures
operator|>=
name|maxRetries
condition|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Error preparing JDBC Statement {}"
argument_list|,
name|stmt
argument_list|,
name|e
argument_list|)
expr_stmt|;
throw|throw
name|e
throw|;
block|}
name|long
name|waitTime
init|=
name|Utilities
operator|.
name|getRandomWaitTime
argument_list|(
name|waitWindow
argument_list|,
name|failures
argument_list|,
name|randGen
argument_list|)
decl_stmt|;
try|try
block|{
name|Thread
operator|.
name|sleep
argument_list|(
name|waitTime
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|InterruptedException
name|e1
parameter_list|)
block|{         }
block|}
catch|catch
parameter_list|(
name|SQLException
name|e
parameter_list|)
block|{
comment|// just throw other types (SQLNonTransientException / SQLRecoverableException)
throw|throw
name|e
throw|;
block|}
block|}
block|}
specifier|public
specifier|static
name|void
name|setQueryTimeout
parameter_list|(
name|java
operator|.
name|sql
operator|.
name|Statement
name|stmt
parameter_list|,
name|int
name|timeout
parameter_list|)
throws|throws
name|SQLException
block|{
if|if
condition|(
name|timeout
operator|<
literal|0
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Invalid query timeout {}"
argument_list|,
name|timeout
argument_list|)
expr_stmt|;
return|return;
block|}
try|try
block|{
name|stmt
operator|.
name|setQueryTimeout
argument_list|(
name|timeout
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|SQLException
name|e
parameter_list|)
block|{
name|String
name|message
init|=
name|e
operator|.
name|getMessage
argument_list|()
operator|==
literal|null
condition|?
literal|null
else|:
name|e
operator|.
name|getMessage
argument_list|()
operator|.
name|toLowerCase
argument_list|()
decl_stmt|;
if|if
condition|(
name|e
operator|instanceof
name|SQLFeatureNotSupportedException
operator|||
operator|(
name|message
operator|!=
literal|null
operator|&&
operator|(
name|message
operator|.
name|contains
argument_list|(
literal|"implemented"
argument_list|)
operator|||
name|message
operator|.
name|contains
argument_list|(
literal|"supported"
argument_list|)
operator|)
operator|)
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"setQueryTimeout is not supported"
argument_list|)
expr_stmt|;
return|return;
block|}
throw|throw
name|e
throw|;
block|}
block|}
comment|/**    * Introducing a random factor to the wait time before another retry.    * The wait time is dependent on # of failures and a random factor.    * At the first time of getting an exception , the wait time    * is a random number between 0..baseWindow msec. If the first retry    * still fails, we will wait baseWindow msec grace period before the 2nd retry.    * Also at the second retry, the waiting window is expanded to 2*baseWindow msec    * alleviating the request rate from the server. Similarly the 3rd retry    * will wait 2*baseWindow msec. grace period before retry and the waiting window is    * expanded to 3*baseWindow msec and so on.    * @param baseWindow the base waiting window.    * @param failures number of failures so far.    * @param r a random generator.    * @return number of milliseconds for the next wait time.    */
specifier|public
specifier|static
name|long
name|getRandomWaitTime
parameter_list|(
name|long
name|baseWindow
parameter_list|,
name|int
name|failures
parameter_list|,
name|Random
name|r
parameter_list|)
block|{
return|return
call|(
name|long
call|)
argument_list|(
name|baseWindow
operator|*
name|failures
operator|+
comment|// grace period for the last round of attempt
name|baseWindow
operator|*
operator|(
name|failures
operator|+
literal|1
operator|)
operator|*
name|r
operator|.
name|nextDouble
argument_list|()
argument_list|)
return|;
comment|// expanding time window for each failure
block|}
specifier|public
specifier|static
specifier|final
name|char
name|sqlEscapeChar
init|=
literal|'\\'
decl_stmt|;
comment|/**    * Escape the '_', '%', as well as the escape characters inside the string key.    * @param key the string that will be used for the SQL LIKE operator.    * @return a string with escaped '_' and '%'.    */
specifier|public
specifier|static
name|String
name|escapeSqlLike
parameter_list|(
name|String
name|key
parameter_list|)
block|{
name|StringBuilder
name|sb
init|=
operator|new
name|StringBuilder
argument_list|(
name|key
operator|.
name|length
argument_list|()
argument_list|)
decl_stmt|;
for|for
control|(
name|char
name|c
range|:
name|key
operator|.
name|toCharArray
argument_list|()
control|)
block|{
switch|switch
condition|(
name|c
condition|)
block|{
case|case
literal|'_'
case|:
case|case
literal|'%'
case|:
case|case
name|sqlEscapeChar
case|:
name|sb
operator|.
name|append
argument_list|(
name|sqlEscapeChar
argument_list|)
expr_stmt|;
comment|// fall through
default|default:
name|sb
operator|.
name|append
argument_list|(
name|c
argument_list|)
expr_stmt|;
break|break;
block|}
block|}
return|return
name|sb
operator|.
name|toString
argument_list|()
return|;
block|}
comment|/**    * Format number of milliseconds to strings    *    * @param msec milliseconds    * @return a formatted string like "x days y hours z minutes a seconds b msec"    */
specifier|public
specifier|static
name|String
name|formatMsecToStr
parameter_list|(
name|long
name|msec
parameter_list|)
block|{
name|long
name|day
init|=
operator|-
literal|1
decl_stmt|,
name|hour
init|=
operator|-
literal|1
decl_stmt|,
name|minute
init|=
operator|-
literal|1
decl_stmt|,
name|second
init|=
operator|-
literal|1
decl_stmt|;
name|long
name|ms
init|=
name|msec
operator|%
literal|1000
decl_stmt|;
name|long
name|timeLeft
init|=
name|msec
operator|/
literal|1000
decl_stmt|;
if|if
condition|(
name|timeLeft
operator|>
literal|0
condition|)
block|{
name|second
operator|=
name|timeLeft
operator|%
literal|60
expr_stmt|;
name|timeLeft
operator|/=
literal|60
expr_stmt|;
if|if
condition|(
name|timeLeft
operator|>
literal|0
condition|)
block|{
name|minute
operator|=
name|timeLeft
operator|%
literal|60
expr_stmt|;
name|timeLeft
operator|/=
literal|60
expr_stmt|;
if|if
condition|(
name|timeLeft
operator|>
literal|0
condition|)
block|{
name|hour
operator|=
name|timeLeft
operator|%
literal|24
expr_stmt|;
name|day
operator|=
name|timeLeft
operator|/
literal|24
expr_stmt|;
block|}
block|}
block|}
name|StringBuilder
name|sb
init|=
operator|new
name|StringBuilder
argument_list|()
decl_stmt|;
if|if
condition|(
name|day
operator|!=
operator|-
literal|1
condition|)
block|{
name|sb
operator|.
name|append
argument_list|(
name|day
operator|+
literal|" days "
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|hour
operator|!=
operator|-
literal|1
condition|)
block|{
name|sb
operator|.
name|append
argument_list|(
name|hour
operator|+
literal|" hours "
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|minute
operator|!=
operator|-
literal|1
condition|)
block|{
name|sb
operator|.
name|append
argument_list|(
name|minute
operator|+
literal|" minutes "
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|second
operator|!=
operator|-
literal|1
condition|)
block|{
name|sb
operator|.
name|append
argument_list|(
name|second
operator|+
literal|" seconds "
argument_list|)
expr_stmt|;
block|}
name|sb
operator|.
name|append
argument_list|(
name|ms
operator|+
literal|" msec"
argument_list|)
expr_stmt|;
return|return
name|sb
operator|.
name|toString
argument_list|()
return|;
block|}
comment|/**    * Estimate the number of reducers needed for this job, based on job input,    * and configuration parameters.    *    * The output of this method should only be used if the output of this    * MapRedTask is not being used to populate a bucketed table and the user    * has not specified the number of reducers to use.    *    * @return the number of reducers.    */
specifier|public
specifier|static
name|int
name|estimateNumberOfReducers
parameter_list|(
name|HiveConf
name|conf
parameter_list|,
name|ContentSummary
name|inputSummary
parameter_list|,
name|MapWork
name|work
parameter_list|,
name|boolean
name|finalMapRed
parameter_list|)
throws|throws
name|IOException
block|{
name|long
name|bytesPerReducer
init|=
name|conf
operator|.
name|getLongVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|BYTESPERREDUCER
argument_list|)
decl_stmt|;
name|int
name|maxReducers
init|=
name|conf
operator|.
name|getIntVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|MAXREDUCERS
argument_list|)
decl_stmt|;
name|double
name|samplePercentage
init|=
name|getHighestSamplePercentage
argument_list|(
name|work
argument_list|)
decl_stmt|;
name|long
name|totalInputFileSize
init|=
name|getTotalInputFileSize
argument_list|(
name|inputSummary
argument_list|,
name|work
argument_list|,
name|samplePercentage
argument_list|)
decl_stmt|;
comment|// if all inputs are sampled, we should shrink the size of reducers accordingly.
if|if
condition|(
name|totalInputFileSize
operator|!=
name|inputSummary
operator|.
name|getLength
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"BytesPerReducer={} maxReducers={} estimated totalInputFileSize={}"
argument_list|,
name|bytesPerReducer
argument_list|,
name|maxReducers
argument_list|,
name|totalInputFileSize
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"BytesPerReducer={} maxReducers={} totalInputFileSize={}"
argument_list|,
name|bytesPerReducer
argument_list|,
name|maxReducers
argument_list|,
name|totalInputFileSize
argument_list|)
expr_stmt|;
block|}
comment|// If this map reduce job writes final data to a table and bucketing is being inferred,
comment|// and the user has configured Hive to do this, make sure the number of reducers is a
comment|// power of two
name|boolean
name|powersOfTwo
init|=
name|conf
operator|.
name|getBoolVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_INFER_BUCKET_SORT_NUM_BUCKETS_POWER_TWO
argument_list|)
operator|&&
name|finalMapRed
operator|&&
operator|!
name|work
operator|.
name|getBucketedColsByDirectory
argument_list|()
operator|.
name|isEmpty
argument_list|()
decl_stmt|;
return|return
name|estimateReducers
argument_list|(
name|totalInputFileSize
argument_list|,
name|bytesPerReducer
argument_list|,
name|maxReducers
argument_list|,
name|powersOfTwo
argument_list|)
return|;
block|}
specifier|public
specifier|static
name|int
name|estimateReducers
parameter_list|(
name|long
name|totalInputFileSize
parameter_list|,
name|long
name|bytesPerReducer
parameter_list|,
name|int
name|maxReducers
parameter_list|,
name|boolean
name|powersOfTwo
parameter_list|)
block|{
name|double
name|bytes
init|=
name|Math
operator|.
name|max
argument_list|(
name|totalInputFileSize
argument_list|,
name|bytesPerReducer
argument_list|)
decl_stmt|;
name|int
name|reducers
init|=
operator|(
name|int
operator|)
name|Math
operator|.
name|ceil
argument_list|(
name|bytes
operator|/
name|bytesPerReducer
argument_list|)
decl_stmt|;
name|reducers
operator|=
name|Math
operator|.
name|max
argument_list|(
literal|1
argument_list|,
name|reducers
argument_list|)
expr_stmt|;
name|reducers
operator|=
name|Math
operator|.
name|min
argument_list|(
name|maxReducers
argument_list|,
name|reducers
argument_list|)
expr_stmt|;
name|int
name|reducersLog
init|=
call|(
name|int
call|)
argument_list|(
name|Math
operator|.
name|log
argument_list|(
name|reducers
argument_list|)
operator|/
name|Math
operator|.
name|log
argument_list|(
literal|2
argument_list|)
argument_list|)
operator|+
literal|1
decl_stmt|;
name|int
name|reducersPowerTwo
init|=
operator|(
name|int
operator|)
name|Math
operator|.
name|pow
argument_list|(
literal|2
argument_list|,
name|reducersLog
argument_list|)
decl_stmt|;
if|if
condition|(
name|powersOfTwo
condition|)
block|{
comment|// If the original number of reducers was a power of two, use that
if|if
condition|(
name|reducersPowerTwo
operator|/
literal|2
operator|==
name|reducers
condition|)
block|{
comment|// nothing to do
block|}
elseif|else
if|if
condition|(
name|reducersPowerTwo
operator|>
name|maxReducers
condition|)
block|{
comment|// If the next power of two greater than the original number of reducers is greater
comment|// than the max number of reducers, use the preceding power of two, which is strictly
comment|// less than the original number of reducers and hence the max
name|reducers
operator|=
name|reducersPowerTwo
operator|/
literal|2
expr_stmt|;
block|}
else|else
block|{
comment|// Otherwise use the smallest power of two greater than the original number of reducers
name|reducers
operator|=
name|reducersPowerTwo
expr_stmt|;
block|}
block|}
return|return
name|reducers
return|;
block|}
comment|/**    * Computes the total input file size. If block sampling was used it will scale this    * value by the highest sample percentage (as an estimate for input).    *    * @param inputSummary    * @param work    * @param highestSamplePercentage    * @return estimated total input size for job    */
specifier|public
specifier|static
name|long
name|getTotalInputFileSize
parameter_list|(
name|ContentSummary
name|inputSummary
parameter_list|,
name|MapWork
name|work
parameter_list|,
name|double
name|highestSamplePercentage
parameter_list|)
block|{
name|long
name|totalInputFileSize
init|=
name|inputSummary
operator|.
name|getLength
argument_list|()
decl_stmt|;
if|if
condition|(
name|MapUtils
operator|.
name|isEmpty
argument_list|(
name|work
operator|.
name|getNameToSplitSample
argument_list|()
argument_list|)
condition|)
block|{
comment|// If percentage block sampling wasn't used, we don't need to do any estimation
return|return
name|totalInputFileSize
return|;
block|}
if|if
condition|(
name|highestSamplePercentage
operator|>=
literal|0
condition|)
block|{
name|totalInputFileSize
operator|=
name|Math
operator|.
name|min
argument_list|(
call|(
name|long
call|)
argument_list|(
name|totalInputFileSize
operator|*
operator|(
name|highestSamplePercentage
operator|/
literal|100D
operator|)
argument_list|)
argument_list|,
name|totalInputFileSize
argument_list|)
expr_stmt|;
block|}
return|return
name|totalInputFileSize
return|;
block|}
comment|/**    * Computes the total number of input files. If block sampling was used it will scale this    * value by the highest sample percentage (as an estimate for # input files).    *    * @param inputSummary    * @param work    * @param highestSamplePercentage    * @return    */
specifier|public
specifier|static
name|long
name|getTotalInputNumFiles
parameter_list|(
name|ContentSummary
name|inputSummary
parameter_list|,
name|MapWork
name|work
parameter_list|,
name|double
name|highestSamplePercentage
parameter_list|)
block|{
name|long
name|totalInputNumFiles
init|=
name|inputSummary
operator|.
name|getFileCount
argument_list|()
decl_stmt|;
if|if
condition|(
name|MapUtils
operator|.
name|isEmpty
argument_list|(
name|work
operator|.
name|getNameToSplitSample
argument_list|()
argument_list|)
condition|)
block|{
comment|// If percentage block sampling wasn't used, we don't need to do any estimation
return|return
name|totalInputNumFiles
return|;
block|}
if|if
condition|(
name|highestSamplePercentage
operator|>=
literal|0
condition|)
block|{
name|totalInputNumFiles
operator|=
name|Math
operator|.
name|min
argument_list|(
call|(
name|long
call|)
argument_list|(
name|totalInputNumFiles
operator|*
operator|(
name|highestSamplePercentage
operator|/
literal|100D
operator|)
argument_list|)
argument_list|,
name|totalInputNumFiles
argument_list|)
expr_stmt|;
block|}
return|return
name|totalInputNumFiles
return|;
block|}
comment|/**    * Returns the highest sample percentage of any alias in the given MapWork    */
specifier|public
specifier|static
name|double
name|getHighestSamplePercentage
parameter_list|(
name|MapWork
name|work
parameter_list|)
block|{
name|double
name|highestSamplePercentage
init|=
literal|0
decl_stmt|;
for|for
control|(
name|String
name|alias
range|:
name|work
operator|.
name|getAliasToWork
argument_list|()
operator|.
name|keySet
argument_list|()
control|)
block|{
if|if
condition|(
name|work
operator|.
name|getNameToSplitSample
argument_list|()
operator|.
name|containsKey
argument_list|(
name|alias
argument_list|)
condition|)
block|{
name|Double
name|rate
init|=
name|work
operator|.
name|getNameToSplitSample
argument_list|()
operator|.
name|get
argument_list|(
name|alias
argument_list|)
operator|.
name|getPercent
argument_list|()
decl_stmt|;
if|if
condition|(
name|rate
operator|!=
literal|null
operator|&&
name|rate
operator|>
name|highestSamplePercentage
condition|)
block|{
name|highestSamplePercentage
operator|=
name|rate
expr_stmt|;
block|}
block|}
else|else
block|{
name|highestSamplePercentage
operator|=
operator|-
literal|1
expr_stmt|;
break|break;
block|}
block|}
return|return
name|highestSamplePercentage
return|;
block|}
comment|/**    * On Tez we're not creating dummy files when getting/setting input paths.    * We let Tez handle the situation. We're also setting the paths in the AM    * so we don't want to depend on scratch dir and context.    */
specifier|public
specifier|static
name|List
argument_list|<
name|Path
argument_list|>
name|getInputPathsTez
parameter_list|(
name|JobConf
name|job
parameter_list|,
name|MapWork
name|work
parameter_list|)
throws|throws
name|Exception
block|{
name|String
name|scratchDir
init|=
name|job
operator|.
name|get
argument_list|(
name|DagUtils
operator|.
name|TEZ_TMP_DIR_KEY
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|Path
argument_list|>
name|paths
init|=
name|getInputPaths
argument_list|(
name|job
argument_list|,
name|work
argument_list|,
operator|new
name|Path
argument_list|(
name|scratchDir
argument_list|)
argument_list|,
literal|null
argument_list|,
literal|true
argument_list|)
decl_stmt|;
return|return
name|paths
return|;
block|}
comment|/**    * Appends vertex name to specified counter name.    *    * @param counter counter to be appended with    * @param vertexName   vertex name    * @return counter name with vertex name appended    */
specifier|public
specifier|static
name|String
name|getVertexCounterName
parameter_list|(
name|String
name|counter
parameter_list|,
name|String
name|vertexName
parameter_list|)
block|{
if|if
condition|(
name|vertexName
operator|!=
literal|null
operator|&&
operator|!
name|vertexName
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|vertexName
operator|=
literal|"_"
operator|+
name|vertexName
operator|.
name|replace
argument_list|(
literal|" "
argument_list|,
literal|"_"
argument_list|)
expr_stmt|;
block|}
return|return
name|counter
operator|+
name|vertexName
return|;
block|}
comment|/**    * Computes a list of all input paths needed to compute the given MapWork. All aliases    * are considered and a merged list of input paths is returned. If any input path points    * to an empty table or partition a dummy file in the scratch dir is instead created and    * added to the list. This is needed to avoid special casing the operator pipeline for    * these cases.    *    * @param job JobConf used to run the job    * @param work MapWork encapsulating the info about the task    * @param hiveScratchDir The tmp dir used to create dummy files if needed    * @param ctx Context object    * @return List of paths to process for the given MapWork    * @throws Exception    */
specifier|public
specifier|static
name|List
argument_list|<
name|Path
argument_list|>
name|getInputPaths
parameter_list|(
name|JobConf
name|job
parameter_list|,
name|MapWork
name|work
parameter_list|,
name|Path
name|hiveScratchDir
parameter_list|,
name|Context
name|ctx
parameter_list|,
name|boolean
name|skipDummy
parameter_list|)
throws|throws
name|Exception
block|{
name|Set
argument_list|<
name|Path
argument_list|>
name|pathsProcessed
init|=
operator|new
name|HashSet
argument_list|<
name|Path
argument_list|>
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|Path
argument_list|>
name|pathsToAdd
init|=
operator|new
name|LinkedList
argument_list|<
name|Path
argument_list|>
argument_list|()
decl_stmt|;
name|LockedDriverState
name|lDrvStat
init|=
name|LockedDriverState
operator|.
name|getLockedDriverState
argument_list|()
decl_stmt|;
comment|// AliasToWork contains all the aliases
name|Collection
argument_list|<
name|String
argument_list|>
name|aliasToWork
init|=
name|work
operator|.
name|getAliasToWork
argument_list|()
operator|.
name|keySet
argument_list|()
decl_stmt|;
if|if
condition|(
operator|!
name|skipDummy
condition|)
block|{
comment|// ConcurrentModification otherwise if adding dummy.
name|aliasToWork
operator|=
operator|new
name|ArrayList
argument_list|<>
argument_list|(
name|aliasToWork
argument_list|)
expr_stmt|;
block|}
for|for
control|(
name|String
name|alias
range|:
name|aliasToWork
control|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Processing alias {}"
argument_list|,
name|alias
argument_list|)
expr_stmt|;
comment|// The alias may not have any path
name|Collection
argument_list|<
name|Map
operator|.
name|Entry
argument_list|<
name|Path
argument_list|,
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|>
argument_list|>
name|pathToAliases
init|=
name|work
operator|.
name|getPathToAliases
argument_list|()
operator|.
name|entrySet
argument_list|()
decl_stmt|;
if|if
condition|(
operator|!
name|skipDummy
condition|)
block|{
comment|// ConcurrentModification otherwise if adding dummy.
name|pathToAliases
operator|=
operator|new
name|ArrayList
argument_list|<>
argument_list|(
name|pathToAliases
argument_list|)
expr_stmt|;
block|}
name|boolean
name|isEmptyTable
init|=
literal|true
decl_stmt|;
name|boolean
name|hasLogged
init|=
literal|false
decl_stmt|;
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|Path
argument_list|,
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|>
name|e
range|:
name|pathToAliases
control|)
block|{
if|if
condition|(
name|lDrvStat
operator|!=
literal|null
operator|&&
name|lDrvStat
operator|.
name|isAborted
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Operation is Canceled."
argument_list|)
throw|;
block|}
name|Path
name|file
init|=
name|e
operator|.
name|getKey
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|aliases
init|=
name|e
operator|.
name|getValue
argument_list|()
decl_stmt|;
if|if
condition|(
name|aliases
operator|.
name|contains
argument_list|(
name|alias
argument_list|)
condition|)
block|{
if|if
condition|(
name|file
operator|!=
literal|null
condition|)
block|{
name|isEmptyTable
operator|=
literal|false
expr_stmt|;
block|}
else|else
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Found a null path for alias {}"
argument_list|,
name|alias
argument_list|)
expr_stmt|;
continue|continue;
block|}
comment|// Multiple aliases can point to the same path - it should be
comment|// processed only once
if|if
condition|(
name|pathsProcessed
operator|.
name|contains
argument_list|(
name|file
argument_list|)
condition|)
block|{
continue|continue;
block|}
name|StringInternUtils
operator|.
name|internUriStringsInPath
argument_list|(
name|file
argument_list|)
expr_stmt|;
name|pathsProcessed
operator|.
name|add
argument_list|(
name|file
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Adding input file {}"
argument_list|,
name|file
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|hasLogged
condition|)
block|{
name|hasLogged
operator|=
literal|true
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Adding {} inputs; the first input is {}"
argument_list|,
name|work
operator|.
name|getPathToAliases
argument_list|()
operator|.
name|size
argument_list|()
argument_list|,
name|file
argument_list|)
expr_stmt|;
block|}
name|pathsToAdd
operator|.
name|add
argument_list|(
name|file
argument_list|)
expr_stmt|;
block|}
block|}
comment|// If the query references non-existent partitions
comment|// We need to add a empty file, it is not acceptable to change the
comment|// operator tree
comment|// Consider the query:
comment|// select * from (select count(1) from T union all select count(1) from
comment|// T2) x;
comment|// If T is empty and T2 contains 100 rows, the user expects: 0, 100 (2
comment|// rows)
if|if
condition|(
name|isEmptyTable
operator|&&
operator|!
name|skipDummy
condition|)
block|{
name|pathsToAdd
operator|.
name|add
argument_list|(
name|createDummyFileForEmptyTable
argument_list|(
name|job
argument_list|,
name|work
argument_list|,
name|hiveScratchDir
argument_list|,
name|alias
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
name|List
argument_list|<
name|Path
argument_list|>
name|finalPathsToAdd
init|=
operator|new
name|LinkedList
argument_list|<>
argument_list|()
decl_stmt|;
name|int
name|numExecutors
init|=
name|getMaxExecutorsForInputListing
argument_list|(
name|job
argument_list|,
name|pathsToAdd
operator|.
name|size
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|numExecutors
operator|>
literal|1
condition|)
block|{
name|ExecutorService
name|pool
init|=
name|Executors
operator|.
name|newFixedThreadPool
argument_list|(
name|numExecutors
argument_list|,
operator|new
name|ThreadFactoryBuilder
argument_list|()
operator|.
name|setDaemon
argument_list|(
literal|true
argument_list|)
operator|.
name|setNameFormat
argument_list|(
literal|"Get-Input-Paths-%d"
argument_list|)
operator|.
name|build
argument_list|()
argument_list|)
decl_stmt|;
name|finalPathsToAdd
operator|.
name|addAll
argument_list|(
name|getInputPathsWithPool
argument_list|(
name|job
argument_list|,
name|work
argument_list|,
name|hiveScratchDir
argument_list|,
name|ctx
argument_list|,
name|skipDummy
argument_list|,
name|pathsToAdd
argument_list|,
name|pool
argument_list|)
argument_list|)
expr_stmt|;
block|}
else|else
block|{
for|for
control|(
specifier|final
name|Path
name|path
range|:
name|pathsToAdd
control|)
block|{
if|if
condition|(
name|lDrvStat
operator|!=
literal|null
operator|&&
name|lDrvStat
operator|.
name|isAborted
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Operation is Canceled."
argument_list|)
throw|;
block|}
name|Path
name|newPath
init|=
operator|new
name|GetInputPathsCallable
argument_list|(
name|path
argument_list|,
name|job
argument_list|,
name|work
argument_list|,
name|hiveScratchDir
argument_list|,
name|ctx
argument_list|,
name|skipDummy
argument_list|)
operator|.
name|call
argument_list|()
decl_stmt|;
name|updatePathForMapWork
argument_list|(
name|newPath
argument_list|,
name|work
argument_list|,
name|path
argument_list|)
expr_stmt|;
name|finalPathsToAdd
operator|.
name|add
argument_list|(
name|newPath
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|finalPathsToAdd
return|;
block|}
annotation|@
name|VisibleForTesting
specifier|static
name|List
argument_list|<
name|Path
argument_list|>
name|getInputPathsWithPool
parameter_list|(
name|JobConf
name|job
parameter_list|,
name|MapWork
name|work
parameter_list|,
name|Path
name|hiveScratchDir
parameter_list|,
name|Context
name|ctx
parameter_list|,
name|boolean
name|skipDummy
parameter_list|,
name|List
argument_list|<
name|Path
argument_list|>
name|pathsToAdd
parameter_list|,
name|ExecutorService
name|pool
parameter_list|)
throws|throws
name|IOException
throws|,
name|ExecutionException
throws|,
name|InterruptedException
block|{
name|LockedDriverState
name|lDrvStat
init|=
name|LockedDriverState
operator|.
name|getLockedDriverState
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|Path
argument_list|>
name|finalPathsToAdd
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|()
decl_stmt|;
try|try
block|{
name|Map
argument_list|<
name|GetInputPathsCallable
argument_list|,
name|Future
argument_list|<
name|Path
argument_list|>
argument_list|>
name|getPathsCallableToFuture
init|=
operator|new
name|LinkedHashMap
argument_list|<>
argument_list|()
decl_stmt|;
for|for
control|(
specifier|final
name|Path
name|path
range|:
name|pathsToAdd
control|)
block|{
if|if
condition|(
name|lDrvStat
operator|!=
literal|null
operator|&&
name|lDrvStat
operator|.
name|isAborted
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Operation is Canceled."
argument_list|)
throw|;
block|}
name|GetInputPathsCallable
name|callable
init|=
operator|new
name|GetInputPathsCallable
argument_list|(
name|path
argument_list|,
name|job
argument_list|,
name|work
argument_list|,
name|hiveScratchDir
argument_list|,
name|ctx
argument_list|,
name|skipDummy
argument_list|)
decl_stmt|;
name|getPathsCallableToFuture
operator|.
name|put
argument_list|(
name|callable
argument_list|,
name|pool
operator|.
name|submit
argument_list|(
name|callable
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|pool
operator|.
name|shutdown
argument_list|()
expr_stmt|;
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|GetInputPathsCallable
argument_list|,
name|Future
argument_list|<
name|Path
argument_list|>
argument_list|>
name|future
range|:
name|getPathsCallableToFuture
operator|.
name|entrySet
argument_list|()
control|)
block|{
if|if
condition|(
name|lDrvStat
operator|!=
literal|null
operator|&&
name|lDrvStat
operator|.
name|isAborted
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Operation is Canceled."
argument_list|)
throw|;
block|}
name|Path
name|newPath
init|=
name|future
operator|.
name|getValue
argument_list|()
operator|.
name|get
argument_list|()
decl_stmt|;
name|updatePathForMapWork
argument_list|(
name|newPath
argument_list|,
name|work
argument_list|,
name|future
operator|.
name|getKey
argument_list|()
operator|.
name|path
argument_list|)
expr_stmt|;
name|finalPathsToAdd
operator|.
name|add
argument_list|(
name|newPath
argument_list|)
expr_stmt|;
block|}
block|}
finally|finally
block|{
name|pool
operator|.
name|shutdownNow
argument_list|()
expr_stmt|;
block|}
return|return
name|finalPathsToAdd
return|;
block|}
specifier|private
specifier|static
class|class
name|GetInputPathsCallable
implements|implements
name|Callable
argument_list|<
name|Path
argument_list|>
block|{
specifier|private
specifier|final
name|Path
name|path
decl_stmt|;
specifier|private
specifier|final
name|JobConf
name|job
decl_stmt|;
specifier|private
specifier|final
name|MapWork
name|work
decl_stmt|;
specifier|private
specifier|final
name|Path
name|hiveScratchDir
decl_stmt|;
specifier|private
specifier|final
name|Context
name|ctx
decl_stmt|;
specifier|private
specifier|final
name|boolean
name|skipDummy
decl_stmt|;
specifier|private
name|GetInputPathsCallable
parameter_list|(
name|Path
name|path
parameter_list|,
name|JobConf
name|job
parameter_list|,
name|MapWork
name|work
parameter_list|,
name|Path
name|hiveScratchDir
parameter_list|,
name|Context
name|ctx
parameter_list|,
name|boolean
name|skipDummy
parameter_list|)
block|{
name|this
operator|.
name|path
operator|=
name|path
expr_stmt|;
name|this
operator|.
name|job
operator|=
name|job
expr_stmt|;
name|this
operator|.
name|work
operator|=
name|work
expr_stmt|;
name|this
operator|.
name|hiveScratchDir
operator|=
name|hiveScratchDir
expr_stmt|;
name|this
operator|.
name|ctx
operator|=
name|ctx
expr_stmt|;
name|this
operator|.
name|skipDummy
operator|=
name|skipDummy
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|Path
name|call
parameter_list|()
throws|throws
name|Exception
block|{
if|if
condition|(
operator|!
name|this
operator|.
name|skipDummy
operator|&&
name|isEmptyPath
argument_list|(
name|this
operator|.
name|job
argument_list|,
name|this
operator|.
name|path
argument_list|,
name|this
operator|.
name|ctx
argument_list|)
condition|)
block|{
return|return
name|createDummyFileForEmptyPartition
argument_list|(
name|this
operator|.
name|path
argument_list|,
name|this
operator|.
name|job
argument_list|,
name|this
operator|.
name|work
operator|.
name|getPathToPartitionInfo
argument_list|()
operator|.
name|get
argument_list|(
name|this
operator|.
name|path
argument_list|)
argument_list|,
name|this
operator|.
name|hiveScratchDir
argument_list|)
return|;
block|}
return|return
name|this
operator|.
name|path
return|;
block|}
block|}
annotation|@
name|SuppressWarnings
argument_list|(
block|{
literal|"rawtypes"
block|,
literal|"unchecked"
block|}
argument_list|)
specifier|private
specifier|static
name|Path
name|createEmptyFile
parameter_list|(
name|Path
name|hiveScratchDir
parameter_list|,
name|HiveOutputFormat
name|outFileFormat
parameter_list|,
name|JobConf
name|job
parameter_list|,
name|Properties
name|props
parameter_list|,
name|boolean
name|dummyRow
parameter_list|)
throws|throws
name|IOException
throws|,
name|InstantiationException
throws|,
name|IllegalAccessException
block|{
comment|// create a dummy empty file in a new directory
name|String
name|newDir
init|=
name|hiveScratchDir
operator|+
name|Path
operator|.
name|SEPARATOR
operator|+
name|UUID
operator|.
name|randomUUID
argument_list|()
operator|.
name|toString
argument_list|()
decl_stmt|;
name|Path
name|newPath
init|=
operator|new
name|Path
argument_list|(
name|newDir
argument_list|)
decl_stmt|;
name|FileSystem
name|fs
init|=
name|newPath
operator|.
name|getFileSystem
argument_list|(
name|job
argument_list|)
decl_stmt|;
name|fs
operator|.
name|mkdirs
argument_list|(
name|newPath
argument_list|)
expr_stmt|;
comment|//Qualify the path against the file system. The user configured path might contain default port which is skipped
comment|//in the file status. This makes sure that all paths which goes into PathToPartitionInfo are always listed status
comment|//file path.
name|newPath
operator|=
name|fs
operator|.
name|makeQualified
argument_list|(
name|newPath
argument_list|)
expr_stmt|;
name|String
name|newFile
init|=
name|newDir
operator|+
name|Path
operator|.
name|SEPARATOR
operator|+
literal|"emptyFile"
decl_stmt|;
name|Path
name|newFilePath
init|=
operator|new
name|Path
argument_list|(
name|newFile
argument_list|)
decl_stmt|;
name|RecordWriter
name|recWriter
init|=
name|outFileFormat
operator|.
name|getHiveRecordWriter
argument_list|(
name|job
argument_list|,
name|newFilePath
argument_list|,
name|Text
operator|.
name|class
argument_list|,
literal|false
argument_list|,
name|props
argument_list|,
literal|null
argument_list|)
decl_stmt|;
if|if
condition|(
name|dummyRow
condition|)
block|{
comment|// empty files are omitted at CombineHiveInputFormat.
comment|// for meta-data only query, it effectively makes partition columns disappear..
comment|// this could be fixed by other methods, but this seemed to be the most easy (HIVEV-2955)
name|recWriter
operator|.
name|write
argument_list|(
operator|new
name|Text
argument_list|(
literal|"empty"
argument_list|)
argument_list|)
expr_stmt|;
comment|// written via HiveIgnoreKeyTextOutputFormat
block|}
name|recWriter
operator|.
name|close
argument_list|(
literal|false
argument_list|)
expr_stmt|;
return|return
name|StringInternUtils
operator|.
name|internUriStringsInPath
argument_list|(
name|newPath
argument_list|)
return|;
block|}
annotation|@
name|SuppressWarnings
argument_list|(
literal|"rawtypes"
argument_list|)
specifier|private
specifier|static
name|Path
name|createDummyFileForEmptyPartition
parameter_list|(
name|Path
name|path
parameter_list|,
name|JobConf
name|job
parameter_list|,
name|PartitionDesc
name|partDesc
parameter_list|,
name|Path
name|hiveScratchDir
parameter_list|)
throws|throws
name|Exception
block|{
name|String
name|strPath
init|=
name|path
operator|.
name|toString
argument_list|()
decl_stmt|;
comment|// The input file does not exist, replace it by a empty file
if|if
condition|(
name|partDesc
operator|.
name|getTableDesc
argument_list|()
operator|.
name|isNonNative
argument_list|()
condition|)
block|{
comment|// if this isn't a hive table we can't create an empty file for it.
return|return
name|path
return|;
block|}
name|Properties
name|props
init|=
name|SerDeUtils
operator|.
name|createOverlayedProperties
argument_list|(
name|partDesc
operator|.
name|getTableDesc
argument_list|()
operator|.
name|getProperties
argument_list|()
argument_list|,
name|partDesc
operator|.
name|getProperties
argument_list|()
argument_list|)
decl_stmt|;
name|HiveOutputFormat
name|outFileFormat
init|=
name|HiveFileFormatUtils
operator|.
name|getHiveOutputFormat
argument_list|(
name|job
argument_list|,
name|partDesc
argument_list|)
decl_stmt|;
name|boolean
name|oneRow
init|=
name|partDesc
operator|.
name|getInputFileFormatClass
argument_list|()
operator|==
name|OneNullRowInputFormat
operator|.
name|class
decl_stmt|;
name|Path
name|newPath
init|=
name|createEmptyFile
argument_list|(
name|hiveScratchDir
argument_list|,
name|outFileFormat
argument_list|,
name|job
argument_list|,
name|props
argument_list|,
name|oneRow
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Changed input file {} to empty file {} ({})"
argument_list|,
name|strPath
argument_list|,
name|newPath
argument_list|,
name|oneRow
argument_list|)
expr_stmt|;
return|return
name|newPath
return|;
block|}
specifier|private
specifier|static
name|void
name|updatePathForMapWork
parameter_list|(
name|Path
name|newPath
parameter_list|,
name|MapWork
name|work
parameter_list|,
name|Path
name|path
parameter_list|)
block|{
comment|// update the work
if|if
condition|(
operator|!
name|newPath
operator|.
name|equals
argument_list|(
name|path
argument_list|)
condition|)
block|{
name|PartitionDesc
name|partDesc
init|=
name|work
operator|.
name|getPathToPartitionInfo
argument_list|()
operator|.
name|get
argument_list|(
name|path
argument_list|)
decl_stmt|;
name|work
operator|.
name|addPathToAlias
argument_list|(
name|newPath
argument_list|,
name|work
operator|.
name|getPathToAliases
argument_list|()
operator|.
name|get
argument_list|(
name|path
argument_list|)
argument_list|)
expr_stmt|;
name|work
operator|.
name|removePathToAlias
argument_list|(
name|path
argument_list|)
expr_stmt|;
name|work
operator|.
name|removePathToPartitionInfo
argument_list|(
name|path
argument_list|)
expr_stmt|;
name|work
operator|.
name|addPathToPartitionInfo
argument_list|(
name|newPath
argument_list|,
name|partDesc
argument_list|)
expr_stmt|;
block|}
block|}
annotation|@
name|SuppressWarnings
argument_list|(
literal|"rawtypes"
argument_list|)
specifier|private
specifier|static
name|Path
name|createDummyFileForEmptyTable
parameter_list|(
name|JobConf
name|job
parameter_list|,
name|MapWork
name|work
parameter_list|,
name|Path
name|hiveScratchDir
parameter_list|,
name|String
name|alias
parameter_list|)
throws|throws
name|Exception
block|{
name|TableDesc
name|tableDesc
init|=
name|work
operator|.
name|getAliasToPartnInfo
argument_list|()
operator|.
name|get
argument_list|(
name|alias
argument_list|)
operator|.
name|getTableDesc
argument_list|()
decl_stmt|;
if|if
condition|(
name|tableDesc
operator|.
name|isNonNative
argument_list|()
condition|)
block|{
comment|// if it does not need native storage, we can't create an empty file for it.
return|return
literal|null
return|;
block|}
name|Properties
name|props
init|=
name|tableDesc
operator|.
name|getProperties
argument_list|()
decl_stmt|;
name|HiveOutputFormat
name|outFileFormat
init|=
name|HiveFileFormatUtils
operator|.
name|getHiveOutputFormat
argument_list|(
name|job
argument_list|,
name|tableDesc
argument_list|)
decl_stmt|;
name|Path
name|newPath
init|=
name|createEmptyFile
argument_list|(
name|hiveScratchDir
argument_list|,
name|outFileFormat
argument_list|,
name|job
argument_list|,
name|props
argument_list|,
literal|false
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Changed input file for alias {} to newPath"
argument_list|,
name|alias
argument_list|,
name|newPath
argument_list|)
expr_stmt|;
comment|// update the work
name|LinkedHashMap
argument_list|<
name|Path
argument_list|,
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|>
name|pathToAliases
init|=
name|work
operator|.
name|getPathToAliases
argument_list|()
decl_stmt|;
name|ArrayList
argument_list|<
name|String
argument_list|>
name|newList
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|(
literal|1
argument_list|)
decl_stmt|;
name|newList
operator|.
name|add
argument_list|(
name|alias
argument_list|)
expr_stmt|;
name|pathToAliases
operator|.
name|put
argument_list|(
name|newPath
argument_list|,
name|newList
argument_list|)
expr_stmt|;
name|work
operator|.
name|setPathToAliases
argument_list|(
name|pathToAliases
argument_list|)
expr_stmt|;
name|PartitionDesc
name|pDesc
init|=
name|work
operator|.
name|getAliasToPartnInfo
argument_list|()
operator|.
name|get
argument_list|(
name|alias
argument_list|)
operator|.
name|clone
argument_list|()
decl_stmt|;
name|work
operator|.
name|addPathToPartitionInfo
argument_list|(
name|newPath
argument_list|,
name|pDesc
argument_list|)
expr_stmt|;
return|return
name|newPath
return|;
block|}
specifier|private
specifier|static
specifier|final
name|Path
index|[]
name|EMPTY_PATH
init|=
operator|new
name|Path
index|[
literal|0
index|]
decl_stmt|;
comment|/**    * setInputPaths add all the paths in the provided list to the Job conf object    * as input paths for the job.    *    * @param job    * @param pathsToAdd    */
specifier|public
specifier|static
name|void
name|setInputPaths
parameter_list|(
name|JobConf
name|job
parameter_list|,
name|List
argument_list|<
name|Path
argument_list|>
name|pathsToAdd
parameter_list|)
block|{
name|Path
index|[]
name|addedPaths
init|=
name|FileInputFormat
operator|.
name|getInputPaths
argument_list|(
name|job
argument_list|)
decl_stmt|;
if|if
condition|(
name|addedPaths
operator|==
literal|null
condition|)
block|{
name|addedPaths
operator|=
name|EMPTY_PATH
expr_stmt|;
block|}
name|Path
index|[]
name|combined
init|=
operator|new
name|Path
index|[
name|addedPaths
operator|.
name|length
operator|+
name|pathsToAdd
operator|.
name|size
argument_list|()
index|]
decl_stmt|;
name|System
operator|.
name|arraycopy
argument_list|(
name|addedPaths
argument_list|,
literal|0
argument_list|,
name|combined
argument_list|,
literal|0
argument_list|,
name|addedPaths
operator|.
name|length
argument_list|)
expr_stmt|;
name|int
name|i
init|=
literal|0
decl_stmt|;
for|for
control|(
name|Path
name|p
range|:
name|pathsToAdd
control|)
block|{
name|combined
index|[
name|addedPaths
operator|.
name|length
operator|+
operator|(
name|i
operator|++
operator|)
index|]
operator|=
name|p
expr_stmt|;
block|}
name|FileInputFormat
operator|.
name|setInputPaths
argument_list|(
name|job
argument_list|,
name|combined
argument_list|)
expr_stmt|;
block|}
comment|/**    * Set hive input format, and input format file if necessary.    */
specifier|public
specifier|static
name|void
name|setInputAttributes
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|MapWork
name|mWork
parameter_list|)
block|{
name|HiveConf
operator|.
name|ConfVars
name|var
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|conf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_EXECUTION_ENGINE
argument_list|)
operator|.
name|equals
argument_list|(
literal|"tez"
argument_list|)
condition|?
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVETEZINPUTFORMAT
else|:
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEINPUTFORMAT
decl_stmt|;
if|if
condition|(
name|mWork
operator|.
name|getInputformat
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|HiveConf
operator|.
name|setVar
argument_list|(
name|conf
argument_list|,
name|var
argument_list|,
name|mWork
operator|.
name|getInputformat
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|mWork
operator|.
name|getIndexIntermediateFile
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|conf
operator|.
name|set
argument_list|(
name|ConfVars
operator|.
name|HIVE_INDEX_COMPACT_FILE
operator|.
name|varname
argument_list|,
name|mWork
operator|.
name|getIndexIntermediateFile
argument_list|()
argument_list|)
expr_stmt|;
name|conf
operator|.
name|set
argument_list|(
name|ConfVars
operator|.
name|HIVE_INDEX_BLOCKFILTER_FILE
operator|.
name|varname
argument_list|,
name|mWork
operator|.
name|getIndexIntermediateFile
argument_list|()
argument_list|)
expr_stmt|;
block|}
comment|// Intentionally overwrites anything the user may have put here
name|conf
operator|.
name|setBoolean
argument_list|(
literal|"hive.input.format.sorted"
argument_list|,
name|mWork
operator|.
name|isInputFormatSorted
argument_list|()
argument_list|)
expr_stmt|;
block|}
comment|/**    * Hive uses tmp directories to capture the output of each FileSinkOperator.    * This method creates all necessary tmp directories for FileSinks in the Mapwork.    *    * @param conf Used to get the right FileSystem    * @param mWork Used to find FileSinkOperators    * @throws IOException    */
specifier|public
specifier|static
name|void
name|createTmpDirs
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|MapWork
name|mWork
parameter_list|)
throws|throws
name|IOException
block|{
name|Map
argument_list|<
name|Path
argument_list|,
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|>
name|pa
init|=
name|mWork
operator|.
name|getPathToAliases
argument_list|()
decl_stmt|;
if|if
condition|(
name|MapUtils
operator|.
name|isNotEmpty
argument_list|(
name|pa
argument_list|)
condition|)
block|{
comment|// common case: 1 table scan per map-work
comment|// rare case: smb joins
name|HashSet
argument_list|<
name|String
argument_list|>
name|aliases
init|=
operator|new
name|HashSet
argument_list|<
name|String
argument_list|>
argument_list|(
literal|1
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|Operator
argument_list|<
name|?
extends|extends
name|OperatorDesc
argument_list|>
argument_list|>
name|ops
init|=
operator|new
name|ArrayList
argument_list|<
name|Operator
argument_list|<
name|?
extends|extends
name|OperatorDesc
argument_list|>
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|List
argument_list|<
name|String
argument_list|>
name|ls
range|:
name|pa
operator|.
name|values
argument_list|()
control|)
block|{
for|for
control|(
name|String
name|a
range|:
name|ls
control|)
block|{
name|aliases
operator|.
name|add
argument_list|(
name|a
argument_list|)
expr_stmt|;
block|}
block|}
for|for
control|(
name|String
name|a
range|:
name|aliases
control|)
block|{
name|ops
operator|.
name|add
argument_list|(
name|mWork
operator|.
name|getAliasToWork
argument_list|()
operator|.
name|get
argument_list|(
name|a
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|createTmpDirs
argument_list|(
name|conf
argument_list|,
name|ops
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * Hive uses tmp directories to capture the output of each FileSinkOperator.    * This method creates all necessary tmp directories for FileSinks in the ReduceWork.    *    * @param conf Used to get the right FileSystem    * @param rWork Used to find FileSinkOperators    * @throws IOException    */
annotation|@
name|SuppressWarnings
argument_list|(
literal|"unchecked"
argument_list|)
specifier|public
specifier|static
name|void
name|createTmpDirs
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|ReduceWork
name|rWork
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|rWork
operator|==
literal|null
condition|)
block|{
return|return;
block|}
name|List
argument_list|<
name|Operator
argument_list|<
name|?
extends|extends
name|OperatorDesc
argument_list|>
argument_list|>
name|ops
init|=
operator|new
name|LinkedList
argument_list|<
name|Operator
argument_list|<
name|?
extends|extends
name|OperatorDesc
argument_list|>
argument_list|>
argument_list|()
decl_stmt|;
name|ops
operator|.
name|add
argument_list|(
name|rWork
operator|.
name|getReducer
argument_list|()
argument_list|)
expr_stmt|;
name|createTmpDirs
argument_list|(
name|conf
argument_list|,
name|ops
argument_list|)
expr_stmt|;
block|}
specifier|private
specifier|static
name|void
name|createTmpDirs
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|List
argument_list|<
name|Operator
argument_list|<
name|?
extends|extends
name|OperatorDesc
argument_list|>
argument_list|>
name|ops
parameter_list|)
throws|throws
name|IOException
block|{
while|while
condition|(
operator|!
name|ops
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|Operator
argument_list|<
name|?
extends|extends
name|OperatorDesc
argument_list|>
name|op
init|=
name|ops
operator|.
name|remove
argument_list|(
literal|0
argument_list|)
decl_stmt|;
if|if
condition|(
name|op
operator|instanceof
name|FileSinkOperator
condition|)
block|{
name|FileSinkDesc
name|fdesc
init|=
operator|(
operator|(
name|FileSinkOperator
operator|)
name|op
operator|)
operator|.
name|getConf
argument_list|()
decl_stmt|;
if|if
condition|(
name|fdesc
operator|.
name|isMmTable
argument_list|()
condition|)
block|{
continue|continue;
comment|// No need to create for MM tables
block|}
name|Path
name|tempDir
init|=
name|fdesc
operator|.
name|getDirName
argument_list|()
decl_stmt|;
if|if
condition|(
name|tempDir
operator|!=
literal|null
condition|)
block|{
name|Path
name|tempPath
init|=
name|Utilities
operator|.
name|toTempPath
argument_list|(
name|tempDir
argument_list|)
decl_stmt|;
name|FileSystem
name|fs
init|=
name|tempPath
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|fs
operator|.
name|mkdirs
argument_list|(
name|tempPath
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
name|op
operator|.
name|getChildOperators
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|ops
operator|.
name|addAll
argument_list|(
name|op
operator|.
name|getChildOperators
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
block|}
specifier|public
specifier|static
name|boolean
name|createDirsWithPermission
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|Path
name|mkdirPath
parameter_list|,
name|FsPermission
name|fsPermission
parameter_list|,
name|boolean
name|recursive
parameter_list|)
throws|throws
name|IOException
block|{
name|String
name|origUmask
init|=
literal|null
decl_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Create dirs {} with permission {} recursive {}"
argument_list|,
name|mkdirPath
argument_list|,
name|fsPermission
argument_list|,
name|recursive
argument_list|)
expr_stmt|;
if|if
condition|(
name|recursive
condition|)
block|{
name|origUmask
operator|=
name|conf
operator|.
name|get
argument_list|(
name|FsPermission
operator|.
name|UMASK_LABEL
argument_list|)
expr_stmt|;
comment|// this umask is required because by default the hdfs mask is 022 resulting in
comment|// all parents getting the fsPermission& !(022) permission instead of fsPermission
name|conf
operator|.
name|set
argument_list|(
name|FsPermission
operator|.
name|UMASK_LABEL
argument_list|,
literal|"000"
argument_list|)
expr_stmt|;
block|}
name|FileSystem
name|fs
init|=
name|ShimLoader
operator|.
name|getHadoopShims
argument_list|()
operator|.
name|getNonCachedFileSystem
argument_list|(
name|mkdirPath
operator|.
name|toUri
argument_list|()
argument_list|,
name|conf
argument_list|)
decl_stmt|;
name|boolean
name|retval
init|=
literal|false
decl_stmt|;
try|try
block|{
name|retval
operator|=
name|fs
operator|.
name|mkdirs
argument_list|(
name|mkdirPath
argument_list|,
name|fsPermission
argument_list|)
expr_stmt|;
name|resetUmaskInConf
argument_list|(
name|conf
argument_list|,
name|recursive
argument_list|,
name|origUmask
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|ioe
parameter_list|)
block|{
name|resetUmaskInConf
argument_list|(
name|conf
argument_list|,
name|recursive
argument_list|,
name|origUmask
argument_list|)
expr_stmt|;
throw|throw
name|ioe
throw|;
block|}
finally|finally
block|{
name|IOUtils
operator|.
name|closeStream
argument_list|(
name|fs
argument_list|)
expr_stmt|;
block|}
return|return
name|retval
return|;
block|}
specifier|private
specifier|static
name|void
name|resetUmaskInConf
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|boolean
name|unsetUmask
parameter_list|,
name|String
name|origUmask
parameter_list|)
block|{
if|if
condition|(
name|unsetUmask
condition|)
block|{
if|if
condition|(
name|origUmask
operator|!=
literal|null
condition|)
block|{
name|conf
operator|.
name|set
argument_list|(
name|FsPermission
operator|.
name|UMASK_LABEL
argument_list|,
name|origUmask
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|conf
operator|.
name|unset
argument_list|(
name|FsPermission
operator|.
name|UMASK_LABEL
argument_list|)
expr_stmt|;
block|}
block|}
block|}
comment|/**    * Returns true if a plan is both configured for vectorized execution    * and the node is vectorized.    *    * The plan may be configured for vectorization    * but vectorization disallowed eg. for FetchOperator execution.    */
specifier|public
specifier|static
name|boolean
name|getIsVectorized
parameter_list|(
name|Configuration
name|conf
parameter_list|)
block|{
if|if
condition|(
name|conf
operator|.
name|get
argument_list|(
name|VECTOR_MODE
argument_list|)
operator|!=
literal|null
condition|)
block|{
comment|// this code path is necessary, because with HS2 and client
comment|// side split generation we end up not finding the map work.
comment|// This is because of thread local madness (tez split
comment|// generation is multi-threaded - HS2 plan cache uses thread
comment|// locals).
return|return
name|conf
operator|.
name|getBoolean
argument_list|(
name|VECTOR_MODE
argument_list|,
literal|false
argument_list|)
return|;
block|}
else|else
block|{
if|if
condition|(
name|HiveConf
operator|.
name|getBoolVar
argument_list|(
name|conf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_VECTORIZATION_ENABLED
argument_list|)
operator|&&
name|Utilities
operator|.
name|getPlanPath
argument_list|(
name|conf
argument_list|)
operator|!=
literal|null
condition|)
block|{
name|MapWork
name|mapWork
init|=
name|Utilities
operator|.
name|getMapWork
argument_list|(
name|conf
argument_list|)
decl_stmt|;
return|return
name|mapWork
operator|.
name|getVectorMode
argument_list|()
return|;
block|}
else|else
block|{
return|return
literal|false
return|;
block|}
block|}
block|}
specifier|public
specifier|static
name|boolean
name|getIsVectorized
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|MapWork
name|mapWork
parameter_list|)
block|{
return|return
name|HiveConf
operator|.
name|getBoolVar
argument_list|(
name|conf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_VECTORIZATION_ENABLED
argument_list|)
operator|&&
name|mapWork
operator|.
name|getVectorMode
argument_list|()
return|;
block|}
comment|/**    * @param conf    * @return the configured VectorizedRowBatchCtx for a MapWork task.    */
specifier|public
specifier|static
name|VectorizedRowBatchCtx
name|getVectorizedRowBatchCtx
parameter_list|(
name|Configuration
name|conf
parameter_list|)
block|{
name|VectorizedRowBatchCtx
name|result
init|=
literal|null
decl_stmt|;
if|if
condition|(
name|HiveConf
operator|.
name|getBoolVar
argument_list|(
name|conf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_VECTORIZATION_ENABLED
argument_list|)
operator|&&
name|Utilities
operator|.
name|getPlanPath
argument_list|(
name|conf
argument_list|)
operator|!=
literal|null
condition|)
block|{
name|MapWork
name|mapWork
init|=
name|Utilities
operator|.
name|getMapWork
argument_list|(
name|conf
argument_list|)
decl_stmt|;
if|if
condition|(
name|mapWork
operator|!=
literal|null
operator|&&
name|mapWork
operator|.
name|getVectorMode
argument_list|()
condition|)
block|{
name|result
operator|=
name|mapWork
operator|.
name|getVectorizedRowBatchCtx
argument_list|()
expr_stmt|;
block|}
block|}
return|return
name|result
return|;
block|}
specifier|public
specifier|static
name|void
name|clearWorkMapForConf
parameter_list|(
name|Configuration
name|conf
parameter_list|)
block|{
comment|// Remove cached query plans for the current query only
name|Path
name|mapPath
init|=
name|getPlanPath
argument_list|(
name|conf
argument_list|,
name|MAP_PLAN_NAME
argument_list|)
decl_stmt|;
name|Path
name|reducePath
init|=
name|getPlanPath
argument_list|(
name|conf
argument_list|,
name|REDUCE_PLAN_NAME
argument_list|)
decl_stmt|;
if|if
condition|(
name|mapPath
operator|!=
literal|null
condition|)
block|{
name|gWorkMap
operator|.
name|get
argument_list|(
name|conf
argument_list|)
operator|.
name|remove
argument_list|(
name|mapPath
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|reducePath
operator|!=
literal|null
condition|)
block|{
name|gWorkMap
operator|.
name|get
argument_list|(
name|conf
argument_list|)
operator|.
name|remove
argument_list|(
name|reducePath
argument_list|)
expr_stmt|;
block|}
comment|// TODO: should this also clean merge work?
block|}
specifier|public
specifier|static
name|void
name|clearWorkMap
parameter_list|(
name|Configuration
name|conf
parameter_list|)
block|{
name|gWorkMap
operator|.
name|get
argument_list|(
name|conf
argument_list|)
operator|.
name|clear
argument_list|()
expr_stmt|;
block|}
comment|/**    * Create a temp dir in specified baseDir    * This can go away once hive moves to support only JDK 7    *  and can use Files.createTempDirectory    *  Guava Files.createTempDir() does not take a base dir    * @param baseDir - directory under which new temp dir will be created    * @return File object for new temp dir    */
specifier|public
specifier|static
name|File
name|createTempDir
parameter_list|(
name|String
name|baseDir
parameter_list|)
block|{
comment|//try creating the temp dir MAX_ATTEMPTS times
specifier|final
name|int
name|MAX_ATTEMPS
init|=
literal|30
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|MAX_ATTEMPS
condition|;
name|i
operator|++
control|)
block|{
comment|//pick a random file name
name|String
name|tempDirName
init|=
literal|"tmp_"
operator|+
operator|(
call|(
name|int
call|)
argument_list|(
literal|100000
operator|*
name|Math
operator|.
name|random
argument_list|()
argument_list|)
operator|)
decl_stmt|;
comment|//return if dir could successfully be created with that file name
name|File
name|tempDir
init|=
operator|new
name|File
argument_list|(
name|baseDir
argument_list|,
name|tempDirName
argument_list|)
decl_stmt|;
if|if
condition|(
name|tempDir
operator|.
name|mkdir
argument_list|()
condition|)
block|{
return|return
name|tempDir
return|;
block|}
block|}
throw|throw
operator|new
name|IllegalStateException
argument_list|(
literal|"Failed to create a temp dir under "
operator|+
name|baseDir
operator|+
literal|" Giving up after "
operator|+
name|MAX_ATTEMPS
operator|+
literal|" attempts"
argument_list|)
throw|;
block|}
comment|/**    * Skip header lines in the table file when reading the record.    *    * @param currRecReader    *          Record reader.    *    * @param headerCount    *          Header line number of the table files.    *    * @param key    *          Key of current reading record.    *    * @param value    *          Value of current reading record.    *    * @return Return true if there are 0 or more records left in the file    *         after skipping all headers, otherwise return false.    */
specifier|public
specifier|static
name|boolean
name|skipHeader
parameter_list|(
name|RecordReader
argument_list|<
name|WritableComparable
argument_list|,
name|Writable
argument_list|>
name|currRecReader
parameter_list|,
name|int
name|headerCount
parameter_list|,
name|WritableComparable
name|key
parameter_list|,
name|Writable
name|value
parameter_list|)
throws|throws
name|IOException
block|{
while|while
condition|(
name|headerCount
operator|>
literal|0
condition|)
block|{
if|if
condition|(
operator|!
name|currRecReader
operator|.
name|next
argument_list|(
name|key
argument_list|,
name|value
argument_list|)
condition|)
block|{
return|return
literal|false
return|;
block|}
name|headerCount
operator|--
expr_stmt|;
block|}
return|return
literal|true
return|;
block|}
comment|/**    * Get header line count for a table.    *    * @param table    *          Table description for target table.    *    */
specifier|public
specifier|static
name|int
name|getHeaderCount
parameter_list|(
name|TableDesc
name|table
parameter_list|)
throws|throws
name|IOException
block|{
name|int
name|headerCount
decl_stmt|;
try|try
block|{
name|headerCount
operator|=
name|Integer
operator|.
name|parseInt
argument_list|(
name|table
operator|.
name|getProperties
argument_list|()
operator|.
name|getProperty
argument_list|(
name|serdeConstants
operator|.
name|HEADER_COUNT
argument_list|,
literal|"0"
argument_list|)
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|NumberFormatException
name|nfe
parameter_list|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
name|nfe
argument_list|)
throw|;
block|}
return|return
name|headerCount
return|;
block|}
comment|/**    * Get footer line count for a table.    *    * @param table    *          Table description for target table.    *    * @param job    *          Job configuration for current job.    */
specifier|public
specifier|static
name|int
name|getFooterCount
parameter_list|(
name|TableDesc
name|table
parameter_list|,
name|JobConf
name|job
parameter_list|)
throws|throws
name|IOException
block|{
name|int
name|footerCount
decl_stmt|;
try|try
block|{
name|footerCount
operator|=
name|Integer
operator|.
name|parseInt
argument_list|(
name|table
operator|.
name|getProperties
argument_list|()
operator|.
name|getProperty
argument_list|(
name|serdeConstants
operator|.
name|FOOTER_COUNT
argument_list|,
literal|"0"
argument_list|)
argument_list|)
expr_stmt|;
if|if
condition|(
name|footerCount
operator|>
name|HiveConf
operator|.
name|getIntVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_FILE_MAX_FOOTER
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"footer number exceeds the limit defined in hive.file.max.footer"
argument_list|)
throw|;
block|}
block|}
catch|catch
parameter_list|(
name|NumberFormatException
name|nfe
parameter_list|)
block|{
comment|// Footer line number must be set as an integer.
throw|throw
operator|new
name|IOException
argument_list|(
name|nfe
argument_list|)
throw|;
block|}
return|return
name|footerCount
return|;
block|}
comment|/**    * Convert path to qualified path.    *    * @param conf    *          Hive configuration.    * @param path    *          Path to convert.    * @return Qualified path    */
specifier|public
specifier|static
name|String
name|getQualifiedPath
parameter_list|(
name|HiveConf
name|conf
parameter_list|,
name|Path
name|path
parameter_list|)
throws|throws
name|HiveException
block|{
name|FileSystem
name|fs
decl_stmt|;
if|if
condition|(
name|path
operator|==
literal|null
condition|)
block|{
return|return
literal|null
return|;
block|}
try|try
block|{
name|fs
operator|=
name|path
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
expr_stmt|;
return|return
name|fs
operator|.
name|makeQualified
argument_list|(
name|path
argument_list|)
operator|.
name|toString
argument_list|()
return|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    * Checks if the current HiveServer2 logging operation level is>= PERFORMANCE.    * @param conf Hive configuration.    * @return true if current HiveServer2 logging operation level is>= PERFORMANCE.    * Else, false.    */
specifier|public
specifier|static
name|boolean
name|isPerfOrAboveLogging
parameter_list|(
name|HiveConf
name|conf
parameter_list|)
block|{
name|String
name|loggingLevel
init|=
name|conf
operator|.
name|getVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_SERVER2_LOGGING_OPERATION_LEVEL
argument_list|)
decl_stmt|;
return|return
name|conf
operator|.
name|getBoolVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_SERVER2_LOGGING_OPERATION_ENABLED
argument_list|)
operator|&&
operator|(
name|loggingLevel
operator|.
name|equalsIgnoreCase
argument_list|(
literal|"PERFORMANCE"
argument_list|)
operator|||
name|loggingLevel
operator|.
name|equalsIgnoreCase
argument_list|(
literal|"VERBOSE"
argument_list|)
operator|)
return|;
block|}
comment|/**    * Returns the full path to the Jar containing the class. It always return a JAR.    *    * @param klass    *          class.    *    * @return path to the Jar containing the class.    */
annotation|@
name|SuppressWarnings
argument_list|(
literal|"rawtypes"
argument_list|)
specifier|public
specifier|static
name|String
name|jarFinderGetJar
parameter_list|(
name|Class
name|klass
parameter_list|)
block|{
name|Preconditions
operator|.
name|checkNotNull
argument_list|(
name|klass
argument_list|,
literal|"klass"
argument_list|)
expr_stmt|;
name|ClassLoader
name|loader
init|=
name|klass
operator|.
name|getClassLoader
argument_list|()
decl_stmt|;
if|if
condition|(
name|loader
operator|!=
literal|null
condition|)
block|{
name|String
name|class_file
init|=
name|klass
operator|.
name|getName
argument_list|()
operator|.
name|replaceAll
argument_list|(
literal|"\\."
argument_list|,
literal|"/"
argument_list|)
operator|+
literal|".class"
decl_stmt|;
try|try
block|{
for|for
control|(
name|Enumeration
name|itr
init|=
name|loader
operator|.
name|getResources
argument_list|(
name|class_file
argument_list|)
init|;
name|itr
operator|.
name|hasMoreElements
argument_list|()
condition|;
control|)
block|{
name|URL
name|url
init|=
operator|(
name|URL
operator|)
name|itr
operator|.
name|nextElement
argument_list|()
decl_stmt|;
name|String
name|path
init|=
name|url
operator|.
name|getPath
argument_list|()
decl_stmt|;
if|if
condition|(
name|path
operator|.
name|startsWith
argument_list|(
literal|"file:"
argument_list|)
condition|)
block|{
name|path
operator|=
name|path
operator|.
name|substring
argument_list|(
literal|"file:"
operator|.
name|length
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|path
operator|=
name|URLDecoder
operator|.
name|decode
argument_list|(
name|path
argument_list|,
literal|"UTF-8"
argument_list|)
expr_stmt|;
if|if
condition|(
literal|"jar"
operator|.
name|equals
argument_list|(
name|url
operator|.
name|getProtocol
argument_list|()
argument_list|)
condition|)
block|{
name|path
operator|=
name|URLDecoder
operator|.
name|decode
argument_list|(
name|path
argument_list|,
literal|"UTF-8"
argument_list|)
expr_stmt|;
return|return
name|path
operator|.
name|replaceAll
argument_list|(
literal|"!.*$"
argument_list|,
literal|""
argument_list|)
return|;
block|}
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
return|return
literal|null
return|;
block|}
specifier|public
specifier|static
name|int
name|getDPColOffset
parameter_list|(
name|FileSinkDesc
name|conf
parameter_list|)
block|{
if|if
condition|(
name|conf
operator|.
name|getWriteType
argument_list|()
operator|==
name|AcidUtils
operator|.
name|Operation
operator|.
name|DELETE
condition|)
block|{
comment|// For deletes, there is only ROW__ID in non-partitioning, non-bucketing columns.
comment|//See : UpdateDeleteSemanticAnalyzer::reparseAndSuperAnalyze() for details.
return|return
literal|1
return|;
block|}
elseif|else
if|if
condition|(
name|conf
operator|.
name|getWriteType
argument_list|()
operator|==
name|AcidUtils
operator|.
name|Operation
operator|.
name|UPDATE
condition|)
block|{
comment|// For updates, ROW__ID is an extra column at index 0.
comment|//See : UpdateDeleteSemanticAnalyzer::reparseAndSuperAnalyze() for details.
return|return
name|getColumnNames
argument_list|(
name|conf
operator|.
name|getTableInfo
argument_list|()
operator|.
name|getProperties
argument_list|()
argument_list|)
operator|.
name|size
argument_list|()
operator|+
literal|1
return|;
block|}
else|else
block|{
return|return
name|getColumnNames
argument_list|(
name|conf
operator|.
name|getTableInfo
argument_list|()
operator|.
name|getProperties
argument_list|()
argument_list|)
operator|.
name|size
argument_list|()
return|;
block|}
block|}
specifier|public
specifier|static
name|List
argument_list|<
name|String
argument_list|>
name|getStatsTmpDirs
parameter_list|(
name|BaseWork
name|work
parameter_list|,
name|Configuration
name|conf
parameter_list|)
block|{
name|List
argument_list|<
name|String
argument_list|>
name|statsTmpDirs
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|()
decl_stmt|;
if|if
condition|(
operator|!
name|StatsSetupConst
operator|.
name|StatDB
operator|.
name|fs
operator|.
name|name
argument_list|()
operator|.
name|equalsIgnoreCase
argument_list|(
name|HiveConf
operator|.
name|getVar
argument_list|(
name|conf
argument_list|,
name|ConfVars
operator|.
name|HIVESTATSDBCLASS
argument_list|)
argument_list|)
condition|)
block|{
comment|// no-op for non-fs stats collection
return|return
name|statsTmpDirs
return|;
block|}
comment|// if its auto-stats gather for inserts or CTAS, stats dir will be in FileSink
name|Set
argument_list|<
name|Operator
argument_list|<
name|?
extends|extends
name|OperatorDesc
argument_list|>
argument_list|>
name|ops
init|=
name|work
operator|.
name|getAllLeafOperators
argument_list|()
decl_stmt|;
if|if
condition|(
name|work
operator|instanceof
name|MapWork
condition|)
block|{
comment|// if its an anlayze statement, stats dir will be in TableScan
name|ops
operator|.
name|addAll
argument_list|(
name|work
operator|.
name|getAllRootOperators
argument_list|()
argument_list|)
expr_stmt|;
block|}
for|for
control|(
name|Operator
argument_list|<
name|?
extends|extends
name|OperatorDesc
argument_list|>
name|op
range|:
name|ops
control|)
block|{
name|OperatorDesc
name|desc
init|=
name|op
operator|.
name|getConf
argument_list|()
decl_stmt|;
name|String
name|statsTmpDir
init|=
literal|null
decl_stmt|;
if|if
condition|(
name|desc
operator|instanceof
name|IStatsGatherDesc
condition|)
block|{
name|statsTmpDir
operator|=
operator|(
operator|(
name|IStatsGatherDesc
operator|)
name|desc
operator|)
operator|.
name|getTmpStatsDir
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
name|statsTmpDir
operator|!=
literal|null
operator|&&
operator|!
name|statsTmpDir
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|statsTmpDirs
operator|.
name|add
argument_list|(
name|statsTmpDir
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|statsTmpDirs
return|;
block|}
specifier|public
specifier|static
name|boolean
name|isSchemaEvolutionEnabled
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|boolean
name|isAcid
parameter_list|)
block|{
return|return
name|isAcid
operator|||
name|HiveConf
operator|.
name|getBoolVar
argument_list|(
name|conf
argument_list|,
name|ConfVars
operator|.
name|HIVE_SCHEMA_EVOLUTION
argument_list|)
return|;
block|}
specifier|public
specifier|static
name|boolean
name|isInputFileFormatSelfDescribing
parameter_list|(
name|PartitionDesc
name|pd
parameter_list|)
block|{
name|Class
argument_list|<
name|?
argument_list|>
name|inputFormatClass
init|=
name|pd
operator|.
name|getInputFileFormatClass
argument_list|()
decl_stmt|;
return|return
name|SelfDescribingInputFormatInterface
operator|.
name|class
operator|.
name|isAssignableFrom
argument_list|(
name|inputFormatClass
argument_list|)
return|;
block|}
specifier|public
specifier|static
name|boolean
name|isInputFileFormatVectorized
parameter_list|(
name|PartitionDesc
name|pd
parameter_list|)
block|{
name|Class
argument_list|<
name|?
argument_list|>
name|inputFormatClass
init|=
name|pd
operator|.
name|getInputFileFormatClass
argument_list|()
decl_stmt|;
return|return
name|VectorizedInputFormatInterface
operator|.
name|class
operator|.
name|isAssignableFrom
argument_list|(
name|inputFormatClass
argument_list|)
return|;
block|}
specifier|public
specifier|static
name|Collection
argument_list|<
name|Class
argument_list|<
name|?
argument_list|>
argument_list|>
name|getClassNamesFromConfig
parameter_list|(
name|HiveConf
name|hiveConf
parameter_list|,
name|ConfVars
name|confVar
parameter_list|)
block|{
name|String
index|[]
name|classNames
init|=
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|StringUtils
operator|.
name|getStrings
argument_list|(
name|HiveConf
operator|.
name|getVar
argument_list|(
name|hiveConf
argument_list|,
name|confVar
argument_list|)
argument_list|)
decl_stmt|;
if|if
condition|(
name|classNames
operator|==
literal|null
condition|)
block|{
return|return
operator|new
name|ArrayList
argument_list|<>
argument_list|(
literal|0
argument_list|)
return|;
block|}
name|Collection
argument_list|<
name|Class
argument_list|<
name|?
argument_list|>
argument_list|>
name|classList
init|=
operator|new
name|ArrayList
argument_list|<
name|Class
argument_list|<
name|?
argument_list|>
argument_list|>
argument_list|(
name|classNames
operator|.
name|length
argument_list|)
decl_stmt|;
for|for
control|(
name|String
name|className
range|:
name|classNames
control|)
block|{
if|if
condition|(
name|StringUtils
operator|.
name|isEmpty
argument_list|(
name|className
argument_list|)
condition|)
block|{
continue|continue;
block|}
try|try
block|{
name|classList
operator|.
name|add
argument_list|(
name|Class
operator|.
name|forName
argument_list|(
name|className
argument_list|)
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|ex
parameter_list|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Cannot create class {} for {} checks"
argument_list|,
name|className
argument_list|,
name|confVar
operator|.
name|varname
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|classList
return|;
block|}
specifier|public
specifier|static
name|void
name|addSchemaEvolutionToTableScanOperator
parameter_list|(
name|Table
name|table
parameter_list|,
name|TableScanOperator
name|tableScanOp
parameter_list|)
block|{
name|String
name|colNames
init|=
name|MetaStoreUtils
operator|.
name|getColumnNamesFromFieldSchema
argument_list|(
name|table
operator|.
name|getSd
argument_list|()
operator|.
name|getCols
argument_list|()
argument_list|)
decl_stmt|;
name|String
name|colTypes
init|=
name|MetaStoreUtils
operator|.
name|getColumnTypesFromFieldSchema
argument_list|(
name|table
operator|.
name|getSd
argument_list|()
operator|.
name|getCols
argument_list|()
argument_list|)
decl_stmt|;
name|tableScanOp
operator|.
name|setSchemaEvolution
argument_list|(
name|colNames
argument_list|,
name|colTypes
argument_list|)
expr_stmt|;
block|}
specifier|public
specifier|static
name|void
name|addSchemaEvolutionToTableScanOperator
parameter_list|(
name|StructObjectInspector
name|structOI
parameter_list|,
name|TableScanOperator
name|tableScanOp
parameter_list|)
block|{
name|String
name|colNames
init|=
name|ObjectInspectorUtils
operator|.
name|getFieldNames
argument_list|(
name|structOI
argument_list|)
decl_stmt|;
name|String
name|colTypes
init|=
name|ObjectInspectorUtils
operator|.
name|getFieldTypes
argument_list|(
name|structOI
argument_list|)
decl_stmt|;
name|tableScanOp
operator|.
name|setSchemaEvolution
argument_list|(
name|colNames
argument_list|,
name|colTypes
argument_list|)
expr_stmt|;
block|}
specifier|public
specifier|static
name|void
name|unsetSchemaEvolution
parameter_list|(
name|Configuration
name|conf
parameter_list|)
block|{
name|conf
operator|.
name|unset
argument_list|(
name|IOConstants
operator|.
name|SCHEMA_EVOLUTION_COLUMNS
argument_list|)
expr_stmt|;
name|conf
operator|.
name|unset
argument_list|(
name|IOConstants
operator|.
name|SCHEMA_EVOLUTION_COLUMNS_TYPES
argument_list|)
expr_stmt|;
block|}
specifier|public
specifier|static
name|void
name|addTableSchemaToConf
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|TableScanOperator
name|tableScanOp
parameter_list|)
block|{
name|String
name|schemaEvolutionColumns
init|=
name|tableScanOp
operator|.
name|getSchemaEvolutionColumns
argument_list|()
decl_stmt|;
if|if
condition|(
name|schemaEvolutionColumns
operator|!=
literal|null
condition|)
block|{
name|conf
operator|.
name|set
argument_list|(
name|IOConstants
operator|.
name|SCHEMA_EVOLUTION_COLUMNS
argument_list|,
name|tableScanOp
operator|.
name|getSchemaEvolutionColumns
argument_list|()
argument_list|)
expr_stmt|;
name|conf
operator|.
name|set
argument_list|(
name|IOConstants
operator|.
name|SCHEMA_EVOLUTION_COLUMNS_TYPES
argument_list|,
name|tableScanOp
operator|.
name|getSchemaEvolutionColumnsTypes
argument_list|()
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"schema.evolution.columns and schema.evolution.columns.types not available"
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * Create row key and value object inspectors for reduce vectorization.    * The row object inspector used by ReduceWork needs to be a **standard**    * struct object inspector, not just any struct object inspector.    * @param keyInspector    * @param valueInspector    * @return OI    * @throws HiveException    */
specifier|public
specifier|static
name|StandardStructObjectInspector
name|constructVectorizedReduceRowOI
parameter_list|(
name|StructObjectInspector
name|keyInspector
parameter_list|,
name|StructObjectInspector
name|valueInspector
parameter_list|)
throws|throws
name|HiveException
block|{
name|ArrayList
argument_list|<
name|String
argument_list|>
name|colNames
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
name|ArrayList
argument_list|<
name|ObjectInspector
argument_list|>
name|ois
init|=
operator|new
name|ArrayList
argument_list|<
name|ObjectInspector
argument_list|>
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|?
extends|extends
name|StructField
argument_list|>
name|fields
init|=
name|keyInspector
operator|.
name|getAllStructFieldRefs
argument_list|()
decl_stmt|;
for|for
control|(
name|StructField
name|field
range|:
name|fields
control|)
block|{
name|colNames
operator|.
name|add
argument_list|(
name|Utilities
operator|.
name|ReduceField
operator|.
name|KEY
operator|.
name|toString
argument_list|()
operator|+
literal|'.'
operator|+
name|field
operator|.
name|getFieldName
argument_list|()
argument_list|)
expr_stmt|;
name|ois
operator|.
name|add
argument_list|(
name|field
operator|.
name|getFieldObjectInspector
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|fields
operator|=
name|valueInspector
operator|.
name|getAllStructFieldRefs
argument_list|()
expr_stmt|;
for|for
control|(
name|StructField
name|field
range|:
name|fields
control|)
block|{
name|colNames
operator|.
name|add
argument_list|(
name|Utilities
operator|.
name|ReduceField
operator|.
name|VALUE
operator|.
name|toString
argument_list|()
operator|+
literal|'.'
operator|+
name|field
operator|.
name|getFieldName
argument_list|()
argument_list|)
expr_stmt|;
name|ois
operator|.
name|add
argument_list|(
name|field
operator|.
name|getFieldObjectInspector
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|StandardStructObjectInspector
name|rowObjectInspector
init|=
name|ObjectInspectorFactory
operator|.
name|getStandardStructObjectInspector
argument_list|(
name|colNames
argument_list|,
name|ois
argument_list|)
decl_stmt|;
return|return
name|rowObjectInspector
return|;
block|}
specifier|public
specifier|static
name|String
name|humanReadableByteCount
parameter_list|(
name|long
name|bytes
parameter_list|)
block|{
name|int
name|unit
init|=
literal|1000
decl_stmt|;
comment|// use binary units instead?
if|if
condition|(
name|bytes
operator|<
name|unit
condition|)
block|{
return|return
name|bytes
operator|+
literal|"B"
return|;
block|}
name|int
name|exp
init|=
call|(
name|int
call|)
argument_list|(
name|Math
operator|.
name|log
argument_list|(
name|bytes
argument_list|)
operator|/
name|Math
operator|.
name|log
argument_list|(
name|unit
argument_list|)
argument_list|)
decl_stmt|;
name|String
name|suffix
init|=
literal|"KMGTPE"
operator|.
name|charAt
argument_list|(
name|exp
operator|-
literal|1
argument_list|)
operator|+
literal|""
decl_stmt|;
return|return
name|String
operator|.
name|format
argument_list|(
literal|"%.2f%sB"
argument_list|,
name|bytes
operator|/
name|Math
operator|.
name|pow
argument_list|(
name|unit
argument_list|,
name|exp
argument_list|)
argument_list|,
name|suffix
argument_list|)
return|;
block|}
specifier|private
specifier|static
specifier|final
name|String
name|MANIFEST_EXTENSION
init|=
literal|".manifest"
decl_stmt|;
specifier|private
specifier|static
name|void
name|tryDelete
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|Path
name|path
parameter_list|)
block|{
try|try
block|{
name|fs
operator|.
name|delete
argument_list|(
name|path
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|ex
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Failed to delete {}"
argument_list|,
name|path
argument_list|,
name|ex
argument_list|)
expr_stmt|;
block|}
block|}
specifier|public
specifier|static
name|Path
index|[]
name|getMmDirectoryCandidates
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|Path
name|path
parameter_list|,
name|int
name|dpLevels
parameter_list|,
name|int
name|lbLevels
parameter_list|,
name|PathFilter
name|filter
parameter_list|,
name|long
name|txnId
parameter_list|,
name|int
name|stmtId
parameter_list|,
name|Configuration
name|conf
parameter_list|,
name|boolean
name|isBaseDir
parameter_list|)
throws|throws
name|IOException
block|{
name|int
name|skipLevels
init|=
name|dpLevels
operator|+
name|lbLevels
decl_stmt|;
if|if
condition|(
name|filter
operator|==
literal|null
condition|)
block|{
name|filter
operator|=
operator|new
name|JavaUtils
operator|.
name|IdPathFilter
argument_list|(
name|txnId
argument_list|,
name|stmtId
argument_list|,
literal|true
argument_list|,
literal|false
argument_list|,
name|isBaseDir
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|skipLevels
operator|==
literal|0
condition|)
block|{
return|return
name|statusToPath
argument_list|(
name|fs
operator|.
name|listStatus
argument_list|(
name|path
argument_list|,
name|filter
argument_list|)
argument_list|)
return|;
block|}
comment|// TODO: for some reason, globStatus doesn't work for masks like "...blah/*/delta_0000007_0000007*"
comment|//       the last star throws it off. So, for now, if stmtId is missing use recursion.
if|if
condition|(
name|stmtId
operator|<
literal|0
operator|||
operator|(
name|HiveConf
operator|.
name|getBoolVar
argument_list|(
name|conf
argument_list|,
name|ConfVars
operator|.
name|HIVE_MM_AVOID_GLOBSTATUS_ON_S3
argument_list|)
operator|&&
name|isS3
argument_list|(
name|fs
argument_list|)
operator|)
condition|)
block|{
return|return
name|getMmDirectoryCandidatesRecursive
argument_list|(
name|fs
argument_list|,
name|path
argument_list|,
name|skipLevels
argument_list|,
name|filter
argument_list|)
return|;
block|}
return|return
name|getMmDirectoryCandidatesGlobStatus
argument_list|(
name|fs
argument_list|,
name|path
argument_list|,
name|skipLevels
argument_list|,
name|filter
argument_list|,
name|txnId
argument_list|,
name|stmtId
argument_list|,
name|isBaseDir
argument_list|)
return|;
block|}
specifier|private
specifier|static
name|boolean
name|isS3
parameter_list|(
name|FileSystem
name|fs
parameter_list|)
block|{
try|try
block|{
return|return
literal|"s3a"
operator|.
name|equalsIgnoreCase
argument_list|(
name|fs
operator|.
name|getScheme
argument_list|()
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|UnsupportedOperationException
name|ex
parameter_list|)
block|{
comment|// Some FS-es do not implement getScheme, e.g. ProxyLocalFileSystem.
return|return
literal|false
return|;
block|}
block|}
specifier|private
specifier|static
name|Path
index|[]
name|statusToPath
parameter_list|(
name|FileStatus
index|[]
name|statuses
parameter_list|)
block|{
if|if
condition|(
name|statuses
operator|==
literal|null
condition|)
block|{
return|return
literal|null
return|;
block|}
name|Path
index|[]
name|paths
init|=
operator|new
name|Path
index|[
name|statuses
operator|.
name|length
index|]
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|statuses
operator|.
name|length
condition|;
operator|++
name|i
control|)
block|{
name|paths
index|[
name|i
index|]
operator|=
name|statuses
index|[
name|i
index|]
operator|.
name|getPath
argument_list|()
expr_stmt|;
block|}
return|return
name|paths
return|;
block|}
specifier|private
specifier|static
name|Path
index|[]
name|getMmDirectoryCandidatesRecursive
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|Path
name|path
parameter_list|,
name|int
name|skipLevels
parameter_list|,
name|PathFilter
name|filter
parameter_list|)
throws|throws
name|IOException
block|{
name|String
name|lastRelDir
init|=
literal|null
decl_stmt|;
name|HashSet
argument_list|<
name|Path
argument_list|>
name|results
init|=
operator|new
name|HashSet
argument_list|<
name|Path
argument_list|>
argument_list|()
decl_stmt|;
name|String
name|relRoot
init|=
name|Path
operator|.
name|getPathWithoutSchemeAndAuthority
argument_list|(
name|path
argument_list|)
operator|.
name|toString
argument_list|()
decl_stmt|;
if|if
condition|(
operator|!
name|relRoot
operator|.
name|endsWith
argument_list|(
name|Path
operator|.
name|SEPARATOR
argument_list|)
condition|)
block|{
name|relRoot
operator|+=
name|Path
operator|.
name|SEPARATOR
expr_stmt|;
block|}
name|RemoteIterator
argument_list|<
name|LocatedFileStatus
argument_list|>
name|allFiles
init|=
name|fs
operator|.
name|listFiles
argument_list|(
name|path
argument_list|,
literal|true
argument_list|)
decl_stmt|;
while|while
condition|(
name|allFiles
operator|.
name|hasNext
argument_list|()
condition|)
block|{
name|LocatedFileStatus
name|lfs
init|=
name|allFiles
operator|.
name|next
argument_list|()
decl_stmt|;
name|Path
name|lfsPath
init|=
name|lfs
operator|.
name|getPath
argument_list|()
decl_stmt|;
name|Path
name|dirPath
init|=
name|Path
operator|.
name|getPathWithoutSchemeAndAuthority
argument_list|(
name|lfsPath
argument_list|)
decl_stmt|;
name|String
name|dir
init|=
name|dirPath
operator|.
name|toString
argument_list|()
decl_stmt|;
if|if
condition|(
operator|!
name|dir
operator|.
name|startsWith
argument_list|(
name|relRoot
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Path "
operator|+
name|lfsPath
operator|+
literal|" is not under "
operator|+
name|relRoot
operator|+
literal|" (when shortened to "
operator|+
name|dir
operator|+
literal|")"
argument_list|)
throw|;
block|}
name|String
name|subDir
init|=
name|dir
operator|.
name|substring
argument_list|(
name|relRoot
operator|.
name|length
argument_list|()
argument_list|)
decl_stmt|;
name|Utilities
operator|.
name|FILE_OP_LOGGER
operator|.
name|trace
argument_list|(
literal|"Looking at {} from {}"
argument_list|,
name|subDir
argument_list|,
name|lfsPath
argument_list|)
expr_stmt|;
comment|// If sorted, we'll skip a bunch of files.
if|if
condition|(
name|lastRelDir
operator|!=
literal|null
operator|&&
name|subDir
operator|.
name|startsWith
argument_list|(
name|lastRelDir
argument_list|)
condition|)
block|{
continue|continue;
block|}
name|int
name|startIx
init|=
name|skipLevels
operator|>
literal|0
condition|?
operator|-
literal|1
else|:
literal|0
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|skipLevels
condition|;
operator|++
name|i
control|)
block|{
name|startIx
operator|=
name|subDir
operator|.
name|indexOf
argument_list|(
name|Path
operator|.
name|SEPARATOR_CHAR
argument_list|,
name|startIx
operator|+
literal|1
argument_list|)
expr_stmt|;
if|if
condition|(
name|startIx
operator|==
operator|-
literal|1
condition|)
block|{
name|Utilities
operator|.
name|FILE_OP_LOGGER
operator|.
name|info
argument_list|(
literal|"Expected level of nesting ({}) is not "
operator|+
literal|" present in {} (from {})"
argument_list|,
name|skipLevels
argument_list|,
name|subDir
argument_list|,
name|lfsPath
argument_list|)
expr_stmt|;
break|break;
block|}
block|}
if|if
condition|(
name|startIx
operator|==
operator|-
literal|1
condition|)
block|{
continue|continue;
block|}
name|int
name|endIx
init|=
name|subDir
operator|.
name|indexOf
argument_list|(
name|Path
operator|.
name|SEPARATOR_CHAR
argument_list|,
name|startIx
operator|+
literal|1
argument_list|)
decl_stmt|;
if|if
condition|(
name|endIx
operator|==
operator|-
literal|1
condition|)
block|{
name|Utilities
operator|.
name|FILE_OP_LOGGER
operator|.
name|info
argument_list|(
literal|"Expected level of nesting ({}) is not present in"
operator|+
literal|" {} (from {})"
argument_list|,
operator|(
name|skipLevels
operator|+
literal|1
operator|)
argument_list|,
name|subDir
argument_list|,
name|lfsPath
argument_list|)
expr_stmt|;
continue|continue;
block|}
name|lastRelDir
operator|=
name|subDir
operator|=
name|subDir
operator|.
name|substring
argument_list|(
literal|0
argument_list|,
name|endIx
argument_list|)
expr_stmt|;
name|Path
name|candidate
init|=
operator|new
name|Path
argument_list|(
name|relRoot
argument_list|,
name|subDir
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|filter
operator|.
name|accept
argument_list|(
name|candidate
argument_list|)
condition|)
block|{
continue|continue;
block|}
name|results
operator|.
name|add
argument_list|(
name|fs
operator|.
name|makeQualified
argument_list|(
name|candidate
argument_list|)
argument_list|)
expr_stmt|;
block|}
return|return
name|results
operator|.
name|toArray
argument_list|(
operator|new
name|Path
index|[
name|results
operator|.
name|size
argument_list|()
index|]
argument_list|)
return|;
block|}
specifier|private
specifier|static
name|Path
index|[]
name|getMmDirectoryCandidatesGlobStatus
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|Path
name|path
parameter_list|,
name|int
name|skipLevels
parameter_list|,
name|PathFilter
name|filter
parameter_list|,
name|long
name|txnId
parameter_list|,
name|int
name|stmtId
parameter_list|,
name|boolean
name|isBaseDir
parameter_list|)
throws|throws
name|IOException
block|{
name|StringBuilder
name|sb
init|=
operator|new
name|StringBuilder
argument_list|(
name|path
operator|.
name|toUri
argument_list|()
operator|.
name|getPath
argument_list|()
argument_list|)
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|skipLevels
condition|;
name|i
operator|++
control|)
block|{
name|sb
operator|.
name|append
argument_list|(
name|Path
operator|.
name|SEPARATOR
argument_list|)
operator|.
name|append
argument_list|(
literal|'*'
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|stmtId
operator|<
literal|0
condition|)
block|{
comment|// Note: this does not work.
comment|// sb.append(Path.SEPARATOR).append(AcidUtils.deltaSubdir(txnId, txnId)).append("_*");
throw|throw
operator|new
name|AssertionError
argument_list|(
literal|"GlobStatus should not be called without a statement ID"
argument_list|)
throw|;
block|}
else|else
block|{
name|sb
operator|.
name|append
argument_list|(
name|Path
operator|.
name|SEPARATOR
argument_list|)
operator|.
name|append
argument_list|(
name|AcidUtils
operator|.
name|baseOrDeltaSubdir
argument_list|(
name|isBaseDir
argument_list|,
name|txnId
argument_list|,
name|txnId
argument_list|,
name|stmtId
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|Path
name|pathPattern
init|=
operator|new
name|Path
argument_list|(
name|path
argument_list|,
name|sb
operator|.
name|toString
argument_list|()
argument_list|)
decl_stmt|;
return|return
name|statusToPath
argument_list|(
name|fs
operator|.
name|globStatus
argument_list|(
name|pathPattern
argument_list|,
name|filter
argument_list|)
argument_list|)
return|;
block|}
specifier|private
specifier|static
name|void
name|tryDeleteAllMmFiles
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|Path
name|specPath
parameter_list|,
name|Path
name|manifestDir
parameter_list|,
name|int
name|dpLevels
parameter_list|,
name|int
name|lbLevels
parameter_list|,
name|JavaUtils
operator|.
name|IdPathFilter
name|filter
parameter_list|,
name|long
name|txnId
parameter_list|,
name|int
name|stmtId
parameter_list|,
name|Configuration
name|conf
parameter_list|,
name|boolean
name|isBaseDir
parameter_list|)
throws|throws
name|IOException
block|{
name|Path
index|[]
name|files
init|=
name|getMmDirectoryCandidates
argument_list|(
name|fs
argument_list|,
name|specPath
argument_list|,
name|dpLevels
argument_list|,
name|lbLevels
argument_list|,
name|filter
argument_list|,
name|txnId
argument_list|,
name|stmtId
argument_list|,
name|conf
argument_list|,
name|isBaseDir
argument_list|)
decl_stmt|;
if|if
condition|(
name|files
operator|!=
literal|null
condition|)
block|{
for|for
control|(
name|Path
name|path
range|:
name|files
control|)
block|{
name|Utilities
operator|.
name|FILE_OP_LOGGER
operator|.
name|info
argument_list|(
literal|"Deleting {} on failure"
argument_list|,
name|path
argument_list|)
expr_stmt|;
name|tryDelete
argument_list|(
name|fs
argument_list|,
name|path
argument_list|)
expr_stmt|;
block|}
block|}
name|Utilities
operator|.
name|FILE_OP_LOGGER
operator|.
name|info
argument_list|(
literal|"Deleting {} on failure"
argument_list|,
name|manifestDir
argument_list|)
expr_stmt|;
name|fs
operator|.
name|delete
argument_list|(
name|manifestDir
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
specifier|public
specifier|static
name|void
name|writeMmCommitManifest
parameter_list|(
name|List
argument_list|<
name|Path
argument_list|>
name|commitPaths
parameter_list|,
name|Path
name|specPath
parameter_list|,
name|FileSystem
name|fs
parameter_list|,
name|String
name|taskId
parameter_list|,
name|Long
name|txnId
parameter_list|,
name|int
name|stmtId
parameter_list|,
name|String
name|unionSuffix
parameter_list|,
name|boolean
name|isInsertOverwrite
parameter_list|)
throws|throws
name|HiveException
block|{
if|if
condition|(
name|commitPaths
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
return|return;
block|}
comment|// We assume one FSOP per task (per specPath), so we create it in specPath.
name|Path
name|manifestPath
init|=
name|getManifestDir
argument_list|(
name|specPath
argument_list|,
name|txnId
argument_list|,
name|stmtId
argument_list|,
name|unionSuffix
argument_list|,
name|isInsertOverwrite
argument_list|)
decl_stmt|;
name|manifestPath
operator|=
operator|new
name|Path
argument_list|(
name|manifestPath
argument_list|,
name|taskId
operator|+
name|MANIFEST_EXTENSION
argument_list|)
expr_stmt|;
name|Utilities
operator|.
name|FILE_OP_LOGGER
operator|.
name|info
argument_list|(
literal|"Writing manifest to {} with {}"
argument_list|,
name|manifestPath
argument_list|,
name|commitPaths
argument_list|)
expr_stmt|;
try|try
block|{
comment|// Don't overwrite the manifest... should fail if we have collisions.
try|try
init|(
name|FSDataOutputStream
name|out
init|=
name|fs
operator|.
name|create
argument_list|(
name|manifestPath
argument_list|,
literal|false
argument_list|)
init|)
block|{
if|if
condition|(
name|out
operator|==
literal|null
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Failed to create manifest at "
operator|+
name|manifestPath
argument_list|)
throw|;
block|}
name|out
operator|.
name|writeInt
argument_list|(
name|commitPaths
operator|.
name|size
argument_list|()
argument_list|)
expr_stmt|;
for|for
control|(
name|Path
name|path
range|:
name|commitPaths
control|)
block|{
name|out
operator|.
name|writeUTF
argument_list|(
name|path
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|private
specifier|static
name|Path
name|getManifestDir
parameter_list|(
name|Path
name|specPath
parameter_list|,
name|long
name|txnId
parameter_list|,
name|int
name|stmtId
parameter_list|,
name|String
name|unionSuffix
parameter_list|,
name|boolean
name|isInsertOverwrite
parameter_list|)
block|{
name|Path
name|manifestPath
init|=
operator|new
name|Path
argument_list|(
name|specPath
argument_list|,
literal|"_tmp."
operator|+
name|AcidUtils
operator|.
name|baseOrDeltaSubdir
argument_list|(
name|isInsertOverwrite
argument_list|,
name|txnId
argument_list|,
name|txnId
argument_list|,
name|stmtId
argument_list|)
argument_list|)
decl_stmt|;
return|return
operator|(
name|unionSuffix
operator|==
literal|null
operator|)
condition|?
name|manifestPath
else|:
operator|new
name|Path
argument_list|(
name|manifestPath
argument_list|,
name|unionSuffix
argument_list|)
return|;
block|}
specifier|public
specifier|static
specifier|final
class|class
name|MissingBucketsContext
block|{
specifier|public
specifier|final
name|TableDesc
name|tableInfo
decl_stmt|;
specifier|public
specifier|final
name|int
name|numBuckets
decl_stmt|;
specifier|public
specifier|final
name|boolean
name|isCompressed
decl_stmt|;
specifier|public
name|MissingBucketsContext
parameter_list|(
name|TableDesc
name|tableInfo
parameter_list|,
name|int
name|numBuckets
parameter_list|,
name|boolean
name|isCompressed
parameter_list|)
block|{
name|this
operator|.
name|tableInfo
operator|=
name|tableInfo
expr_stmt|;
name|this
operator|.
name|numBuckets
operator|=
name|numBuckets
expr_stmt|;
name|this
operator|.
name|isCompressed
operator|=
name|isCompressed
expr_stmt|;
block|}
block|}
specifier|public
specifier|static
name|void
name|handleMmTableFinalPath
parameter_list|(
name|Path
name|specPath
parameter_list|,
name|String
name|unionSuffix
parameter_list|,
name|Configuration
name|hconf
parameter_list|,
name|boolean
name|success
parameter_list|,
name|int
name|dpLevels
parameter_list|,
name|int
name|lbLevels
parameter_list|,
name|MissingBucketsContext
name|mbc
parameter_list|,
name|long
name|txnId
parameter_list|,
name|int
name|stmtId
parameter_list|,
name|Reporter
name|reporter
parameter_list|,
name|boolean
name|isMmTable
parameter_list|,
name|boolean
name|isMmCtas
parameter_list|,
name|boolean
name|isInsertOverwrite
parameter_list|)
throws|throws
name|IOException
throws|,
name|HiveException
block|{
name|FileSystem
name|fs
init|=
name|specPath
operator|.
name|getFileSystem
argument_list|(
name|hconf
argument_list|)
decl_stmt|;
name|Path
name|manifestDir
init|=
name|getManifestDir
argument_list|(
name|specPath
argument_list|,
name|txnId
argument_list|,
name|stmtId
argument_list|,
name|unionSuffix
argument_list|,
name|isInsertOverwrite
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|success
condition|)
block|{
name|JavaUtils
operator|.
name|IdPathFilter
name|filter
init|=
operator|new
name|JavaUtils
operator|.
name|IdPathFilter
argument_list|(
name|txnId
argument_list|,
name|stmtId
argument_list|,
literal|true
argument_list|)
decl_stmt|;
name|tryDeleteAllMmFiles
argument_list|(
name|fs
argument_list|,
name|specPath
argument_list|,
name|manifestDir
argument_list|,
name|dpLevels
argument_list|,
name|lbLevels
argument_list|,
name|filter
argument_list|,
name|txnId
argument_list|,
name|stmtId
argument_list|,
name|hconf
argument_list|,
name|isInsertOverwrite
argument_list|)
expr_stmt|;
return|return;
block|}
name|Utilities
operator|.
name|FILE_OP_LOGGER
operator|.
name|debug
argument_list|(
literal|"Looking for manifests in: {} ({})"
argument_list|,
name|manifestDir
argument_list|,
name|txnId
argument_list|)
expr_stmt|;
name|List
argument_list|<
name|Path
argument_list|>
name|manifests
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|()
decl_stmt|;
if|if
condition|(
name|fs
operator|.
name|exists
argument_list|(
name|manifestDir
argument_list|)
condition|)
block|{
name|FileStatus
index|[]
name|manifestFiles
init|=
name|fs
operator|.
name|listStatus
argument_list|(
name|manifestDir
argument_list|)
decl_stmt|;
if|if
condition|(
name|manifestFiles
operator|!=
literal|null
condition|)
block|{
for|for
control|(
name|FileStatus
name|status
range|:
name|manifestFiles
control|)
block|{
name|Path
name|path
init|=
name|status
operator|.
name|getPath
argument_list|()
decl_stmt|;
if|if
condition|(
name|path
operator|.
name|getName
argument_list|()
operator|.
name|endsWith
argument_list|(
name|MANIFEST_EXTENSION
argument_list|)
condition|)
block|{
name|Utilities
operator|.
name|FILE_OP_LOGGER
operator|.
name|info
argument_list|(
literal|"Reading manifest {}"
argument_list|,
name|path
argument_list|)
expr_stmt|;
name|manifests
operator|.
name|add
argument_list|(
name|path
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
else|else
block|{
name|Utilities
operator|.
name|FILE_OP_LOGGER
operator|.
name|info
argument_list|(
literal|"No manifests found - query produced no output"
argument_list|)
expr_stmt|;
name|manifestDir
operator|=
literal|null
expr_stmt|;
block|}
name|Utilities
operator|.
name|FILE_OP_LOGGER
operator|.
name|debug
argument_list|(
literal|"Looking for files in: {}"
argument_list|,
name|specPath
argument_list|)
expr_stmt|;
name|JavaUtils
operator|.
name|IdPathFilter
name|filter
init|=
operator|new
name|JavaUtils
operator|.
name|IdPathFilter
argument_list|(
name|txnId
argument_list|,
name|stmtId
argument_list|,
literal|true
argument_list|,
literal|false
argument_list|,
name|isInsertOverwrite
argument_list|)
decl_stmt|;
if|if
condition|(
name|isMmCtas
operator|&&
operator|!
name|fs
operator|.
name|exists
argument_list|(
name|specPath
argument_list|)
condition|)
block|{
name|Utilities
operator|.
name|FILE_OP_LOGGER
operator|.
name|info
argument_list|(
literal|"Creating table directory for CTAS with no output at {}"
argument_list|,
name|specPath
argument_list|)
expr_stmt|;
name|FileUtils
operator|.
name|mkdir
argument_list|(
name|fs
argument_list|,
name|specPath
argument_list|,
name|hconf
argument_list|)
expr_stmt|;
block|}
name|Path
index|[]
name|files
init|=
name|getMmDirectoryCandidates
argument_list|(
name|fs
argument_list|,
name|specPath
argument_list|,
name|dpLevels
argument_list|,
name|lbLevels
argument_list|,
name|filter
argument_list|,
name|txnId
argument_list|,
name|stmtId
argument_list|,
name|hconf
argument_list|,
name|isInsertOverwrite
argument_list|)
decl_stmt|;
name|ArrayList
argument_list|<
name|Path
argument_list|>
name|mmDirectories
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|()
decl_stmt|;
if|if
condition|(
name|files
operator|!=
literal|null
condition|)
block|{
for|for
control|(
name|Path
name|path
range|:
name|files
control|)
block|{
name|Utilities
operator|.
name|FILE_OP_LOGGER
operator|.
name|trace
argument_list|(
literal|"Looking at path: {}"
argument_list|,
name|path
argument_list|)
expr_stmt|;
name|mmDirectories
operator|.
name|add
argument_list|(
name|path
argument_list|)
expr_stmt|;
block|}
block|}
name|HashSet
argument_list|<
name|String
argument_list|>
name|committed
init|=
operator|new
name|HashSet
argument_list|<>
argument_list|()
decl_stmt|;
for|for
control|(
name|Path
name|mfp
range|:
name|manifests
control|)
block|{
try|try
init|(
name|FSDataInputStream
name|mdis
init|=
name|fs
operator|.
name|open
argument_list|(
name|mfp
argument_list|)
init|)
block|{
name|int
name|fileCount
init|=
name|mdis
operator|.
name|readInt
argument_list|()
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|fileCount
condition|;
operator|++
name|i
control|)
block|{
name|String
name|nextFile
init|=
name|mdis
operator|.
name|readUTF
argument_list|()
decl_stmt|;
if|if
condition|(
operator|!
name|committed
operator|.
name|add
argument_list|(
name|nextFile
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|nextFile
operator|+
literal|" was specified in multiple manifests"
argument_list|)
throw|;
block|}
block|}
block|}
block|}
if|if
condition|(
name|manifestDir
operator|!=
literal|null
condition|)
block|{
name|Utilities
operator|.
name|FILE_OP_LOGGER
operator|.
name|info
argument_list|(
literal|"Deleting manifest directory {}"
argument_list|,
name|manifestDir
argument_list|)
expr_stmt|;
name|tryDelete
argument_list|(
name|fs
argument_list|,
name|manifestDir
argument_list|)
expr_stmt|;
if|if
condition|(
name|unionSuffix
operator|!=
literal|null
condition|)
block|{
comment|// Also delete the parent directory if we are the last union FSOP to execute.
name|manifestDir
operator|=
name|manifestDir
operator|.
name|getParent
argument_list|()
expr_stmt|;
name|FileStatus
index|[]
name|remainingFiles
init|=
name|fs
operator|.
name|listStatus
argument_list|(
name|manifestDir
argument_list|)
decl_stmt|;
if|if
condition|(
name|remainingFiles
operator|==
literal|null
operator|||
name|remainingFiles
operator|.
name|length
operator|==
literal|0
condition|)
block|{
name|Utilities
operator|.
name|FILE_OP_LOGGER
operator|.
name|info
argument_list|(
literal|"Deleting manifest directory {}"
argument_list|,
name|manifestDir
argument_list|)
expr_stmt|;
name|tryDelete
argument_list|(
name|fs
argument_list|,
name|manifestDir
argument_list|)
expr_stmt|;
block|}
block|}
block|}
for|for
control|(
name|Path
name|path
range|:
name|mmDirectories
control|)
block|{
name|cleanMmDirectory
argument_list|(
name|path
argument_list|,
name|fs
argument_list|,
name|unionSuffix
argument_list|,
name|committed
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
operator|!
name|committed
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"The following files were committed but not found: "
operator|+
name|committed
argument_list|)
throw|;
block|}
if|if
condition|(
name|mmDirectories
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
return|return;
block|}
comment|// TODO: see HIVE-14886 - removeTempOrDuplicateFiles is broken for list bucketing,
comment|//       so maintain parity here by not calling it at all.
if|if
condition|(
name|lbLevels
operator|!=
literal|0
condition|)
block|{
return|return;
block|}
comment|// Create fake file statuses to avoid querying the file system. removeTempOrDuplicateFiles
comment|// doesn't need tocheck anything except path and directory status for MM directories.
name|FileStatus
index|[]
name|finalResults
init|=
operator|new
name|FileStatus
index|[
name|mmDirectories
operator|.
name|size
argument_list|()
index|]
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|mmDirectories
operator|.
name|size
argument_list|()
condition|;
operator|++
name|i
control|)
block|{
name|finalResults
index|[
name|i
index|]
operator|=
operator|new
name|PathOnlyFileStatus
argument_list|(
name|mmDirectories
operator|.
name|get
argument_list|(
name|i
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|List
argument_list|<
name|Path
argument_list|>
name|emptyBuckets
init|=
name|Utilities
operator|.
name|removeTempOrDuplicateFiles
argument_list|(
name|fs
argument_list|,
name|finalResults
argument_list|,
name|unionSuffix
argument_list|,
name|dpLevels
argument_list|,
name|mbc
operator|==
literal|null
condition|?
literal|0
else|:
name|mbc
operator|.
name|numBuckets
argument_list|,
name|hconf
argument_list|,
name|txnId
argument_list|,
name|stmtId
argument_list|,
name|isMmTable
argument_list|,
literal|null
argument_list|,
name|isInsertOverwrite
argument_list|)
decl_stmt|;
comment|// create empty buckets if necessary
if|if
condition|(
operator|!
name|emptyBuckets
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
assert|assert
name|mbc
operator|!=
literal|null
assert|;
name|Utilities
operator|.
name|createEmptyBuckets
argument_list|(
name|hconf
argument_list|,
name|emptyBuckets
argument_list|,
name|mbc
operator|.
name|isCompressed
argument_list|,
name|mbc
operator|.
name|tableInfo
argument_list|,
name|reporter
argument_list|)
expr_stmt|;
block|}
block|}
specifier|private
specifier|static
specifier|final
class|class
name|PathOnlyFileStatus
extends|extends
name|FileStatus
block|{
specifier|public
name|PathOnlyFileStatus
parameter_list|(
name|Path
name|path
parameter_list|)
block|{
name|super
argument_list|(
literal|0
argument_list|,
literal|true
argument_list|,
literal|0
argument_list|,
literal|0
argument_list|,
literal|0
argument_list|,
name|path
argument_list|)
expr_stmt|;
block|}
block|}
specifier|private
specifier|static
name|void
name|cleanMmDirectory
parameter_list|(
name|Path
name|dir
parameter_list|,
name|FileSystem
name|fs
parameter_list|,
name|String
name|unionSuffix
parameter_list|,
name|HashSet
argument_list|<
name|String
argument_list|>
name|committed
parameter_list|)
throws|throws
name|IOException
throws|,
name|HiveException
block|{
for|for
control|(
name|FileStatus
name|child
range|:
name|fs
operator|.
name|listStatus
argument_list|(
name|dir
argument_list|)
control|)
block|{
name|Path
name|childPath
init|=
name|child
operator|.
name|getPath
argument_list|()
decl_stmt|;
if|if
condition|(
name|unionSuffix
operator|==
literal|null
condition|)
block|{
if|if
condition|(
name|committed
operator|.
name|remove
argument_list|(
name|childPath
operator|.
name|toString
argument_list|()
argument_list|)
condition|)
block|{
continue|continue;
comment|// A good file.
block|}
name|deleteUncommitedFile
argument_list|(
name|childPath
argument_list|,
name|fs
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
operator|!
name|child
operator|.
name|isDirectory
argument_list|()
condition|)
block|{
if|if
condition|(
name|committed
operator|.
name|contains
argument_list|(
name|childPath
operator|.
name|toString
argument_list|()
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Union FSOP has commited "
operator|+
name|childPath
operator|+
literal|" outside of union directory"
operator|+
name|unionSuffix
argument_list|)
throw|;
block|}
name|deleteUncommitedFile
argument_list|(
name|childPath
argument_list|,
name|fs
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|childPath
operator|.
name|getName
argument_list|()
operator|.
name|equals
argument_list|(
name|unionSuffix
argument_list|)
condition|)
block|{
comment|// Found the right union directory; treat it as "our" MM directory.
name|cleanMmDirectory
argument_list|(
name|childPath
argument_list|,
name|fs
argument_list|,
literal|null
argument_list|,
name|committed
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|Utilities
operator|.
name|FILE_OP_LOGGER
operator|.
name|trace
argument_list|(
literal|"FSOP for {} is ignoring the other side of the union {}"
argument_list|,
name|unionSuffix
argument_list|,
name|childPath
argument_list|)
expr_stmt|;
block|}
block|}
block|}
specifier|private
specifier|static
name|void
name|deleteUncommitedFile
parameter_list|(
name|Path
name|childPath
parameter_list|,
name|FileSystem
name|fs
parameter_list|)
throws|throws
name|IOException
throws|,
name|HiveException
block|{
name|Utilities
operator|.
name|FILE_OP_LOGGER
operator|.
name|info
argument_list|(
literal|"Deleting {} that was not committed"
argument_list|,
name|childPath
argument_list|)
expr_stmt|;
comment|// We should actually succeed here - if we fail, don't commit the query.
if|if
condition|(
operator|!
name|fs
operator|.
name|delete
argument_list|(
name|childPath
argument_list|,
literal|true
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Failed to delete an uncommitted path "
operator|+
name|childPath
argument_list|)
throw|;
block|}
block|}
comment|/**    * @return the complete list of valid MM directories under a table/partition path; null    * if the entire directory is valid (has no uncommitted/temporary files).    */
specifier|public
specifier|static
name|List
argument_list|<
name|Path
argument_list|>
name|getValidMmDirectoriesFromTableOrPart
parameter_list|(
name|Path
name|path
parameter_list|,
name|Configuration
name|conf
parameter_list|,
name|ValidTxnList
name|validTxnList
parameter_list|,
name|int
name|lbLevels
parameter_list|)
throws|throws
name|IOException
block|{
name|Utilities
operator|.
name|FILE_OP_LOGGER
operator|.
name|trace
argument_list|(
literal|"Looking for valid MM paths under {}"
argument_list|,
name|path
argument_list|)
expr_stmt|;
comment|// NULL means this directory is entirely valid.
name|List
argument_list|<
name|Path
argument_list|>
name|result
init|=
literal|null
decl_stmt|;
name|FileSystem
name|fs
init|=
name|path
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|FileStatus
index|[]
name|children
init|=
operator|(
name|lbLevels
operator|==
literal|0
operator|)
condition|?
name|fs
operator|.
name|listStatus
argument_list|(
name|path
argument_list|)
else|:
name|fs
operator|.
name|globStatus
argument_list|(
operator|new
name|Path
argument_list|(
name|path
argument_list|,
name|StringUtils
operator|.
name|repeat
argument_list|(
literal|"*"
operator|+
name|Path
operator|.
name|SEPARATOR
argument_list|,
name|lbLevels
argument_list|)
operator|+
literal|"*"
argument_list|)
argument_list|)
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|children
operator|.
name|length
condition|;
operator|++
name|i
control|)
block|{
name|FileStatus
name|file
init|=
name|children
index|[
name|i
index|]
decl_stmt|;
name|Path
name|childPath
init|=
name|file
operator|.
name|getPath
argument_list|()
decl_stmt|;
name|Long
name|txnId
init|=
name|JavaUtils
operator|.
name|extractTxnId
argument_list|(
name|childPath
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|file
operator|.
name|isDirectory
argument_list|()
operator|||
name|txnId
operator|==
literal|null
operator|||
operator|!
name|validTxnList
operator|.
name|isTxnValid
argument_list|(
name|txnId
argument_list|)
condition|)
block|{
name|Utilities
operator|.
name|FILE_OP_LOGGER
operator|.
name|debug
argument_list|(
literal|"Skipping path {}"
argument_list|,
name|childPath
argument_list|)
expr_stmt|;
if|if
condition|(
name|result
operator|==
literal|null
condition|)
block|{
name|result
operator|=
operator|new
name|ArrayList
argument_list|<>
argument_list|(
name|children
operator|.
name|length
operator|-
literal|1
argument_list|)
expr_stmt|;
for|for
control|(
name|int
name|j
init|=
literal|0
init|;
name|j
operator|<
name|i
condition|;
operator|++
name|j
control|)
block|{
name|result
operator|.
name|add
argument_list|(
name|children
index|[
name|j
index|]
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
block|}
elseif|else
if|if
condition|(
name|result
operator|!=
literal|null
condition|)
block|{
name|result
operator|.
name|add
argument_list|(
name|childPath
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|result
return|;
block|}
specifier|public
specifier|static
name|String
name|getAclStringWithHiveModification
parameter_list|(
name|Configuration
name|tezConf
parameter_list|,
name|String
name|propertyName
parameter_list|,
name|boolean
name|addHs2User
parameter_list|,
name|String
name|user
parameter_list|,
name|String
name|hs2User
parameter_list|)
throws|throws
name|IOException
block|{
comment|// Start with initial ACLs
name|ACLConfigurationParser
name|aclConf
init|=
operator|new
name|ACLConfigurationParser
argument_list|(
name|tezConf
argument_list|,
name|propertyName
argument_list|)
decl_stmt|;
comment|// Always give access to the user
name|aclConf
operator|.
name|addAllowedUser
argument_list|(
name|user
argument_list|)
expr_stmt|;
comment|// Give access to the process user if the config is set.
if|if
condition|(
name|addHs2User
operator|&&
name|hs2User
operator|!=
literal|null
condition|)
block|{
name|aclConf
operator|.
name|addAllowedUser
argument_list|(
name|hs2User
argument_list|)
expr_stmt|;
block|}
return|return
name|aclConf
operator|.
name|toAclString
argument_list|()
return|;
block|}
specifier|public
specifier|static
name|boolean
name|isHiveManagedFile
parameter_list|(
name|Path
name|path
parameter_list|)
block|{
return|return
name|AcidUtils
operator|.
name|ORIGINAL_PATTERN
operator|.
name|matcher
argument_list|(
name|path
operator|.
name|getName
argument_list|()
argument_list|)
operator|.
name|matches
argument_list|()
operator|||
name|AcidUtils
operator|.
name|ORIGINAL_PATTERN_COPY
operator|.
name|matcher
argument_list|(
name|path
operator|.
name|getName
argument_list|()
argument_list|)
operator|.
name|matches
argument_list|()
return|;
block|}
block|}
end_class

end_unit

