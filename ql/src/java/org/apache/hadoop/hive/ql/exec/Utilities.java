begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/**  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
package|;
end_package

begin_import
import|import
name|java
operator|.
name|beans
operator|.
name|DefaultPersistenceDelegate
import|;
end_import

begin_import
import|import
name|java
operator|.
name|beans
operator|.
name|Encoder
import|;
end_import

begin_import
import|import
name|java
operator|.
name|beans
operator|.
name|ExceptionListener
import|;
end_import

begin_import
import|import
name|java
operator|.
name|beans
operator|.
name|Expression
import|;
end_import

begin_import
import|import
name|java
operator|.
name|beans
operator|.
name|Statement
import|;
end_import

begin_import
import|import
name|java
operator|.
name|beans
operator|.
name|XMLDecoder
import|;
end_import

begin_import
import|import
name|java
operator|.
name|beans
operator|.
name|XMLEncoder
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|BufferedReader
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|ByteArrayInputStream
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|ByteArrayOutputStream
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|DataInput
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|EOFException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|File
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|FileInputStream
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|FileNotFoundException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|InputStream
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|InputStreamReader
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|OutputStream
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|PrintStream
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|Serializable
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|UnsupportedEncodingException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|net
operator|.
name|URI
import|;
end_import

begin_import
import|import
name|java
operator|.
name|net
operator|.
name|URL
import|;
end_import

begin_import
import|import
name|java
operator|.
name|net
operator|.
name|URLClassLoader
import|;
end_import

begin_import
import|import
name|java
operator|.
name|sql
operator|.
name|Connection
import|;
end_import

begin_import
import|import
name|java
operator|.
name|sql
operator|.
name|DriverManager
import|;
end_import

begin_import
import|import
name|java
operator|.
name|sql
operator|.
name|PreparedStatement
import|;
end_import

begin_import
import|import
name|java
operator|.
name|sql
operator|.
name|SQLException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|sql
operator|.
name|SQLTransientException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|text
operator|.
name|SimpleDateFormat
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Arrays
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Calendar
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Collection
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Collections
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashSet
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Iterator
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|LinkedHashMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Properties
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Random
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Set
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|UUID
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|ConcurrentHashMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|ExecutionException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|Future
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|LinkedBlockingQueue
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|ThreadPoolExecutor
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|TimeUnit
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|regex
operator|.
name|Matcher
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|regex
operator|.
name|Pattern
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|lang
operator|.
name|StringUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|lang
operator|.
name|WordUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|Log
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|LogFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configuration
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|filecache
operator|.
name|DistributedCache
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|ContentSummary
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FSDataOutputStream
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileStatus
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|PathFilter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|HiveInterruptCallback
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|HiveInterruptUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|Warehouse
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|FieldSchema
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Order
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|Context
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|QueryPlan
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|FileSinkOperator
operator|.
name|RecordWriter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|ContentSummaryInputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|HiveFileFormatUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|HiveInputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|HiveOutputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|HiveSequenceFileOutputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|RCFile
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|ReworkMapredInputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|HiveException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|Partition
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|Table
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|parse
operator|.
name|ErrorMsg
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|parse
operator|.
name|SemanticException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|DynamicPartitionCtx
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|ExprNodeColumnDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|ExprNodeConstantDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|ExprNodeDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|ExprNodeGenericFuncDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|FileSinkDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|GroupByDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|MapredLocalWork
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|MapredWork
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|PartitionDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|PlanUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|PlanUtils
operator|.
name|ExpressionTypes
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|TableDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|stats
operator|.
name|StatsFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|stats
operator|.
name|StatsPublisher
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|udf
operator|.
name|generic
operator|.
name|GenericUDF
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|udf
operator|.
name|generic
operator|.
name|GenericUDFOPAnd
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|udf
operator|.
name|generic
operator|.
name|GenericUDFOPEqual
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|udf
operator|.
name|generic
operator|.
name|GenericUDFOPOr
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde
operator|.
name|Constants
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|SerDeException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|Serializer
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|lazy
operator|.
name|LazySimpleSerDe
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|typeinfo
operator|.
name|TypeInfo
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|shims
operator|.
name|ShimLoader
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|IOUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|SequenceFile
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|SequenceFile
operator|.
name|CompressionType
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|Writable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|compress
operator|.
name|CompressionCodec
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|compress
operator|.
name|DefaultCodec
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|FileOutputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|InputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|JobConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|SequenceFileInputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|SequenceFileOutputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|ReflectionUtils
import|;
end_import

begin_comment
comment|/**  * Utilities.  *  */
end_comment

begin_class
annotation|@
name|SuppressWarnings
argument_list|(
literal|"nls"
argument_list|)
specifier|public
specifier|final
class|class
name|Utilities
block|{
comment|/**    * The object in the reducer are composed of these top level fields.    */
specifier|public
specifier|static
name|String
name|HADOOP_LOCAL_FS
init|=
literal|"file:///"
decl_stmt|;
comment|/**    * ReduceField.    *    */
specifier|public
specifier|static
enum|enum
name|ReduceField
block|{
name|KEY
block|,
name|VALUE
block|,
name|ALIAS
block|}
empty_stmt|;
specifier|private
name|Utilities
parameter_list|()
block|{
comment|// prevent instantiation
block|}
specifier|private
specifier|static
name|Map
argument_list|<
name|String
argument_list|,
name|MapredWork
argument_list|>
name|gWorkMap
init|=
name|Collections
operator|.
name|synchronizedMap
argument_list|(
operator|new
name|HashMap
argument_list|<
name|String
argument_list|,
name|MapredWork
argument_list|>
argument_list|()
argument_list|)
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|Log
name|LOG
init|=
name|LogFactory
operator|.
name|getLog
argument_list|(
name|Utilities
operator|.
name|class
operator|.
name|getName
argument_list|()
argument_list|)
decl_stmt|;
specifier|public
specifier|static
name|void
name|clearMapRedWork
parameter_list|(
name|Configuration
name|job
parameter_list|)
block|{
try|try
block|{
name|Path
name|planPath
init|=
operator|new
name|Path
argument_list|(
name|HiveConf
operator|.
name|getVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|PLAN
argument_list|)
argument_list|)
decl_stmt|;
name|FileSystem
name|fs
init|=
name|planPath
operator|.
name|getFileSystem
argument_list|(
name|job
argument_list|)
decl_stmt|;
if|if
condition|(
name|fs
operator|.
name|exists
argument_list|(
name|planPath
argument_list|)
condition|)
block|{
try|try
block|{
name|fs
operator|.
name|delete
argument_list|(
name|planPath
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|e
operator|.
name|printStackTrace
argument_list|()
expr_stmt|;
block|}
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{     }
finally|finally
block|{
comment|// where a single process works with multiple plans - we must clear
comment|// the cache before working with the next plan.
name|String
name|jobID
init|=
name|getHiveJobID
argument_list|(
name|job
argument_list|)
decl_stmt|;
if|if
condition|(
name|jobID
operator|!=
literal|null
condition|)
block|{
name|gWorkMap
operator|.
name|remove
argument_list|(
name|jobID
argument_list|)
expr_stmt|;
block|}
block|}
block|}
specifier|public
specifier|static
name|MapredWork
name|getMapRedWork
parameter_list|(
name|Configuration
name|job
parameter_list|)
block|{
name|MapredWork
name|gWork
init|=
literal|null
decl_stmt|;
try|try
block|{
name|String
name|jobID
init|=
name|getHiveJobID
argument_list|(
name|job
argument_list|)
decl_stmt|;
assert|assert
name|jobID
operator|!=
literal|null
assert|;
name|gWork
operator|=
name|gWorkMap
operator|.
name|get
argument_list|(
name|jobID
argument_list|)
expr_stmt|;
if|if
condition|(
name|gWork
operator|==
literal|null
condition|)
block|{
name|String
name|jtConf
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HADOOPJT
argument_list|)
decl_stmt|;
name|String
name|path
decl_stmt|;
if|if
condition|(
name|jtConf
operator|.
name|equals
argument_list|(
literal|"local"
argument_list|)
condition|)
block|{
name|String
name|planPath
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|PLAN
argument_list|)
decl_stmt|;
name|path
operator|=
operator|new
name|Path
argument_list|(
name|planPath
argument_list|)
operator|.
name|toUri
argument_list|()
operator|.
name|getPath
argument_list|()
expr_stmt|;
block|}
else|else
block|{
name|path
operator|=
literal|"HIVE_PLAN"
operator|+
name|jobID
expr_stmt|;
block|}
name|InputStream
name|in
init|=
operator|new
name|FileInputStream
argument_list|(
name|path
argument_list|)
decl_stmt|;
name|MapredWork
name|ret
init|=
name|deserializeMapRedWork
argument_list|(
name|in
argument_list|,
name|job
argument_list|)
decl_stmt|;
name|gWork
operator|=
name|ret
expr_stmt|;
name|gWork
operator|.
name|initialize
argument_list|()
expr_stmt|;
name|gWorkMap
operator|.
name|put
argument_list|(
name|jobID
argument_list|,
name|gWork
argument_list|)
expr_stmt|;
block|}
return|return
operator|(
name|gWork
operator|)
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|e
operator|.
name|printStackTrace
argument_list|()
expr_stmt|;
throw|throw
operator|new
name|RuntimeException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
specifier|static
name|List
argument_list|<
name|String
argument_list|>
name|getFieldSchemaString
parameter_list|(
name|List
argument_list|<
name|FieldSchema
argument_list|>
name|fl
parameter_list|)
block|{
if|if
condition|(
name|fl
operator|==
literal|null
condition|)
block|{
return|return
literal|null
return|;
block|}
name|ArrayList
argument_list|<
name|String
argument_list|>
name|ret
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|FieldSchema
name|f
range|:
name|fl
control|)
block|{
name|ret
operator|.
name|add
argument_list|(
name|f
operator|.
name|getName
argument_list|()
operator|+
literal|" "
operator|+
name|f
operator|.
name|getType
argument_list|()
operator|+
operator|(
name|f
operator|.
name|getComment
argument_list|()
operator|!=
literal|null
condition|?
operator|(
literal|" "
operator|+
name|f
operator|.
name|getComment
argument_list|()
operator|)
else|:
literal|""
operator|)
argument_list|)
expr_stmt|;
block|}
return|return
name|ret
return|;
block|}
comment|/**    * Java 1.5 workaround. From http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=5015403    */
specifier|public
specifier|static
class|class
name|EnumDelegate
extends|extends
name|DefaultPersistenceDelegate
block|{
annotation|@
name|Override
specifier|protected
name|Expression
name|instantiate
parameter_list|(
name|Object
name|oldInstance
parameter_list|,
name|Encoder
name|out
parameter_list|)
block|{
return|return
operator|new
name|Expression
argument_list|(
name|Enum
operator|.
name|class
argument_list|,
literal|"valueOf"
argument_list|,
operator|new
name|Object
index|[]
block|{
name|oldInstance
operator|.
name|getClass
argument_list|()
block|,
operator|(
operator|(
name|Enum
argument_list|<
name|?
argument_list|>
operator|)
name|oldInstance
operator|)
operator|.
name|name
argument_list|()
block|}
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|protected
name|boolean
name|mutatesTo
parameter_list|(
name|Object
name|oldInstance
parameter_list|,
name|Object
name|newInstance
parameter_list|)
block|{
return|return
name|oldInstance
operator|==
name|newInstance
return|;
block|}
block|}
specifier|public
specifier|static
class|class
name|MapDelegate
extends|extends
name|DefaultPersistenceDelegate
block|{
annotation|@
name|Override
specifier|protected
name|Expression
name|instantiate
parameter_list|(
name|Object
name|oldInstance
parameter_list|,
name|Encoder
name|out
parameter_list|)
block|{
name|Map
name|oldMap
init|=
operator|(
name|Map
operator|)
name|oldInstance
decl_stmt|;
name|HashMap
name|newMap
init|=
operator|new
name|HashMap
argument_list|(
name|oldMap
argument_list|)
decl_stmt|;
return|return
operator|new
name|Expression
argument_list|(
name|newMap
argument_list|,
name|HashMap
operator|.
name|class
argument_list|,
literal|"new"
argument_list|,
operator|new
name|Object
index|[]
block|{}
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|protected
name|boolean
name|mutatesTo
parameter_list|(
name|Object
name|oldInstance
parameter_list|,
name|Object
name|newInstance
parameter_list|)
block|{
return|return
literal|false
return|;
block|}
annotation|@
name|Override
specifier|protected
name|void
name|initialize
parameter_list|(
name|Class
argument_list|<
name|?
argument_list|>
name|type
parameter_list|,
name|Object
name|oldInstance
parameter_list|,
name|Object
name|newInstance
parameter_list|,
name|Encoder
name|out
parameter_list|)
block|{
name|java
operator|.
name|util
operator|.
name|Collection
name|oldO
init|=
operator|(
name|java
operator|.
name|util
operator|.
name|Collection
operator|)
name|oldInstance
decl_stmt|;
name|java
operator|.
name|util
operator|.
name|Collection
name|newO
init|=
operator|(
name|java
operator|.
name|util
operator|.
name|Collection
operator|)
name|newInstance
decl_stmt|;
if|if
condition|(
name|newO
operator|.
name|size
argument_list|()
operator|!=
literal|0
condition|)
block|{
name|out
operator|.
name|writeStatement
argument_list|(
operator|new
name|Statement
argument_list|(
name|oldInstance
argument_list|,
literal|"clear"
argument_list|,
operator|new
name|Object
index|[]
block|{}
argument_list|)
argument_list|)
expr_stmt|;
block|}
for|for
control|(
name|Iterator
name|i
init|=
name|oldO
operator|.
name|iterator
argument_list|()
init|;
name|i
operator|.
name|hasNext
argument_list|()
condition|;
control|)
block|{
name|out
operator|.
name|writeStatement
argument_list|(
operator|new
name|Statement
argument_list|(
name|oldInstance
argument_list|,
literal|"add"
argument_list|,
operator|new
name|Object
index|[]
block|{
name|i
operator|.
name|next
argument_list|()
block|}
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
block|}
specifier|public
specifier|static
class|class
name|SetDelegate
extends|extends
name|DefaultPersistenceDelegate
block|{
annotation|@
name|Override
specifier|protected
name|Expression
name|instantiate
parameter_list|(
name|Object
name|oldInstance
parameter_list|,
name|Encoder
name|out
parameter_list|)
block|{
name|Set
name|oldSet
init|=
operator|(
name|Set
operator|)
name|oldInstance
decl_stmt|;
name|HashSet
name|newSet
init|=
operator|new
name|HashSet
argument_list|(
name|oldSet
argument_list|)
decl_stmt|;
return|return
operator|new
name|Expression
argument_list|(
name|newSet
argument_list|,
name|HashSet
operator|.
name|class
argument_list|,
literal|"new"
argument_list|,
operator|new
name|Object
index|[]
block|{}
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|protected
name|boolean
name|mutatesTo
parameter_list|(
name|Object
name|oldInstance
parameter_list|,
name|Object
name|newInstance
parameter_list|)
block|{
return|return
literal|false
return|;
block|}
annotation|@
name|Override
specifier|protected
name|void
name|initialize
parameter_list|(
name|Class
argument_list|<
name|?
argument_list|>
name|type
parameter_list|,
name|Object
name|oldInstance
parameter_list|,
name|Object
name|newInstance
parameter_list|,
name|Encoder
name|out
parameter_list|)
block|{
name|java
operator|.
name|util
operator|.
name|Collection
name|oldO
init|=
operator|(
name|java
operator|.
name|util
operator|.
name|Collection
operator|)
name|oldInstance
decl_stmt|;
name|java
operator|.
name|util
operator|.
name|Collection
name|newO
init|=
operator|(
name|java
operator|.
name|util
operator|.
name|Collection
operator|)
name|newInstance
decl_stmt|;
if|if
condition|(
name|newO
operator|.
name|size
argument_list|()
operator|!=
literal|0
condition|)
block|{
name|out
operator|.
name|writeStatement
argument_list|(
operator|new
name|Statement
argument_list|(
name|oldInstance
argument_list|,
literal|"clear"
argument_list|,
operator|new
name|Object
index|[]
block|{}
argument_list|)
argument_list|)
expr_stmt|;
block|}
for|for
control|(
name|Iterator
name|i
init|=
name|oldO
operator|.
name|iterator
argument_list|()
init|;
name|i
operator|.
name|hasNext
argument_list|()
condition|;
control|)
block|{
name|out
operator|.
name|writeStatement
argument_list|(
operator|new
name|Statement
argument_list|(
name|oldInstance
argument_list|,
literal|"add"
argument_list|,
operator|new
name|Object
index|[]
block|{
name|i
operator|.
name|next
argument_list|()
block|}
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
block|}
specifier|public
specifier|static
class|class
name|ListDelegate
extends|extends
name|DefaultPersistenceDelegate
block|{
annotation|@
name|Override
specifier|protected
name|Expression
name|instantiate
parameter_list|(
name|Object
name|oldInstance
parameter_list|,
name|Encoder
name|out
parameter_list|)
block|{
name|List
name|oldList
init|=
operator|(
name|List
operator|)
name|oldInstance
decl_stmt|;
name|ArrayList
name|newList
init|=
operator|new
name|ArrayList
argument_list|(
name|oldList
argument_list|)
decl_stmt|;
return|return
operator|new
name|Expression
argument_list|(
name|newList
argument_list|,
name|ArrayList
operator|.
name|class
argument_list|,
literal|"new"
argument_list|,
operator|new
name|Object
index|[]
block|{}
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|protected
name|boolean
name|mutatesTo
parameter_list|(
name|Object
name|oldInstance
parameter_list|,
name|Object
name|newInstance
parameter_list|)
block|{
return|return
literal|false
return|;
block|}
annotation|@
name|Override
specifier|protected
name|void
name|initialize
parameter_list|(
name|Class
argument_list|<
name|?
argument_list|>
name|type
parameter_list|,
name|Object
name|oldInstance
parameter_list|,
name|Object
name|newInstance
parameter_list|,
name|Encoder
name|out
parameter_list|)
block|{
name|java
operator|.
name|util
operator|.
name|Collection
name|oldO
init|=
operator|(
name|java
operator|.
name|util
operator|.
name|Collection
operator|)
name|oldInstance
decl_stmt|;
name|java
operator|.
name|util
operator|.
name|Collection
name|newO
init|=
operator|(
name|java
operator|.
name|util
operator|.
name|Collection
operator|)
name|newInstance
decl_stmt|;
if|if
condition|(
name|newO
operator|.
name|size
argument_list|()
operator|!=
literal|0
condition|)
block|{
name|out
operator|.
name|writeStatement
argument_list|(
operator|new
name|Statement
argument_list|(
name|oldInstance
argument_list|,
literal|"clear"
argument_list|,
operator|new
name|Object
index|[]
block|{}
argument_list|)
argument_list|)
expr_stmt|;
block|}
for|for
control|(
name|Iterator
name|i
init|=
name|oldO
operator|.
name|iterator
argument_list|()
init|;
name|i
operator|.
name|hasNext
argument_list|()
condition|;
control|)
block|{
name|out
operator|.
name|writeStatement
argument_list|(
operator|new
name|Statement
argument_list|(
name|oldInstance
argument_list|,
literal|"add"
argument_list|,
operator|new
name|Object
index|[]
block|{
name|i
operator|.
name|next
argument_list|()
block|}
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
block|}
specifier|public
specifier|static
name|void
name|setMapRedWork
parameter_list|(
name|Configuration
name|job
parameter_list|,
name|MapredWork
name|w
parameter_list|,
name|String
name|hiveScratchDir
parameter_list|)
block|{
try|try
block|{
comment|// this is the unique job ID, which is kept in JobConf as part of the plan file name
name|String
name|jobID
init|=
name|UUID
operator|.
name|randomUUID
argument_list|()
operator|.
name|toString
argument_list|()
decl_stmt|;
name|Path
name|planPath
init|=
operator|new
name|Path
argument_list|(
name|hiveScratchDir
argument_list|,
name|jobID
argument_list|)
decl_stmt|;
name|HiveConf
operator|.
name|setVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|PLAN
argument_list|,
name|planPath
operator|.
name|toUri
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
comment|// use the default file system of the job
name|FileSystem
name|fs
init|=
name|planPath
operator|.
name|getFileSystem
argument_list|(
name|job
argument_list|)
decl_stmt|;
name|FSDataOutputStream
name|out
init|=
name|fs
operator|.
name|create
argument_list|(
name|planPath
argument_list|)
decl_stmt|;
name|serializeMapRedWork
argument_list|(
name|w
argument_list|,
name|out
argument_list|)
expr_stmt|;
comment|// Serialize the plan to the default hdfs instance
comment|// Except for hadoop local mode execution where we should be
comment|// able to get the plan directly from the cache
if|if
condition|(
operator|!
name|HiveConf
operator|.
name|getVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HADOOPJT
argument_list|)
operator|.
name|equals
argument_list|(
literal|"local"
argument_list|)
condition|)
block|{
comment|// Set up distributed cache
name|DistributedCache
operator|.
name|createSymlink
argument_list|(
name|job
argument_list|)
expr_stmt|;
name|String
name|uriWithLink
init|=
name|planPath
operator|.
name|toUri
argument_list|()
operator|.
name|toString
argument_list|()
operator|+
literal|"#HIVE_PLAN"
operator|+
name|jobID
decl_stmt|;
name|DistributedCache
operator|.
name|addCacheFile
argument_list|(
operator|new
name|URI
argument_list|(
name|uriWithLink
argument_list|)
argument_list|,
name|job
argument_list|)
expr_stmt|;
comment|// set replication of the plan file to a high number. we use the same
comment|// replication factor as used by the hadoop jobclient for job.xml etc.
name|short
name|replication
init|=
operator|(
name|short
operator|)
name|job
operator|.
name|getInt
argument_list|(
literal|"mapred.submit.replication"
argument_list|,
literal|10
argument_list|)
decl_stmt|;
name|fs
operator|.
name|setReplication
argument_list|(
name|planPath
argument_list|,
name|replication
argument_list|)
expr_stmt|;
block|}
comment|// Cache the plan in this process
name|w
operator|.
name|initialize
argument_list|()
expr_stmt|;
name|gWorkMap
operator|.
name|put
argument_list|(
name|jobID
argument_list|,
name|w
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|e
operator|.
name|printStackTrace
argument_list|()
expr_stmt|;
throw|throw
operator|new
name|RuntimeException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
specifier|static
name|String
name|getHiveJobID
parameter_list|(
name|Configuration
name|job
parameter_list|)
block|{
name|String
name|planPath
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|PLAN
argument_list|)
decl_stmt|;
if|if
condition|(
name|planPath
operator|!=
literal|null
condition|)
block|{
return|return
operator|(
operator|new
name|Path
argument_list|(
name|planPath
argument_list|)
operator|)
operator|.
name|getName
argument_list|()
return|;
block|}
return|return
literal|null
return|;
block|}
specifier|public
specifier|static
name|String
name|serializeExpression
parameter_list|(
name|ExprNodeDesc
name|expr
parameter_list|)
block|{
name|ByteArrayOutputStream
name|baos
init|=
operator|new
name|ByteArrayOutputStream
argument_list|()
decl_stmt|;
name|XMLEncoder
name|encoder
init|=
operator|new
name|XMLEncoder
argument_list|(
name|baos
argument_list|)
decl_stmt|;
try|try
block|{
name|encoder
operator|.
name|writeObject
argument_list|(
name|expr
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
name|encoder
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
try|try
block|{
return|return
name|baos
operator|.
name|toString
argument_list|(
literal|"UTF-8"
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|UnsupportedEncodingException
name|ex
parameter_list|)
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
literal|"UTF-8 support required"
argument_list|,
name|ex
argument_list|)
throw|;
block|}
block|}
specifier|public
specifier|static
name|ExprNodeDesc
name|deserializeExpression
parameter_list|(
name|String
name|s
parameter_list|,
name|Configuration
name|conf
parameter_list|)
block|{
name|byte
index|[]
name|bytes
decl_stmt|;
try|try
block|{
name|bytes
operator|=
name|s
operator|.
name|getBytes
argument_list|(
literal|"UTF-8"
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|UnsupportedEncodingException
name|ex
parameter_list|)
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
literal|"UTF-8 support required"
argument_list|,
name|ex
argument_list|)
throw|;
block|}
name|ByteArrayInputStream
name|bais
init|=
operator|new
name|ByteArrayInputStream
argument_list|(
name|bytes
argument_list|)
decl_stmt|;
name|XMLDecoder
name|decoder
init|=
operator|new
name|XMLDecoder
argument_list|(
name|bais
argument_list|,
literal|null
argument_list|,
literal|null
argument_list|,
name|conf
operator|.
name|getClassLoader
argument_list|()
argument_list|)
decl_stmt|;
try|try
block|{
name|ExprNodeDesc
name|expr
init|=
operator|(
name|ExprNodeDesc
operator|)
name|decoder
operator|.
name|readObject
argument_list|()
decl_stmt|;
return|return
name|expr
return|;
block|}
finally|finally
block|{
name|decoder
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
comment|/**    * Serialize a single Task.    */
specifier|public
specifier|static
name|void
name|serializeTasks
parameter_list|(
name|Task
argument_list|<
name|?
extends|extends
name|Serializable
argument_list|>
name|t
parameter_list|,
name|OutputStream
name|out
parameter_list|)
block|{
name|XMLEncoder
name|e
init|=
literal|null
decl_stmt|;
try|try
block|{
name|e
operator|=
operator|new
name|XMLEncoder
argument_list|(
name|out
argument_list|)
expr_stmt|;
comment|// workaround for java 1.5
name|e
operator|.
name|setPersistenceDelegate
argument_list|(
name|ExpressionTypes
operator|.
name|class
argument_list|,
operator|new
name|EnumDelegate
argument_list|()
argument_list|)
expr_stmt|;
name|e
operator|.
name|setPersistenceDelegate
argument_list|(
name|GroupByDesc
operator|.
name|Mode
operator|.
name|class
argument_list|,
operator|new
name|EnumDelegate
argument_list|()
argument_list|)
expr_stmt|;
name|e
operator|.
name|setPersistenceDelegate
argument_list|(
name|Operator
operator|.
name|ProgressCounter
operator|.
name|class
argument_list|,
operator|new
name|EnumDelegate
argument_list|()
argument_list|)
expr_stmt|;
name|e
operator|.
name|writeObject
argument_list|(
name|t
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
if|if
condition|(
literal|null
operator|!=
name|e
condition|)
block|{
name|e
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
block|}
specifier|public
specifier|static
class|class
name|CollectionPersistenceDelegate
extends|extends
name|DefaultPersistenceDelegate
block|{
annotation|@
name|Override
specifier|protected
name|Expression
name|instantiate
parameter_list|(
name|Object
name|oldInstance
parameter_list|,
name|Encoder
name|out
parameter_list|)
block|{
return|return
operator|new
name|Expression
argument_list|(
name|oldInstance
argument_list|,
name|oldInstance
operator|.
name|getClass
argument_list|()
argument_list|,
literal|"new"
argument_list|,
literal|null
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|protected
name|void
name|initialize
parameter_list|(
name|Class
name|type
parameter_list|,
name|Object
name|oldInstance
parameter_list|,
name|Object
name|newInstance
parameter_list|,
name|Encoder
name|out
parameter_list|)
block|{
name|Iterator
name|ite
init|=
operator|(
operator|(
name|Collection
operator|)
name|oldInstance
operator|)
operator|.
name|iterator
argument_list|()
decl_stmt|;
while|while
condition|(
name|ite
operator|.
name|hasNext
argument_list|()
condition|)
block|{
name|out
operator|.
name|writeStatement
argument_list|(
operator|new
name|Statement
argument_list|(
name|oldInstance
argument_list|,
literal|"add"
argument_list|,
operator|new
name|Object
index|[]
block|{
name|ite
operator|.
name|next
argument_list|()
block|}
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
block|}
comment|/**    * Serialize the whole query plan.    */
specifier|public
specifier|static
name|void
name|serializeQueryPlan
parameter_list|(
name|QueryPlan
name|plan
parameter_list|,
name|OutputStream
name|out
parameter_list|)
block|{
name|XMLEncoder
name|e
init|=
operator|new
name|XMLEncoder
argument_list|(
name|out
argument_list|)
decl_stmt|;
name|e
operator|.
name|setExceptionListener
argument_list|(
operator|new
name|ExceptionListener
argument_list|()
block|{
specifier|public
name|void
name|exceptionThrown
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|RuntimeException
argument_list|(
literal|"Cannot serialize the query plan"
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
argument_list|)
expr_stmt|;
comment|// workaround for java 1.5
name|e
operator|.
name|setPersistenceDelegate
argument_list|(
name|ExpressionTypes
operator|.
name|class
argument_list|,
operator|new
name|EnumDelegate
argument_list|()
argument_list|)
expr_stmt|;
name|e
operator|.
name|setPersistenceDelegate
argument_list|(
name|GroupByDesc
operator|.
name|Mode
operator|.
name|class
argument_list|,
operator|new
name|EnumDelegate
argument_list|()
argument_list|)
expr_stmt|;
name|e
operator|.
name|setPersistenceDelegate
argument_list|(
name|Operator
operator|.
name|ProgressCounter
operator|.
name|class
argument_list|,
operator|new
name|EnumDelegate
argument_list|()
argument_list|)
expr_stmt|;
name|e
operator|.
name|setPersistenceDelegate
argument_list|(
name|org
operator|.
name|datanucleus
operator|.
name|sco
operator|.
name|backed
operator|.
name|Map
operator|.
name|class
argument_list|,
operator|new
name|MapDelegate
argument_list|()
argument_list|)
expr_stmt|;
name|e
operator|.
name|setPersistenceDelegate
argument_list|(
name|org
operator|.
name|datanucleus
operator|.
name|sco
operator|.
name|backed
operator|.
name|List
operator|.
name|class
argument_list|,
operator|new
name|ListDelegate
argument_list|()
argument_list|)
expr_stmt|;
name|e
operator|.
name|writeObject
argument_list|(
name|plan
argument_list|)
expr_stmt|;
name|e
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
comment|/**    * Deserialize the whole query plan.    */
specifier|public
specifier|static
name|QueryPlan
name|deserializeQueryPlan
parameter_list|(
name|InputStream
name|in
parameter_list|,
name|Configuration
name|conf
parameter_list|)
block|{
name|XMLDecoder
name|d
init|=
literal|null
decl_stmt|;
try|try
block|{
name|d
operator|=
operator|new
name|XMLDecoder
argument_list|(
name|in
argument_list|,
literal|null
argument_list|,
literal|null
argument_list|,
name|conf
operator|.
name|getClassLoader
argument_list|()
argument_list|)
expr_stmt|;
name|QueryPlan
name|ret
init|=
operator|(
name|QueryPlan
operator|)
name|d
operator|.
name|readObject
argument_list|()
decl_stmt|;
return|return
operator|(
name|ret
operator|)
return|;
block|}
finally|finally
block|{
if|if
condition|(
literal|null
operator|!=
name|d
condition|)
block|{
name|d
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
block|}
comment|/**    * Serialize the mapredWork object to an output stream. DO NOT use this to write to standard    * output since it closes the output stream. DO USE mapredWork.toXML() instead.    */
specifier|public
specifier|static
name|void
name|serializeMapRedWork
parameter_list|(
name|MapredWork
name|w
parameter_list|,
name|OutputStream
name|out
parameter_list|)
block|{
name|XMLEncoder
name|e
init|=
literal|null
decl_stmt|;
try|try
block|{
name|e
operator|=
operator|new
name|XMLEncoder
argument_list|(
name|out
argument_list|)
expr_stmt|;
comment|// workaround for java 1.5
name|e
operator|.
name|setPersistenceDelegate
argument_list|(
name|ExpressionTypes
operator|.
name|class
argument_list|,
operator|new
name|EnumDelegate
argument_list|()
argument_list|)
expr_stmt|;
name|e
operator|.
name|setPersistenceDelegate
argument_list|(
name|GroupByDesc
operator|.
name|Mode
operator|.
name|class
argument_list|,
operator|new
name|EnumDelegate
argument_list|()
argument_list|)
expr_stmt|;
name|e
operator|.
name|writeObject
argument_list|(
name|w
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
if|if
condition|(
literal|null
operator|!=
name|e
condition|)
block|{
name|e
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
block|}
specifier|public
specifier|static
name|MapredWork
name|deserializeMapRedWork
parameter_list|(
name|InputStream
name|in
parameter_list|,
name|Configuration
name|conf
parameter_list|)
block|{
name|XMLDecoder
name|d
init|=
literal|null
decl_stmt|;
try|try
block|{
name|d
operator|=
operator|new
name|XMLDecoder
argument_list|(
name|in
argument_list|,
literal|null
argument_list|,
literal|null
argument_list|,
name|conf
operator|.
name|getClassLoader
argument_list|()
argument_list|)
expr_stmt|;
name|MapredWork
name|ret
init|=
operator|(
name|MapredWork
operator|)
name|d
operator|.
name|readObject
argument_list|()
decl_stmt|;
return|return
operator|(
name|ret
operator|)
return|;
block|}
finally|finally
block|{
if|if
condition|(
literal|null
operator|!=
name|d
condition|)
block|{
name|d
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
block|}
comment|/**    * Serialize the mapredLocalWork object to an output stream. DO NOT use this to write to standard    * output since it closes the output stream. DO USE mapredWork.toXML() instead.    */
specifier|public
specifier|static
name|void
name|serializeMapRedLocalWork
parameter_list|(
name|MapredLocalWork
name|w
parameter_list|,
name|OutputStream
name|out
parameter_list|)
block|{
name|XMLEncoder
name|e
init|=
literal|null
decl_stmt|;
try|try
block|{
name|e
operator|=
operator|new
name|XMLEncoder
argument_list|(
name|out
argument_list|)
expr_stmt|;
comment|// workaround for java 1.5
name|e
operator|.
name|setPersistenceDelegate
argument_list|(
name|ExpressionTypes
operator|.
name|class
argument_list|,
operator|new
name|EnumDelegate
argument_list|()
argument_list|)
expr_stmt|;
name|e
operator|.
name|setPersistenceDelegate
argument_list|(
name|GroupByDesc
operator|.
name|Mode
operator|.
name|class
argument_list|,
operator|new
name|EnumDelegate
argument_list|()
argument_list|)
expr_stmt|;
name|e
operator|.
name|writeObject
argument_list|(
name|w
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
if|if
condition|(
literal|null
operator|!=
name|e
condition|)
block|{
name|e
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
block|}
specifier|public
specifier|static
name|MapredLocalWork
name|deserializeMapRedLocalWork
parameter_list|(
name|InputStream
name|in
parameter_list|,
name|Configuration
name|conf
parameter_list|)
block|{
name|XMLDecoder
name|d
init|=
literal|null
decl_stmt|;
try|try
block|{
name|d
operator|=
operator|new
name|XMLDecoder
argument_list|(
name|in
argument_list|,
literal|null
argument_list|,
literal|null
argument_list|,
name|conf
operator|.
name|getClassLoader
argument_list|()
argument_list|)
expr_stmt|;
name|MapredLocalWork
name|ret
init|=
operator|(
name|MapredLocalWork
operator|)
name|d
operator|.
name|readObject
argument_list|()
decl_stmt|;
return|return
operator|(
name|ret
operator|)
return|;
block|}
finally|finally
block|{
if|if
condition|(
literal|null
operator|!=
name|d
condition|)
block|{
name|d
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
block|}
comment|/**    * Tuple.    *    * @param<T>    * @param<V>    */
specifier|public
specifier|static
class|class
name|Tuple
parameter_list|<
name|T
parameter_list|,
name|V
parameter_list|>
block|{
specifier|private
specifier|final
name|T
name|one
decl_stmt|;
specifier|private
specifier|final
name|V
name|two
decl_stmt|;
specifier|public
name|Tuple
parameter_list|(
name|T
name|one
parameter_list|,
name|V
name|two
parameter_list|)
block|{
name|this
operator|.
name|one
operator|=
name|one
expr_stmt|;
name|this
operator|.
name|two
operator|=
name|two
expr_stmt|;
block|}
specifier|public
name|T
name|getOne
parameter_list|()
block|{
return|return
name|this
operator|.
name|one
return|;
block|}
specifier|public
name|V
name|getTwo
parameter_list|()
block|{
return|return
name|this
operator|.
name|two
return|;
block|}
block|}
specifier|public
specifier|static
name|TableDesc
name|defaultTd
decl_stmt|;
static|static
block|{
comment|// by default we expect ^A separated strings
comment|// This tableDesc does not provide column names. We should always use
comment|// PlanUtils.getDefaultTableDesc(String separatorCode, String columns)
comment|// or getBinarySortableTableDesc(List<FieldSchema> fieldSchemas) when
comment|// we know the column names.
name|defaultTd
operator|=
name|PlanUtils
operator|.
name|getDefaultTableDesc
argument_list|(
literal|""
operator|+
name|Utilities
operator|.
name|ctrlaCode
argument_list|)
expr_stmt|;
block|}
specifier|public
specifier|static
specifier|final
name|int
name|newLineCode
init|=
literal|10
decl_stmt|;
specifier|public
specifier|static
specifier|final
name|int
name|tabCode
init|=
literal|9
decl_stmt|;
specifier|public
specifier|static
specifier|final
name|int
name|ctrlaCode
init|=
literal|1
decl_stmt|;
specifier|public
specifier|static
specifier|final
name|String
name|INDENT
init|=
literal|"  "
decl_stmt|;
comment|// Note: When DDL supports specifying what string to represent null,
comment|// we should specify "NULL" to represent null in the temp table, and then
comment|// we can make the following translation deprecated.
specifier|public
specifier|static
name|String
name|nullStringStorage
init|=
literal|"\\N"
decl_stmt|;
specifier|public
specifier|static
name|String
name|nullStringOutput
init|=
literal|"NULL"
decl_stmt|;
specifier|public
specifier|static
name|Random
name|randGen
init|=
operator|new
name|Random
argument_list|()
decl_stmt|;
comment|/**    * Gets the task id if we are running as a Hadoop job. Gets a random number otherwise.    */
specifier|public
specifier|static
name|String
name|getTaskId
parameter_list|(
name|Configuration
name|hconf
parameter_list|)
block|{
name|String
name|taskid
init|=
operator|(
name|hconf
operator|==
literal|null
operator|)
condition|?
literal|null
else|:
name|hconf
operator|.
name|get
argument_list|(
literal|"mapred.task.id"
argument_list|)
decl_stmt|;
if|if
condition|(
operator|(
name|taskid
operator|==
literal|null
operator|)
operator|||
name|taskid
operator|.
name|equals
argument_list|(
literal|""
argument_list|)
condition|)
block|{
return|return
operator|(
literal|""
operator|+
name|Math
operator|.
name|abs
argument_list|(
name|randGen
operator|.
name|nextInt
argument_list|()
argument_list|)
operator|)
return|;
block|}
else|else
block|{
comment|/*        * extract the task and attempt id from the hadoop taskid. in version 17 the leading component        * was 'task_'. thereafter the leading component is 'attempt_'. in 17 - hadoop also seems to        * have used _map_ and _reduce_ to denote map/reduce task types        */
name|String
name|ret
init|=
name|taskid
operator|.
name|replaceAll
argument_list|(
literal|".*_[mr]_"
argument_list|,
literal|""
argument_list|)
operator|.
name|replaceAll
argument_list|(
literal|".*_(map|reduce)_"
argument_list|,
literal|""
argument_list|)
decl_stmt|;
return|return
operator|(
name|ret
operator|)
return|;
block|}
block|}
specifier|public
specifier|static
name|HashMap
name|makeMap
parameter_list|(
name|Object
modifier|...
name|olist
parameter_list|)
block|{
name|HashMap
name|ret
init|=
operator|new
name|HashMap
argument_list|()
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|olist
operator|.
name|length
condition|;
name|i
operator|+=
literal|2
control|)
block|{
name|ret
operator|.
name|put
argument_list|(
name|olist
index|[
name|i
index|]
argument_list|,
name|olist
index|[
name|i
operator|+
literal|1
index|]
argument_list|)
expr_stmt|;
block|}
return|return
operator|(
name|ret
operator|)
return|;
block|}
specifier|public
specifier|static
name|Properties
name|makeProperties
parameter_list|(
name|String
modifier|...
name|olist
parameter_list|)
block|{
name|Properties
name|ret
init|=
operator|new
name|Properties
argument_list|()
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|olist
operator|.
name|length
condition|;
name|i
operator|+=
literal|2
control|)
block|{
name|ret
operator|.
name|setProperty
argument_list|(
name|olist
index|[
name|i
index|]
argument_list|,
name|olist
index|[
name|i
operator|+
literal|1
index|]
argument_list|)
expr_stmt|;
block|}
return|return
operator|(
name|ret
operator|)
return|;
block|}
specifier|public
specifier|static
name|ArrayList
name|makeList
parameter_list|(
name|Object
modifier|...
name|olist
parameter_list|)
block|{
name|ArrayList
name|ret
init|=
operator|new
name|ArrayList
argument_list|()
decl_stmt|;
for|for
control|(
name|Object
name|element
range|:
name|olist
control|)
block|{
name|ret
operator|.
name|add
argument_list|(
name|element
argument_list|)
expr_stmt|;
block|}
return|return
operator|(
name|ret
operator|)
return|;
block|}
comment|/**    * StreamPrinter.    *    */
specifier|public
specifier|static
class|class
name|StreamPrinter
extends|extends
name|Thread
block|{
name|InputStream
name|is
decl_stmt|;
name|String
name|type
decl_stmt|;
name|PrintStream
name|os
decl_stmt|;
specifier|public
name|StreamPrinter
parameter_list|(
name|InputStream
name|is
parameter_list|,
name|String
name|type
parameter_list|,
name|PrintStream
name|os
parameter_list|)
block|{
name|this
operator|.
name|is
operator|=
name|is
expr_stmt|;
name|this
operator|.
name|type
operator|=
name|type
expr_stmt|;
name|this
operator|.
name|os
operator|=
name|os
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|run
parameter_list|()
block|{
name|BufferedReader
name|br
init|=
literal|null
decl_stmt|;
try|try
block|{
name|InputStreamReader
name|isr
init|=
operator|new
name|InputStreamReader
argument_list|(
name|is
argument_list|)
decl_stmt|;
name|br
operator|=
operator|new
name|BufferedReader
argument_list|(
name|isr
argument_list|)
expr_stmt|;
name|String
name|line
init|=
literal|null
decl_stmt|;
if|if
condition|(
name|type
operator|!=
literal|null
condition|)
block|{
while|while
condition|(
operator|(
name|line
operator|=
name|br
operator|.
name|readLine
argument_list|()
operator|)
operator|!=
literal|null
condition|)
block|{
name|os
operator|.
name|println
argument_list|(
name|type
operator|+
literal|">"
operator|+
name|line
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
while|while
condition|(
operator|(
name|line
operator|=
name|br
operator|.
name|readLine
argument_list|()
operator|)
operator|!=
literal|null
condition|)
block|{
name|os
operator|.
name|println
argument_list|(
name|line
argument_list|)
expr_stmt|;
block|}
block|}
name|br
operator|.
name|close
argument_list|()
expr_stmt|;
name|br
operator|=
literal|null
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|ioe
parameter_list|)
block|{
name|ioe
operator|.
name|printStackTrace
argument_list|()
expr_stmt|;
block|}
finally|finally
block|{
name|IOUtils
operator|.
name|closeStream
argument_list|(
name|br
argument_list|)
expr_stmt|;
block|}
block|}
block|}
specifier|public
specifier|static
name|TableDesc
name|getTableDesc
parameter_list|(
name|Table
name|tbl
parameter_list|)
block|{
return|return
operator|(
operator|new
name|TableDesc
argument_list|(
name|tbl
operator|.
name|getDeserializer
argument_list|()
operator|.
name|getClass
argument_list|()
argument_list|,
name|tbl
operator|.
name|getInputFormatClass
argument_list|()
argument_list|,
name|tbl
operator|.
name|getOutputFormatClass
argument_list|()
argument_list|,
name|tbl
operator|.
name|getSchema
argument_list|()
argument_list|)
operator|)
return|;
block|}
comment|// column names and column types are all delimited by comma
specifier|public
specifier|static
name|TableDesc
name|getTableDesc
parameter_list|(
name|String
name|cols
parameter_list|,
name|String
name|colTypes
parameter_list|)
block|{
return|return
operator|(
operator|new
name|TableDesc
argument_list|(
name|LazySimpleSerDe
operator|.
name|class
argument_list|,
name|SequenceFileInputFormat
operator|.
name|class
argument_list|,
name|HiveSequenceFileOutputFormat
operator|.
name|class
argument_list|,
name|Utilities
operator|.
name|makeProperties
argument_list|(
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde
operator|.
name|Constants
operator|.
name|SERIALIZATION_FORMAT
argument_list|,
literal|""
operator|+
name|Utilities
operator|.
name|ctrlaCode
argument_list|,
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde
operator|.
name|Constants
operator|.
name|LIST_COLUMNS
argument_list|,
name|cols
argument_list|,
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde
operator|.
name|Constants
operator|.
name|LIST_COLUMN_TYPES
argument_list|,
name|colTypes
argument_list|)
argument_list|)
operator|)
return|;
block|}
specifier|public
specifier|static
name|PartitionDesc
name|getPartitionDesc
parameter_list|(
name|Partition
name|part
parameter_list|)
throws|throws
name|HiveException
block|{
return|return
operator|(
operator|new
name|PartitionDesc
argument_list|(
name|part
argument_list|)
operator|)
return|;
block|}
specifier|public
specifier|static
name|PartitionDesc
name|getPartitionDescFromTableDesc
parameter_list|(
name|TableDesc
name|tblDesc
parameter_list|,
name|Partition
name|part
parameter_list|)
throws|throws
name|HiveException
block|{
return|return
operator|new
name|PartitionDesc
argument_list|(
name|part
argument_list|,
name|tblDesc
argument_list|)
return|;
block|}
specifier|public
specifier|static
name|void
name|addMapWork
parameter_list|(
name|MapredWork
name|mr
parameter_list|,
name|Table
name|tbl
parameter_list|,
name|String
name|alias
parameter_list|,
name|Operator
argument_list|<
name|?
argument_list|>
name|work
parameter_list|)
block|{
name|mr
operator|.
name|addMapWork
argument_list|(
name|tbl
operator|.
name|getDataLocation
argument_list|()
operator|.
name|getPath
argument_list|()
argument_list|,
name|alias
argument_list|,
name|work
argument_list|,
operator|new
name|PartitionDesc
argument_list|(
name|getTableDesc
argument_list|(
name|tbl
argument_list|)
argument_list|,
operator|(
name|LinkedHashMap
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
operator|)
literal|null
argument_list|)
argument_list|)
expr_stmt|;
block|}
specifier|private
specifier|static
name|String
name|getOpTreeSkel_helper
parameter_list|(
name|Operator
argument_list|<
name|?
argument_list|>
name|op
parameter_list|,
name|String
name|indent
parameter_list|)
block|{
if|if
condition|(
name|op
operator|==
literal|null
condition|)
block|{
return|return
literal|""
return|;
block|}
name|StringBuilder
name|sb
init|=
operator|new
name|StringBuilder
argument_list|()
decl_stmt|;
name|sb
operator|.
name|append
argument_list|(
name|indent
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
name|op
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
literal|"\n"
argument_list|)
expr_stmt|;
if|if
condition|(
name|op
operator|.
name|getChildOperators
argument_list|()
operator|!=
literal|null
condition|)
block|{
for|for
control|(
name|Object
name|child
range|:
name|op
operator|.
name|getChildOperators
argument_list|()
control|)
block|{
name|sb
operator|.
name|append
argument_list|(
name|getOpTreeSkel_helper
argument_list|(
operator|(
name|Operator
argument_list|<
name|?
argument_list|>
operator|)
name|child
argument_list|,
name|indent
operator|+
literal|"  "
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|sb
operator|.
name|toString
argument_list|()
return|;
block|}
specifier|public
specifier|static
name|String
name|getOpTreeSkel
parameter_list|(
name|Operator
argument_list|<
name|?
argument_list|>
name|op
parameter_list|)
block|{
return|return
name|getOpTreeSkel_helper
argument_list|(
name|op
argument_list|,
literal|""
argument_list|)
return|;
block|}
specifier|private
specifier|static
name|boolean
name|isWhitespace
parameter_list|(
name|int
name|c
parameter_list|)
block|{
if|if
condition|(
name|c
operator|==
operator|-
literal|1
condition|)
block|{
return|return
literal|false
return|;
block|}
return|return
name|Character
operator|.
name|isWhitespace
argument_list|(
operator|(
name|char
operator|)
name|c
argument_list|)
return|;
block|}
specifier|public
specifier|static
name|boolean
name|contentsEqual
parameter_list|(
name|InputStream
name|is1
parameter_list|,
name|InputStream
name|is2
parameter_list|,
name|boolean
name|ignoreWhitespace
parameter_list|)
throws|throws
name|IOException
block|{
try|try
block|{
if|if
condition|(
operator|(
name|is1
operator|==
name|is2
operator|)
operator|||
operator|(
name|is1
operator|==
literal|null
operator|&&
name|is2
operator|==
literal|null
operator|)
condition|)
block|{
return|return
literal|true
return|;
block|}
if|if
condition|(
name|is1
operator|==
literal|null
operator|||
name|is2
operator|==
literal|null
condition|)
block|{
return|return
literal|false
return|;
block|}
while|while
condition|(
literal|true
condition|)
block|{
name|int
name|c1
init|=
name|is1
operator|.
name|read
argument_list|()
decl_stmt|;
while|while
condition|(
name|ignoreWhitespace
operator|&&
name|isWhitespace
argument_list|(
name|c1
argument_list|)
condition|)
block|{
name|c1
operator|=
name|is1
operator|.
name|read
argument_list|()
expr_stmt|;
block|}
name|int
name|c2
init|=
name|is2
operator|.
name|read
argument_list|()
decl_stmt|;
while|while
condition|(
name|ignoreWhitespace
operator|&&
name|isWhitespace
argument_list|(
name|c2
argument_list|)
condition|)
block|{
name|c2
operator|=
name|is2
operator|.
name|read
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
name|c1
operator|==
operator|-
literal|1
operator|&&
name|c2
operator|==
operator|-
literal|1
condition|)
block|{
return|return
literal|true
return|;
block|}
if|if
condition|(
name|c1
operator|!=
name|c2
condition|)
block|{
break|break;
block|}
block|}
block|}
catch|catch
parameter_list|(
name|FileNotFoundException
name|e
parameter_list|)
block|{
name|e
operator|.
name|printStackTrace
argument_list|()
expr_stmt|;
block|}
return|return
literal|false
return|;
block|}
comment|/**    * convert "From src insert blah blah" to "From src insert ... blah"    */
specifier|public
specifier|static
name|String
name|abbreviate
parameter_list|(
name|String
name|str
parameter_list|,
name|int
name|max
parameter_list|)
block|{
name|str
operator|=
name|str
operator|.
name|trim
argument_list|()
expr_stmt|;
name|int
name|len
init|=
name|str
operator|.
name|length
argument_list|()
decl_stmt|;
name|int
name|suffixlength
init|=
literal|20
decl_stmt|;
if|if
condition|(
name|len
operator|<=
name|max
condition|)
block|{
return|return
name|str
return|;
block|}
name|suffixlength
operator|=
name|Math
operator|.
name|min
argument_list|(
name|suffixlength
argument_list|,
operator|(
name|max
operator|-
literal|3
operator|)
operator|/
literal|2
argument_list|)
expr_stmt|;
name|String
name|rev
init|=
name|StringUtils
operator|.
name|reverse
argument_list|(
name|str
argument_list|)
decl_stmt|;
comment|// get the last few words
name|String
name|suffix
init|=
name|WordUtils
operator|.
name|abbreviate
argument_list|(
name|rev
argument_list|,
literal|0
argument_list|,
name|suffixlength
argument_list|,
literal|""
argument_list|)
decl_stmt|;
name|suffix
operator|=
name|StringUtils
operator|.
name|reverse
argument_list|(
name|suffix
argument_list|)
expr_stmt|;
comment|// first few ..
name|String
name|prefix
init|=
name|StringUtils
operator|.
name|abbreviate
argument_list|(
name|str
argument_list|,
name|max
operator|-
name|suffix
operator|.
name|length
argument_list|()
argument_list|)
decl_stmt|;
return|return
name|prefix
operator|+
name|suffix
return|;
block|}
specifier|public
specifier|static
specifier|final
name|String
name|NSTR
init|=
literal|""
decl_stmt|;
comment|/**    * StreamStatus.    *    */
specifier|public
specifier|static
enum|enum
name|StreamStatus
block|{
name|EOF
block|,
name|TERMINATED
block|}
specifier|public
specifier|static
name|StreamStatus
name|readColumn
parameter_list|(
name|DataInput
name|in
parameter_list|,
name|OutputStream
name|out
parameter_list|)
throws|throws
name|IOException
block|{
while|while
condition|(
literal|true
condition|)
block|{
name|int
name|b
decl_stmt|;
try|try
block|{
name|b
operator|=
name|in
operator|.
name|readByte
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|EOFException
name|e
parameter_list|)
block|{
return|return
name|StreamStatus
operator|.
name|EOF
return|;
block|}
if|if
condition|(
name|b
operator|==
name|Utilities
operator|.
name|newLineCode
condition|)
block|{
return|return
name|StreamStatus
operator|.
name|TERMINATED
return|;
block|}
name|out
operator|.
name|write
argument_list|(
name|b
argument_list|)
expr_stmt|;
block|}
comment|// Unreachable
block|}
comment|/**    * Convert an output stream to a compressed output stream based on codecs and compression options    * specified in the Job Configuration.    *    * @param jc    *          Job Configuration    * @param out    *          Output Stream to be converted into compressed output stream    * @return compressed output stream    */
specifier|public
specifier|static
name|OutputStream
name|createCompressedStream
parameter_list|(
name|JobConf
name|jc
parameter_list|,
name|OutputStream
name|out
parameter_list|)
throws|throws
name|IOException
block|{
name|boolean
name|isCompressed
init|=
name|FileOutputFormat
operator|.
name|getCompressOutput
argument_list|(
name|jc
argument_list|)
decl_stmt|;
return|return
name|createCompressedStream
argument_list|(
name|jc
argument_list|,
name|out
argument_list|,
name|isCompressed
argument_list|)
return|;
block|}
comment|/**    * Convert an output stream to a compressed output stream based on codecs codecs in the Job    * Configuration. Caller specifies directly whether file is compressed or not    *    * @param jc    *          Job Configuration    * @param out    *          Output Stream to be converted into compressed output stream    * @param isCompressed    *          whether the output stream needs to be compressed or not    * @return compressed output stream    */
specifier|public
specifier|static
name|OutputStream
name|createCompressedStream
parameter_list|(
name|JobConf
name|jc
parameter_list|,
name|OutputStream
name|out
parameter_list|,
name|boolean
name|isCompressed
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|isCompressed
condition|)
block|{
name|Class
argument_list|<
name|?
extends|extends
name|CompressionCodec
argument_list|>
name|codecClass
init|=
name|FileOutputFormat
operator|.
name|getOutputCompressorClass
argument_list|(
name|jc
argument_list|,
name|DefaultCodec
operator|.
name|class
argument_list|)
decl_stmt|;
name|CompressionCodec
name|codec
init|=
operator|(
name|CompressionCodec
operator|)
name|ReflectionUtils
operator|.
name|newInstance
argument_list|(
name|codecClass
argument_list|,
name|jc
argument_list|)
decl_stmt|;
return|return
name|codec
operator|.
name|createOutputStream
argument_list|(
name|out
argument_list|)
return|;
block|}
else|else
block|{
return|return
operator|(
name|out
operator|)
return|;
block|}
block|}
comment|/**    * Based on compression option and configured output codec - get extension for output file. This    * is only required for text files - not sequencefiles    *    * @param jc    *          Job Configuration    * @param isCompressed    *          Whether the output file is compressed or not    * @return the required file extension (example: .gz)    */
specifier|public
specifier|static
name|String
name|getFileExtension
parameter_list|(
name|JobConf
name|jc
parameter_list|,
name|boolean
name|isCompressed
parameter_list|)
block|{
if|if
condition|(
operator|!
name|isCompressed
condition|)
block|{
return|return
literal|""
return|;
block|}
else|else
block|{
name|Class
argument_list|<
name|?
extends|extends
name|CompressionCodec
argument_list|>
name|codecClass
init|=
name|FileOutputFormat
operator|.
name|getOutputCompressorClass
argument_list|(
name|jc
argument_list|,
name|DefaultCodec
operator|.
name|class
argument_list|)
decl_stmt|;
name|CompressionCodec
name|codec
init|=
operator|(
name|CompressionCodec
operator|)
name|ReflectionUtils
operator|.
name|newInstance
argument_list|(
name|codecClass
argument_list|,
name|jc
argument_list|)
decl_stmt|;
return|return
name|codec
operator|.
name|getDefaultExtension
argument_list|()
return|;
block|}
block|}
comment|/**    * Create a sequencefile output stream based on job configuration.    *    * @param jc    *          Job configuration    * @param fs    *          File System to create file in    * @param file    *          Path to be created    * @param keyClass    *          Java Class for key    * @param valClass    *          Java Class for value    * @return output stream over the created sequencefile    */
specifier|public
specifier|static
name|SequenceFile
operator|.
name|Writer
name|createSequenceWriter
parameter_list|(
name|JobConf
name|jc
parameter_list|,
name|FileSystem
name|fs
parameter_list|,
name|Path
name|file
parameter_list|,
name|Class
argument_list|<
name|?
argument_list|>
name|keyClass
parameter_list|,
name|Class
argument_list|<
name|?
argument_list|>
name|valClass
parameter_list|)
throws|throws
name|IOException
block|{
name|boolean
name|isCompressed
init|=
name|FileOutputFormat
operator|.
name|getCompressOutput
argument_list|(
name|jc
argument_list|)
decl_stmt|;
return|return
name|createSequenceWriter
argument_list|(
name|jc
argument_list|,
name|fs
argument_list|,
name|file
argument_list|,
name|keyClass
argument_list|,
name|valClass
argument_list|,
name|isCompressed
argument_list|)
return|;
block|}
comment|/**    * Create a sequencefile output stream based on job configuration Uses user supplied compression    * flag (rather than obtaining it from the Job Configuration).    *    * @param jc    *          Job configuration    * @param fs    *          File System to create file in    * @param file    *          Path to be created    * @param keyClass    *          Java Class for key    * @param valClass    *          Java Class for value    * @return output stream over the created sequencefile    */
specifier|public
specifier|static
name|SequenceFile
operator|.
name|Writer
name|createSequenceWriter
parameter_list|(
name|JobConf
name|jc
parameter_list|,
name|FileSystem
name|fs
parameter_list|,
name|Path
name|file
parameter_list|,
name|Class
argument_list|<
name|?
argument_list|>
name|keyClass
parameter_list|,
name|Class
argument_list|<
name|?
argument_list|>
name|valClass
parameter_list|,
name|boolean
name|isCompressed
parameter_list|)
throws|throws
name|IOException
block|{
name|CompressionCodec
name|codec
init|=
literal|null
decl_stmt|;
name|CompressionType
name|compressionType
init|=
name|CompressionType
operator|.
name|NONE
decl_stmt|;
name|Class
name|codecClass
init|=
literal|null
decl_stmt|;
if|if
condition|(
name|isCompressed
condition|)
block|{
name|compressionType
operator|=
name|SequenceFileOutputFormat
operator|.
name|getOutputCompressionType
argument_list|(
name|jc
argument_list|)
expr_stmt|;
name|codecClass
operator|=
name|FileOutputFormat
operator|.
name|getOutputCompressorClass
argument_list|(
name|jc
argument_list|,
name|DefaultCodec
operator|.
name|class
argument_list|)
expr_stmt|;
name|codec
operator|=
operator|(
name|CompressionCodec
operator|)
name|ReflectionUtils
operator|.
name|newInstance
argument_list|(
name|codecClass
argument_list|,
name|jc
argument_list|)
expr_stmt|;
block|}
return|return
operator|(
name|SequenceFile
operator|.
name|createWriter
argument_list|(
name|fs
argument_list|,
name|jc
argument_list|,
name|file
argument_list|,
name|keyClass
argument_list|,
name|valClass
argument_list|,
name|compressionType
argument_list|,
name|codec
argument_list|)
operator|)
return|;
block|}
comment|/**    * Create a RCFile output stream based on job configuration Uses user supplied compression flag    * (rather than obtaining it from the Job Configuration).    *    * @param jc    *          Job configuration    * @param fs    *          File System to create file in    * @param file    *          Path to be created    * @return output stream over the created rcfile    */
specifier|public
specifier|static
name|RCFile
operator|.
name|Writer
name|createRCFileWriter
parameter_list|(
name|JobConf
name|jc
parameter_list|,
name|FileSystem
name|fs
parameter_list|,
name|Path
name|file
parameter_list|,
name|boolean
name|isCompressed
parameter_list|)
throws|throws
name|IOException
block|{
name|CompressionCodec
name|codec
init|=
literal|null
decl_stmt|;
name|Class
argument_list|<
name|?
argument_list|>
name|codecClass
init|=
literal|null
decl_stmt|;
if|if
condition|(
name|isCompressed
condition|)
block|{
name|codecClass
operator|=
name|FileOutputFormat
operator|.
name|getOutputCompressorClass
argument_list|(
name|jc
argument_list|,
name|DefaultCodec
operator|.
name|class
argument_list|)
expr_stmt|;
name|codec
operator|=
operator|(
name|CompressionCodec
operator|)
name|ReflectionUtils
operator|.
name|newInstance
argument_list|(
name|codecClass
argument_list|,
name|jc
argument_list|)
expr_stmt|;
block|}
return|return
operator|new
name|RCFile
operator|.
name|Writer
argument_list|(
name|fs
argument_list|,
name|jc
argument_list|,
name|file
argument_list|,
literal|null
argument_list|,
name|codec
argument_list|)
return|;
block|}
comment|/**    * Shamelessly cloned from GenericOptionsParser.    */
specifier|public
specifier|static
name|String
name|realFile
parameter_list|(
name|String
name|newFile
parameter_list|,
name|Configuration
name|conf
parameter_list|)
throws|throws
name|IOException
block|{
name|Path
name|path
init|=
operator|new
name|Path
argument_list|(
name|newFile
argument_list|)
decl_stmt|;
name|URI
name|pathURI
init|=
name|path
operator|.
name|toUri
argument_list|()
decl_stmt|;
name|FileSystem
name|fs
decl_stmt|;
if|if
condition|(
name|pathURI
operator|.
name|getScheme
argument_list|()
operator|==
literal|null
condition|)
block|{
name|fs
operator|=
name|FileSystem
operator|.
name|getLocal
argument_list|(
name|conf
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|fs
operator|=
name|path
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
operator|!
name|fs
operator|.
name|exists
argument_list|(
name|path
argument_list|)
condition|)
block|{
return|return
literal|null
return|;
block|}
try|try
block|{
name|fs
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{     }
name|String
name|file
init|=
name|path
operator|.
name|makeQualified
argument_list|(
name|fs
argument_list|)
operator|.
name|toString
argument_list|()
decl_stmt|;
comment|// For compatibility with hadoop 0.17, change file:/a/b/c to file:///a/b/c
if|if
condition|(
name|StringUtils
operator|.
name|startsWith
argument_list|(
name|file
argument_list|,
literal|"file:/"
argument_list|)
operator|&&
operator|!
name|StringUtils
operator|.
name|startsWith
argument_list|(
name|file
argument_list|,
literal|"file:///"
argument_list|)
condition|)
block|{
name|file
operator|=
literal|"file:///"
operator|+
name|file
operator|.
name|substring
argument_list|(
literal|"file:/"
operator|.
name|length
argument_list|()
argument_list|)
expr_stmt|;
block|}
return|return
name|file
return|;
block|}
specifier|public
specifier|static
name|List
argument_list|<
name|String
argument_list|>
name|mergeUniqElems
parameter_list|(
name|List
argument_list|<
name|String
argument_list|>
name|src
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|dest
parameter_list|)
block|{
if|if
condition|(
name|dest
operator|==
literal|null
condition|)
block|{
return|return
name|src
return|;
block|}
if|if
condition|(
name|src
operator|==
literal|null
condition|)
block|{
return|return
name|dest
return|;
block|}
name|int
name|pos
init|=
literal|0
decl_stmt|;
while|while
condition|(
name|pos
operator|<
name|dest
operator|.
name|size
argument_list|()
condition|)
block|{
if|if
condition|(
operator|!
name|src
operator|.
name|contains
argument_list|(
name|dest
operator|.
name|get
argument_list|(
name|pos
argument_list|)
argument_list|)
condition|)
block|{
name|src
operator|.
name|add
argument_list|(
name|dest
operator|.
name|get
argument_list|(
name|pos
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|pos
operator|++
expr_stmt|;
block|}
return|return
name|src
return|;
block|}
specifier|private
specifier|static
specifier|final
name|String
name|tmpPrefix
init|=
literal|"_tmp."
decl_stmt|;
specifier|public
specifier|static
name|Path
name|toTempPath
parameter_list|(
name|Path
name|orig
parameter_list|)
block|{
if|if
condition|(
name|orig
operator|.
name|getName
argument_list|()
operator|.
name|indexOf
argument_list|(
name|tmpPrefix
argument_list|)
operator|==
literal|0
condition|)
block|{
return|return
name|orig
return|;
block|}
return|return
operator|new
name|Path
argument_list|(
name|orig
operator|.
name|getParent
argument_list|()
argument_list|,
name|tmpPrefix
operator|+
name|orig
operator|.
name|getName
argument_list|()
argument_list|)
return|;
block|}
comment|/**    * Given a path, convert to a temporary path.    */
specifier|public
specifier|static
name|Path
name|toTempPath
parameter_list|(
name|String
name|orig
parameter_list|)
block|{
return|return
name|toTempPath
argument_list|(
operator|new
name|Path
argument_list|(
name|orig
argument_list|)
argument_list|)
return|;
block|}
comment|/**    * Detect if the supplied file is a temporary path.    */
specifier|public
specifier|static
name|boolean
name|isTempPath
parameter_list|(
name|FileStatus
name|file
parameter_list|)
block|{
name|String
name|name
init|=
name|file
operator|.
name|getPath
argument_list|()
operator|.
name|getName
argument_list|()
decl_stmt|;
comment|// in addition to detecting hive temporary files, we also check hadoop
comment|// temporary folders that used to show up in older releases
return|return
operator|(
name|name
operator|.
name|startsWith
argument_list|(
literal|"_task"
argument_list|)
operator|||
name|name
operator|.
name|startsWith
argument_list|(
name|tmpPrefix
argument_list|)
operator|)
return|;
block|}
comment|/**    * Rename src to dst, or in the case dst already exists, move files in src to dst. If there is an    * existing file with the same name, the new file's name will be appended with "_1", "_2", etc.    *    * @param fs    *          the FileSystem where src and dst are on.    * @param src    *          the src directory    * @param dst    *          the target directory    * @throws IOException    */
specifier|public
specifier|static
name|void
name|rename
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|Path
name|src
parameter_list|,
name|Path
name|dst
parameter_list|)
throws|throws
name|IOException
throws|,
name|HiveException
block|{
if|if
condition|(
operator|!
name|fs
operator|.
name|rename
argument_list|(
name|src
argument_list|,
name|dst
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Unable to move: "
operator|+
name|src
operator|+
literal|" to: "
operator|+
name|dst
argument_list|)
throw|;
block|}
block|}
comment|/**    * Rename src to dst, or in the case dst already exists, move files in src to dst. If there is an    * existing file with the same name, the new file's name will be appended with "_1", "_2", etc.    *    * @param fs    *          the FileSystem where src and dst are on.    * @param src    *          the src directory    * @param dst    *          the target directory    * @throws IOException    */
specifier|public
specifier|static
name|void
name|renameOrMoveFiles
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|Path
name|src
parameter_list|,
name|Path
name|dst
parameter_list|)
throws|throws
name|IOException
throws|,
name|HiveException
block|{
if|if
condition|(
operator|!
name|fs
operator|.
name|exists
argument_list|(
name|dst
argument_list|)
condition|)
block|{
if|if
condition|(
operator|!
name|fs
operator|.
name|rename
argument_list|(
name|src
argument_list|,
name|dst
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Unable to move: "
operator|+
name|src
operator|+
literal|" to: "
operator|+
name|dst
argument_list|)
throw|;
block|}
block|}
else|else
block|{
comment|// move file by file
name|FileStatus
index|[]
name|files
init|=
name|fs
operator|.
name|listStatus
argument_list|(
name|src
argument_list|)
decl_stmt|;
for|for
control|(
name|FileStatus
name|file
range|:
name|files
control|)
block|{
name|Path
name|srcFilePath
init|=
name|file
operator|.
name|getPath
argument_list|()
decl_stmt|;
name|String
name|fileName
init|=
name|srcFilePath
operator|.
name|getName
argument_list|()
decl_stmt|;
name|Path
name|dstFilePath
init|=
operator|new
name|Path
argument_list|(
name|dst
argument_list|,
name|fileName
argument_list|)
decl_stmt|;
if|if
condition|(
name|fs
operator|.
name|exists
argument_list|(
name|dstFilePath
argument_list|)
condition|)
block|{
name|int
name|suffix
init|=
literal|0
decl_stmt|;
do|do
block|{
name|suffix
operator|++
expr_stmt|;
name|dstFilePath
operator|=
operator|new
name|Path
argument_list|(
name|dst
argument_list|,
name|fileName
operator|+
literal|"_"
operator|+
name|suffix
argument_list|)
expr_stmt|;
block|}
do|while
condition|(
name|fs
operator|.
name|exists
argument_list|(
name|dstFilePath
argument_list|)
condition|)
do|;
block|}
if|if
condition|(
operator|!
name|fs
operator|.
name|rename
argument_list|(
name|srcFilePath
argument_list|,
name|dstFilePath
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Unable to move: "
operator|+
name|src
operator|+
literal|" to: "
operator|+
name|dst
argument_list|)
throw|;
block|}
block|}
block|}
block|}
comment|/**    * The first group will contain the task id. The second group is the optional extension. The file    * name looks like: "0_0" or "0_0.gz". There may be a leading prefix (tmp_). Since getTaskId() can    * return an integer only - this should match a pure integer as well    */
specifier|private
specifier|static
name|Pattern
name|fileNameTaskIdRegex
init|=
name|Pattern
operator|.
name|compile
argument_list|(
literal|"^.*?([0-9]+)(_[0-9])?(\\..*)?$"
argument_list|)
decl_stmt|;
comment|/**    * Get the task id from the filename. It is assumed that the filename is derived from the output    * of getTaskId    *    * @param filename    *          filename to extract taskid from    */
specifier|public
specifier|static
name|String
name|getTaskIdFromFilename
parameter_list|(
name|String
name|filename
parameter_list|)
block|{
name|String
name|taskId
init|=
name|filename
decl_stmt|;
name|int
name|dirEnd
init|=
name|filename
operator|.
name|lastIndexOf
argument_list|(
name|Path
operator|.
name|SEPARATOR
argument_list|)
decl_stmt|;
if|if
condition|(
name|dirEnd
operator|!=
operator|-
literal|1
condition|)
block|{
name|taskId
operator|=
name|filename
operator|.
name|substring
argument_list|(
name|dirEnd
operator|+
literal|1
argument_list|)
expr_stmt|;
block|}
name|Matcher
name|m
init|=
name|fileNameTaskIdRegex
operator|.
name|matcher
argument_list|(
name|taskId
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|m
operator|.
name|matches
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Unable to get task id from file name: "
operator|+
name|filename
operator|+
literal|". Using last component"
operator|+
name|taskId
operator|+
literal|" as task id."
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|taskId
operator|=
name|m
operator|.
name|group
argument_list|(
literal|1
argument_list|)
expr_stmt|;
block|}
name|LOG
operator|.
name|debug
argument_list|(
literal|"TaskId for "
operator|+
name|filename
operator|+
literal|" = "
operator|+
name|taskId
argument_list|)
expr_stmt|;
return|return
name|taskId
return|;
block|}
comment|/**    * Replace the task id from the filename. It is assumed that the filename is derived from the    * output of getTaskId    *    * @param filename    *          filename to replace taskid "0_0" or "0_0.gz" by 33 to "33_0" or "33_0.gz"    */
specifier|public
specifier|static
name|String
name|replaceTaskIdFromFilename
parameter_list|(
name|String
name|filename
parameter_list|,
name|int
name|bucketNum
parameter_list|)
block|{
name|String
name|taskId
init|=
name|getTaskIdFromFilename
argument_list|(
name|filename
argument_list|)
decl_stmt|;
name|String
name|newTaskId
init|=
name|replaceTaskId
argument_list|(
name|taskId
argument_list|,
name|bucketNum
argument_list|)
decl_stmt|;
name|String
name|ret
init|=
name|replaceTaskIdFromFilename
argument_list|(
name|filename
argument_list|,
name|taskId
argument_list|,
name|newTaskId
argument_list|)
decl_stmt|;
return|return
operator|(
name|ret
operator|)
return|;
block|}
specifier|private
specifier|static
name|String
name|replaceTaskId
parameter_list|(
name|String
name|taskId
parameter_list|,
name|int
name|bucketNum
parameter_list|)
block|{
name|String
name|strBucketNum
init|=
name|String
operator|.
name|valueOf
argument_list|(
name|bucketNum
argument_list|)
decl_stmt|;
name|int
name|bucketNumLen
init|=
name|strBucketNum
operator|.
name|length
argument_list|()
decl_stmt|;
name|int
name|taskIdLen
init|=
name|taskId
operator|.
name|length
argument_list|()
decl_stmt|;
name|StringBuffer
name|s
init|=
operator|new
name|StringBuffer
argument_list|()
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|taskIdLen
operator|-
name|bucketNumLen
condition|;
name|i
operator|++
control|)
block|{
name|s
operator|.
name|append
argument_list|(
literal|"0"
argument_list|)
expr_stmt|;
block|}
return|return
name|s
operator|.
name|toString
argument_list|()
operator|+
name|strBucketNum
return|;
block|}
comment|/**    * Replace the oldTaskId appearing in the filename by the newTaskId. The string oldTaskId could    * appear multiple times, we should only replace the last one.    *    * @param filename    * @param oldTaskId    * @param newTaskId    * @return    */
specifier|private
specifier|static
name|String
name|replaceTaskIdFromFilename
parameter_list|(
name|String
name|filename
parameter_list|,
name|String
name|oldTaskId
parameter_list|,
name|String
name|newTaskId
parameter_list|)
block|{
name|String
index|[]
name|spl
init|=
name|filename
operator|.
name|split
argument_list|(
name|oldTaskId
argument_list|)
decl_stmt|;
if|if
condition|(
operator|(
name|spl
operator|.
name|length
operator|==
literal|0
operator|)
operator|||
operator|(
name|spl
operator|.
name|length
operator|==
literal|1
operator|)
condition|)
block|{
return|return
name|filename
operator|.
name|replaceAll
argument_list|(
name|oldTaskId
argument_list|,
name|newTaskId
argument_list|)
return|;
block|}
name|StringBuffer
name|snew
init|=
operator|new
name|StringBuffer
argument_list|()
decl_stmt|;
for|for
control|(
name|int
name|idx
init|=
literal|0
init|;
name|idx
operator|<
name|spl
operator|.
name|length
operator|-
literal|1
condition|;
name|idx
operator|++
control|)
block|{
if|if
condition|(
name|idx
operator|>
literal|0
condition|)
block|{
name|snew
operator|.
name|append
argument_list|(
name|oldTaskId
argument_list|)
expr_stmt|;
block|}
name|snew
operator|.
name|append
argument_list|(
name|spl
index|[
name|idx
index|]
argument_list|)
expr_stmt|;
block|}
name|snew
operator|.
name|append
argument_list|(
name|newTaskId
argument_list|)
expr_stmt|;
name|snew
operator|.
name|append
argument_list|(
name|spl
index|[
name|spl
operator|.
name|length
operator|-
literal|1
index|]
argument_list|)
expr_stmt|;
return|return
name|snew
operator|.
name|toString
argument_list|()
return|;
block|}
comment|/**    * Get all file status from a root path and recursively go deep into certain levels.    *    * @param path    *          the root path    * @param level    *          the depth of directory should explore    * @param fs    *          the file system    * @return array of FileStatus    * @throws IOException    */
specifier|public
specifier|static
name|FileStatus
index|[]
name|getFileStatusRecurse
parameter_list|(
name|Path
name|path
parameter_list|,
name|int
name|level
parameter_list|,
name|FileSystem
name|fs
parameter_list|)
throws|throws
name|IOException
block|{
comment|// construct a path pattern (e.g., /*/*) to find all dynamically generated paths
name|StringBuilder
name|sb
init|=
operator|new
name|StringBuilder
argument_list|(
name|path
operator|.
name|toUri
argument_list|()
operator|.
name|getPath
argument_list|()
argument_list|)
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|level
condition|;
operator|++
name|i
control|)
block|{
name|sb
operator|.
name|append
argument_list|(
name|Path
operator|.
name|SEPARATOR
argument_list|)
operator|.
name|append
argument_list|(
literal|"*"
argument_list|)
expr_stmt|;
block|}
name|Path
name|pathPattern
init|=
operator|new
name|Path
argument_list|(
name|path
argument_list|,
name|sb
operator|.
name|toString
argument_list|()
argument_list|)
decl_stmt|;
return|return
name|fs
operator|.
name|globStatus
argument_list|(
name|pathPattern
argument_list|)
return|;
block|}
specifier|public
specifier|static
name|void
name|mvFileToFinalPath
parameter_list|(
name|String
name|specPath
parameter_list|,
name|Configuration
name|hconf
parameter_list|,
name|boolean
name|success
parameter_list|,
name|Log
name|log
parameter_list|,
name|DynamicPartitionCtx
name|dpCtx
parameter_list|,
name|FileSinkDesc
name|conf
parameter_list|)
throws|throws
name|IOException
throws|,
name|HiveException
block|{
name|FileSystem
name|fs
init|=
operator|(
operator|new
name|Path
argument_list|(
name|specPath
argument_list|)
operator|)
operator|.
name|getFileSystem
argument_list|(
name|hconf
argument_list|)
decl_stmt|;
name|Path
name|tmpPath
init|=
name|Utilities
operator|.
name|toTempPath
argument_list|(
name|specPath
argument_list|)
decl_stmt|;
name|Path
name|intermediatePath
init|=
operator|new
name|Path
argument_list|(
name|tmpPath
operator|.
name|getParent
argument_list|()
argument_list|,
name|tmpPath
operator|.
name|getName
argument_list|()
operator|+
literal|".intermediate"
argument_list|)
decl_stmt|;
name|Path
name|finalPath
init|=
operator|new
name|Path
argument_list|(
name|specPath
argument_list|)
decl_stmt|;
if|if
condition|(
name|success
condition|)
block|{
if|if
condition|(
name|fs
operator|.
name|exists
argument_list|(
name|tmpPath
argument_list|)
condition|)
block|{
comment|// Step1: rename tmp output folder to intermediate path. After this
comment|// point, updates from speculative tasks still writing to tmpPath
comment|// will not appear in finalPath.
name|log
operator|.
name|info
argument_list|(
literal|"Moving tmp dir: "
operator|+
name|tmpPath
operator|+
literal|" to: "
operator|+
name|intermediatePath
argument_list|)
expr_stmt|;
name|Utilities
operator|.
name|rename
argument_list|(
name|fs
argument_list|,
name|tmpPath
argument_list|,
name|intermediatePath
argument_list|)
expr_stmt|;
comment|// Step2: remove any tmp file or double-committed output files
name|ArrayList
argument_list|<
name|String
argument_list|>
name|emptyBuckets
init|=
name|Utilities
operator|.
name|removeTempOrDuplicateFiles
argument_list|(
name|fs
argument_list|,
name|intermediatePath
argument_list|,
name|dpCtx
argument_list|)
decl_stmt|;
comment|// create empty buckets if necessary
if|if
condition|(
name|emptyBuckets
operator|.
name|size
argument_list|()
operator|>
literal|0
condition|)
block|{
name|createEmptyBuckets
argument_list|(
name|hconf
argument_list|,
name|emptyBuckets
argument_list|,
name|conf
argument_list|)
expr_stmt|;
block|}
comment|// Step3: move to the file destination
name|log
operator|.
name|info
argument_list|(
literal|"Moving tmp dir: "
operator|+
name|intermediatePath
operator|+
literal|" to: "
operator|+
name|finalPath
argument_list|)
expr_stmt|;
name|Utilities
operator|.
name|renameOrMoveFiles
argument_list|(
name|fs
argument_list|,
name|intermediatePath
argument_list|,
name|finalPath
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
name|fs
operator|.
name|delete
argument_list|(
name|tmpPath
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * Check the existence of buckets according to bucket specification. Create empty buckets if    * needed.    *    * @param specPath    *          The final path where the dynamic partitions should be in.    * @param conf    *          FileSinkDesc.    * @param dpCtx    *          dynamic partition context.    * @throws HiveException    * @throws IOException    */
specifier|private
specifier|static
name|void
name|createEmptyBuckets
parameter_list|(
name|Configuration
name|hconf
parameter_list|,
name|ArrayList
argument_list|<
name|String
argument_list|>
name|paths
parameter_list|,
name|FileSinkDesc
name|conf
parameter_list|)
throws|throws
name|HiveException
throws|,
name|IOException
block|{
name|JobConf
name|jc
decl_stmt|;
if|if
condition|(
name|hconf
operator|instanceof
name|JobConf
condition|)
block|{
name|jc
operator|=
operator|new
name|JobConf
argument_list|(
name|hconf
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// test code path
name|jc
operator|=
operator|new
name|JobConf
argument_list|(
name|hconf
argument_list|,
name|ExecDriver
operator|.
name|class
argument_list|)
expr_stmt|;
block|}
name|HiveOutputFormat
argument_list|<
name|?
argument_list|,
name|?
argument_list|>
name|hiveOutputFormat
init|=
literal|null
decl_stmt|;
name|Class
argument_list|<
name|?
extends|extends
name|Writable
argument_list|>
name|outputClass
init|=
literal|null
decl_stmt|;
name|boolean
name|isCompressed
init|=
name|conf
operator|.
name|getCompressed
argument_list|()
decl_stmt|;
name|TableDesc
name|tableInfo
init|=
name|conf
operator|.
name|getTableInfo
argument_list|()
decl_stmt|;
try|try
block|{
name|Serializer
name|serializer
init|=
operator|(
name|Serializer
operator|)
name|tableInfo
operator|.
name|getDeserializerClass
argument_list|()
operator|.
name|newInstance
argument_list|()
decl_stmt|;
name|serializer
operator|.
name|initialize
argument_list|(
literal|null
argument_list|,
name|tableInfo
operator|.
name|getProperties
argument_list|()
argument_list|)
expr_stmt|;
name|outputClass
operator|=
name|serializer
operator|.
name|getSerializedClass
argument_list|()
expr_stmt|;
name|hiveOutputFormat
operator|=
name|conf
operator|.
name|getTableInfo
argument_list|()
operator|.
name|getOutputFileFormatClass
argument_list|()
operator|.
name|newInstance
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|SerDeException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
catch|catch
parameter_list|(
name|InstantiationException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
catch|catch
parameter_list|(
name|IllegalAccessException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
for|for
control|(
name|String
name|p
range|:
name|paths
control|)
block|{
name|Path
name|path
init|=
operator|new
name|Path
argument_list|(
name|p
argument_list|)
decl_stmt|;
name|RecordWriter
name|writer
init|=
name|HiveFileFormatUtils
operator|.
name|getRecordWriter
argument_list|(
name|jc
argument_list|,
name|hiveOutputFormat
argument_list|,
name|outputClass
argument_list|,
name|isCompressed
argument_list|,
name|tableInfo
operator|.
name|getProperties
argument_list|()
argument_list|,
name|path
argument_list|)
decl_stmt|;
name|writer
operator|.
name|close
argument_list|(
literal|false
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"created empty bucket for enforcing bucketing at "
operator|+
name|path
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * Remove all temporary files and duplicate (double-committed) files from a given directory.    *    * @return a list of path names corresponding to should-be-created empty buckets.    */
specifier|public
specifier|static
name|void
name|removeTempOrDuplicateFiles
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|Path
name|path
parameter_list|)
throws|throws
name|IOException
block|{
name|removeTempOrDuplicateFiles
argument_list|(
name|fs
argument_list|,
name|path
argument_list|,
literal|null
argument_list|)
expr_stmt|;
block|}
comment|/**    * Remove all temporary files and duplicate (double-committed) files from a given directory.    *    * @return a list of path names corresponding to should-be-created empty buckets.    */
specifier|public
specifier|static
name|ArrayList
argument_list|<
name|String
argument_list|>
name|removeTempOrDuplicateFiles
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|Path
name|path
parameter_list|,
name|DynamicPartitionCtx
name|dpCtx
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|path
operator|==
literal|null
condition|)
block|{
return|return
literal|null
return|;
block|}
name|ArrayList
argument_list|<
name|String
argument_list|>
name|result
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
if|if
condition|(
name|dpCtx
operator|!=
literal|null
condition|)
block|{
name|FileStatus
name|parts
index|[]
init|=
name|getFileStatusRecurse
argument_list|(
name|path
argument_list|,
name|dpCtx
operator|.
name|getNumDPCols
argument_list|()
argument_list|,
name|fs
argument_list|)
decl_stmt|;
name|HashMap
argument_list|<
name|String
argument_list|,
name|FileStatus
argument_list|>
name|taskIDToFile
init|=
literal|null
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|parts
operator|.
name|length
condition|;
operator|++
name|i
control|)
block|{
assert|assert
name|parts
index|[
name|i
index|]
operator|.
name|isDir
argument_list|()
operator|:
literal|"dynamic partition "
operator|+
name|parts
index|[
name|i
index|]
operator|.
name|getPath
argument_list|()
operator|+
literal|" is not a direcgtory"
assert|;
name|FileStatus
index|[]
name|items
init|=
name|fs
operator|.
name|listStatus
argument_list|(
name|parts
index|[
name|i
index|]
operator|.
name|getPath
argument_list|()
argument_list|)
decl_stmt|;
comment|// remove empty directory since DP insert should not generate empty partitions.
comment|// empty directories could be generated by crashed Task/ScriptOperator
if|if
condition|(
name|items
operator|.
name|length
operator|==
literal|0
condition|)
block|{
if|if
condition|(
operator|!
name|fs
operator|.
name|delete
argument_list|(
name|parts
index|[
name|i
index|]
operator|.
name|getPath
argument_list|()
argument_list|,
literal|true
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Cannot delete empty directory "
operator|+
name|parts
index|[
name|i
index|]
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Cannot delete empty directory "
operator|+
name|parts
index|[
name|i
index|]
operator|.
name|getPath
argument_list|()
argument_list|)
throw|;
block|}
block|}
name|taskIDToFile
operator|=
name|removeTempOrDuplicateFiles
argument_list|(
name|items
argument_list|,
name|fs
argument_list|)
expr_stmt|;
comment|// if the table is bucketed and enforce bucketing, we should check and generate all buckets
if|if
condition|(
name|dpCtx
operator|.
name|getNumBuckets
argument_list|()
operator|>
literal|0
operator|&&
name|taskIDToFile
operator|!=
literal|null
condition|)
block|{
comment|// refresh the file list
name|items
operator|=
name|fs
operator|.
name|listStatus
argument_list|(
name|parts
index|[
name|i
index|]
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
comment|// get the missing buckets and generate empty buckets
name|String
name|taskID1
init|=
name|taskIDToFile
operator|.
name|keySet
argument_list|()
operator|.
name|iterator
argument_list|()
operator|.
name|next
argument_list|()
decl_stmt|;
name|Path
name|bucketPath
init|=
name|taskIDToFile
operator|.
name|values
argument_list|()
operator|.
name|iterator
argument_list|()
operator|.
name|next
argument_list|()
operator|.
name|getPath
argument_list|()
decl_stmt|;
for|for
control|(
name|int
name|j
init|=
literal|0
init|;
name|j
operator|<
name|dpCtx
operator|.
name|getNumBuckets
argument_list|()
condition|;
operator|++
name|j
control|)
block|{
name|String
name|taskID2
init|=
name|replaceTaskId
argument_list|(
name|taskID1
argument_list|,
name|j
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|taskIDToFile
operator|.
name|containsKey
argument_list|(
name|taskID2
argument_list|)
condition|)
block|{
comment|// create empty bucket, file name should be derived from taskID2
name|String
name|path2
init|=
name|replaceTaskIdFromFilename
argument_list|(
name|bucketPath
operator|.
name|toUri
argument_list|()
operator|.
name|getPath
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|,
name|j
argument_list|)
decl_stmt|;
name|result
operator|.
name|add
argument_list|(
name|path2
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
block|}
else|else
block|{
name|FileStatus
index|[]
name|items
init|=
name|fs
operator|.
name|listStatus
argument_list|(
name|path
argument_list|)
decl_stmt|;
name|removeTempOrDuplicateFiles
argument_list|(
name|items
argument_list|,
name|fs
argument_list|)
expr_stmt|;
block|}
return|return
name|result
return|;
block|}
specifier|public
specifier|static
name|HashMap
argument_list|<
name|String
argument_list|,
name|FileStatus
argument_list|>
name|removeTempOrDuplicateFiles
parameter_list|(
name|FileStatus
index|[]
name|items
parameter_list|,
name|FileSystem
name|fs
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|items
operator|==
literal|null
operator|||
name|fs
operator|==
literal|null
condition|)
block|{
return|return
literal|null
return|;
block|}
name|HashMap
argument_list|<
name|String
argument_list|,
name|FileStatus
argument_list|>
name|taskIdToFile
init|=
operator|new
name|HashMap
argument_list|<
name|String
argument_list|,
name|FileStatus
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|FileStatus
name|one
range|:
name|items
control|)
block|{
if|if
condition|(
name|isTempPath
argument_list|(
name|one
argument_list|)
condition|)
block|{
if|if
condition|(
operator|!
name|fs
operator|.
name|delete
argument_list|(
name|one
operator|.
name|getPath
argument_list|()
argument_list|,
literal|true
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Unable to delete tmp file: "
operator|+
name|one
operator|.
name|getPath
argument_list|()
argument_list|)
throw|;
block|}
block|}
else|else
block|{
name|String
name|taskId
init|=
name|getTaskIdFromFilename
argument_list|(
name|one
operator|.
name|getPath
argument_list|()
operator|.
name|getName
argument_list|()
argument_list|)
decl_stmt|;
name|FileStatus
name|otherFile
init|=
name|taskIdToFile
operator|.
name|get
argument_list|(
name|taskId
argument_list|)
decl_stmt|;
if|if
condition|(
name|otherFile
operator|==
literal|null
condition|)
block|{
name|taskIdToFile
operator|.
name|put
argument_list|(
name|taskId
argument_list|,
name|one
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// Compare the file sizes of all the attempt files for the same task, the largest win
comment|// any attempt files could contain partial results (due to task failures or
comment|// speculative runs), but the largest should be the correct one since the result
comment|// of a successful run should never be smaller than a failed/speculative run.
name|FileStatus
name|toDelete
init|=
literal|null
decl_stmt|;
if|if
condition|(
name|otherFile
operator|.
name|getLen
argument_list|()
operator|>=
name|one
operator|.
name|getLen
argument_list|()
condition|)
block|{
name|toDelete
operator|=
name|one
expr_stmt|;
block|}
else|else
block|{
name|toDelete
operator|=
name|otherFile
expr_stmt|;
name|taskIdToFile
operator|.
name|put
argument_list|(
name|taskId
argument_list|,
name|one
argument_list|)
expr_stmt|;
block|}
name|long
name|len1
init|=
name|toDelete
operator|.
name|getLen
argument_list|()
decl_stmt|;
name|long
name|len2
init|=
name|taskIdToFile
operator|.
name|get
argument_list|(
name|taskId
argument_list|)
operator|.
name|getLen
argument_list|()
decl_stmt|;
if|if
condition|(
operator|!
name|fs
operator|.
name|delete
argument_list|(
name|toDelete
operator|.
name|getPath
argument_list|()
argument_list|,
literal|true
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Unable to delete duplicate file: "
operator|+
name|toDelete
operator|.
name|getPath
argument_list|()
operator|+
literal|". Existing file: "
operator|+
name|taskIdToFile
operator|.
name|get
argument_list|(
name|taskId
argument_list|)
operator|.
name|getPath
argument_list|()
argument_list|)
throw|;
block|}
else|else
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Duplicate taskid file removed: "
operator|+
name|toDelete
operator|.
name|getPath
argument_list|()
operator|+
literal|" with length "
operator|+
name|len1
operator|+
literal|". Existing file: "
operator|+
name|taskIdToFile
operator|.
name|get
argument_list|(
name|taskId
argument_list|)
operator|.
name|getPath
argument_list|()
operator|+
literal|" with length "
operator|+
name|len2
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
return|return
name|taskIdToFile
return|;
block|}
specifier|public
specifier|static
name|String
name|getNameMessage
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
return|return
name|e
operator|.
name|getClass
argument_list|()
operator|.
name|getName
argument_list|()
operator|+
literal|"("
operator|+
name|e
operator|.
name|getMessage
argument_list|()
operator|+
literal|")"
return|;
block|}
comment|/**    * Add new elements to the classpath.    *    * @param newPaths    *          Array of classpath elements    */
specifier|public
specifier|static
name|ClassLoader
name|addToClassPath
parameter_list|(
name|ClassLoader
name|cloader
parameter_list|,
name|String
index|[]
name|newPaths
parameter_list|)
throws|throws
name|Exception
block|{
name|URLClassLoader
name|loader
init|=
operator|(
name|URLClassLoader
operator|)
name|cloader
decl_stmt|;
name|List
argument_list|<
name|URL
argument_list|>
name|curPath
init|=
name|Arrays
operator|.
name|asList
argument_list|(
name|loader
operator|.
name|getURLs
argument_list|()
argument_list|)
decl_stmt|;
name|ArrayList
argument_list|<
name|URL
argument_list|>
name|newPath
init|=
operator|new
name|ArrayList
argument_list|<
name|URL
argument_list|>
argument_list|()
decl_stmt|;
comment|// get a list with the current classpath components
for|for
control|(
name|URL
name|onePath
range|:
name|curPath
control|)
block|{
name|newPath
operator|.
name|add
argument_list|(
name|onePath
argument_list|)
expr_stmt|;
block|}
name|curPath
operator|=
name|newPath
expr_stmt|;
for|for
control|(
name|String
name|onestr
range|:
name|newPaths
control|)
block|{
comment|// special processing for hadoop-17. file:// needs to be removed
if|if
condition|(
name|StringUtils
operator|.
name|indexOf
argument_list|(
name|onestr
argument_list|,
literal|"file://"
argument_list|)
operator|==
literal|0
condition|)
block|{
name|onestr
operator|=
name|StringUtils
operator|.
name|substring
argument_list|(
name|onestr
argument_list|,
literal|7
argument_list|)
expr_stmt|;
block|}
name|URL
name|oneurl
init|=
operator|(
operator|new
name|File
argument_list|(
name|onestr
argument_list|)
operator|)
operator|.
name|toURL
argument_list|()
decl_stmt|;
if|if
condition|(
operator|!
name|curPath
operator|.
name|contains
argument_list|(
name|oneurl
argument_list|)
condition|)
block|{
name|curPath
operator|.
name|add
argument_list|(
name|oneurl
argument_list|)
expr_stmt|;
block|}
block|}
return|return
operator|new
name|URLClassLoader
argument_list|(
name|curPath
operator|.
name|toArray
argument_list|(
operator|new
name|URL
index|[
literal|0
index|]
argument_list|)
argument_list|,
name|loader
argument_list|)
return|;
block|}
comment|/**    * remove elements from the classpath.    *    * @param pathsToRemove    *          Array of classpath elements    */
specifier|public
specifier|static
name|void
name|removeFromClassPath
parameter_list|(
name|String
index|[]
name|pathsToRemove
parameter_list|)
throws|throws
name|Exception
block|{
name|Thread
name|curThread
init|=
name|Thread
operator|.
name|currentThread
argument_list|()
decl_stmt|;
name|URLClassLoader
name|loader
init|=
operator|(
name|URLClassLoader
operator|)
name|curThread
operator|.
name|getContextClassLoader
argument_list|()
decl_stmt|;
name|Set
argument_list|<
name|URL
argument_list|>
name|newPath
init|=
operator|new
name|HashSet
argument_list|<
name|URL
argument_list|>
argument_list|(
name|Arrays
operator|.
name|asList
argument_list|(
name|loader
operator|.
name|getURLs
argument_list|()
argument_list|)
argument_list|)
decl_stmt|;
for|for
control|(
name|String
name|onestr
range|:
name|pathsToRemove
control|)
block|{
comment|// special processing for hadoop-17. file:// needs to be removed
if|if
condition|(
name|StringUtils
operator|.
name|indexOf
argument_list|(
name|onestr
argument_list|,
literal|"file://"
argument_list|)
operator|==
literal|0
condition|)
block|{
name|onestr
operator|=
name|StringUtils
operator|.
name|substring
argument_list|(
name|onestr
argument_list|,
literal|7
argument_list|)
expr_stmt|;
block|}
name|URL
name|oneurl
init|=
operator|(
operator|new
name|File
argument_list|(
name|onestr
argument_list|)
operator|)
operator|.
name|toURL
argument_list|()
decl_stmt|;
name|newPath
operator|.
name|remove
argument_list|(
name|oneurl
argument_list|)
expr_stmt|;
block|}
name|loader
operator|=
operator|new
name|URLClassLoader
argument_list|(
name|newPath
operator|.
name|toArray
argument_list|(
operator|new
name|URL
index|[
literal|0
index|]
argument_list|)
argument_list|)
expr_stmt|;
name|curThread
operator|.
name|setContextClassLoader
argument_list|(
name|loader
argument_list|)
expr_stmt|;
block|}
specifier|public
specifier|static
name|String
name|formatBinaryString
parameter_list|(
name|byte
index|[]
name|array
parameter_list|,
name|int
name|start
parameter_list|,
name|int
name|length
parameter_list|)
block|{
name|StringBuilder
name|sb
init|=
operator|new
name|StringBuilder
argument_list|()
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
name|start
init|;
name|i
operator|<
name|start
operator|+
name|length
condition|;
name|i
operator|++
control|)
block|{
name|sb
operator|.
name|append
argument_list|(
literal|"x"
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
name|array
index|[
name|i
index|]
operator|<
literal|0
condition|?
name|array
index|[
name|i
index|]
operator|+
literal|256
else|:
name|array
index|[
name|i
index|]
operator|+
literal|0
argument_list|)
expr_stmt|;
block|}
return|return
name|sb
operator|.
name|toString
argument_list|()
return|;
block|}
specifier|public
specifier|static
name|List
argument_list|<
name|String
argument_list|>
name|getColumnNamesFromSortCols
parameter_list|(
name|List
argument_list|<
name|Order
argument_list|>
name|sortCols
parameter_list|)
block|{
name|List
argument_list|<
name|String
argument_list|>
name|names
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|Order
name|o
range|:
name|sortCols
control|)
block|{
name|names
operator|.
name|add
argument_list|(
name|o
operator|.
name|getCol
argument_list|()
argument_list|)
expr_stmt|;
block|}
return|return
name|names
return|;
block|}
specifier|public
specifier|static
name|List
argument_list|<
name|String
argument_list|>
name|getColumnNamesFromFieldSchema
parameter_list|(
name|List
argument_list|<
name|FieldSchema
argument_list|>
name|partCols
parameter_list|)
block|{
name|List
argument_list|<
name|String
argument_list|>
name|names
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|FieldSchema
name|o
range|:
name|partCols
control|)
block|{
name|names
operator|.
name|add
argument_list|(
name|o
operator|.
name|getName
argument_list|()
argument_list|)
expr_stmt|;
block|}
return|return
name|names
return|;
block|}
specifier|public
specifier|static
name|List
argument_list|<
name|String
argument_list|>
name|getColumnNames
parameter_list|(
name|Properties
name|props
parameter_list|)
block|{
name|List
argument_list|<
name|String
argument_list|>
name|names
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
name|String
name|colNames
init|=
name|props
operator|.
name|getProperty
argument_list|(
name|Constants
operator|.
name|LIST_COLUMNS
argument_list|)
decl_stmt|;
name|String
index|[]
name|cols
init|=
name|colNames
operator|.
name|trim
argument_list|()
operator|.
name|split
argument_list|(
literal|","
argument_list|)
decl_stmt|;
if|if
condition|(
name|cols
operator|!=
literal|null
condition|)
block|{
for|for
control|(
name|String
name|col
range|:
name|cols
control|)
block|{
if|if
condition|(
name|col
operator|!=
literal|null
operator|&&
operator|!
name|col
operator|.
name|trim
argument_list|()
operator|.
name|equals
argument_list|(
literal|""
argument_list|)
condition|)
block|{
name|names
operator|.
name|add
argument_list|(
name|col
argument_list|)
expr_stmt|;
block|}
block|}
block|}
return|return
name|names
return|;
block|}
specifier|public
specifier|static
name|List
argument_list|<
name|String
argument_list|>
name|getColumnTypes
parameter_list|(
name|Properties
name|props
parameter_list|)
block|{
name|List
argument_list|<
name|String
argument_list|>
name|names
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
name|String
name|colNames
init|=
name|props
operator|.
name|getProperty
argument_list|(
name|Constants
operator|.
name|LIST_COLUMN_TYPES
argument_list|)
decl_stmt|;
name|String
index|[]
name|cols
init|=
name|colNames
operator|.
name|trim
argument_list|()
operator|.
name|split
argument_list|(
literal|","
argument_list|)
decl_stmt|;
if|if
condition|(
name|cols
operator|!=
literal|null
condition|)
block|{
for|for
control|(
name|String
name|col
range|:
name|cols
control|)
block|{
if|if
condition|(
name|col
operator|!=
literal|null
operator|&&
operator|!
name|col
operator|.
name|trim
argument_list|()
operator|.
name|equals
argument_list|(
literal|""
argument_list|)
condition|)
block|{
name|names
operator|.
name|add
argument_list|(
name|col
argument_list|)
expr_stmt|;
block|}
block|}
block|}
return|return
name|names
return|;
block|}
specifier|public
specifier|static
name|void
name|validateColumnNames
parameter_list|(
name|List
argument_list|<
name|String
argument_list|>
name|colNames
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|checkCols
parameter_list|)
throws|throws
name|SemanticException
block|{
name|Iterator
argument_list|<
name|String
argument_list|>
name|checkColsIter
init|=
name|checkCols
operator|.
name|iterator
argument_list|()
decl_stmt|;
while|while
condition|(
name|checkColsIter
operator|.
name|hasNext
argument_list|()
condition|)
block|{
name|String
name|toCheck
init|=
name|checkColsIter
operator|.
name|next
argument_list|()
decl_stmt|;
name|boolean
name|found
init|=
literal|false
decl_stmt|;
name|Iterator
argument_list|<
name|String
argument_list|>
name|colNamesIter
init|=
name|colNames
operator|.
name|iterator
argument_list|()
decl_stmt|;
while|while
condition|(
name|colNamesIter
operator|.
name|hasNext
argument_list|()
condition|)
block|{
name|String
name|colName
init|=
name|colNamesIter
operator|.
name|next
argument_list|()
decl_stmt|;
if|if
condition|(
name|toCheck
operator|.
name|equalsIgnoreCase
argument_list|(
name|colName
argument_list|)
condition|)
block|{
name|found
operator|=
literal|true
expr_stmt|;
break|break;
block|}
block|}
if|if
condition|(
operator|!
name|found
condition|)
block|{
throw|throw
operator|new
name|SemanticException
argument_list|(
name|ErrorMsg
operator|.
name|INVALID_COLUMN
operator|.
name|getMsg
argument_list|()
argument_list|)
throw|;
block|}
block|}
block|}
comment|/**    * Gets the default notification interval to send progress updates to the tracker. Useful for    * operators that may not output data for a while.    *    * @param hconf    * @return the interval in milliseconds    */
specifier|public
specifier|static
name|int
name|getDefaultNotificationInterval
parameter_list|(
name|Configuration
name|hconf
parameter_list|)
block|{
name|int
name|notificationInterval
decl_stmt|;
name|Integer
name|expInterval
init|=
name|Integer
operator|.
name|decode
argument_list|(
name|hconf
operator|.
name|get
argument_list|(
literal|"mapred.tasktracker.expiry.interval"
argument_list|)
argument_list|)
decl_stmt|;
if|if
condition|(
name|expInterval
operator|!=
literal|null
condition|)
block|{
name|notificationInterval
operator|=
name|expInterval
operator|.
name|intValue
argument_list|()
operator|/
literal|2
expr_stmt|;
block|}
else|else
block|{
comment|// 5 minutes
name|notificationInterval
operator|=
literal|5
operator|*
literal|60
operator|*
literal|1000
expr_stmt|;
block|}
return|return
name|notificationInterval
return|;
block|}
comment|/**    * Copies the storage handler properties configured for a table descriptor to a runtime job    * configuration.    *    * @param tbl    *          table descriptor from which to read    *    * @param job    *          configuration which receives configured properties    */
specifier|public
specifier|static
name|void
name|copyTableJobPropertiesToConf
parameter_list|(
name|TableDesc
name|tbl
parameter_list|,
name|JobConf
name|job
parameter_list|)
block|{
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|jobProperties
init|=
name|tbl
operator|.
name|getJobProperties
argument_list|()
decl_stmt|;
if|if
condition|(
name|jobProperties
operator|==
literal|null
condition|)
block|{
return|return;
block|}
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|entry
range|:
name|jobProperties
operator|.
name|entrySet
argument_list|()
control|)
block|{
name|job
operator|.
name|set
argument_list|(
name|entry
operator|.
name|getKey
argument_list|()
argument_list|,
name|entry
operator|.
name|getValue
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
specifier|public
specifier|static
name|Object
name|getInputSummaryLock
init|=
operator|new
name|Object
argument_list|()
decl_stmt|;
comment|/**    * Calculate the total size of input files.    *    * @param job    *          the hadoop job conf.    * @param work    *          map reduce job plan    * @param filter    *          filter to apply to the input paths before calculating size    * @return the summary of all the input paths.    * @throws IOException    */
specifier|public
specifier|static
name|ContentSummary
name|getInputSummary
parameter_list|(
name|Context
name|ctx
parameter_list|,
name|MapredWork
name|work
parameter_list|,
name|PathFilter
name|filter
parameter_list|)
throws|throws
name|IOException
block|{
name|long
index|[]
name|summary
init|=
block|{
literal|0
block|,
literal|0
block|,
literal|0
block|}
decl_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|pathNeedProcess
init|=
operator|new
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
comment|// Since multiple threads could call this method concurrently, locking
comment|// this method will avoid number of threads out of control.
synchronized|synchronized
init|(
name|getInputSummaryLock
init|)
block|{
comment|// For each input path, calculate the total size.
for|for
control|(
name|String
name|path
range|:
name|work
operator|.
name|getPathToAliases
argument_list|()
operator|.
name|keySet
argument_list|()
control|)
block|{
name|Path
name|p
init|=
operator|new
name|Path
argument_list|(
name|path
argument_list|)
decl_stmt|;
if|if
condition|(
name|filter
operator|!=
literal|null
operator|&&
operator|!
name|filter
operator|.
name|accept
argument_list|(
name|p
argument_list|)
condition|)
block|{
continue|continue;
block|}
name|ContentSummary
name|cs
init|=
name|ctx
operator|.
name|getCS
argument_list|(
name|path
argument_list|)
decl_stmt|;
if|if
condition|(
name|cs
operator|==
literal|null
condition|)
block|{
if|if
condition|(
name|path
operator|==
literal|null
condition|)
block|{
continue|continue;
block|}
name|pathNeedProcess
operator|.
name|add
argument_list|(
name|path
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|summary
index|[
literal|0
index|]
operator|+=
name|cs
operator|.
name|getLength
argument_list|()
expr_stmt|;
name|summary
index|[
literal|1
index|]
operator|+=
name|cs
operator|.
name|getFileCount
argument_list|()
expr_stmt|;
name|summary
index|[
literal|2
index|]
operator|+=
name|cs
operator|.
name|getDirectoryCount
argument_list|()
expr_stmt|;
block|}
block|}
comment|// Process the case when name node call is needed
specifier|final
name|Map
argument_list|<
name|String
argument_list|,
name|ContentSummary
argument_list|>
name|resultMap
init|=
operator|new
name|ConcurrentHashMap
argument_list|<
name|String
argument_list|,
name|ContentSummary
argument_list|>
argument_list|()
decl_stmt|;
name|ArrayList
argument_list|<
name|Future
argument_list|<
name|?
argument_list|>
argument_list|>
name|results
init|=
operator|new
name|ArrayList
argument_list|<
name|Future
argument_list|<
name|?
argument_list|>
argument_list|>
argument_list|()
decl_stmt|;
specifier|final
name|ThreadPoolExecutor
name|executor
decl_stmt|;
name|int
name|maxThreads
init|=
name|ctx
operator|.
name|getConf
argument_list|()
operator|.
name|getInt
argument_list|(
literal|"mapred.dfsclient.parallelism.max"
argument_list|,
literal|0
argument_list|)
decl_stmt|;
if|if
condition|(
name|pathNeedProcess
operator|.
name|size
argument_list|()
operator|>
literal|1
operator|&&
name|maxThreads
operator|>
literal|1
condition|)
block|{
name|int
name|numExecutors
init|=
name|Math
operator|.
name|min
argument_list|(
name|pathNeedProcess
operator|.
name|size
argument_list|()
argument_list|,
name|maxThreads
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Using "
operator|+
name|numExecutors
operator|+
literal|" threads for getContentSummary"
argument_list|)
expr_stmt|;
name|executor
operator|=
operator|new
name|ThreadPoolExecutor
argument_list|(
name|numExecutors
argument_list|,
name|numExecutors
argument_list|,
literal|60
argument_list|,
name|TimeUnit
operator|.
name|SECONDS
argument_list|,
operator|new
name|LinkedBlockingQueue
argument_list|<
name|Runnable
argument_list|>
argument_list|()
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|executor
operator|=
literal|null
expr_stmt|;
block|}
name|HiveInterruptCallback
name|interrup
init|=
name|HiveInterruptUtils
operator|.
name|add
argument_list|(
operator|new
name|HiveInterruptCallback
argument_list|()
block|{
annotation|@
name|Override
specifier|public
name|void
name|interrupt
parameter_list|()
block|{
if|if
condition|(
name|executor
operator|!=
literal|null
condition|)
block|{
name|executor
operator|.
name|shutdownNow
argument_list|()
expr_stmt|;
block|}
block|}
block|}
argument_list|)
decl_stmt|;
try|try
block|{
name|Configuration
name|conf
init|=
name|ctx
operator|.
name|getConf
argument_list|()
decl_stmt|;
name|JobConf
name|jobConf
init|=
operator|new
name|JobConf
argument_list|(
name|conf
argument_list|)
decl_stmt|;
for|for
control|(
name|String
name|path
range|:
name|pathNeedProcess
control|)
block|{
specifier|final
name|Path
name|p
init|=
operator|new
name|Path
argument_list|(
name|path
argument_list|)
decl_stmt|;
specifier|final
name|String
name|pathStr
init|=
name|path
decl_stmt|;
comment|// All threads share the same Configuration and JobConf based on the
comment|// assumption that they are thread safe if only read operations are
comment|// executed. It is not stated in Hadoop's javadoc, the sourcce codes
comment|// clearly showed that they made efforts for it and we believe it is
comment|// thread safe. Will revisit this piece of codes if we find the assumption
comment|// is not correct.
specifier|final
name|Configuration
name|myConf
init|=
name|conf
decl_stmt|;
specifier|final
name|JobConf
name|myJobConf
init|=
name|jobConf
decl_stmt|;
specifier|final
name|PartitionDesc
name|partDesc
init|=
name|work
operator|.
name|getPathToPartitionInfo
argument_list|()
operator|.
name|get
argument_list|(
name|p
operator|.
name|toString
argument_list|()
argument_list|)
decl_stmt|;
name|Runnable
name|r
init|=
operator|new
name|Runnable
argument_list|()
block|{
specifier|public
name|void
name|run
parameter_list|()
block|{
try|try
block|{
name|ContentSummary
name|resultCs
decl_stmt|;
name|Class
argument_list|<
name|?
extends|extends
name|InputFormat
argument_list|>
name|inputFormatCls
init|=
name|partDesc
operator|.
name|getInputFileFormatClass
argument_list|()
decl_stmt|;
name|InputFormat
name|inputFormatObj
init|=
name|HiveInputFormat
operator|.
name|getInputFormatFromCache
argument_list|(
name|inputFormatCls
argument_list|,
name|myJobConf
argument_list|)
decl_stmt|;
if|if
condition|(
name|inputFormatObj
operator|instanceof
name|ContentSummaryInputFormat
condition|)
block|{
name|resultCs
operator|=
operator|(
operator|(
name|ContentSummaryInputFormat
operator|)
name|inputFormatObj
operator|)
operator|.
name|getContentSummary
argument_list|(
name|p
argument_list|,
name|myJobConf
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|FileSystem
name|fs
init|=
name|p
operator|.
name|getFileSystem
argument_list|(
name|myConf
argument_list|)
decl_stmt|;
name|resultCs
operator|=
name|fs
operator|.
name|getContentSummary
argument_list|(
name|p
argument_list|)
expr_stmt|;
block|}
name|resultMap
operator|.
name|put
argument_list|(
name|pathStr
argument_list|,
name|resultCs
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
comment|// We safely ignore this exception for summary data.
comment|// We don't update the cache to protect it from polluting other
comment|// usages. The worst case is that IOException will always be
comment|// retried for another getInputSummary(), which is fine as
comment|// IOException is not considered as a common case.
name|LOG
operator|.
name|info
argument_list|(
literal|"Cannot get size of "
operator|+
name|pathStr
operator|+
literal|". Safely ignored."
argument_list|)
expr_stmt|;
block|}
block|}
block|}
decl_stmt|;
if|if
condition|(
name|executor
operator|==
literal|null
condition|)
block|{
name|r
operator|.
name|run
argument_list|()
expr_stmt|;
block|}
else|else
block|{
name|Future
argument_list|<
name|?
argument_list|>
name|result
init|=
name|executor
operator|.
name|submit
argument_list|(
name|r
argument_list|)
decl_stmt|;
name|results
operator|.
name|add
argument_list|(
name|result
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
name|executor
operator|!=
literal|null
condition|)
block|{
for|for
control|(
name|Future
argument_list|<
name|?
argument_list|>
name|result
range|:
name|results
control|)
block|{
name|boolean
name|executorDone
init|=
literal|false
decl_stmt|;
do|do
block|{
try|try
block|{
name|result
operator|.
name|get
argument_list|()
expr_stmt|;
name|executorDone
operator|=
literal|true
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|InterruptedException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Interrupted when waiting threads: "
argument_list|,
name|e
argument_list|)
expr_stmt|;
name|Thread
operator|.
name|currentThread
argument_list|()
operator|.
name|interrupt
argument_list|()
expr_stmt|;
break|break;
block|}
catch|catch
parameter_list|(
name|ExecutionException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|IOException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
do|while
condition|(
operator|!
name|executorDone
condition|)
do|;
block|}
name|executor
operator|.
name|shutdown
argument_list|()
expr_stmt|;
block|}
name|HiveInterruptUtils
operator|.
name|checkInterrupted
argument_list|()
expr_stmt|;
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|String
argument_list|,
name|ContentSummary
argument_list|>
name|entry
range|:
name|resultMap
operator|.
name|entrySet
argument_list|()
control|)
block|{
name|ContentSummary
name|cs
init|=
name|entry
operator|.
name|getValue
argument_list|()
decl_stmt|;
name|summary
index|[
literal|0
index|]
operator|+=
name|cs
operator|.
name|getLength
argument_list|()
expr_stmt|;
name|summary
index|[
literal|1
index|]
operator|+=
name|cs
operator|.
name|getFileCount
argument_list|()
expr_stmt|;
name|summary
index|[
literal|2
index|]
operator|+=
name|cs
operator|.
name|getDirectoryCount
argument_list|()
expr_stmt|;
name|ctx
operator|.
name|addCS
argument_list|(
name|entry
operator|.
name|getKey
argument_list|()
argument_list|,
name|cs
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Cache Content Summary for "
operator|+
name|entry
operator|.
name|getKey
argument_list|()
operator|+
literal|" length: "
operator|+
name|cs
operator|.
name|getLength
argument_list|()
operator|+
literal|" file count: "
operator|+
name|cs
operator|.
name|getFileCount
argument_list|()
operator|+
literal|" directory count: "
operator|+
name|cs
operator|.
name|getDirectoryCount
argument_list|()
argument_list|)
expr_stmt|;
block|}
return|return
operator|new
name|ContentSummary
argument_list|(
name|summary
index|[
literal|0
index|]
argument_list|,
name|summary
index|[
literal|1
index|]
argument_list|,
name|summary
index|[
literal|2
index|]
argument_list|)
return|;
block|}
finally|finally
block|{
name|HiveInterruptUtils
operator|.
name|remove
argument_list|(
name|interrup
argument_list|)
expr_stmt|;
block|}
block|}
block|}
specifier|public
specifier|static
name|boolean
name|isEmptyPath
parameter_list|(
name|JobConf
name|job
parameter_list|,
name|String
name|dirPath
parameter_list|,
name|Context
name|ctx
parameter_list|)
throws|throws
name|Exception
block|{
name|ContentSummary
name|cs
init|=
name|ctx
operator|.
name|getCS
argument_list|(
name|dirPath
argument_list|)
decl_stmt|;
if|if
condition|(
name|cs
operator|!=
literal|null
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Content Summary "
operator|+
name|dirPath
operator|+
literal|"length: "
operator|+
name|cs
operator|.
name|getLength
argument_list|()
operator|+
literal|" num files: "
operator|+
name|cs
operator|.
name|getFileCount
argument_list|()
operator|+
literal|" num directories: "
operator|+
name|cs
operator|.
name|getDirectoryCount
argument_list|()
argument_list|)
expr_stmt|;
return|return
operator|(
name|cs
operator|.
name|getLength
argument_list|()
operator|==
literal|0
operator|&&
name|cs
operator|.
name|getFileCount
argument_list|()
operator|==
literal|0
operator|&&
name|cs
operator|.
name|getDirectoryCount
argument_list|()
operator|<=
literal|1
operator|)
return|;
block|}
else|else
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Content Summary not cached for "
operator|+
name|dirPath
argument_list|)
expr_stmt|;
block|}
name|Path
name|p
init|=
operator|new
name|Path
argument_list|(
name|dirPath
argument_list|)
decl_stmt|;
return|return
name|isEmptyPath
argument_list|(
name|job
argument_list|,
name|p
argument_list|)
return|;
block|}
specifier|public
specifier|static
name|boolean
name|isEmptyPath
parameter_list|(
name|JobConf
name|job
parameter_list|,
name|Path
name|dirPath
parameter_list|)
throws|throws
name|Exception
block|{
name|FileSystem
name|inpFs
init|=
name|dirPath
operator|.
name|getFileSystem
argument_list|(
name|job
argument_list|)
decl_stmt|;
if|if
condition|(
name|inpFs
operator|.
name|exists
argument_list|(
name|dirPath
argument_list|)
condition|)
block|{
name|FileStatus
index|[]
name|fStats
init|=
name|inpFs
operator|.
name|listStatus
argument_list|(
name|dirPath
argument_list|)
decl_stmt|;
if|if
condition|(
name|fStats
operator|.
name|length
operator|>
literal|0
condition|)
block|{
return|return
literal|false
return|;
block|}
block|}
return|return
literal|true
return|;
block|}
specifier|public
specifier|static
name|List
argument_list|<
name|ExecDriver
argument_list|>
name|getMRTasks
parameter_list|(
name|List
argument_list|<
name|Task
argument_list|<
name|?
extends|extends
name|Serializable
argument_list|>
argument_list|>
name|tasks
parameter_list|)
block|{
name|List
argument_list|<
name|ExecDriver
argument_list|>
name|mrTasks
init|=
operator|new
name|ArrayList
argument_list|<
name|ExecDriver
argument_list|>
argument_list|()
decl_stmt|;
if|if
condition|(
name|tasks
operator|!=
literal|null
condition|)
block|{
name|getMRTasks
argument_list|(
name|tasks
argument_list|,
name|mrTasks
argument_list|)
expr_stmt|;
block|}
return|return
name|mrTasks
return|;
block|}
specifier|private
specifier|static
name|void
name|getMRTasks
parameter_list|(
name|List
argument_list|<
name|Task
argument_list|<
name|?
extends|extends
name|Serializable
argument_list|>
argument_list|>
name|tasks
parameter_list|,
name|List
argument_list|<
name|ExecDriver
argument_list|>
name|mrTasks
parameter_list|)
block|{
for|for
control|(
name|Task
argument_list|<
name|?
extends|extends
name|Serializable
argument_list|>
name|task
range|:
name|tasks
control|)
block|{
if|if
condition|(
name|task
operator|instanceof
name|ExecDriver
operator|&&
operator|!
name|mrTasks
operator|.
name|contains
argument_list|(
operator|(
name|ExecDriver
operator|)
name|task
argument_list|)
condition|)
block|{
name|mrTasks
operator|.
name|add
argument_list|(
operator|(
name|ExecDriver
operator|)
name|task
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|task
operator|.
name|getDependentTasks
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|getMRTasks
argument_list|(
name|task
operator|.
name|getDependentTasks
argument_list|()
argument_list|,
name|mrTasks
argument_list|)
expr_stmt|;
block|}
block|}
block|}
specifier|public
specifier|static
name|boolean
name|supportCombineFileInputFormat
parameter_list|()
block|{
return|return
name|ShimLoader
operator|.
name|getHadoopShims
argument_list|()
operator|.
name|getCombineFileInputFormat
argument_list|()
operator|!=
literal|null
return|;
block|}
comment|/**    * Construct a list of full partition spec from Dynamic Partition Context and the directory names    * corresponding to these dynamic partitions.    */
specifier|public
specifier|static
name|List
argument_list|<
name|LinkedHashMap
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
argument_list|>
name|getFullDPSpecs
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|DynamicPartitionCtx
name|dpCtx
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|Path
name|loadPath
init|=
operator|new
name|Path
argument_list|(
name|dpCtx
operator|.
name|getRootPath
argument_list|()
argument_list|)
decl_stmt|;
name|FileSystem
name|fs
init|=
name|loadPath
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|int
name|numDPCols
init|=
name|dpCtx
operator|.
name|getNumDPCols
argument_list|()
decl_stmt|;
name|FileStatus
index|[]
name|status
init|=
name|Utilities
operator|.
name|getFileStatusRecurse
argument_list|(
name|loadPath
argument_list|,
name|numDPCols
argument_list|,
name|fs
argument_list|)
decl_stmt|;
if|if
condition|(
name|status
operator|.
name|length
operator|==
literal|0
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"No partition is genereated by dynamic partitioning"
argument_list|)
expr_stmt|;
return|return
literal|null
return|;
block|}
comment|// partial partition specification
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|partSpec
init|=
name|dpCtx
operator|.
name|getPartSpec
argument_list|()
decl_stmt|;
comment|// list of full partition specification
name|List
argument_list|<
name|LinkedHashMap
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
argument_list|>
name|fullPartSpecs
init|=
operator|new
name|ArrayList
argument_list|<
name|LinkedHashMap
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
argument_list|>
argument_list|()
decl_stmt|;
comment|// for each dynamically created DP directory, construct a full partition spec
comment|// and load the partition based on that
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|status
operator|.
name|length
condition|;
operator|++
name|i
control|)
block|{
comment|// get the dynamically created directory
name|Path
name|partPath
init|=
name|status
index|[
name|i
index|]
operator|.
name|getPath
argument_list|()
decl_stmt|;
assert|assert
name|fs
operator|.
name|getFileStatus
argument_list|(
name|partPath
argument_list|)
operator|.
name|isDir
argument_list|()
operator|:
literal|"partitions "
operator|+
name|partPath
operator|+
literal|" is not a directory !"
assert|;
comment|// generate a full partition specification
name|LinkedHashMap
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|fullPartSpec
init|=
operator|new
name|LinkedHashMap
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
argument_list|(
name|partSpec
argument_list|)
decl_stmt|;
name|Warehouse
operator|.
name|makeSpecFromName
argument_list|(
name|fullPartSpec
argument_list|,
name|partPath
argument_list|)
expr_stmt|;
name|fullPartSpecs
operator|.
name|add
argument_list|(
name|fullPartSpec
argument_list|)
expr_stmt|;
block|}
return|return
name|fullPartSpecs
return|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|public
specifier|static
name|StatsPublisher
name|getStatsPublisher
parameter_list|(
name|JobConf
name|jc
parameter_list|)
block|{
name|String
name|statsImplementationClass
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|jc
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVESTATSDBCLASS
argument_list|)
decl_stmt|;
if|if
condition|(
name|StatsFactory
operator|.
name|setImplementation
argument_list|(
name|statsImplementationClass
argument_list|,
name|jc
argument_list|)
condition|)
block|{
return|return
name|StatsFactory
operator|.
name|getStatsPublisher
argument_list|()
return|;
block|}
else|else
block|{
return|return
literal|null
return|;
block|}
block|}
specifier|public
specifier|static
name|void
name|setColumnNameList
parameter_list|(
name|JobConf
name|jobConf
parameter_list|,
name|Operator
name|op
parameter_list|)
block|{
name|RowSchema
name|rowSchema
init|=
name|op
operator|.
name|getSchema
argument_list|()
decl_stmt|;
if|if
condition|(
name|rowSchema
operator|==
literal|null
condition|)
block|{
return|return;
block|}
name|StringBuilder
name|columnNames
init|=
operator|new
name|StringBuilder
argument_list|()
decl_stmt|;
for|for
control|(
name|ColumnInfo
name|colInfo
range|:
name|rowSchema
operator|.
name|getSignature
argument_list|()
control|)
block|{
if|if
condition|(
name|columnNames
operator|.
name|length
argument_list|()
operator|>
literal|0
condition|)
block|{
name|columnNames
operator|.
name|append
argument_list|(
literal|","
argument_list|)
expr_stmt|;
block|}
name|columnNames
operator|.
name|append
argument_list|(
name|colInfo
operator|.
name|getInternalName
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|String
name|columnNamesString
init|=
name|columnNames
operator|.
name|toString
argument_list|()
decl_stmt|;
name|jobConf
operator|.
name|set
argument_list|(
name|Constants
operator|.
name|LIST_COLUMNS
argument_list|,
name|columnNamesString
argument_list|)
expr_stmt|;
block|}
specifier|public
specifier|static
name|void
name|validatePartSpec
parameter_list|(
name|Table
name|tbl
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|partSpec
parameter_list|)
throws|throws
name|SemanticException
block|{
name|List
argument_list|<
name|FieldSchema
argument_list|>
name|parts
init|=
name|tbl
operator|.
name|getPartitionKeys
argument_list|()
decl_stmt|;
name|Set
argument_list|<
name|String
argument_list|>
name|partCols
init|=
operator|new
name|HashSet
argument_list|<
name|String
argument_list|>
argument_list|(
name|parts
operator|.
name|size
argument_list|()
argument_list|)
decl_stmt|;
for|for
control|(
name|FieldSchema
name|col
range|:
name|parts
control|)
block|{
name|partCols
operator|.
name|add
argument_list|(
name|col
operator|.
name|getName
argument_list|()
argument_list|)
expr_stmt|;
block|}
for|for
control|(
name|String
name|col
range|:
name|partSpec
operator|.
name|keySet
argument_list|()
control|)
block|{
if|if
condition|(
operator|!
name|partCols
operator|.
name|contains
argument_list|(
name|col
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|SemanticException
argument_list|(
name|ErrorMsg
operator|.
name|NONEXISTPARTCOL
operator|.
name|getMsg
argument_list|(
name|col
argument_list|)
argument_list|)
throw|;
block|}
block|}
block|}
specifier|public
specifier|static
name|String
name|suffix
init|=
literal|".hashtable"
decl_stmt|;
specifier|public
specifier|static
name|String
name|generatePath
parameter_list|(
name|String
name|baseURI
parameter_list|,
name|Byte
name|tag
parameter_list|,
name|String
name|bigBucketFileName
parameter_list|)
block|{
name|String
name|path
init|=
operator|new
name|String
argument_list|(
name|baseURI
operator|+
name|Path
operator|.
name|SEPARATOR
operator|+
literal|"MapJoin-"
operator|+
name|tag
operator|+
literal|"-"
operator|+
name|bigBucketFileName
operator|+
name|suffix
argument_list|)
decl_stmt|;
return|return
name|path
return|;
block|}
specifier|public
specifier|static
name|String
name|generateFileName
parameter_list|(
name|Byte
name|tag
parameter_list|,
name|String
name|bigBucketFileName
parameter_list|)
block|{
name|String
name|fileName
init|=
operator|new
name|String
argument_list|(
literal|"MapJoin-"
operator|+
name|tag
operator|+
literal|"-"
operator|+
name|bigBucketFileName
operator|+
name|suffix
argument_list|)
decl_stmt|;
return|return
name|fileName
return|;
block|}
specifier|public
specifier|static
name|String
name|generateTmpURI
parameter_list|(
name|String
name|baseURI
parameter_list|,
name|String
name|id
parameter_list|)
block|{
name|String
name|tmpFileURI
init|=
operator|new
name|String
argument_list|(
name|baseURI
operator|+
name|Path
operator|.
name|SEPARATOR
operator|+
literal|"HashTable-"
operator|+
name|id
argument_list|)
decl_stmt|;
return|return
name|tmpFileURI
return|;
block|}
specifier|public
specifier|static
name|String
name|generateTarURI
parameter_list|(
name|String
name|baseURI
parameter_list|,
name|String
name|filename
parameter_list|)
block|{
name|String
name|tmpFileURI
init|=
operator|new
name|String
argument_list|(
name|baseURI
operator|+
name|Path
operator|.
name|SEPARATOR
operator|+
name|filename
operator|+
literal|".tar.gz"
argument_list|)
decl_stmt|;
return|return
name|tmpFileURI
return|;
block|}
specifier|public
specifier|static
name|String
name|generateTarURI
parameter_list|(
name|Path
name|baseURI
parameter_list|,
name|String
name|filename
parameter_list|)
block|{
name|String
name|tmpFileURI
init|=
operator|new
name|String
argument_list|(
name|baseURI
operator|+
name|Path
operator|.
name|SEPARATOR
operator|+
name|filename
operator|+
literal|".tar.gz"
argument_list|)
decl_stmt|;
return|return
name|tmpFileURI
return|;
block|}
specifier|public
specifier|static
name|String
name|generateTarFileName
parameter_list|(
name|String
name|name
parameter_list|)
block|{
name|String
name|tmpFileURI
init|=
operator|new
name|String
argument_list|(
name|name
operator|+
literal|".tar.gz"
argument_list|)
decl_stmt|;
return|return
name|tmpFileURI
return|;
block|}
specifier|public
specifier|static
name|String
name|generatePath
parameter_list|(
name|Path
name|baseURI
parameter_list|,
name|String
name|filename
parameter_list|)
block|{
name|String
name|path
init|=
operator|new
name|String
argument_list|(
name|baseURI
operator|+
name|Path
operator|.
name|SEPARATOR
operator|+
name|filename
argument_list|)
decl_stmt|;
return|return
name|path
return|;
block|}
specifier|public
specifier|static
name|String
name|now
parameter_list|()
block|{
name|Calendar
name|cal
init|=
name|Calendar
operator|.
name|getInstance
argument_list|()
decl_stmt|;
name|SimpleDateFormat
name|sdf
init|=
operator|new
name|SimpleDateFormat
argument_list|(
literal|"yyyy-MM-dd hh:mm:ss"
argument_list|)
decl_stmt|;
return|return
name|sdf
operator|.
name|format
argument_list|(
name|cal
operator|.
name|getTime
argument_list|()
argument_list|)
return|;
block|}
specifier|public
specifier|static
name|double
name|showTime
parameter_list|(
name|long
name|time
parameter_list|)
block|{
name|double
name|result
init|=
operator|(
name|double
operator|)
name|time
operator|/
operator|(
name|double
operator|)
literal|1000
decl_stmt|;
return|return
name|result
return|;
block|}
comment|/**    * Determines whether a partition has been archived    *    * @param p    * @return    */
specifier|public
specifier|static
name|boolean
name|isArchived
parameter_list|(
name|Partition
name|p
parameter_list|)
block|{
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|params
init|=
name|p
operator|.
name|getParameters
argument_list|()
decl_stmt|;
if|if
condition|(
literal|"true"
operator|.
name|equalsIgnoreCase
argument_list|(
name|params
operator|.
name|get
argument_list|(
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Constants
operator|.
name|IS_ARCHIVED
argument_list|)
argument_list|)
condition|)
block|{
return|return
literal|true
return|;
block|}
else|else
block|{
return|return
literal|false
return|;
block|}
block|}
comment|/**    * Check if a function can be pushed down to JDO.    * Now only {=, AND, OR} are supported.    * @param func a generic function.    * @return true if this function can be pushed down to JDO filter.    */
specifier|private
specifier|static
name|boolean
name|supportedJDOFuncs
parameter_list|(
name|GenericUDF
name|func
parameter_list|)
block|{
return|return
name|func
operator|instanceof
name|GenericUDFOPEqual
operator|||
name|func
operator|instanceof
name|GenericUDFOPAnd
operator|||
name|func
operator|instanceof
name|GenericUDFOPOr
return|;
block|}
comment|/**    * Check if the partition pruning expression can be pushed down to JDO filtering.    * The partition expression contains only partition columns.    * The criteria that an expression can be pushed down are that:    *  1) the expression only contains function specified in supportedJDOFuncs().    *     Now only {=, AND, OR} can be pushed down.    *  2) the partition column type and the constant type have to be String. This is    *     restriction by the current JDO filtering implementation.    * @param tab The table that contains the partition columns.    * @param expr the partition pruning expression    * @return true if the partition pruning expression can be pushed down to JDO filtering.    */
specifier|public
specifier|static
name|boolean
name|checkJDOPushDown
parameter_list|(
name|Table
name|tab
parameter_list|,
name|ExprNodeDesc
name|expr
parameter_list|)
block|{
if|if
condition|(
name|expr
operator|instanceof
name|ExprNodeConstantDesc
condition|)
block|{
comment|// JDO filter now only support String typed literal -- see Filter.g and ExpressionTree.java
name|Object
name|value
init|=
operator|(
operator|(
name|ExprNodeConstantDesc
operator|)
name|expr
operator|)
operator|.
name|getValue
argument_list|()
decl_stmt|;
return|return
operator|(
name|value
operator|instanceof
name|String
operator|)
return|;
block|}
elseif|else
if|if
condition|(
name|expr
operator|instanceof
name|ExprNodeColumnDesc
condition|)
block|{
comment|// JDO filter now only support String typed literal -- see Filter.g and ExpressionTree.java
name|TypeInfo
name|type
init|=
name|expr
operator|.
name|getTypeInfo
argument_list|()
decl_stmt|;
if|if
condition|(
name|type
operator|.
name|getTypeName
argument_list|()
operator|.
name|equals
argument_list|(
name|Constants
operator|.
name|STRING_TYPE_NAME
argument_list|)
condition|)
block|{
name|String
name|colName
init|=
operator|(
operator|(
name|ExprNodeColumnDesc
operator|)
name|expr
operator|)
operator|.
name|getColumn
argument_list|()
decl_stmt|;
for|for
control|(
name|FieldSchema
name|fs
range|:
name|tab
operator|.
name|getPartCols
argument_list|()
control|)
block|{
if|if
condition|(
name|fs
operator|.
name|getName
argument_list|()
operator|.
name|equals
argument_list|(
name|colName
argument_list|)
condition|)
block|{
return|return
name|fs
operator|.
name|getType
argument_list|()
operator|.
name|equals
argument_list|(
name|Constants
operator|.
name|STRING_TYPE_NAME
argument_list|)
return|;
block|}
block|}
assert|assert
operator|(
literal|false
operator|)
assert|;
comment|// cannot find the partition column!
block|}
else|else
block|{
return|return
literal|false
return|;
block|}
block|}
elseif|else
if|if
condition|(
name|expr
operator|instanceof
name|ExprNodeGenericFuncDesc
condition|)
block|{
name|ExprNodeGenericFuncDesc
name|funcDesc
init|=
operator|(
name|ExprNodeGenericFuncDesc
operator|)
name|expr
decl_stmt|;
name|GenericUDF
name|func
init|=
name|funcDesc
operator|.
name|getGenericUDF
argument_list|()
decl_stmt|;
if|if
condition|(
operator|!
name|supportedJDOFuncs
argument_list|(
name|func
argument_list|)
condition|)
block|{
return|return
literal|false
return|;
block|}
name|List
argument_list|<
name|ExprNodeDesc
argument_list|>
name|children
init|=
name|funcDesc
operator|.
name|getChildExprs
argument_list|()
decl_stmt|;
for|for
control|(
name|ExprNodeDesc
name|child
range|:
name|children
control|)
block|{
if|if
condition|(
operator|!
name|checkJDOPushDown
argument_list|(
name|tab
argument_list|,
name|child
argument_list|)
condition|)
block|{
return|return
literal|false
return|;
block|}
block|}
return|return
literal|true
return|;
block|}
return|return
literal|false
return|;
block|}
specifier|private
specifier|static
name|ThreadLocal
argument_list|<
name|Map
argument_list|<
name|String
argument_list|,
name|Long
argument_list|>
argument_list|>
name|perfKeyMaps
init|=
operator|new
name|ThreadLocal
argument_list|<
name|Map
argument_list|<
name|String
argument_list|,
name|Long
argument_list|>
argument_list|>
argument_list|()
decl_stmt|;
comment|/**    * Call this function when you start to measure time spent by a piece of code.    * @param _log the logging object to be used.    * @param method method or ID that identifies this perf log element.    */
specifier|public
specifier|static
name|void
name|PerfLogBegin
parameter_list|(
name|Log
name|_log
parameter_list|,
name|String
name|method
parameter_list|)
block|{
name|long
name|startTime
init|=
name|System
operator|.
name|currentTimeMillis
argument_list|()
decl_stmt|;
name|_log
operator|.
name|info
argument_list|(
literal|"<PERFLOG method="
operator|+
name|method
operator|+
literal|">"
argument_list|)
expr_stmt|;
if|if
condition|(
name|perfKeyMaps
operator|.
name|get
argument_list|()
operator|==
literal|null
condition|)
block|{
name|perfKeyMaps
operator|.
name|set
argument_list|(
operator|new
name|HashMap
argument_list|<
name|String
argument_list|,
name|Long
argument_list|>
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|perfKeyMaps
operator|.
name|get
argument_list|()
operator|.
name|put
argument_list|(
name|method
argument_list|,
operator|new
name|Long
argument_list|(
name|startTime
argument_list|)
argument_list|)
expr_stmt|;
block|}
comment|/**    * Call this function in correspondence of PerfLogBegin to mark the end of the measurement.    * @param _log    * @param method    */
specifier|public
specifier|static
name|void
name|PerfLogEnd
parameter_list|(
name|Log
name|_log
parameter_list|,
name|String
name|method
parameter_list|)
block|{
name|Long
name|startTime
init|=
name|perfKeyMaps
operator|.
name|get
argument_list|()
operator|.
name|get
argument_list|(
name|method
argument_list|)
decl_stmt|;
name|long
name|endTime
init|=
name|System
operator|.
name|currentTimeMillis
argument_list|()
decl_stmt|;
name|StringBuilder
name|sb
init|=
operator|new
name|StringBuilder
argument_list|(
literal|"</PERFLOG method="
argument_list|)
operator|.
name|append
argument_list|(
name|method
argument_list|)
decl_stmt|;
if|if
condition|(
name|startTime
operator|!=
literal|null
condition|)
block|{
name|sb
operator|.
name|append
argument_list|(
literal|" start="
argument_list|)
operator|.
name|append
argument_list|(
name|startTime
argument_list|)
expr_stmt|;
block|}
name|sb
operator|.
name|append
argument_list|(
literal|" end="
argument_list|)
operator|.
name|append
argument_list|(
name|endTime
argument_list|)
expr_stmt|;
if|if
condition|(
name|startTime
operator|!=
literal|null
condition|)
block|{
name|sb
operator|.
name|append
argument_list|(
literal|" duration="
argument_list|)
operator|.
name|append
argument_list|(
name|endTime
operator|-
name|startTime
operator|.
name|longValue
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|sb
operator|.
name|append
argument_list|(
literal|">"
argument_list|)
expr_stmt|;
name|_log
operator|.
name|info
argument_list|(
name|sb
argument_list|)
expr_stmt|;
block|}
comment|/**    * The check here is kind of not clean. It first use a for loop to go through    * all input formats, and choose the ones that extend ReworkMapredInputFormat    * to a set. And finally go through the ReworkMapredInputFormat set, and call    * rework for each one.    *    * Technically all these can be avoided if all Hive's input formats can share    * a same interface. As in today's hive and Hadoop, it is not possible because    * a lot of Hive's input formats are in Hadoop's code. And most of Hadoop's    * input formats just extend InputFormat interface.    *    * @param task    * @param reworkMapredWork    * @param conf    * @throws SemanticException    */
specifier|public
specifier|static
name|void
name|reworkMapRedWork
parameter_list|(
name|Task
argument_list|<
name|?
extends|extends
name|Serializable
argument_list|>
name|task
parameter_list|,
name|boolean
name|reworkMapredWork
parameter_list|,
name|HiveConf
name|conf
parameter_list|)
throws|throws
name|SemanticException
block|{
if|if
condition|(
name|reworkMapredWork
operator|&&
operator|(
name|task
operator|instanceof
name|MapRedTask
operator|)
condition|)
block|{
try|try
block|{
name|MapredWork
name|mapredWork
init|=
operator|(
operator|(
name|MapRedTask
operator|)
name|task
operator|)
operator|.
name|getWork
argument_list|()
decl_stmt|;
name|Set
argument_list|<
name|Class
argument_list|<
name|?
extends|extends
name|InputFormat
argument_list|>
argument_list|>
name|reworkInputFormats
init|=
operator|new
name|HashSet
argument_list|<
name|Class
argument_list|<
name|?
extends|extends
name|InputFormat
argument_list|>
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|PartitionDesc
name|part
range|:
name|mapredWork
operator|.
name|getPathToPartitionInfo
argument_list|()
operator|.
name|values
argument_list|()
control|)
block|{
name|Class
argument_list|<
name|?
extends|extends
name|InputFormat
argument_list|>
name|inputFormatCls
init|=
name|part
operator|.
name|getInputFileFormatClass
argument_list|()
decl_stmt|;
if|if
condition|(
name|ReworkMapredInputFormat
operator|.
name|class
operator|.
name|isAssignableFrom
argument_list|(
name|inputFormatCls
argument_list|)
condition|)
block|{
name|reworkInputFormats
operator|.
name|add
argument_list|(
name|inputFormatCls
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
name|reworkInputFormats
operator|.
name|size
argument_list|()
operator|>
literal|0
condition|)
block|{
for|for
control|(
name|Class
argument_list|<
name|?
extends|extends
name|InputFormat
argument_list|>
name|inputFormatCls
range|:
name|reworkInputFormats
control|)
block|{
name|ReworkMapredInputFormat
name|inst
init|=
operator|(
name|ReworkMapredInputFormat
operator|)
name|ReflectionUtils
operator|.
name|newInstance
argument_list|(
name|inputFormatCls
argument_list|,
literal|null
argument_list|)
decl_stmt|;
name|inst
operator|.
name|rework
argument_list|(
name|conf
argument_list|,
name|mapredWork
argument_list|)
expr_stmt|;
block|}
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|SemanticException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
block|}
specifier|public
specifier|static
class|class
name|SQLCommand
parameter_list|<
name|T
parameter_list|>
block|{
specifier|public
name|T
name|run
parameter_list|(
name|PreparedStatement
name|stmt
parameter_list|)
throws|throws
name|SQLException
block|{
return|return
literal|null
return|;
block|}
block|}
comment|/**    * Retry SQL execution with random backoff (same as the one implemented in HDFS-767).    * This function only retries when the SQL query throws a SQLTransientException (which    * might be able to succeed with a simple retry). It doesn't retry when the exception    * is a SQLRecoverableException or SQLNonTransientException. For SQLRecoverableException    * the caller needs to reconnect to the database and restart the whole transaction.    *    * @param query the prepared statement of SQL.    * @param type either SQLCommandType.QUERY or SQLCommandType.UPDATE    * @param baseWindow  The base time window (in milliseconds) before the next retry.    * see {@getRandomWaitTime} for details.    * @param maxRetries the maximum # of retries when getting a SQLTransientException.    * @throws SQLException throws SQLRecoverableException or SQLNonTransientException the    * first time it is caught, or SQLTransientException when the maxRetries has reached.    */
specifier|public
specifier|static
parameter_list|<
name|T
parameter_list|>
name|T
name|executeWithRetry
parameter_list|(
name|SQLCommand
argument_list|<
name|T
argument_list|>
name|cmd
parameter_list|,
name|PreparedStatement
name|stmt
parameter_list|,
name|int
name|baseWindow
parameter_list|,
name|int
name|maxRetries
parameter_list|)
throws|throws
name|SQLException
block|{
name|Random
name|r
init|=
operator|new
name|Random
argument_list|()
decl_stmt|;
name|T
name|result
init|=
literal|null
decl_stmt|;
comment|// retry with # of maxRetries before throwing exception
for|for
control|(
name|int
name|failures
init|=
literal|0
init|;
condition|;
name|failures
operator|++
control|)
block|{
try|try
block|{
name|result
operator|=
name|cmd
operator|.
name|run
argument_list|(
name|stmt
argument_list|)
expr_stmt|;
return|return
name|result
return|;
block|}
catch|catch
parameter_list|(
name|SQLTransientException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Failure and retry #"
operator|+
name|failures
operator|+
literal|" with exception "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|failures
operator|>=
name|maxRetries
condition|)
block|{
throw|throw
name|e
throw|;
block|}
name|long
name|waitTime
init|=
name|getRandomWaitTime
argument_list|(
name|baseWindow
argument_list|,
name|failures
argument_list|,
name|r
argument_list|)
decl_stmt|;
try|try
block|{
name|Thread
operator|.
name|sleep
argument_list|(
name|waitTime
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|InterruptedException
name|iex
parameter_list|)
block|{      	  }
block|}
catch|catch
parameter_list|(
name|SQLException
name|e
parameter_list|)
block|{
comment|// throw other types of SQLExceptions (SQLNonTransientException / SQLRecoverableException)
throw|throw
name|e
throw|;
block|}
block|}
block|}
comment|/**    * Retry connecting to a database with random backoff (same as the one implemented in HDFS-767).    * This function only retries when the SQL query throws a SQLTransientException (which    * might be able to succeed with a simple retry). It doesn't retry when the exception    * is a SQLRecoverableException or SQLNonTransientException. For SQLRecoverableException    * the caller needs to reconnect to the database and restart the whole transaction.    *    * @param connectionString the JDBC connection string.    * @param baseWindow  The base time window (in milliseconds) before the next retry.    * see {@getRandomWaitTime} for details.    * @param maxRetries the maximum # of retries when getting a SQLTransientException.    * @throws SQLException throws SQLRecoverableException or SQLNonTransientException the    * first time it is caught, or SQLTransientException when the maxRetries has reached.    */
specifier|public
specifier|static
name|Connection
name|connectWithRetry
parameter_list|(
name|String
name|connectionString
parameter_list|,
name|int
name|waitWindow
parameter_list|,
name|int
name|maxRetries
parameter_list|)
throws|throws
name|SQLException
block|{
name|Random
name|r
init|=
operator|new
name|Random
argument_list|()
decl_stmt|;
comment|// retry with # of maxRetries before throwing exception
for|for
control|(
name|int
name|failures
init|=
literal|0
init|;
condition|;
name|failures
operator|++
control|)
block|{
try|try
block|{
name|Connection
name|conn
init|=
name|DriverManager
operator|.
name|getConnection
argument_list|(
name|connectionString
argument_list|)
decl_stmt|;
return|return
name|conn
return|;
block|}
catch|catch
parameter_list|(
name|SQLTransientException
name|e
parameter_list|)
block|{
if|if
condition|(
name|failures
operator|>=
name|maxRetries
condition|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Error during JDBC connection. "
operator|+
name|e
argument_list|)
expr_stmt|;
throw|throw
name|e
throw|;
block|}
name|long
name|waitTime
init|=
name|Utilities
operator|.
name|getRandomWaitTime
argument_list|(
name|waitWindow
argument_list|,
name|failures
argument_list|,
name|r
argument_list|)
decl_stmt|;
try|try
block|{
name|Thread
operator|.
name|sleep
argument_list|(
name|waitTime
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|InterruptedException
name|e1
parameter_list|)
block|{         }
block|}
catch|catch
parameter_list|(
name|SQLException
name|e
parameter_list|)
block|{
comment|// just throw other types (SQLNonTransientException / SQLRecoverableException)
throw|throw
name|e
throw|;
block|}
block|}
block|}
comment|/**    * Retry preparing a SQL statement with random backoff (same as the one implemented in HDFS-767).    * This function only retries when the SQL query throws a SQLTransientException (which    * might be able to succeed with a simple retry). It doesn't retry when the exception    * is a SQLRecoverableException or SQLNonTransientException. For SQLRecoverableException    * the caller needs to reconnect to the database and restart the whole transaction.    *    * @param conn a JDBC connection.    * @param stmt the SQL statement to be prepared.    * @param baseWindow  The base time window (in milliseconds) before the next retry.    * see {@getRandomWaitTime} for details.    * @param maxRetries the maximum # of retries when getting a SQLTransientException.    * @throws SQLException throws SQLRecoverableException or SQLNonTransientException the    * first time it is caught, or SQLTransientException when the maxRetries has reached.    */
specifier|public
specifier|static
name|PreparedStatement
name|prepareWithRetry
parameter_list|(
name|Connection
name|conn
parameter_list|,
name|String
name|stmt
parameter_list|,
name|int
name|waitWindow
parameter_list|,
name|int
name|maxRetries
parameter_list|)
throws|throws
name|SQLException
block|{
name|Random
name|r
init|=
operator|new
name|Random
argument_list|()
decl_stmt|;
comment|// retry with # of maxRetries before throwing exception
for|for
control|(
name|int
name|failures
init|=
literal|0
init|;
condition|;
name|failures
operator|++
control|)
block|{
try|try
block|{
return|return
name|conn
operator|.
name|prepareStatement
argument_list|(
name|stmt
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|SQLTransientException
name|e
parameter_list|)
block|{
if|if
condition|(
name|failures
operator|>=
name|maxRetries
condition|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Error preparing JDBC Statement "
operator|+
name|stmt
operator|+
literal|" :"
operator|+
name|e
argument_list|)
expr_stmt|;
throw|throw
name|e
throw|;
block|}
name|long
name|waitTime
init|=
name|Utilities
operator|.
name|getRandomWaitTime
argument_list|(
name|waitWindow
argument_list|,
name|failures
argument_list|,
name|r
argument_list|)
decl_stmt|;
try|try
block|{
name|Thread
operator|.
name|sleep
argument_list|(
name|waitTime
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|InterruptedException
name|e1
parameter_list|)
block|{         }
block|}
catch|catch
parameter_list|(
name|SQLException
name|e
parameter_list|)
block|{
comment|// just throw other types (SQLNonTransientException / SQLRecoverableException)
throw|throw
name|e
throw|;
block|}
block|}
block|}
comment|/**    * Introducing a random factor to the wait time before another retry. 	 * The wait time is dependent on # of failures and a random factor. 	 * At the first time of getting an exception , the wait time 	 * is a random number between 0..baseWindow msec. If the first retry 	 * still fails, we will wait baseWindow msec grace period before the 2nd retry. 	 * Also at the second retry, the waiting window is expanded to 2*baseWindow msec 	 * alleviating the request rate from the server. Similarly the 3rd retry 	 * will wait 2*baseWindow msec. grace period before retry and the waiting window is 	 * expanded to 3*baseWindow msec and so on.    * @param baseWindow the base waiting window.    * @param failures number of failures so far.    * @param r a random generator.    * @return number of milliseconds for the next wait time.    */
specifier|public
specifier|static
name|long
name|getRandomWaitTime
parameter_list|(
name|int
name|baseWindow
parameter_list|,
name|int
name|failures
parameter_list|,
name|Random
name|r
parameter_list|)
block|{
return|return
call|(
name|long
call|)
argument_list|(
name|baseWindow
operator|*
name|failures
operator|+
comment|// grace period for the last round of attempt
name|baseWindow
operator|*
operator|(
name|failures
operator|+
literal|1
operator|)
operator|*
name|r
operator|.
name|nextDouble
argument_list|()
argument_list|)
return|;
comment|// expanding time window for each failure
block|}
comment|/**    * Escape the '_', '%', as well as the escape characters inside the string key.    * @param key the string that will be used for the SQL LIKE operator.    * @param escape the escape character    * @return a string with escaped '_' and '%'.    */
specifier|public
specifier|static
specifier|final
name|char
name|sqlEscapeChar
init|=
literal|'\\'
decl_stmt|;
specifier|public
specifier|static
name|String
name|escapeSqlLike
parameter_list|(
name|String
name|key
parameter_list|)
block|{
name|StringBuffer
name|sb
init|=
operator|new
name|StringBuffer
argument_list|(
name|key
operator|.
name|length
argument_list|()
argument_list|)
decl_stmt|;
for|for
control|(
name|char
name|c
range|:
name|key
operator|.
name|toCharArray
argument_list|()
control|)
block|{
switch|switch
condition|(
name|c
condition|)
block|{
case|case
literal|'_'
case|:
case|case
literal|'%'
case|:
case|case
name|sqlEscapeChar
case|:
name|sb
operator|.
name|append
argument_list|(
name|sqlEscapeChar
argument_list|)
expr_stmt|;
comment|// fall through
default|default:
name|sb
operator|.
name|append
argument_list|(
name|c
argument_list|)
expr_stmt|;
break|break;
block|}
block|}
return|return
name|sb
operator|.
name|toString
argument_list|()
return|;
block|}
block|}
end_class

end_unit

