begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/**  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
package|;
end_package

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|Serializable
import|;
end_import

begin_import
import|import
name|java
operator|.
name|lang
operator|.
name|Exception
import|;
end_import

begin_import
import|import
name|java
operator|.
name|net
operator|.
name|MalformedURLException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|text
operator|.
name|SimpleDateFormat
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Calendar
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Collections
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Enumeration
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashSet
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Set
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|Log
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|LogFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|JavaUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|MapRedStats
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|Operator
operator|.
name|ProgressCounter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|errors
operator|.
name|ErrorAndSolution
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|errors
operator|.
name|TaskLogProcessor
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|history
operator|.
name|HiveHistory
operator|.
name|Keys
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|session
operator|.
name|SessionState
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|session
operator|.
name|SessionState
operator|.
name|LogHelper
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|stats
operator|.
name|ClientStatsPublisher
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|shims
operator|.
name|ShimLoader
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|Counters
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|Counters
operator|.
name|Counter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|JobClient
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|JobConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|RunningJob
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|TaskCompletionEvent
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|TaskReport
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|log4j
operator|.
name|Appender
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|log4j
operator|.
name|FileAppender
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|log4j
operator|.
name|LogManager
import|;
end_import

begin_class
specifier|public
class|class
name|HadoopJobExecHelper
block|{
specifier|static
specifier|final
specifier|private
name|Log
name|LOG
init|=
name|LogFactory
operator|.
name|getLog
argument_list|(
name|HadoopJobExecHelper
operator|.
name|class
operator|.
name|getName
argument_list|()
argument_list|)
decl_stmt|;
specifier|protected
specifier|transient
name|JobConf
name|job
decl_stmt|;
specifier|protected
name|Task
argument_list|<
name|?
extends|extends
name|Serializable
argument_list|>
name|task
decl_stmt|;
specifier|protected
specifier|transient
name|int
name|mapProgress
init|=
literal|0
decl_stmt|;
specifier|protected
specifier|transient
name|int
name|reduceProgress
init|=
literal|0
decl_stmt|;
specifier|public
specifier|transient
name|String
name|jobId
decl_stmt|;
specifier|private
name|LogHelper
name|console
decl_stmt|;
specifier|private
name|HadoopJobExecHook
name|callBackObj
decl_stmt|;
comment|/**    * Update counters relevant to this task.    */
specifier|private
name|void
name|updateCounters
parameter_list|(
name|Counters
name|ctrs
parameter_list|,
name|RunningJob
name|rj
parameter_list|)
throws|throws
name|IOException
block|{
name|mapProgress
operator|=
name|Math
operator|.
name|round
argument_list|(
name|rj
operator|.
name|mapProgress
argument_list|()
operator|*
literal|100
argument_list|)
expr_stmt|;
name|reduceProgress
operator|=
name|Math
operator|.
name|round
argument_list|(
name|rj
operator|.
name|reduceProgress
argument_list|()
operator|*
literal|100
argument_list|)
expr_stmt|;
name|task
operator|.
name|taskCounters
operator|.
name|put
argument_list|(
literal|"CNTR_NAME_"
operator|+
name|task
operator|.
name|getId
argument_list|()
operator|+
literal|"_MAP_PROGRESS"
argument_list|,
name|Long
operator|.
name|valueOf
argument_list|(
name|mapProgress
argument_list|)
argument_list|)
expr_stmt|;
name|task
operator|.
name|taskCounters
operator|.
name|put
argument_list|(
literal|"CNTR_NAME_"
operator|+
name|task
operator|.
name|getId
argument_list|()
operator|+
literal|"_REDUCE_PROGRESS"
argument_list|,
name|Long
operator|.
name|valueOf
argument_list|(
name|reduceProgress
argument_list|)
argument_list|)
expr_stmt|;
if|if
condition|(
name|ctrs
operator|==
literal|null
condition|)
block|{
comment|// hadoop might return null if it cannot locate the job.
comment|// we may still be able to retrieve the job status - so ignore
return|return;
block|}
if|if
condition|(
name|callBackObj
operator|!=
literal|null
condition|)
block|{
name|callBackObj
operator|.
name|updateCounters
argument_list|(
name|ctrs
argument_list|,
name|rj
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * This msg pattern is used to track when a job is started.    *    * @param jobId    * @return    */
specifier|private
specifier|static
name|String
name|getJobStartMsg
parameter_list|(
name|String
name|jobId
parameter_list|)
block|{
return|return
literal|"Starting Job = "
operator|+
name|jobId
return|;
block|}
comment|/**    * this msg pattern is used to track when a job is successfully done.    *    * @param jobId    * @return the job end message    */
specifier|public
specifier|static
name|String
name|getJobEndMsg
parameter_list|(
name|String
name|jobId
parameter_list|)
block|{
return|return
literal|"Ended Job = "
operator|+
name|jobId
return|;
block|}
specifier|public
name|boolean
name|mapStarted
parameter_list|()
block|{
return|return
name|mapProgress
operator|>
literal|0
return|;
block|}
specifier|public
name|boolean
name|reduceStarted
parameter_list|()
block|{
return|return
name|reduceProgress
operator|>
literal|0
return|;
block|}
specifier|public
name|boolean
name|mapDone
parameter_list|()
block|{
return|return
name|mapProgress
operator|==
literal|100
return|;
block|}
specifier|public
name|boolean
name|reduceDone
parameter_list|()
block|{
return|return
name|reduceProgress
operator|==
literal|100
return|;
block|}
specifier|public
name|String
name|getJobId
parameter_list|()
block|{
return|return
name|jobId
return|;
block|}
specifier|public
name|void
name|setJobId
parameter_list|(
name|String
name|jobId
parameter_list|)
block|{
name|this
operator|.
name|jobId
operator|=
name|jobId
expr_stmt|;
block|}
specifier|public
name|HadoopJobExecHelper
parameter_list|()
block|{   }
specifier|public
name|HadoopJobExecHelper
parameter_list|(
name|JobConf
name|job
parameter_list|,
name|LogHelper
name|console
parameter_list|,
name|Task
argument_list|<
name|?
extends|extends
name|Serializable
argument_list|>
name|task
parameter_list|,
name|HadoopJobExecHook
name|hookCallBack
parameter_list|)
block|{
name|this
operator|.
name|job
operator|=
name|job
expr_stmt|;
name|this
operator|.
name|console
operator|=
name|console
expr_stmt|;
name|this
operator|.
name|task
operator|=
name|task
expr_stmt|;
name|this
operator|.
name|callBackObj
operator|=
name|hookCallBack
expr_stmt|;
block|}
comment|/**    * A list of the currently running jobs spawned in this Hive instance that is used to kill all    * running jobs in the event of an unexpected shutdown - i.e., the JVM shuts down while there are    * still jobs running.    */
specifier|public
specifier|static
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|runningJobKillURIs
init|=
name|Collections
operator|.
name|synchronizedMap
argument_list|(
operator|new
name|HashMap
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
argument_list|()
argument_list|)
decl_stmt|;
comment|/**    * In Hive, when the user control-c's the command line, any running jobs spawned from that command    * line are best-effort killed.    *    * This static constructor registers a shutdown thread to iterate over all the running job kill    * URLs and do a get on them.    *    */
static|static
block|{
if|if
condition|(
operator|new
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configuration
argument_list|()
operator|.
name|getBoolean
argument_list|(
literal|"webinterface.private.actions"
argument_list|,
literal|false
argument_list|)
condition|)
block|{
name|Runtime
operator|.
name|getRuntime
argument_list|()
operator|.
name|addShutdownHook
argument_list|(
operator|new
name|Thread
argument_list|()
block|{
annotation|@
name|Override
specifier|public
name|void
name|run
parameter_list|()
block|{
name|killRunningJobs
argument_list|()
expr_stmt|;
block|}
block|}
argument_list|)
expr_stmt|;
block|}
block|}
specifier|public
specifier|static
name|void
name|killRunningJobs
parameter_list|()
block|{
synchronized|synchronized
init|(
name|runningJobKillURIs
init|)
block|{
for|for
control|(
name|String
name|uri
range|:
name|runningJobKillURIs
operator|.
name|values
argument_list|()
control|)
block|{
try|try
block|{
name|System
operator|.
name|err
operator|.
name|println
argument_list|(
literal|"killing job with: "
operator|+
name|uri
argument_list|)
expr_stmt|;
name|java
operator|.
name|net
operator|.
name|HttpURLConnection
name|conn
init|=
operator|(
name|java
operator|.
name|net
operator|.
name|HttpURLConnection
operator|)
operator|new
name|java
operator|.
name|net
operator|.
name|URL
argument_list|(
name|uri
argument_list|)
operator|.
name|openConnection
argument_list|()
decl_stmt|;
name|conn
operator|.
name|setRequestMethod
argument_list|(
literal|"POST"
argument_list|)
expr_stmt|;
name|int
name|retCode
init|=
name|conn
operator|.
name|getResponseCode
argument_list|()
decl_stmt|;
if|if
condition|(
name|retCode
operator|!=
literal|200
condition|)
block|{
name|System
operator|.
name|err
operator|.
name|println
argument_list|(
literal|"Got an error trying to kill job with URI: "
operator|+
name|uri
operator|+
literal|" = "
operator|+
name|retCode
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|System
operator|.
name|err
operator|.
name|println
argument_list|(
literal|"trying to kill job, caught: "
operator|+
name|e
argument_list|)
expr_stmt|;
comment|// do nothing
block|}
block|}
block|}
block|}
specifier|public
name|boolean
name|checkFatalErrors
parameter_list|(
name|Counters
name|ctrs
parameter_list|,
name|StringBuilder
name|errMsg
parameter_list|)
block|{
if|if
condition|(
name|ctrs
operator|==
literal|null
condition|)
block|{
comment|// hadoop might return null if it cannot locate the job.
comment|// we may still be able to retrieve the job status - so ignore
return|return
literal|false
return|;
block|}
comment|// check for number of created files
name|long
name|numFiles
init|=
name|ctrs
operator|.
name|getCounter
argument_list|(
name|ProgressCounter
operator|.
name|CREATED_FILES
argument_list|)
decl_stmt|;
name|long
name|upperLimit
init|=
name|HiveConf
operator|.
name|getLongVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|MAXCREATEDFILES
argument_list|)
decl_stmt|;
if|if
condition|(
name|numFiles
operator|>
name|upperLimit
condition|)
block|{
name|errMsg
operator|.
name|append
argument_list|(
literal|"total number of created files now is "
operator|+
name|numFiles
operator|+
literal|", which exceeds "
argument_list|)
operator|.
name|append
argument_list|(
name|upperLimit
argument_list|)
expr_stmt|;
return|return
literal|true
return|;
block|}
return|return
name|this
operator|.
name|callBackObj
operator|.
name|checkFatalErrors
argument_list|(
name|ctrs
argument_list|,
name|errMsg
argument_list|)
return|;
block|}
specifier|private
name|MapRedStats
name|progress
parameter_list|(
name|ExecDriverTaskHandle
name|th
parameter_list|)
throws|throws
name|IOException
block|{
name|JobClient
name|jc
init|=
name|th
operator|.
name|getJobClient
argument_list|()
decl_stmt|;
name|RunningJob
name|rj
init|=
name|th
operator|.
name|getRunningJob
argument_list|()
decl_stmt|;
name|String
name|lastReport
init|=
literal|""
decl_stmt|;
name|SimpleDateFormat
name|dateFormat
init|=
operator|new
name|SimpleDateFormat
argument_list|(
literal|"yyyy-MM-dd HH:mm:ss,SSS"
argument_list|)
decl_stmt|;
comment|//DecimalFormat longFormatter = new DecimalFormat("###,###");
name|long
name|reportTime
init|=
name|System
operator|.
name|currentTimeMillis
argument_list|()
decl_stmt|;
name|long
name|maxReportInterval
init|=
name|HiveConf
operator|.
name|getLongVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_LOG_INCREMENTAL_PLAN_PROGRESS_INTERVAL
argument_list|)
decl_stmt|;
name|boolean
name|fatal
init|=
literal|false
decl_stmt|;
name|StringBuilder
name|errMsg
init|=
operator|new
name|StringBuilder
argument_list|()
decl_stmt|;
name|long
name|pullInterval
init|=
name|HiveConf
operator|.
name|getLongVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVECOUNTERSPULLINTERVAL
argument_list|)
decl_stmt|;
name|boolean
name|initializing
init|=
literal|true
decl_stmt|;
name|boolean
name|initOutputPrinted
init|=
literal|false
decl_stmt|;
name|long
name|cpuMsec
init|=
operator|-
literal|1
decl_stmt|;
name|int
name|numMap
init|=
operator|-
literal|1
decl_stmt|;
name|int
name|numReduce
init|=
operator|-
literal|1
decl_stmt|;
name|List
argument_list|<
name|ClientStatsPublisher
argument_list|>
name|clientStatPublishers
init|=
name|getClientStatPublishers
argument_list|()
decl_stmt|;
while|while
condition|(
operator|!
name|rj
operator|.
name|isComplete
argument_list|()
condition|)
block|{
try|try
block|{
name|Thread
operator|.
name|sleep
argument_list|(
name|pullInterval
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|InterruptedException
name|e
parameter_list|)
block|{       }
if|if
condition|(
name|initializing
operator|&&
name|ShimLoader
operator|.
name|getHadoopShims
argument_list|()
operator|.
name|isJobPreparing
argument_list|(
name|rj
argument_list|)
condition|)
block|{
comment|// No reason to poll untill the job is initialized
continue|continue;
block|}
else|else
block|{
comment|// By now the job is initialized so no reason to do
comment|// rj.getJobState() again and we do not want to do an extra RPC call
name|initializing
operator|=
literal|false
expr_stmt|;
block|}
if|if
condition|(
operator|!
name|initOutputPrinted
condition|)
block|{
name|SessionState
name|ss
init|=
name|SessionState
operator|.
name|get
argument_list|()
decl_stmt|;
name|String
name|logMapper
decl_stmt|;
name|String
name|logReducer
decl_stmt|;
name|TaskReport
index|[]
name|mappers
init|=
name|jc
operator|.
name|getMapTaskReports
argument_list|(
name|rj
operator|.
name|getJobID
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|mappers
operator|==
literal|null
condition|)
block|{
name|logMapper
operator|=
literal|"no information for number of mappers; "
expr_stmt|;
block|}
else|else
block|{
name|numMap
operator|=
name|mappers
operator|.
name|length
expr_stmt|;
if|if
condition|(
name|ss
operator|!=
literal|null
condition|)
block|{
name|ss
operator|.
name|getHiveHistory
argument_list|()
operator|.
name|setTaskProperty
argument_list|(
name|SessionState
operator|.
name|get
argument_list|()
operator|.
name|getQueryId
argument_list|()
argument_list|,
name|getId
argument_list|()
argument_list|,
name|Keys
operator|.
name|TASK_NUM_MAPPERS
argument_list|,
name|Integer
operator|.
name|toString
argument_list|(
name|numMap
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|logMapper
operator|=
literal|"number of mappers: "
operator|+
name|numMap
operator|+
literal|"; "
expr_stmt|;
block|}
name|TaskReport
index|[]
name|reducers
init|=
name|jc
operator|.
name|getReduceTaskReports
argument_list|(
name|rj
operator|.
name|getJobID
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|reducers
operator|==
literal|null
condition|)
block|{
name|logReducer
operator|=
literal|"no information for number of reducers. "
expr_stmt|;
block|}
else|else
block|{
name|numReduce
operator|=
name|reducers
operator|.
name|length
expr_stmt|;
if|if
condition|(
name|ss
operator|!=
literal|null
condition|)
block|{
name|ss
operator|.
name|getHiveHistory
argument_list|()
operator|.
name|setTaskProperty
argument_list|(
name|SessionState
operator|.
name|get
argument_list|()
operator|.
name|getQueryId
argument_list|()
argument_list|,
name|getId
argument_list|()
argument_list|,
name|Keys
operator|.
name|TASK_NUM_REDUCERS
argument_list|,
name|Integer
operator|.
name|toString
argument_list|(
name|numReduce
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|logReducer
operator|=
literal|"number of reducers: "
operator|+
name|numReduce
expr_stmt|;
block|}
name|console
operator|.
name|printInfo
argument_list|(
literal|"Hadoop job information for "
operator|+
name|getId
argument_list|()
operator|+
literal|": "
operator|+
name|logMapper
operator|+
name|logReducer
argument_list|)
expr_stmt|;
name|initOutputPrinted
operator|=
literal|true
expr_stmt|;
block|}
name|RunningJob
name|newRj
init|=
name|jc
operator|.
name|getJob
argument_list|(
name|rj
operator|.
name|getJobID
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|newRj
operator|==
literal|null
condition|)
block|{
comment|// under exceptional load, hadoop may not be able to look up status
comment|// of finished jobs (because it has purged them from memory). From
comment|// hive's perspective - it's equivalent to the job having failed.
comment|// So raise a meaningful exception
throw|throw
operator|new
name|IOException
argument_list|(
literal|"Could not find status of job:"
operator|+
name|rj
operator|.
name|getJobID
argument_list|()
argument_list|)
throw|;
block|}
else|else
block|{
name|th
operator|.
name|setRunningJob
argument_list|(
name|newRj
argument_list|)
expr_stmt|;
name|rj
operator|=
name|newRj
expr_stmt|;
block|}
comment|// If fatal errors happen we should kill the job immediately rather than
comment|// let the job retry several times, which eventually lead to failure.
if|if
condition|(
name|fatal
condition|)
block|{
continue|continue;
comment|// wait until rj.isComplete
block|}
name|Counters
name|ctrs
init|=
name|th
operator|.
name|getCounters
argument_list|()
decl_stmt|;
if|if
condition|(
name|fatal
operator|=
name|checkFatalErrors
argument_list|(
name|ctrs
argument_list|,
name|errMsg
argument_list|)
condition|)
block|{
name|console
operator|.
name|printError
argument_list|(
literal|"[Fatal Error] "
operator|+
name|errMsg
operator|.
name|toString
argument_list|()
operator|+
literal|". Killing the job."
argument_list|)
expr_stmt|;
name|rj
operator|.
name|killJob
argument_list|()
expr_stmt|;
continue|continue;
block|}
name|errMsg
operator|.
name|setLength
argument_list|(
literal|0
argument_list|)
expr_stmt|;
name|updateCounters
argument_list|(
name|ctrs
argument_list|,
name|rj
argument_list|)
expr_stmt|;
comment|// Prepare data for Client Stat Publishers (if any present) and execute them
if|if
condition|(
name|clientStatPublishers
operator|.
name|size
argument_list|()
operator|>
literal|0
operator|&&
name|ctrs
operator|!=
literal|null
condition|)
block|{
name|Map
argument_list|<
name|String
argument_list|,
name|Double
argument_list|>
name|exctractedCounters
init|=
name|extractAllCounterValues
argument_list|(
name|ctrs
argument_list|)
decl_stmt|;
for|for
control|(
name|ClientStatsPublisher
name|clientStatPublisher
range|:
name|clientStatPublishers
control|)
block|{
try|try
block|{
name|clientStatPublisher
operator|.
name|run
argument_list|(
name|exctractedCounters
argument_list|,
name|rj
operator|.
name|getID
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|RuntimeException
name|runtimeException
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Exception "
operator|+
name|runtimeException
operator|.
name|getClass
argument_list|()
operator|.
name|getCanonicalName
argument_list|()
operator|+
literal|" thrown when running clientStatsPublishers. The stack trace is: "
argument_list|,
name|runtimeException
argument_list|)
expr_stmt|;
block|}
block|}
block|}
name|String
name|report
init|=
literal|" "
operator|+
name|getId
argument_list|()
operator|+
literal|" map = "
operator|+
name|mapProgress
operator|+
literal|"%,  reduce = "
operator|+
name|reduceProgress
operator|+
literal|"%"
decl_stmt|;
if|if
condition|(
operator|!
name|report
operator|.
name|equals
argument_list|(
name|lastReport
argument_list|)
operator|||
name|System
operator|.
name|currentTimeMillis
argument_list|()
operator|>=
name|reportTime
operator|+
name|maxReportInterval
condition|)
block|{
comment|// find out CPU msecs
comment|// In the case that we can't find out this number, we just skip the step to print
comment|// it out.
if|if
condition|(
name|ctrs
operator|!=
literal|null
condition|)
block|{
name|Counter
name|counterCpuMsec
init|=
name|ctrs
operator|.
name|findCounter
argument_list|(
literal|"org.apache.hadoop.mapred.Task$Counter"
argument_list|,
literal|"CPU_MILLISECONDS"
argument_list|)
decl_stmt|;
if|if
condition|(
name|counterCpuMsec
operator|!=
literal|null
condition|)
block|{
name|long
name|newCpuMSec
init|=
name|counterCpuMsec
operator|.
name|getValue
argument_list|()
decl_stmt|;
if|if
condition|(
name|newCpuMSec
operator|>
literal|0
condition|)
block|{
name|cpuMsec
operator|=
name|newCpuMSec
expr_stmt|;
name|report
operator|+=
literal|", Cumulative CPU "
operator|+
operator|(
name|cpuMsec
operator|/
literal|1000D
operator|)
operator|+
literal|" sec"
expr_stmt|;
block|}
block|}
block|}
comment|// write out serialized plan with counters to log file
comment|// LOG.info(queryPlan);
name|String
name|output
init|=
name|dateFormat
operator|.
name|format
argument_list|(
name|Calendar
operator|.
name|getInstance
argument_list|()
operator|.
name|getTime
argument_list|()
argument_list|)
operator|+
name|report
decl_stmt|;
name|SessionState
name|ss
init|=
name|SessionState
operator|.
name|get
argument_list|()
decl_stmt|;
if|if
condition|(
name|ss
operator|!=
literal|null
condition|)
block|{
name|ss
operator|.
name|getHiveHistory
argument_list|()
operator|.
name|setTaskCounters
argument_list|(
name|SessionState
operator|.
name|get
argument_list|()
operator|.
name|getQueryId
argument_list|()
argument_list|,
name|getId
argument_list|()
argument_list|,
name|ctrs
argument_list|)
expr_stmt|;
name|ss
operator|.
name|getHiveHistory
argument_list|()
operator|.
name|setTaskProperty
argument_list|(
name|SessionState
operator|.
name|get
argument_list|()
operator|.
name|getQueryId
argument_list|()
argument_list|,
name|getId
argument_list|()
argument_list|,
name|Keys
operator|.
name|TASK_HADOOP_PROGRESS
argument_list|,
name|output
argument_list|)
expr_stmt|;
if|if
condition|(
name|ss
operator|.
name|getConf
argument_list|()
operator|.
name|getBoolVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_LOG_INCREMENTAL_PLAN_PROGRESS
argument_list|)
condition|)
block|{
name|ss
operator|.
name|getHiveHistory
argument_list|()
operator|.
name|progressTask
argument_list|(
name|SessionState
operator|.
name|get
argument_list|()
operator|.
name|getQueryId
argument_list|()
argument_list|,
name|this
operator|.
name|task
argument_list|)
expr_stmt|;
name|this
operator|.
name|callBackObj
operator|.
name|logPlanProgress
argument_list|(
name|ss
argument_list|)
expr_stmt|;
block|}
block|}
name|console
operator|.
name|printInfo
argument_list|(
name|output
argument_list|)
expr_stmt|;
name|lastReport
operator|=
name|report
expr_stmt|;
name|reportTime
operator|=
name|System
operator|.
name|currentTimeMillis
argument_list|()
expr_stmt|;
block|}
block|}
if|if
condition|(
name|cpuMsec
operator|>
literal|0
condition|)
block|{
name|console
operator|.
name|printInfo
argument_list|(
literal|"MapReduce Total cumulative CPU time: "
operator|+
name|Utilities
operator|.
name|formatMsecToStr
argument_list|(
name|cpuMsec
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|boolean
name|success
decl_stmt|;
name|Counters
name|ctrs
init|=
name|th
operator|.
name|getCounters
argument_list|()
decl_stmt|;
if|if
condition|(
name|fatal
condition|)
block|{
name|success
operator|=
literal|false
expr_stmt|;
block|}
else|else
block|{
comment|// check for fatal error again in case it occurred after
comment|// the last check before the job is completed
if|if
condition|(
name|checkFatalErrors
argument_list|(
name|ctrs
argument_list|,
name|errMsg
argument_list|)
condition|)
block|{
name|console
operator|.
name|printError
argument_list|(
literal|"[Fatal Error] "
operator|+
name|errMsg
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
name|success
operator|=
literal|false
expr_stmt|;
block|}
else|else
block|{
name|SessionState
name|ss
init|=
name|SessionState
operator|.
name|get
argument_list|()
decl_stmt|;
if|if
condition|(
name|ss
operator|!=
literal|null
condition|)
block|{
name|ss
operator|.
name|getHiveHistory
argument_list|()
operator|.
name|setTaskCounters
argument_list|(
name|SessionState
operator|.
name|get
argument_list|()
operator|.
name|getQueryId
argument_list|()
argument_list|,
name|getId
argument_list|()
argument_list|,
name|ctrs
argument_list|)
expr_stmt|;
block|}
name|success
operator|=
name|rj
operator|.
name|isSuccessful
argument_list|()
expr_stmt|;
block|}
block|}
if|if
condition|(
name|ctrs
operator|!=
literal|null
condition|)
block|{
name|Counter
name|counterCpuMsec
init|=
name|ctrs
operator|.
name|findCounter
argument_list|(
literal|"org.apache.hadoop.mapred.Task$Counter"
argument_list|,
literal|"CPU_MILLISECONDS"
argument_list|)
decl_stmt|;
if|if
condition|(
name|counterCpuMsec
operator|!=
literal|null
condition|)
block|{
name|long
name|newCpuMSec
init|=
name|counterCpuMsec
operator|.
name|getValue
argument_list|()
decl_stmt|;
if|if
condition|(
name|newCpuMSec
operator|>
name|cpuMsec
condition|)
block|{
name|cpuMsec
operator|=
name|newCpuMSec
expr_stmt|;
block|}
block|}
block|}
name|MapRedStats
name|mapRedStats
init|=
operator|new
name|MapRedStats
argument_list|(
name|numMap
argument_list|,
name|numReduce
argument_list|,
name|cpuMsec
argument_list|,
name|success
argument_list|,
name|rj
operator|.
name|getID
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|)
decl_stmt|;
name|mapRedStats
operator|.
name|setCounters
argument_list|(
name|ctrs
argument_list|)
expr_stmt|;
comment|// update based on the final value of the counters
name|updateCounters
argument_list|(
name|ctrs
argument_list|,
name|rj
argument_list|)
expr_stmt|;
name|SessionState
name|ss
init|=
name|SessionState
operator|.
name|get
argument_list|()
decl_stmt|;
if|if
condition|(
name|ss
operator|!=
literal|null
condition|)
block|{
name|this
operator|.
name|callBackObj
operator|.
name|logPlanProgress
argument_list|(
name|ss
argument_list|)
expr_stmt|;
block|}
comment|// LOG.info(queryPlan);
return|return
name|mapRedStats
return|;
block|}
specifier|private
name|String
name|getId
parameter_list|()
block|{
return|return
name|this
operator|.
name|task
operator|.
name|getId
argument_list|()
return|;
block|}
comment|/**    * from StreamJob.java.    */
specifier|public
name|void
name|jobInfo
parameter_list|(
name|RunningJob
name|rj
parameter_list|)
block|{
if|if
condition|(
name|job
operator|.
name|get
argument_list|(
literal|"mapred.job.tracker"
argument_list|,
literal|"local"
argument_list|)
operator|.
name|equals
argument_list|(
literal|"local"
argument_list|)
condition|)
block|{
name|console
operator|.
name|printInfo
argument_list|(
literal|"Job running in-process (local Hadoop)"
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|String
name|hp
init|=
name|job
operator|.
name|get
argument_list|(
literal|"mapred.job.tracker"
argument_list|)
decl_stmt|;
if|if
condition|(
name|SessionState
operator|.
name|get
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|SessionState
operator|.
name|get
argument_list|()
operator|.
name|getHiveHistory
argument_list|()
operator|.
name|setTaskProperty
argument_list|(
name|SessionState
operator|.
name|get
argument_list|()
operator|.
name|getQueryId
argument_list|()
argument_list|,
name|getId
argument_list|()
argument_list|,
name|Keys
operator|.
name|TASK_HADOOP_ID
argument_list|,
name|rj
operator|.
name|getJobID
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|console
operator|.
name|printInfo
argument_list|(
name|getJobStartMsg
argument_list|(
name|rj
operator|.
name|getJobID
argument_list|()
argument_list|)
operator|+
literal|", Tracking URL = "
operator|+
name|rj
operator|.
name|getTrackingURL
argument_list|()
argument_list|)
expr_stmt|;
name|console
operator|.
name|printInfo
argument_list|(
literal|"Kill Command = "
operator|+
name|HiveConf
operator|.
name|getVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HADOOPBIN
argument_list|)
operator|+
literal|" job  -Dmapred.job.tracker="
operator|+
name|hp
operator|+
literal|" -kill "
operator|+
name|rj
operator|.
name|getJobID
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * This class contains the state of the running task Going forward, we will return this handle    * from execute and Driver can split execute into start, monitorProgess and postProcess.    */
specifier|private
specifier|static
class|class
name|ExecDriverTaskHandle
extends|extends
name|TaskHandle
block|{
name|JobClient
name|jc
decl_stmt|;
name|RunningJob
name|rj
decl_stmt|;
name|JobClient
name|getJobClient
parameter_list|()
block|{
return|return
name|jc
return|;
block|}
name|RunningJob
name|getRunningJob
parameter_list|()
block|{
return|return
name|rj
return|;
block|}
specifier|public
name|ExecDriverTaskHandle
parameter_list|(
name|JobClient
name|jc
parameter_list|,
name|RunningJob
name|rj
parameter_list|)
block|{
name|this
operator|.
name|jc
operator|=
name|jc
expr_stmt|;
name|this
operator|.
name|rj
operator|=
name|rj
expr_stmt|;
block|}
specifier|public
name|void
name|setRunningJob
parameter_list|(
name|RunningJob
name|job
parameter_list|)
block|{
name|rj
operator|=
name|job
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|Counters
name|getCounters
parameter_list|()
throws|throws
name|IOException
block|{
return|return
name|rj
operator|.
name|getCounters
argument_list|()
return|;
block|}
block|}
comment|// Used for showJobFailDebugInfo
specifier|private
specifier|static
class|class
name|TaskInfo
block|{
name|String
name|jobId
decl_stmt|;
name|HashSet
argument_list|<
name|String
argument_list|>
name|logUrls
decl_stmt|;
specifier|public
name|TaskInfo
parameter_list|(
name|String
name|jobId
parameter_list|)
block|{
name|this
operator|.
name|jobId
operator|=
name|jobId
expr_stmt|;
name|logUrls
operator|=
operator|new
name|HashSet
argument_list|<
name|String
argument_list|>
argument_list|()
expr_stmt|;
block|}
specifier|public
name|void
name|addLogUrl
parameter_list|(
name|String
name|logUrl
parameter_list|)
block|{
name|logUrls
operator|.
name|add
argument_list|(
name|logUrl
argument_list|)
expr_stmt|;
block|}
specifier|public
name|HashSet
argument_list|<
name|String
argument_list|>
name|getLogUrls
parameter_list|()
block|{
return|return
name|logUrls
return|;
block|}
specifier|public
name|String
name|getJobId
parameter_list|()
block|{
return|return
name|jobId
return|;
block|}
block|}
annotation|@
name|SuppressWarnings
argument_list|(
literal|"deprecation"
argument_list|)
specifier|private
name|void
name|showJobFailDebugInfo
parameter_list|(
name|JobConf
name|conf
parameter_list|,
name|RunningJob
name|rj
parameter_list|)
throws|throws
name|IOException
throws|,
name|MalformedURLException
block|{
comment|// Mapping from task ID to the number of failures
name|Map
argument_list|<
name|String
argument_list|,
name|Integer
argument_list|>
name|failures
init|=
operator|new
name|HashMap
argument_list|<
name|String
argument_list|,
name|Integer
argument_list|>
argument_list|()
decl_stmt|;
comment|// Successful task ID's
name|Set
argument_list|<
name|String
argument_list|>
name|successes
init|=
operator|new
name|HashSet
argument_list|<
name|String
argument_list|>
argument_list|()
decl_stmt|;
name|Map
argument_list|<
name|String
argument_list|,
name|TaskInfo
argument_list|>
name|taskIdToInfo
init|=
operator|new
name|HashMap
argument_list|<
name|String
argument_list|,
name|TaskInfo
argument_list|>
argument_list|()
decl_stmt|;
name|int
name|startIndex
init|=
literal|0
decl_stmt|;
name|console
operator|.
name|printError
argument_list|(
literal|"Error during job, obtaining debugging information..."
argument_list|)
expr_stmt|;
comment|// Loop to get all task completion events because getTaskCompletionEvents
comment|// only returns a subset per call
while|while
condition|(
literal|true
condition|)
block|{
name|TaskCompletionEvent
index|[]
name|taskCompletions
init|=
name|rj
operator|.
name|getTaskCompletionEvents
argument_list|(
name|startIndex
argument_list|)
decl_stmt|;
if|if
condition|(
name|taskCompletions
operator|==
literal|null
operator|||
name|taskCompletions
operator|.
name|length
operator|==
literal|0
condition|)
block|{
break|break;
block|}
name|boolean
name|more
init|=
literal|true
decl_stmt|;
name|boolean
name|firstError
init|=
literal|true
decl_stmt|;
for|for
control|(
name|TaskCompletionEvent
name|t
range|:
name|taskCompletions
control|)
block|{
comment|// getTaskJobIDs returns Strings for compatibility with Hadoop versions
comment|// without TaskID or TaskAttemptID
name|String
index|[]
name|taskJobIds
init|=
name|ShimLoader
operator|.
name|getHadoopShims
argument_list|()
operator|.
name|getTaskJobIDs
argument_list|(
name|t
argument_list|)
decl_stmt|;
if|if
condition|(
name|taskJobIds
operator|==
literal|null
condition|)
block|{
name|console
operator|.
name|printError
argument_list|(
literal|"Task attempt info is unavailable in this Hadoop version"
argument_list|)
expr_stmt|;
name|more
operator|=
literal|false
expr_stmt|;
break|break;
block|}
comment|// For each task completion event, get the associated task id, job id
comment|// and the logs
name|String
name|taskId
init|=
name|taskJobIds
index|[
literal|0
index|]
decl_stmt|;
name|String
name|jobId
init|=
name|taskJobIds
index|[
literal|1
index|]
decl_stmt|;
if|if
condition|(
name|firstError
condition|)
block|{
name|console
operator|.
name|printError
argument_list|(
literal|"Examining task ID: "
operator|+
name|taskId
operator|+
literal|" (and more) from job "
operator|+
name|jobId
argument_list|)
expr_stmt|;
name|firstError
operator|=
literal|false
expr_stmt|;
block|}
name|TaskInfo
name|ti
init|=
name|taskIdToInfo
operator|.
name|get
argument_list|(
name|taskId
argument_list|)
decl_stmt|;
if|if
condition|(
name|ti
operator|==
literal|null
condition|)
block|{
name|ti
operator|=
operator|new
name|TaskInfo
argument_list|(
name|jobId
argument_list|)
expr_stmt|;
name|taskIdToInfo
operator|.
name|put
argument_list|(
name|taskId
argument_list|,
name|ti
argument_list|)
expr_stmt|;
block|}
comment|// These tasks should have come from the same job.
assert|assert
operator|(
name|ti
operator|.
name|getJobId
argument_list|()
operator|!=
literal|null
operator|&&
name|ti
operator|.
name|getJobId
argument_list|()
operator|.
name|equals
argument_list|(
name|jobId
argument_list|)
operator|)
assert|;
name|String
name|taskAttemptLogUrl
init|=
name|ShimLoader
operator|.
name|getHadoopShims
argument_list|()
operator|.
name|getTaskAttemptLogUrl
argument_list|(
name|conf
argument_list|,
name|t
operator|.
name|getTaskTrackerHttp
argument_list|()
argument_list|,
name|t
operator|.
name|getTaskId
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|taskAttemptLogUrl
operator|!=
literal|null
condition|)
block|{
name|ti
operator|.
name|getLogUrls
argument_list|()
operator|.
name|add
argument_list|(
name|taskAttemptLogUrl
argument_list|)
expr_stmt|;
block|}
comment|// If a task failed, then keep track of the total number of failures
comment|// for that task (typically, a task gets re-run up to 4 times if it
comment|// fails
if|if
condition|(
name|t
operator|.
name|getTaskStatus
argument_list|()
operator|!=
name|TaskCompletionEvent
operator|.
name|Status
operator|.
name|SUCCEEDED
condition|)
block|{
name|Integer
name|failAttempts
init|=
name|failures
operator|.
name|get
argument_list|(
name|taskId
argument_list|)
decl_stmt|;
if|if
condition|(
name|failAttempts
operator|==
literal|null
condition|)
block|{
name|failAttempts
operator|=
name|Integer
operator|.
name|valueOf
argument_list|(
literal|0
argument_list|)
expr_stmt|;
block|}
name|failAttempts
operator|=
name|Integer
operator|.
name|valueOf
argument_list|(
name|failAttempts
operator|.
name|intValue
argument_list|()
operator|+
literal|1
argument_list|)
expr_stmt|;
name|failures
operator|.
name|put
argument_list|(
name|taskId
argument_list|,
name|failAttempts
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|successes
operator|.
name|add
argument_list|(
name|taskId
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
operator|!
name|more
condition|)
block|{
break|break;
block|}
name|startIndex
operator|+=
name|taskCompletions
operator|.
name|length
expr_stmt|;
block|}
comment|// Remove failures for tasks that succeeded
for|for
control|(
name|String
name|task
range|:
name|successes
control|)
block|{
name|failures
operator|.
name|remove
argument_list|(
name|task
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|failures
operator|.
name|keySet
argument_list|()
operator|.
name|size
argument_list|()
operator|==
literal|0
condition|)
block|{
return|return;
block|}
comment|// Find the highest failure count
name|int
name|maxFailures
init|=
literal|0
decl_stmt|;
for|for
control|(
name|Integer
name|failCount
range|:
name|failures
operator|.
name|values
argument_list|()
control|)
block|{
if|if
condition|(
name|maxFailures
operator|<
name|failCount
operator|.
name|intValue
argument_list|()
condition|)
block|{
name|maxFailures
operator|=
name|failCount
operator|.
name|intValue
argument_list|()
expr_stmt|;
block|}
block|}
comment|// Display Error Message for tasks with the highest failure count
name|String
name|jtUrl
init|=
name|JobTrackerURLResolver
operator|.
name|getURL
argument_list|(
name|conf
argument_list|)
decl_stmt|;
for|for
control|(
name|String
name|task
range|:
name|failures
operator|.
name|keySet
argument_list|()
control|)
block|{
if|if
condition|(
name|failures
operator|.
name|get
argument_list|(
name|task
argument_list|)
operator|.
name|intValue
argument_list|()
operator|==
name|maxFailures
condition|)
block|{
name|TaskInfo
name|ti
init|=
name|taskIdToInfo
operator|.
name|get
argument_list|(
name|task
argument_list|)
decl_stmt|;
name|String
name|jobId
init|=
name|ti
operator|.
name|getJobId
argument_list|()
decl_stmt|;
name|String
name|taskUrl
init|=
name|jtUrl
operator|+
literal|"/taskdetails.jsp?jobid="
operator|+
name|jobId
operator|+
literal|"&tipid="
operator|+
name|task
operator|.
name|toString
argument_list|()
decl_stmt|;
name|TaskLogProcessor
name|tlp
init|=
operator|new
name|TaskLogProcessor
argument_list|(
name|conf
argument_list|)
decl_stmt|;
for|for
control|(
name|String
name|logUrl
range|:
name|ti
operator|.
name|getLogUrls
argument_list|()
control|)
block|{
name|tlp
operator|.
name|addTaskAttemptLogUrl
argument_list|(
name|logUrl
argument_list|)
expr_stmt|;
block|}
name|List
argument_list|<
name|ErrorAndSolution
argument_list|>
name|errors
init|=
name|tlp
operator|.
name|getErrors
argument_list|()
decl_stmt|;
name|StringBuilder
name|sb
init|=
operator|new
name|StringBuilder
argument_list|()
decl_stmt|;
comment|// We use a StringBuilder and then call printError only once as
comment|// printError will write to both stderr and the error log file. In
comment|// situations where both the stderr and the log file output is
comment|// simultaneously output to a single stream, this will look cleaner.
name|sb
operator|.
name|append
argument_list|(
literal|"\n"
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
literal|"Task with the most failures("
operator|+
name|maxFailures
operator|+
literal|"): \n"
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
literal|"-----\n"
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
literal|"Task ID:\n  "
operator|+
name|task
operator|+
literal|"\n\n"
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
literal|"URL:\n  "
operator|+
name|taskUrl
operator|+
literal|"\n"
argument_list|)
expr_stmt|;
for|for
control|(
name|ErrorAndSolution
name|e
range|:
name|errors
control|)
block|{
name|sb
operator|.
name|append
argument_list|(
literal|"\n"
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
literal|"Possible error:\n  "
operator|+
name|e
operator|.
name|getError
argument_list|()
operator|+
literal|"\n\n"
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
literal|"Solution:\n  "
operator|+
name|e
operator|.
name|getSolution
argument_list|()
operator|+
literal|"\n"
argument_list|)
expr_stmt|;
block|}
name|sb
operator|.
name|append
argument_list|(
literal|"-----\n"
argument_list|)
expr_stmt|;
name|console
operator|.
name|printError
argument_list|(
name|sb
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
comment|// Only print out one task because that's good enough for debugging.
break|break;
block|}
block|}
return|return;
block|}
specifier|public
name|void
name|localJobDebugger
parameter_list|(
name|int
name|exitVal
parameter_list|,
name|String
name|taskId
parameter_list|)
block|{
name|StringBuilder
name|sb
init|=
operator|new
name|StringBuilder
argument_list|()
decl_stmt|;
name|sb
operator|.
name|append
argument_list|(
literal|"\n"
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
literal|"Task failed!\n"
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
literal|"Task ID:\n  "
operator|+
name|taskId
operator|+
literal|"\n\n"
argument_list|)
expr_stmt|;
name|sb
operator|.
name|append
argument_list|(
literal|"Logs:\n"
argument_list|)
expr_stmt|;
name|console
operator|.
name|printError
argument_list|(
name|sb
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
for|for
control|(
name|Appender
name|a
range|:
name|Collections
operator|.
name|list
argument_list|(
operator|(
name|Enumeration
argument_list|<
name|Appender
argument_list|>
operator|)
name|LogManager
operator|.
name|getRootLogger
argument_list|()
operator|.
name|getAllAppenders
argument_list|()
argument_list|)
control|)
block|{
if|if
condition|(
name|a
operator|instanceof
name|FileAppender
condition|)
block|{
name|console
operator|.
name|printError
argument_list|(
operator|(
operator|(
name|FileAppender
operator|)
name|a
operator|)
operator|.
name|getFile
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
block|}
specifier|public
name|int
name|progressLocal
parameter_list|(
name|Process
name|runningJob
parameter_list|,
name|String
name|taskId
parameter_list|)
block|{
name|int
name|exitVal
init|=
operator|-
literal|101
decl_stmt|;
try|try
block|{
name|exitVal
operator|=
name|runningJob
operator|.
name|waitFor
argument_list|()
expr_stmt|;
comment|//TODO: poll periodically
block|}
catch|catch
parameter_list|(
name|InterruptedException
name|e
parameter_list|)
block|{     }
if|if
condition|(
name|exitVal
operator|!=
literal|0
condition|)
block|{
name|console
operator|.
name|printError
argument_list|(
literal|"Execution failed with exit status: "
operator|+
name|exitVal
argument_list|)
expr_stmt|;
name|console
operator|.
name|printError
argument_list|(
literal|"Obtaining error information"
argument_list|)
expr_stmt|;
if|if
condition|(
name|HiveConf
operator|.
name|getBoolVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|SHOW_JOB_FAIL_DEBUG_INFO
argument_list|)
condition|)
block|{
comment|// Since local jobs are run sequentially, all relevant information is already available
comment|// Therefore, no need to fetch job debug info asynchronously
name|localJobDebugger
argument_list|(
name|exitVal
argument_list|,
name|taskId
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
name|console
operator|.
name|printInfo
argument_list|(
literal|"Execution completed successfully"
argument_list|)
expr_stmt|;
name|console
operator|.
name|printInfo
argument_list|(
literal|"Mapred Local Task Succeeded . Convert the Join into MapJoin"
argument_list|)
expr_stmt|;
block|}
return|return
name|exitVal
return|;
block|}
specifier|public
name|int
name|progress
parameter_list|(
name|RunningJob
name|rj
parameter_list|,
name|JobClient
name|jc
parameter_list|)
throws|throws
name|IOException
block|{
name|jobId
operator|=
name|rj
operator|.
name|getJobID
argument_list|()
expr_stmt|;
name|int
name|returnVal
init|=
literal|0
decl_stmt|;
comment|// remove the pwd from conf file so that job tracker doesn't show this
comment|// logs
name|String
name|pwd
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|METASTOREPWD
argument_list|)
decl_stmt|;
if|if
condition|(
name|pwd
operator|!=
literal|null
condition|)
block|{
name|HiveConf
operator|.
name|setVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|METASTOREPWD
argument_list|,
literal|"HIVE"
argument_list|)
expr_stmt|;
block|}
comment|// replace it back
if|if
condition|(
name|pwd
operator|!=
literal|null
condition|)
block|{
name|HiveConf
operator|.
name|setVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|METASTOREPWD
argument_list|,
name|pwd
argument_list|)
expr_stmt|;
block|}
comment|// add to list of running jobs to kill in case of abnormal shutdown
name|runningJobKillURIs
operator|.
name|put
argument_list|(
name|rj
operator|.
name|getJobID
argument_list|()
argument_list|,
name|rj
operator|.
name|getTrackingURL
argument_list|()
operator|+
literal|"&action=kill"
argument_list|)
expr_stmt|;
name|ExecDriverTaskHandle
name|th
init|=
operator|new
name|ExecDriverTaskHandle
argument_list|(
name|jc
argument_list|,
name|rj
argument_list|)
decl_stmt|;
name|jobInfo
argument_list|(
name|rj
argument_list|)
expr_stmt|;
name|MapRedStats
name|mapRedStats
init|=
name|progress
argument_list|(
name|th
argument_list|)
decl_stmt|;
comment|// Not always there is a SessionState. Sometimes ExeDriver is directly invoked
comment|// for special modes. In that case, SessionState.get() is empty.
if|if
condition|(
name|SessionState
operator|.
name|get
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|SessionState
operator|.
name|get
argument_list|()
operator|.
name|getLastMapRedStatsList
argument_list|()
operator|.
name|add
argument_list|(
name|mapRedStats
argument_list|)
expr_stmt|;
block|}
name|boolean
name|success
init|=
name|mapRedStats
operator|.
name|isSuccess
argument_list|()
decl_stmt|;
name|String
name|statusMesg
init|=
name|getJobEndMsg
argument_list|(
name|rj
operator|.
name|getJobID
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|success
condition|)
block|{
name|statusMesg
operator|+=
literal|" with errors"
expr_stmt|;
name|returnVal
operator|=
literal|2
expr_stmt|;
name|console
operator|.
name|printError
argument_list|(
name|statusMesg
argument_list|)
expr_stmt|;
if|if
condition|(
name|HiveConf
operator|.
name|getBoolVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|SHOW_JOB_FAIL_DEBUG_INFO
argument_list|)
operator|||
name|HiveConf
operator|.
name|getBoolVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|JOB_DEBUG_CAPTURE_STACKTRACES
argument_list|)
condition|)
block|{
try|try
block|{
name|JobDebugger
name|jd
decl_stmt|;
if|if
condition|(
name|SessionState
operator|.
name|get
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|jd
operator|=
operator|new
name|JobDebugger
argument_list|(
name|job
argument_list|,
name|rj
argument_list|,
name|console
argument_list|,
name|SessionState
operator|.
name|get
argument_list|()
operator|.
name|getStackTraces
argument_list|()
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|jd
operator|=
operator|new
name|JobDebugger
argument_list|(
name|job
argument_list|,
name|rj
argument_list|,
name|console
argument_list|)
expr_stmt|;
block|}
name|Thread
name|t
init|=
operator|new
name|Thread
argument_list|(
name|jd
argument_list|)
decl_stmt|;
name|t
operator|.
name|start
argument_list|()
expr_stmt|;
name|t
operator|.
name|join
argument_list|(
name|HiveConf
operator|.
name|getIntVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|JOB_DEBUG_TIMEOUT
argument_list|)
argument_list|)
expr_stmt|;
name|int
name|ec
init|=
name|jd
operator|.
name|getErrorCode
argument_list|()
decl_stmt|;
if|if
condition|(
name|ec
operator|>
literal|0
condition|)
block|{
name|returnVal
operator|=
name|ec
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|InterruptedException
name|e
parameter_list|)
block|{
name|console
operator|.
name|printError
argument_list|(
literal|"Timed out trying to grab more detailed job failure"
operator|+
literal|" information, please check jobtracker for more info"
argument_list|)
expr_stmt|;
block|}
block|}
block|}
else|else
block|{
name|console
operator|.
name|printInfo
argument_list|(
name|statusMesg
argument_list|)
expr_stmt|;
block|}
return|return
name|returnVal
return|;
block|}
specifier|private
name|Map
argument_list|<
name|String
argument_list|,
name|Double
argument_list|>
name|extractAllCounterValues
parameter_list|(
name|Counters
name|counters
parameter_list|)
block|{
name|Map
argument_list|<
name|String
argument_list|,
name|Double
argument_list|>
name|exctractedCounters
init|=
operator|new
name|HashMap
argument_list|<
name|String
argument_list|,
name|Double
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|Counters
operator|.
name|Group
name|cg
range|:
name|counters
control|)
block|{
for|for
control|(
name|Counter
name|c
range|:
name|cg
control|)
block|{
name|exctractedCounters
operator|.
name|put
argument_list|(
name|cg
operator|.
name|getName
argument_list|()
operator|+
literal|"::"
operator|+
name|c
operator|.
name|getName
argument_list|()
argument_list|,
operator|new
name|Double
argument_list|(
name|c
operator|.
name|getCounter
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|exctractedCounters
return|;
block|}
specifier|private
name|List
argument_list|<
name|ClientStatsPublisher
argument_list|>
name|getClientStatPublishers
parameter_list|()
block|{
name|List
argument_list|<
name|ClientStatsPublisher
argument_list|>
name|clientStatsPublishers
init|=
operator|new
name|ArrayList
argument_list|<
name|ClientStatsPublisher
argument_list|>
argument_list|()
decl_stmt|;
name|String
name|confString
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|job
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|CLIENTSTATSPUBLISHERS
argument_list|)
decl_stmt|;
name|confString
operator|=
name|confString
operator|.
name|trim
argument_list|()
expr_stmt|;
if|if
condition|(
name|confString
operator|.
name|equals
argument_list|(
literal|""
argument_list|)
condition|)
block|{
return|return
name|clientStatsPublishers
return|;
block|}
name|String
index|[]
name|clientStatsPublisherClasses
init|=
name|confString
operator|.
name|split
argument_list|(
literal|","
argument_list|)
decl_stmt|;
for|for
control|(
name|String
name|clientStatsPublisherClass
range|:
name|clientStatsPublisherClasses
control|)
block|{
try|try
block|{
name|clientStatsPublishers
operator|.
name|add
argument_list|(
operator|(
name|ClientStatsPublisher
operator|)
name|Class
operator|.
name|forName
argument_list|(
name|clientStatsPublisherClass
operator|.
name|trim
argument_list|()
argument_list|,
literal|true
argument_list|,
name|JavaUtils
operator|.
name|getClassLoader
argument_list|()
argument_list|)
operator|.
name|newInstance
argument_list|()
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
name|e
operator|.
name|getClass
argument_list|()
operator|.
name|getName
argument_list|()
operator|+
literal|" occured when trying to create class: "
operator|+
name|clientStatsPublisherClass
operator|.
name|trim
argument_list|()
operator|+
literal|" implementing ClientStatsPublisher interface"
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|warn
argument_list|(
literal|"The exception message is: "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|warn
argument_list|(
literal|"Program will continue, but without this ClientStatsPublisher working"
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|clientStatsPublishers
return|;
block|}
block|}
end_class

end_unit

