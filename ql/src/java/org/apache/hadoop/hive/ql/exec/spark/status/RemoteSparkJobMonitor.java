begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/**  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|spark
operator|.
name|status
package|;
end_package

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Arrays
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|base
operator|.
name|Preconditions
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|Utilities
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|spark
operator|.
name|status
operator|.
name|impl
operator|.
name|RemoteSparkJobStatus
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|log
operator|.
name|PerfLogger
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hive
operator|.
name|spark
operator|.
name|client
operator|.
name|JobHandle
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|spark
operator|.
name|JobExecutionStatus
import|;
end_import

begin_comment
comment|/**  * RemoteSparkJobMonitor monitor a RSC remote job status in a loop until job finished/failed/killed.  * It print current job status to console and sleep current thread between monitor interval.  */
end_comment

begin_class
specifier|public
class|class
name|RemoteSparkJobMonitor
extends|extends
name|SparkJobMonitor
block|{
specifier|private
name|int
name|sparkJobMaxTaskCount
init|=
operator|-
literal|1
decl_stmt|;
specifier|private
name|int
name|totalTaskCount
init|=
literal|0
decl_stmt|;
specifier|private
name|RemoteSparkJobStatus
name|sparkJobStatus
decl_stmt|;
specifier|private
specifier|final
name|HiveConf
name|hiveConf
decl_stmt|;
specifier|public
name|RemoteSparkJobMonitor
parameter_list|(
name|HiveConf
name|hiveConf
parameter_list|,
name|RemoteSparkJobStatus
name|sparkJobStatus
parameter_list|)
block|{
name|super
argument_list|(
name|hiveConf
argument_list|)
expr_stmt|;
name|this
operator|.
name|sparkJobStatus
operator|=
name|sparkJobStatus
expr_stmt|;
name|this
operator|.
name|hiveConf
operator|=
name|hiveConf
expr_stmt|;
name|sparkJobMaxTaskCount
operator|=
name|hiveConf
operator|.
name|getIntVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|SPARK_JOB_MAX_TASKS
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|int
name|startMonitor
parameter_list|()
block|{
name|boolean
name|running
init|=
literal|false
decl_stmt|;
name|boolean
name|done
init|=
literal|false
decl_stmt|;
name|int
name|rc
init|=
literal|0
decl_stmt|;
name|Map
argument_list|<
name|String
argument_list|,
name|SparkStageProgress
argument_list|>
name|lastProgressMap
init|=
literal|null
decl_stmt|;
name|perfLogger
operator|.
name|PerfLogBegin
argument_list|(
name|CLASS_NAME
argument_list|,
name|PerfLogger
operator|.
name|SPARK_RUN_JOB
argument_list|)
expr_stmt|;
name|perfLogger
operator|.
name|PerfLogBegin
argument_list|(
name|CLASS_NAME
argument_list|,
name|PerfLogger
operator|.
name|SPARK_SUBMIT_TO_RUNNING
argument_list|)
expr_stmt|;
name|startTime
operator|=
name|System
operator|.
name|currentTimeMillis
argument_list|()
expr_stmt|;
while|while
condition|(
literal|true
condition|)
block|{
try|try
block|{
name|JobHandle
operator|.
name|State
name|state
init|=
name|sparkJobStatus
operator|.
name|getRemoteJobState
argument_list|()
decl_stmt|;
switch|switch
condition|(
name|state
condition|)
block|{
case|case
name|SENT
case|:
case|case
name|QUEUED
case|:
name|long
name|timeCount
init|=
operator|(
name|System
operator|.
name|currentTimeMillis
argument_list|()
operator|-
name|startTime
operator|)
operator|/
literal|1000
decl_stmt|;
if|if
condition|(
operator|(
name|timeCount
operator|>
name|monitorTimeoutInterval
operator|)
condition|)
block|{
name|console
operator|.
name|printError
argument_list|(
literal|"Job hasn't been submitted after "
operator|+
name|timeCount
operator|+
literal|"s."
operator|+
literal|" Aborting it.\nPossible reasons include network issues, "
operator|+
literal|"errors in remote driver or the cluster has no available resources, etc.\n"
operator|+
literal|"Please check YARN or Spark driver's logs for further information."
argument_list|)
expr_stmt|;
name|console
operator|.
name|printError
argument_list|(
literal|"Status: "
operator|+
name|state
argument_list|)
expr_stmt|;
name|running
operator|=
literal|false
expr_stmt|;
name|done
operator|=
literal|true
expr_stmt|;
name|rc
operator|=
literal|2
expr_stmt|;
block|}
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|console
operator|.
name|printInfo
argument_list|(
literal|"state = "
operator|+
name|state
argument_list|)
expr_stmt|;
block|}
break|break;
case|case
name|STARTED
case|:
name|JobExecutionStatus
name|sparkJobState
init|=
name|sparkJobStatus
operator|.
name|getState
argument_list|()
decl_stmt|;
if|if
condition|(
name|sparkJobState
operator|==
name|JobExecutionStatus
operator|.
name|RUNNING
condition|)
block|{
name|Map
argument_list|<
name|String
argument_list|,
name|SparkStageProgress
argument_list|>
name|progressMap
init|=
name|sparkJobStatus
operator|.
name|getSparkStageProgress
argument_list|()
decl_stmt|;
if|if
condition|(
operator|!
name|running
condition|)
block|{
name|perfLogger
operator|.
name|PerfLogEnd
argument_list|(
name|CLASS_NAME
argument_list|,
name|PerfLogger
operator|.
name|SPARK_SUBMIT_TO_RUNNING
argument_list|)
expr_stmt|;
name|printAppInfo
argument_list|()
expr_stmt|;
comment|// print job stages.
name|console
operator|.
name|printInfo
argument_list|(
literal|"\nQuery Hive on Spark job["
operator|+
name|sparkJobStatus
operator|.
name|getJobId
argument_list|()
operator|+
literal|"] stages: "
operator|+
name|Arrays
operator|.
name|toString
argument_list|(
name|sparkJobStatus
operator|.
name|getStageIds
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
name|console
operator|.
name|printInfo
argument_list|(
literal|"\nStatus: Running (Hive on Spark job["
operator|+
name|sparkJobStatus
operator|.
name|getJobId
argument_list|()
operator|+
literal|"])"
argument_list|)
expr_stmt|;
name|running
operator|=
literal|true
expr_stmt|;
name|String
name|format
init|=
literal|"Job Progress Format\nCurrentTime StageId_StageAttemptId: "
operator|+
literal|"SucceededTasksCount(+RunningTasksCount-FailedTasksCount)/TotalTasksCount"
decl_stmt|;
if|if
condition|(
operator|!
name|inPlaceUpdate
condition|)
block|{
name|console
operator|.
name|printInfo
argument_list|(
name|format
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|console
operator|.
name|logInfo
argument_list|(
name|format
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
comment|// Count the number of tasks, and kill application if it goes beyond the limit.
if|if
condition|(
name|sparkJobMaxTaskCount
operator|!=
operator|-
literal|1
operator|&&
name|totalTaskCount
operator|==
literal|0
condition|)
block|{
name|totalTaskCount
operator|=
name|getTotalTaskCount
argument_list|(
name|progressMap
argument_list|)
expr_stmt|;
if|if
condition|(
name|totalTaskCount
operator|>
name|sparkJobMaxTaskCount
condition|)
block|{
name|rc
operator|=
literal|4
expr_stmt|;
name|done
operator|=
literal|true
expr_stmt|;
name|console
operator|.
name|printInfo
argument_list|(
literal|"\nThe total number of task in the Spark job ["
operator|+
name|totalTaskCount
operator|+
literal|"] is greater than the limit ["
operator|+
name|sparkJobMaxTaskCount
operator|+
literal|"]. The Spark job will be cancelled."
argument_list|)
expr_stmt|;
block|}
block|}
block|}
name|printStatus
argument_list|(
name|progressMap
argument_list|,
name|lastProgressMap
argument_list|)
expr_stmt|;
name|lastProgressMap
operator|=
name|progressMap
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|sparkJobState
operator|==
literal|null
condition|)
block|{
comment|// in case the remote context crashes between JobStarted and JobSubmitted
name|Preconditions
operator|.
name|checkState
argument_list|(
name|sparkJobStatus
operator|.
name|isRemoteActive
argument_list|()
argument_list|,
literal|"Remote context becomes inactive."
argument_list|)
expr_stmt|;
block|}
break|break;
case|case
name|SUCCEEDED
case|:
name|Map
argument_list|<
name|String
argument_list|,
name|SparkStageProgress
argument_list|>
name|progressMap
init|=
name|sparkJobStatus
operator|.
name|getSparkStageProgress
argument_list|()
decl_stmt|;
name|printStatus
argument_list|(
name|progressMap
argument_list|,
name|lastProgressMap
argument_list|)
expr_stmt|;
name|lastProgressMap
operator|=
name|progressMap
expr_stmt|;
name|double
name|duration
init|=
operator|(
name|System
operator|.
name|currentTimeMillis
argument_list|()
operator|-
name|startTime
operator|)
operator|/
literal|1000.0
decl_stmt|;
name|console
operator|.
name|printInfo
argument_list|(
literal|"Status: Finished successfully in "
operator|+
name|String
operator|.
name|format
argument_list|(
literal|"%.2f seconds"
argument_list|,
name|duration
argument_list|)
argument_list|)
expr_stmt|;
name|running
operator|=
literal|false
expr_stmt|;
name|done
operator|=
literal|true
expr_stmt|;
break|break;
case|case
name|FAILED
case|:
name|String
name|detail
init|=
name|sparkJobStatus
operator|.
name|getError
argument_list|()
operator|.
name|getMessage
argument_list|()
decl_stmt|;
name|StringBuilder
name|errBuilder
init|=
operator|new
name|StringBuilder
argument_list|()
decl_stmt|;
name|errBuilder
operator|.
name|append
argument_list|(
literal|"Job failed with "
argument_list|)
expr_stmt|;
if|if
condition|(
name|detail
operator|==
literal|null
condition|)
block|{
name|errBuilder
operator|.
name|append
argument_list|(
literal|"UNKNOWN reason"
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// We SerDe the Throwable as String, parse it for the root cause
specifier|final
name|String
name|CAUSE_CAPTION
init|=
literal|"Caused by: "
decl_stmt|;
name|int
name|index
init|=
name|detail
operator|.
name|lastIndexOf
argument_list|(
name|CAUSE_CAPTION
argument_list|)
decl_stmt|;
if|if
condition|(
name|index
operator|!=
operator|-
literal|1
condition|)
block|{
name|String
name|rootCause
init|=
name|detail
operator|.
name|substring
argument_list|(
name|index
operator|+
name|CAUSE_CAPTION
operator|.
name|length
argument_list|()
argument_list|)
decl_stmt|;
name|index
operator|=
name|rootCause
operator|.
name|indexOf
argument_list|(
name|System
operator|.
name|getProperty
argument_list|(
literal|"line.separator"
argument_list|)
argument_list|)
expr_stmt|;
if|if
condition|(
name|index
operator|!=
operator|-
literal|1
condition|)
block|{
name|errBuilder
operator|.
name|append
argument_list|(
name|rootCause
operator|.
name|substring
argument_list|(
literal|0
argument_list|,
name|index
argument_list|)
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|errBuilder
operator|.
name|append
argument_list|(
name|rootCause
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
name|errBuilder
operator|.
name|append
argument_list|(
name|detail
argument_list|)
expr_stmt|;
block|}
name|detail
operator|=
name|System
operator|.
name|getProperty
argument_list|(
literal|"line.separator"
argument_list|)
operator|+
name|detail
expr_stmt|;
block|}
name|console
operator|.
name|printError
argument_list|(
name|errBuilder
operator|.
name|toString
argument_list|()
argument_list|,
name|detail
argument_list|)
expr_stmt|;
name|running
operator|=
literal|false
expr_stmt|;
name|done
operator|=
literal|true
expr_stmt|;
name|rc
operator|=
literal|3
expr_stmt|;
break|break;
block|}
if|if
condition|(
operator|!
name|done
condition|)
block|{
name|Thread
operator|.
name|sleep
argument_list|(
name|checkInterval
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|String
name|msg
init|=
literal|" with exception '"
operator|+
name|Utilities
operator|.
name|getNameMessage
argument_list|(
name|e
argument_list|)
operator|+
literal|"'"
decl_stmt|;
name|msg
operator|=
literal|"Failed to monitor Job["
operator|+
name|sparkJobStatus
operator|.
name|getJobId
argument_list|()
operator|+
literal|"]"
operator|+
name|msg
expr_stmt|;
comment|// Has to use full name to make sure it does not conflict with
comment|// org.apache.commons.lang.StringUtils
name|LOG
operator|.
name|error
argument_list|(
name|msg
argument_list|,
name|e
argument_list|)
expr_stmt|;
name|console
operator|.
name|printError
argument_list|(
name|msg
argument_list|,
literal|"\n"
operator|+
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
name|rc
operator|=
literal|1
expr_stmt|;
name|done
operator|=
literal|true
expr_stmt|;
name|sparkJobStatus
operator|.
name|setError
argument_list|(
name|e
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
if|if
condition|(
name|done
condition|)
block|{
break|break;
block|}
block|}
block|}
name|perfLogger
operator|.
name|PerfLogEnd
argument_list|(
name|CLASS_NAME
argument_list|,
name|PerfLogger
operator|.
name|SPARK_RUN_JOB
argument_list|)
expr_stmt|;
return|return
name|rc
return|;
block|}
specifier|private
name|void
name|printAppInfo
parameter_list|()
block|{
name|String
name|sparkMaster
init|=
name|hiveConf
operator|.
name|get
argument_list|(
literal|"spark.master"
argument_list|)
decl_stmt|;
if|if
condition|(
name|sparkMaster
operator|!=
literal|null
operator|&&
name|sparkMaster
operator|.
name|startsWith
argument_list|(
literal|"yarn"
argument_list|)
condition|)
block|{
name|String
name|appID
init|=
name|sparkJobStatus
operator|.
name|getAppID
argument_list|()
decl_stmt|;
if|if
condition|(
name|appID
operator|!=
literal|null
condition|)
block|{
name|console
operator|.
name|printInfo
argument_list|(
literal|"Running with YARN Application = "
operator|+
name|appID
argument_list|)
expr_stmt|;
name|console
operator|.
name|printInfo
argument_list|(
literal|"Kill Command = "
operator|+
name|HiveConf
operator|.
name|getVar
argument_list|(
name|hiveConf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|YARNBIN
argument_list|)
operator|+
literal|" application -kill "
operator|+
name|appID
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
end_class

end_unit

