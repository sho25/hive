begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/*  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|spark
operator|.
name|status
package|;
end_package

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Arrays
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|base
operator|.
name|Preconditions
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|ErrorMsg
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|Utilities
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|spark
operator|.
name|status
operator|.
name|impl
operator|.
name|RemoteSparkJobStatus
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|log
operator|.
name|PerfLogger
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|HiveException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hive
operator|.
name|spark
operator|.
name|client
operator|.
name|JobHandle
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|spark
operator|.
name|JobExecutionStatus
import|;
end_import

begin_comment
comment|/**  * RemoteSparkJobMonitor monitor a RSC remote job status in a loop until job finished/failed/killed.  * It print current job status to console and sleep current thread between monitor interval.  */
end_comment

begin_class
specifier|public
class|class
name|RemoteSparkJobMonitor
extends|extends
name|SparkJobMonitor
block|{
specifier|private
name|int
name|sparkJobMaxTaskCount
init|=
operator|-
literal|1
decl_stmt|;
specifier|private
name|int
name|sparkStageMaxTaskCount
init|=
operator|-
literal|1
decl_stmt|;
specifier|private
name|int
name|totalTaskCount
init|=
literal|0
decl_stmt|;
specifier|private
name|int
name|stageMaxTaskCount
init|=
literal|0
decl_stmt|;
specifier|private
name|RemoteSparkJobStatus
name|sparkJobStatus
decl_stmt|;
specifier|private
specifier|final
name|HiveConf
name|hiveConf
decl_stmt|;
specifier|public
name|RemoteSparkJobMonitor
parameter_list|(
name|HiveConf
name|hiveConf
parameter_list|,
name|RemoteSparkJobStatus
name|sparkJobStatus
parameter_list|)
block|{
name|super
argument_list|(
name|hiveConf
argument_list|)
expr_stmt|;
name|this
operator|.
name|sparkJobStatus
operator|=
name|sparkJobStatus
expr_stmt|;
name|this
operator|.
name|hiveConf
operator|=
name|hiveConf
expr_stmt|;
name|sparkJobMaxTaskCount
operator|=
name|hiveConf
operator|.
name|getIntVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|SPARK_JOB_MAX_TASKS
argument_list|)
expr_stmt|;
name|sparkStageMaxTaskCount
operator|=
name|hiveConf
operator|.
name|getIntVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|SPARK_STAGE_MAX_TASKS
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|int
name|startMonitor
parameter_list|()
block|{
name|boolean
name|running
init|=
literal|false
decl_stmt|;
name|boolean
name|done
init|=
literal|false
decl_stmt|;
name|int
name|rc
init|=
literal|0
decl_stmt|;
name|Map
argument_list|<
name|SparkStage
argument_list|,
name|SparkStageProgress
argument_list|>
name|lastProgressMap
init|=
literal|null
decl_stmt|;
name|perfLogger
operator|.
name|PerfLogBegin
argument_list|(
name|CLASS_NAME
argument_list|,
name|PerfLogger
operator|.
name|SPARK_RUN_JOB
argument_list|)
expr_stmt|;
name|perfLogger
operator|.
name|PerfLogBegin
argument_list|(
name|CLASS_NAME
argument_list|,
name|PerfLogger
operator|.
name|SPARK_SUBMIT_TO_RUNNING
argument_list|)
expr_stmt|;
name|startTime
operator|=
name|System
operator|.
name|currentTimeMillis
argument_list|()
expr_stmt|;
name|JobHandle
operator|.
name|State
name|state
init|=
literal|null
decl_stmt|;
while|while
condition|(
literal|true
condition|)
block|{
try|try
block|{
name|state
operator|=
name|sparkJobStatus
operator|.
name|getRemoteJobState
argument_list|()
expr_stmt|;
name|Preconditions
operator|.
name|checkState
argument_list|(
name|sparkJobStatus
operator|.
name|isRemoteActive
argument_list|()
argument_list|,
literal|"Connection to remote Spark driver was lost"
argument_list|)
expr_stmt|;
switch|switch
condition|(
name|state
condition|)
block|{
case|case
name|SENT
case|:
case|case
name|QUEUED
case|:
name|long
name|timeCount
init|=
operator|(
name|System
operator|.
name|currentTimeMillis
argument_list|()
operator|-
name|startTime
operator|)
operator|/
literal|1000
decl_stmt|;
if|if
condition|(
operator|(
name|timeCount
operator|>
name|monitorTimeoutInterval
operator|)
condition|)
block|{
name|HiveException
name|he
init|=
operator|new
name|HiveException
argument_list|(
name|ErrorMsg
operator|.
name|SPARK_JOB_MONITOR_TIMEOUT
argument_list|,
name|Long
operator|.
name|toString
argument_list|(
name|timeCount
argument_list|)
argument_list|)
decl_stmt|;
name|sparkJobStatus
operator|.
name|setMonitorError
argument_list|(
name|he
argument_list|)
expr_stmt|;
name|running
operator|=
literal|false
expr_stmt|;
name|done
operator|=
literal|true
expr_stmt|;
name|rc
operator|=
literal|2
expr_stmt|;
block|}
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|console
operator|.
name|printInfo
argument_list|(
literal|"Spark job state = "
operator|+
name|state
argument_list|)
expr_stmt|;
block|}
break|break;
case|case
name|STARTED
case|:
name|JobExecutionStatus
name|sparkJobState
init|=
name|sparkJobStatus
operator|.
name|getState
argument_list|()
decl_stmt|;
if|if
condition|(
name|sparkJobState
operator|==
name|JobExecutionStatus
operator|.
name|RUNNING
condition|)
block|{
name|Map
argument_list|<
name|SparkStage
argument_list|,
name|SparkStageProgress
argument_list|>
name|progressMap
init|=
name|sparkJobStatus
operator|.
name|getSparkStageProgress
argument_list|()
decl_stmt|;
if|if
condition|(
operator|!
name|running
condition|)
block|{
name|perfLogger
operator|.
name|PerfLogEnd
argument_list|(
name|CLASS_NAME
argument_list|,
name|PerfLogger
operator|.
name|SPARK_SUBMIT_TO_RUNNING
argument_list|)
expr_stmt|;
name|printAppInfo
argument_list|()
expr_stmt|;
name|console
operator|.
name|printInfo
argument_list|(
literal|"Hive on Spark Session Web UI URL: "
operator|+
name|sparkJobStatus
operator|.
name|getWebUIURL
argument_list|()
argument_list|)
expr_stmt|;
comment|// print job stages.
name|console
operator|.
name|printInfo
argument_list|(
literal|"\nQuery Hive on Spark job["
operator|+
name|sparkJobStatus
operator|.
name|getJobId
argument_list|()
operator|+
literal|"] stages: "
operator|+
name|Arrays
operator|.
name|toString
argument_list|(
name|sparkJobStatus
operator|.
name|getStageIds
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
name|console
operator|.
name|printInfo
argument_list|(
literal|"Spark job["
operator|+
name|sparkJobStatus
operator|.
name|getJobId
argument_list|()
operator|+
literal|"] status = RUNNING"
argument_list|)
expr_stmt|;
name|running
operator|=
literal|true
expr_stmt|;
name|String
name|format
init|=
literal|"Job Progress Format\nCurrentTime StageId_StageAttemptId: "
operator|+
literal|"SucceededTasksCount(+RunningTasksCount-FailedTasksCount)/TotalTasksCount"
decl_stmt|;
if|if
condition|(
operator|!
name|inPlaceUpdate
condition|)
block|{
name|console
operator|.
name|printInfo
argument_list|(
name|format
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|console
operator|.
name|logInfo
argument_list|(
name|format
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
comment|// Get the maximum of the number of tasks in the stages of the job and cancel the job if it goes beyond the limit.
if|if
condition|(
name|sparkStageMaxTaskCount
operator|!=
operator|-
literal|1
operator|&&
name|stageMaxTaskCount
operator|==
literal|0
condition|)
block|{
name|stageMaxTaskCount
operator|=
name|getStageMaxTaskCount
argument_list|(
name|progressMap
argument_list|)
expr_stmt|;
if|if
condition|(
name|stageMaxTaskCount
operator|>
name|sparkStageMaxTaskCount
condition|)
block|{
name|rc
operator|=
literal|4
expr_stmt|;
name|done
operator|=
literal|true
expr_stmt|;
name|console
operator|.
name|printInfo
argument_list|(
literal|"\nThe number of task in one stage of the Spark job ["
operator|+
name|stageMaxTaskCount
operator|+
literal|"] is greater than the limit ["
operator|+
name|sparkStageMaxTaskCount
operator|+
literal|"]. The Spark job will be cancelled."
argument_list|)
expr_stmt|;
block|}
block|}
comment|// Count the number of tasks, and kill application if it goes beyond the limit.
if|if
condition|(
name|sparkJobMaxTaskCount
operator|!=
operator|-
literal|1
operator|&&
name|totalTaskCount
operator|==
literal|0
condition|)
block|{
name|totalTaskCount
operator|=
name|getTotalTaskCount
argument_list|(
name|progressMap
argument_list|)
expr_stmt|;
if|if
condition|(
name|totalTaskCount
operator|>
name|sparkJobMaxTaskCount
condition|)
block|{
name|rc
operator|=
literal|4
expr_stmt|;
name|done
operator|=
literal|true
expr_stmt|;
name|console
operator|.
name|printInfo
argument_list|(
literal|"\nThe total number of task in the Spark job ["
operator|+
name|totalTaskCount
operator|+
literal|"] is greater than the limit ["
operator|+
name|sparkJobMaxTaskCount
operator|+
literal|"]. The Spark job will be cancelled."
argument_list|)
expr_stmt|;
block|}
block|}
block|}
name|updateFunction
operator|.
name|printStatus
argument_list|(
name|progressMap
argument_list|,
name|lastProgressMap
argument_list|)
expr_stmt|;
name|lastProgressMap
operator|=
name|progressMap
expr_stmt|;
block|}
break|break;
case|case
name|SUCCEEDED
case|:
name|Map
argument_list|<
name|SparkStage
argument_list|,
name|SparkStageProgress
argument_list|>
name|progressMap
init|=
name|sparkJobStatus
operator|.
name|getSparkStageProgress
argument_list|()
decl_stmt|;
name|updateFunction
operator|.
name|printStatus
argument_list|(
name|progressMap
argument_list|,
name|lastProgressMap
argument_list|)
expr_stmt|;
name|lastProgressMap
operator|=
name|progressMap
expr_stmt|;
name|double
name|duration
init|=
operator|(
name|System
operator|.
name|currentTimeMillis
argument_list|()
operator|-
name|startTime
operator|)
operator|/
literal|1000.0
decl_stmt|;
name|console
operator|.
name|printInfo
argument_list|(
literal|"Spark job["
operator|+
name|sparkJobStatus
operator|.
name|getJobId
argument_list|()
operator|+
literal|"] finished successfully in "
operator|+
name|String
operator|.
name|format
argument_list|(
literal|"%.2f second(s)"
argument_list|,
name|duration
argument_list|)
argument_list|)
expr_stmt|;
name|running
operator|=
literal|false
expr_stmt|;
name|done
operator|=
literal|true
expr_stmt|;
break|break;
case|case
name|FAILED
case|:
name|LOG
operator|.
name|error
argument_list|(
literal|"Spark job["
operator|+
name|sparkJobStatus
operator|.
name|getJobId
argument_list|()
operator|+
literal|"] failed"
argument_list|,
name|sparkJobStatus
operator|.
name|getSparkJobException
argument_list|()
argument_list|)
expr_stmt|;
name|running
operator|=
literal|false
expr_stmt|;
name|done
operator|=
literal|true
expr_stmt|;
name|rc
operator|=
literal|3
expr_stmt|;
break|break;
case|case
name|CANCELLED
case|:
name|console
operator|.
name|printInfo
argument_list|(
literal|"Spark job["
operator|+
name|sparkJobStatus
operator|.
name|getJobId
argument_list|()
operator|+
literal|" was cancelled"
argument_list|)
expr_stmt|;
name|running
operator|=
literal|false
expr_stmt|;
name|done
operator|=
literal|true
expr_stmt|;
name|rc
operator|=
literal|3
expr_stmt|;
break|break;
block|}
if|if
condition|(
operator|!
name|done
condition|)
block|{
name|Thread
operator|.
name|sleep
argument_list|(
name|checkInterval
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|Exception
name|finalException
init|=
name|e
decl_stmt|;
if|if
condition|(
name|e
operator|instanceof
name|InterruptedException
operator|||
operator|(
name|e
operator|instanceof
name|HiveException
operator|&&
name|e
operator|.
name|getCause
argument_list|()
operator|instanceof
name|InterruptedException
operator|)
condition|)
block|{
name|finalException
operator|=
operator|new
name|HiveException
argument_list|(
name|e
argument_list|,
name|ErrorMsg
operator|.
name|SPARK_JOB_INTERRUPTED
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|warn
argument_list|(
literal|"Interrupted while monitoring the Hive on Spark application, exiting"
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|String
name|msg
init|=
literal|" with exception '"
operator|+
name|Utilities
operator|.
name|getNameMessage
argument_list|(
name|e
argument_list|)
operator|+
literal|"' Last known state = "
operator|+
operator|(
name|state
operator|!=
literal|null
condition|?
name|state
operator|.
name|name
argument_list|()
else|:
literal|"UNKNOWN"
operator|)
decl_stmt|;
name|msg
operator|=
literal|"Failed to monitor Job["
operator|+
name|sparkJobStatus
operator|.
name|getJobId
argument_list|()
operator|+
literal|"]"
operator|+
name|msg
expr_stmt|;
comment|// Has to use full name to make sure it does not conflict with
comment|// org.apache.commons.lang.StringUtils
name|console
operator|.
name|printError
argument_list|(
name|msg
argument_list|,
literal|"\n"
operator|+
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|StringUtils
operator|.
name|stringifyException
argument_list|(
name|e
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|rc
operator|=
literal|1
expr_stmt|;
name|done
operator|=
literal|true
expr_stmt|;
name|sparkJobStatus
operator|.
name|setMonitorError
argument_list|(
name|finalException
argument_list|)
expr_stmt|;
block|}
finally|finally
block|{
if|if
condition|(
name|done
condition|)
block|{
break|break;
block|}
block|}
block|}
name|perfLogger
operator|.
name|PerfLogEnd
argument_list|(
name|CLASS_NAME
argument_list|,
name|PerfLogger
operator|.
name|SPARK_RUN_JOB
argument_list|)
expr_stmt|;
return|return
name|rc
return|;
block|}
specifier|private
name|void
name|printAppInfo
parameter_list|()
block|{
name|String
name|sparkMaster
init|=
name|hiveConf
operator|.
name|get
argument_list|(
literal|"spark.master"
argument_list|)
decl_stmt|;
if|if
condition|(
name|sparkMaster
operator|!=
literal|null
operator|&&
name|sparkMaster
operator|.
name|startsWith
argument_list|(
literal|"yarn"
argument_list|)
condition|)
block|{
name|String
name|appID
init|=
name|sparkJobStatus
operator|.
name|getAppID
argument_list|()
decl_stmt|;
if|if
condition|(
name|appID
operator|!=
literal|null
condition|)
block|{
name|console
operator|.
name|printInfo
argument_list|(
literal|"Running with YARN Application = "
operator|+
name|appID
argument_list|)
expr_stmt|;
name|console
operator|.
name|printInfo
argument_list|(
literal|"Kill Command = "
operator|+
name|HiveConf
operator|.
name|getVar
argument_list|(
name|hiveConf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|YARNBIN
argument_list|)
operator|+
literal|" application -kill "
operator|+
name|appID
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
end_class

end_unit

