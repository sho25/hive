begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/**  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
package|;
end_package

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|Serializable
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Collection
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashSet
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|Callable
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|Future
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|lang3
operator|.
name|tuple
operator|.
name|ImmutablePair
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|lang3
operator|.
name|tuple
operator|.
name|Pair
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|Log
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|LogFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configuration
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|ObjectPair
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
operator|.
name|ConfVars
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|HashTableLoaderFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|mr
operator|.
name|ExecMapperContext
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|persistence
operator|.
name|BytesBytesMultiHashMap
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|persistence
operator|.
name|HybridHashTableContainer
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|persistence
operator|.
name|HybridHashTableContainer
operator|.
name|HashPartition
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|persistence
operator|.
name|KeyValueContainer
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|persistence
operator|.
name|MapJoinBytesTableContainer
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|persistence
operator|.
name|MapJoinBytesTableContainer
operator|.
name|KeyValueHelper
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|persistence
operator|.
name|MapJoinKey
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|persistence
operator|.
name|MapJoinObjectSerDeContext
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|persistence
operator|.
name|MapJoinRowContainer
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|persistence
operator|.
name|MapJoinTableContainer
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|persistence
operator|.
name|MapJoinTableContainer
operator|.
name|ReusableGetAdaptor
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|persistence
operator|.
name|MapJoinTableContainerSerDe
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|persistence
operator|.
name|ObjectContainer
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|persistence
operator|.
name|UnwrapRowContainer
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|spark
operator|.
name|SparkUtilities
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|HiveKey
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|log
operator|.
name|PerfLogger
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|HiveException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|JoinCondDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|JoinDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|MapJoinDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|TableDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|api
operator|.
name|OperatorType
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|session
operator|.
name|SessionState
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|SerDe
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|SerDeException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|SerDeUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|objectinspector
operator|.
name|ObjectInspector
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|BytesWritable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|Writable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hive
operator|.
name|common
operator|.
name|util
operator|.
name|ReflectionUtil
import|;
end_import

begin_comment
comment|/**  * Map side Join operator implementation.  */
end_comment

begin_class
specifier|public
class|class
name|MapJoinOperator
extends|extends
name|AbstractMapJoinOperator
argument_list|<
name|MapJoinDesc
argument_list|>
implements|implements
name|Serializable
block|{
specifier|private
specifier|static
specifier|final
name|long
name|serialVersionUID
init|=
literal|1L
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|Log
name|LOG
init|=
name|LogFactory
operator|.
name|getLog
argument_list|(
name|MapJoinOperator
operator|.
name|class
operator|.
name|getName
argument_list|()
argument_list|)
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|String
name|CLASS_NAME
init|=
name|MapJoinOperator
operator|.
name|class
operator|.
name|getName
argument_list|()
decl_stmt|;
specifier|private
specifier|final
name|PerfLogger
name|perfLogger
init|=
name|SessionState
operator|.
name|getPerfLogger
argument_list|()
decl_stmt|;
specifier|private
specifier|transient
name|String
name|cacheKey
decl_stmt|;
specifier|private
specifier|transient
name|ObjectCache
name|cache
decl_stmt|;
specifier|protected
name|HashTableLoader
name|loader
decl_stmt|;
specifier|private
name|boolean
name|loadCalled
decl_stmt|;
specifier|protected
specifier|transient
name|MapJoinTableContainer
index|[]
name|mapJoinTables
decl_stmt|;
specifier|private
specifier|transient
name|MapJoinTableContainerSerDe
index|[]
name|mapJoinTableSerdes
decl_stmt|;
specifier|private
specifier|transient
name|boolean
name|hashTblInitedOnce
decl_stmt|;
specifier|protected
specifier|transient
name|ReusableGetAdaptor
index|[]
name|hashMapRowGetters
decl_stmt|;
specifier|private
name|UnwrapRowContainer
index|[]
name|unwrapContainer
decl_stmt|;
specifier|private
specifier|transient
name|Configuration
name|hconf
decl_stmt|;
specifier|private
specifier|transient
name|boolean
name|hybridMapJoinLeftover
decl_stmt|;
comment|// whether there's spilled data to be processed
specifier|protected
specifier|transient
name|MapJoinBytesTableContainer
index|[]
name|spilledMapJoinTables
decl_stmt|;
comment|// used to hold restored
comment|// spilled small tables
specifier|protected
name|HybridHashTableContainer
name|firstSmallTable
decl_stmt|;
comment|// The first small table;
comment|// Only this table has spilled big table rows
specifier|public
name|MapJoinOperator
parameter_list|()
block|{   }
specifier|public
name|MapJoinOperator
parameter_list|(
name|AbstractMapJoinOperator
argument_list|<
name|?
extends|extends
name|MapJoinDesc
argument_list|>
name|mjop
parameter_list|)
block|{
name|super
argument_list|(
name|mjop
argument_list|)
expr_stmt|;
block|}
comment|/*    * We need the base (operator.java) implementation of start/endGroup.    * The parent class has functionality in those that map join can't use.    * Note: The mapjoin can be run in the reducer only on Tez.    */
annotation|@
name|Override
specifier|public
name|void
name|endGroup
parameter_list|()
throws|throws
name|HiveException
block|{
name|defaultEndGroup
argument_list|()
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|startGroup
parameter_list|()
throws|throws
name|HiveException
block|{
name|defaultStartGroup
argument_list|()
expr_stmt|;
block|}
specifier|protected
name|HashTableLoader
name|getHashTableLoader
parameter_list|(
name|Configuration
name|hconf
parameter_list|)
block|{
return|return
name|HashTableLoaderFactory
operator|.
name|getLoader
argument_list|(
name|hconf
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|protected
name|void
name|initializeOp
parameter_list|(
name|Configuration
name|hconf
parameter_list|)
throws|throws
name|HiveException
block|{
name|this
operator|.
name|hconf
operator|=
name|hconf
expr_stmt|;
name|unwrapContainer
operator|=
operator|new
name|UnwrapRowContainer
index|[
name|conf
operator|.
name|getTagLength
argument_list|()
index|]
expr_stmt|;
name|super
operator|.
name|initializeOp
argument_list|(
name|hconf
argument_list|)
expr_stmt|;
name|int
name|tagLen
init|=
name|conf
operator|.
name|getTagLength
argument_list|()
decl_stmt|;
comment|// On Tez only: The hash map might already be cached in the container we run
comment|// the task in. On MR: The cache is a no-op.
name|String
name|queryId
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|hconf
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEQUERYID
argument_list|)
decl_stmt|;
name|cacheKey
operator|=
literal|"HASH_MAP_"
operator|+
name|this
operator|.
name|getOperatorId
argument_list|()
operator|+
literal|"_container"
expr_stmt|;
name|cache
operator|=
name|ObjectCacheFactory
operator|.
name|getCache
argument_list|(
name|hconf
argument_list|,
name|queryId
argument_list|)
expr_stmt|;
name|loader
operator|=
name|getHashTableLoader
argument_list|(
name|hconf
argument_list|)
expr_stmt|;
name|hashMapRowGetters
operator|=
literal|null
expr_stmt|;
name|mapJoinTables
operator|=
operator|new
name|MapJoinTableContainer
index|[
name|tagLen
index|]
expr_stmt|;
name|mapJoinTableSerdes
operator|=
operator|new
name|MapJoinTableContainerSerDe
index|[
name|tagLen
index|]
expr_stmt|;
name|hashTblInitedOnce
operator|=
literal|false
expr_stmt|;
name|generateMapMetaData
argument_list|()
expr_stmt|;
specifier|final
name|ExecMapperContext
name|mapContext
init|=
name|getExecContext
argument_list|()
decl_stmt|;
specifier|final
name|MapredContext
name|mrContext
init|=
name|MapredContext
operator|.
name|get
argument_list|()
decl_stmt|;
if|if
condition|(
operator|!
name|conf
operator|.
name|isBucketMapJoin
argument_list|()
operator|&&
operator|!
name|conf
operator|.
name|isDynamicPartitionHashJoin
argument_list|()
condition|)
block|{
comment|/*        * The issue with caching in case of bucket map join is that different tasks        * process different buckets and if the container is reused to join a different bucket,        * join results can be incorrect. The cache is keyed on operator id and for bucket map join        * the operator does not change but data needed is different. For a proper fix, this        * requires changes in the Tez API with regard to finding bucket id and        * also ability to schedule tasks to re-use containers that have cached the specific bucket.        */
if|if
condition|(
name|isLogInfoEnabled
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"This is not bucket map join, so cache"
argument_list|)
expr_stmt|;
block|}
name|Future
argument_list|<
name|Pair
argument_list|<
name|MapJoinTableContainer
index|[]
argument_list|,
name|MapJoinTableContainerSerDe
index|[]
argument_list|>
argument_list|>
name|future
init|=
name|cache
operator|.
name|retrieveAsync
argument_list|(
name|cacheKey
argument_list|,
operator|new
name|Callable
argument_list|<
name|Pair
argument_list|<
name|MapJoinTableContainer
index|[]
argument_list|,
name|MapJoinTableContainerSerDe
index|[]
argument_list|>
argument_list|>
argument_list|()
block|{
annotation|@
name|Override
specifier|public
name|Pair
argument_list|<
name|MapJoinTableContainer
index|[]
argument_list|,
name|MapJoinTableContainerSerDe
index|[]
argument_list|>
name|call
parameter_list|()
throws|throws
name|HiveException
block|{
return|return
name|loadHashTable
argument_list|(
name|mapContext
argument_list|,
name|mrContext
argument_list|)
return|;
block|}
block|}
argument_list|)
decl_stmt|;
name|asyncInitOperations
operator|.
name|add
argument_list|(
name|future
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
operator|!
name|isInputFileChangeSensitive
argument_list|(
name|mapContext
argument_list|)
condition|)
block|{
name|loadHashTable
argument_list|(
name|mapContext
argument_list|,
name|mrContext
argument_list|)
expr_stmt|;
name|hashTblInitedOnce
operator|=
literal|true
expr_stmt|;
block|}
block|}
annotation|@
name|SuppressWarnings
argument_list|(
literal|"unchecked"
argument_list|)
annotation|@
name|Override
specifier|protected
specifier|final
name|void
name|completeInitializationOp
parameter_list|(
name|Object
index|[]
name|os
parameter_list|)
throws|throws
name|HiveException
block|{
if|if
condition|(
name|os
operator|.
name|length
operator|!=
literal|0
condition|)
block|{
name|Pair
argument_list|<
name|MapJoinTableContainer
index|[]
argument_list|,
name|MapJoinTableContainerSerDe
index|[]
argument_list|>
name|pair
init|=
operator|(
name|Pair
argument_list|<
name|MapJoinTableContainer
index|[]
argument_list|,
name|MapJoinTableContainerSerDe
index|[]
argument_list|>
operator|)
name|os
index|[
literal|0
index|]
decl_stmt|;
name|boolean
name|spilled
init|=
literal|false
decl_stmt|;
for|for
control|(
name|MapJoinTableContainer
name|container
range|:
name|pair
operator|.
name|getLeft
argument_list|()
control|)
block|{
if|if
condition|(
name|container
operator|!=
literal|null
condition|)
block|{
name|spilled
operator|=
name|spilled
operator|||
name|container
operator|.
name|hasSpill
argument_list|()
expr_stmt|;
block|}
block|}
if|if
condition|(
operator|!
name|loadCalled
operator|&&
name|spilled
condition|)
block|{
comment|// we can't use the cached table because it has spilled.
name|loadHashTable
argument_list|(
name|getExecContext
argument_list|()
argument_list|,
name|MapredContext
operator|.
name|get
argument_list|()
argument_list|)
expr_stmt|;
block|}
else|else
block|{
if|if
condition|(
name|LOG
operator|.
name|isInfoEnabled
argument_list|()
condition|)
block|{
name|String
name|s
init|=
literal|"Using tables from cache: ["
decl_stmt|;
for|for
control|(
name|MapJoinTableContainer
name|c
range|:
name|pair
operator|.
name|getLeft
argument_list|()
control|)
block|{
name|s
operator|+=
operator|(
operator|(
name|c
operator|==
literal|null
operator|)
condition|?
literal|"null"
else|:
name|c
operator|.
name|getClass
argument_list|()
operator|.
name|getSimpleName
argument_list|()
operator|)
operator|+
literal|", "
expr_stmt|;
block|}
name|LOG
operator|.
name|info
argument_list|(
name|s
operator|+
literal|"]"
argument_list|)
expr_stmt|;
block|}
comment|// let's use the table from the cache.
name|mapJoinTables
operator|=
name|pair
operator|.
name|getLeft
argument_list|()
expr_stmt|;
name|mapJoinTableSerdes
operator|=
name|pair
operator|.
name|getRight
argument_list|()
expr_stmt|;
block|}
name|hashTblInitedOnce
operator|=
literal|true
expr_stmt|;
block|}
if|if
condition|(
name|this
operator|.
name|getExecContext
argument_list|()
operator|!=
literal|null
condition|)
block|{
comment|// reset exec context so that initialization of the map operator happens
comment|// properly
name|this
operator|.
name|getExecContext
argument_list|()
operator|.
name|setLastInputPath
argument_list|(
literal|null
argument_list|)
expr_stmt|;
name|this
operator|.
name|getExecContext
argument_list|()
operator|.
name|setCurrentInputPath
argument_list|(
literal|null
argument_list|)
expr_stmt|;
block|}
block|}
annotation|@
name|Override
specifier|protected
name|List
argument_list|<
name|ObjectInspector
argument_list|>
name|getValueObjectInspectors
parameter_list|(
name|byte
name|alias
parameter_list|,
name|List
argument_list|<
name|ObjectInspector
argument_list|>
index|[]
name|aliasToObjectInspectors
parameter_list|)
block|{
name|int
index|[]
name|valueIndex
init|=
name|conf
operator|.
name|getValueIndex
argument_list|(
name|alias
argument_list|)
decl_stmt|;
if|if
condition|(
name|valueIndex
operator|==
literal|null
condition|)
block|{
return|return
name|super
operator|.
name|getValueObjectInspectors
argument_list|(
name|alias
argument_list|,
name|aliasToObjectInspectors
argument_list|)
return|;
block|}
name|unwrapContainer
index|[
name|alias
index|]
operator|=
operator|new
name|UnwrapRowContainer
argument_list|(
name|alias
argument_list|,
name|valueIndex
argument_list|,
name|hasFilter
argument_list|(
name|alias
argument_list|)
argument_list|)
expr_stmt|;
name|List
argument_list|<
name|ObjectInspector
argument_list|>
name|inspectors
init|=
name|aliasToObjectInspectors
index|[
name|alias
index|]
decl_stmt|;
name|int
name|bigPos
init|=
name|conf
operator|.
name|getPosBigTable
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|ObjectInspector
argument_list|>
name|valueOI
init|=
operator|new
name|ArrayList
argument_list|<
name|ObjectInspector
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|valueIndex
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
if|if
condition|(
name|valueIndex
index|[
name|i
index|]
operator|>=
literal|0
operator|&&
operator|!
name|joinKeysObjectInspectors
index|[
name|bigPos
index|]
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|valueOI
operator|.
name|add
argument_list|(
name|joinKeysObjectInspectors
index|[
name|bigPos
index|]
operator|.
name|get
argument_list|(
name|valueIndex
index|[
name|i
index|]
argument_list|)
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|valueOI
operator|.
name|add
argument_list|(
name|inspectors
operator|.
name|get
argument_list|(
name|i
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|valueOI
return|;
block|}
specifier|public
name|void
name|generateMapMetaData
parameter_list|()
throws|throws
name|HiveException
block|{
comment|// generate the meta data for key
comment|// index for key is -1
try|try
block|{
name|TableDesc
name|keyTableDesc
init|=
name|conf
operator|.
name|getKeyTblDesc
argument_list|()
decl_stmt|;
name|SerDe
name|keySerializer
init|=
operator|(
name|SerDe
operator|)
name|ReflectionUtil
operator|.
name|newInstance
argument_list|(
name|keyTableDesc
operator|.
name|getDeserializerClass
argument_list|()
argument_list|,
literal|null
argument_list|)
decl_stmt|;
name|SerDeUtils
operator|.
name|initializeSerDe
argument_list|(
name|keySerializer
argument_list|,
literal|null
argument_list|,
name|keyTableDesc
operator|.
name|getProperties
argument_list|()
argument_list|,
literal|null
argument_list|)
expr_stmt|;
name|MapJoinObjectSerDeContext
name|keyContext
init|=
operator|new
name|MapJoinObjectSerDeContext
argument_list|(
name|keySerializer
argument_list|,
literal|false
argument_list|)
decl_stmt|;
for|for
control|(
name|int
name|pos
init|=
literal|0
init|;
name|pos
operator|<
name|order
operator|.
name|length
condition|;
name|pos
operator|++
control|)
block|{
if|if
condition|(
name|pos
operator|==
name|posBigTable
condition|)
block|{
continue|continue;
block|}
name|TableDesc
name|valueTableDesc
decl_stmt|;
if|if
condition|(
name|conf
operator|.
name|getNoOuterJoin
argument_list|()
condition|)
block|{
name|valueTableDesc
operator|=
name|conf
operator|.
name|getValueTblDescs
argument_list|()
operator|.
name|get
argument_list|(
name|pos
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|valueTableDesc
operator|=
name|conf
operator|.
name|getValueFilteredTblDescs
argument_list|()
operator|.
name|get
argument_list|(
name|pos
argument_list|)
expr_stmt|;
block|}
name|SerDe
name|valueSerDe
init|=
operator|(
name|SerDe
operator|)
name|ReflectionUtil
operator|.
name|newInstance
argument_list|(
name|valueTableDesc
operator|.
name|getDeserializerClass
argument_list|()
argument_list|,
literal|null
argument_list|)
decl_stmt|;
name|SerDeUtils
operator|.
name|initializeSerDe
argument_list|(
name|valueSerDe
argument_list|,
literal|null
argument_list|,
name|valueTableDesc
operator|.
name|getProperties
argument_list|()
argument_list|,
literal|null
argument_list|)
expr_stmt|;
name|MapJoinObjectSerDeContext
name|valueContext
init|=
operator|new
name|MapJoinObjectSerDeContext
argument_list|(
name|valueSerDe
argument_list|,
name|hasFilter
argument_list|(
name|pos
argument_list|)
argument_list|)
decl_stmt|;
name|mapJoinTableSerdes
index|[
name|pos
index|]
operator|=
operator|new
name|MapJoinTableContainerSerDe
argument_list|(
name|keyContext
argument_list|,
name|valueContext
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|SerDeException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|protected
name|Pair
argument_list|<
name|MapJoinTableContainer
index|[]
argument_list|,
name|MapJoinTableContainerSerDe
index|[]
argument_list|>
name|loadHashTable
parameter_list|(
name|ExecMapperContext
name|mapContext
parameter_list|,
name|MapredContext
name|mrContext
parameter_list|)
throws|throws
name|HiveException
block|{
name|loadCalled
operator|=
literal|true
expr_stmt|;
if|if
condition|(
name|canSkipReload
argument_list|(
name|mapContext
argument_list|)
condition|)
block|{
comment|// no need to reload
return|return
operator|new
name|ImmutablePair
argument_list|<
name|MapJoinTableContainer
index|[]
argument_list|,
name|MapJoinTableContainerSerDe
index|[]
argument_list|>
argument_list|(
name|mapJoinTables
argument_list|,
name|mapJoinTableSerdes
argument_list|)
return|;
block|}
name|perfLogger
operator|.
name|PerfLogBegin
argument_list|(
name|CLASS_NAME
argument_list|,
name|PerfLogger
operator|.
name|LOAD_HASHTABLE
argument_list|)
expr_stmt|;
name|loader
operator|.
name|init
argument_list|(
name|mapContext
argument_list|,
name|mrContext
argument_list|,
name|hconf
argument_list|,
name|this
argument_list|)
expr_stmt|;
try|try
block|{
name|loader
operator|.
name|load
argument_list|(
name|mapJoinTables
argument_list|,
name|mapJoinTableSerdes
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|HiveException
name|e
parameter_list|)
block|{
if|if
condition|(
name|isLogInfoEnabled
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Exception loading hash tables. Clearing partially loaded hash table containers."
argument_list|)
expr_stmt|;
block|}
comment|// there could be some spilled partitions which needs to be cleaned up
name|clearAllTableContainers
argument_list|()
expr_stmt|;
throw|throw
name|e
throw|;
block|}
name|hashTblInitedOnce
operator|=
literal|true
expr_stmt|;
name|Pair
argument_list|<
name|MapJoinTableContainer
index|[]
argument_list|,
name|MapJoinTableContainerSerDe
index|[]
argument_list|>
name|pair
init|=
operator|new
name|ImmutablePair
argument_list|<
name|MapJoinTableContainer
index|[]
argument_list|,
name|MapJoinTableContainerSerDe
index|[]
argument_list|>
argument_list|(
name|mapJoinTables
argument_list|,
name|mapJoinTableSerdes
argument_list|)
decl_stmt|;
name|perfLogger
operator|.
name|PerfLogEnd
argument_list|(
name|CLASS_NAME
argument_list|,
name|PerfLogger
operator|.
name|LOAD_HASHTABLE
argument_list|)
expr_stmt|;
if|if
condition|(
name|canSkipJoinProcessing
argument_list|(
name|mapContext
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Skipping big table join processing for "
operator|+
name|this
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
name|this
operator|.
name|setDone
argument_list|(
literal|true
argument_list|)
expr_stmt|;
block|}
return|return
name|pair
return|;
block|}
comment|// Load the hash table
annotation|@
name|Override
specifier|public
name|void
name|cleanUpInputFileChangedOp
parameter_list|()
throws|throws
name|HiveException
block|{
name|loadHashTable
argument_list|(
name|getExecContext
argument_list|()
argument_list|,
name|MapredContext
operator|.
name|get
argument_list|()
argument_list|)
expr_stmt|;
block|}
specifier|protected
name|JoinUtil
operator|.
name|JoinResult
name|setMapJoinKey
parameter_list|(
name|ReusableGetAdaptor
name|dest
parameter_list|,
name|Object
name|row
parameter_list|,
name|byte
name|alias
parameter_list|)
throws|throws
name|HiveException
block|{
return|return
name|dest
operator|.
name|setFromRow
argument_list|(
name|row
argument_list|,
name|joinKeys
index|[
name|alias
index|]
argument_list|,
name|joinKeysObjectInspectors
index|[
name|alias
index|]
argument_list|)
return|;
block|}
specifier|protected
name|MapJoinKey
name|getRefKey
parameter_list|(
name|byte
name|alias
parameter_list|)
block|{
comment|// We assume that since we are joining on the same key, all tables would have either
comment|// optimized or non-optimized key; hence, we can pass any key in any table as reference.
comment|// We do it so that MJKB could determine whether it can use optimized keys.
for|for
control|(
name|byte
name|pos
init|=
literal|0
init|;
name|pos
operator|<
name|order
operator|.
name|length
condition|;
name|pos
operator|++
control|)
block|{
if|if
condition|(
name|pos
operator|==
name|alias
condition|)
continue|continue;
name|MapJoinKey
name|refKey
init|=
name|mapJoinTables
index|[
name|pos
index|]
operator|.
name|getAnyKey
argument_list|()
decl_stmt|;
if|if
condition|(
name|refKey
operator|!=
literal|null
condition|)
return|return
name|refKey
return|;
block|}
return|return
literal|null
return|;
comment|// All join tables have 0 keys, doesn't matter what we generate.
block|}
annotation|@
name|Override
specifier|public
name|void
name|process
parameter_list|(
name|Object
name|row
parameter_list|,
name|int
name|tag
parameter_list|)
throws|throws
name|HiveException
block|{
try|try
block|{
name|alias
operator|=
operator|(
name|byte
operator|)
name|tag
expr_stmt|;
if|if
condition|(
name|hashMapRowGetters
operator|==
literal|null
condition|)
block|{
name|hashMapRowGetters
operator|=
operator|new
name|ReusableGetAdaptor
index|[
name|mapJoinTables
operator|.
name|length
index|]
expr_stmt|;
name|MapJoinKey
name|refKey
init|=
name|getRefKey
argument_list|(
name|alias
argument_list|)
decl_stmt|;
for|for
control|(
name|byte
name|pos
init|=
literal|0
init|;
name|pos
operator|<
name|order
operator|.
name|length
condition|;
name|pos
operator|++
control|)
block|{
if|if
condition|(
name|pos
operator|!=
name|alias
condition|)
block|{
name|hashMapRowGetters
index|[
name|pos
index|]
operator|=
name|mapJoinTables
index|[
name|pos
index|]
operator|.
name|createGetter
argument_list|(
name|refKey
argument_list|)
expr_stmt|;
block|}
block|}
block|}
comment|// As we're calling processOp again to process the leftover "tuples", we know the "row" is
comment|// coming from the spilled matchfile. We need to recreate hashMapRowGetter against new hashtables
if|if
condition|(
name|hybridMapJoinLeftover
condition|)
block|{
name|MapJoinKey
name|refKey
init|=
name|getRefKey
argument_list|(
name|alias
argument_list|)
decl_stmt|;
for|for
control|(
name|byte
name|pos
init|=
literal|0
init|;
name|pos
operator|<
name|order
operator|.
name|length
condition|;
name|pos
operator|++
control|)
block|{
if|if
condition|(
name|pos
operator|!=
name|alias
operator|&&
name|spilledMapJoinTables
index|[
name|pos
index|]
operator|!=
literal|null
condition|)
block|{
name|hashMapRowGetters
index|[
name|pos
index|]
operator|=
name|spilledMapJoinTables
index|[
name|pos
index|]
operator|.
name|createGetter
argument_list|(
name|refKey
argument_list|)
expr_stmt|;
block|}
block|}
block|}
comment|// compute keys and values as StandardObjects
name|ReusableGetAdaptor
name|firstSetKey
init|=
literal|null
decl_stmt|;
name|int
name|fieldCount
init|=
name|joinKeys
index|[
name|alias
index|]
operator|.
name|size
argument_list|()
decl_stmt|;
name|boolean
name|joinNeeded
init|=
literal|false
decl_stmt|;
name|boolean
name|bigTableRowSpilled
init|=
literal|false
decl_stmt|;
for|for
control|(
name|byte
name|pos
init|=
literal|0
init|;
name|pos
operator|<
name|order
operator|.
name|length
condition|;
name|pos
operator|++
control|)
block|{
if|if
condition|(
name|pos
operator|!=
name|alias
condition|)
block|{
name|JoinUtil
operator|.
name|JoinResult
name|joinResult
decl_stmt|;
name|ReusableGetAdaptor
name|adaptor
decl_stmt|;
if|if
condition|(
name|firstSetKey
operator|==
literal|null
condition|)
block|{
name|adaptor
operator|=
name|firstSetKey
operator|=
name|hashMapRowGetters
index|[
name|pos
index|]
expr_stmt|;
name|joinResult
operator|=
name|setMapJoinKey
argument_list|(
name|firstSetKey
argument_list|,
name|row
argument_list|,
name|alias
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// Keys for all tables are the same, so only the first has to deserialize them.
name|adaptor
operator|=
name|hashMapRowGetters
index|[
name|pos
index|]
expr_stmt|;
name|joinResult
operator|=
name|adaptor
operator|.
name|setFromOther
argument_list|(
name|firstSetKey
argument_list|)
expr_stmt|;
block|}
name|MapJoinRowContainer
name|rowContainer
init|=
name|adaptor
operator|.
name|getCurrentRows
argument_list|()
decl_stmt|;
if|if
condition|(
name|rowContainer
operator|!=
literal|null
operator|&&
name|unwrapContainer
index|[
name|pos
index|]
operator|!=
literal|null
condition|)
block|{
name|Object
index|[]
name|currentKey
init|=
name|firstSetKey
operator|.
name|getCurrentKey
argument_list|()
decl_stmt|;
name|rowContainer
operator|=
name|unwrapContainer
index|[
name|pos
index|]
operator|.
name|setInternal
argument_list|(
name|rowContainer
argument_list|,
name|currentKey
argument_list|)
expr_stmt|;
block|}
comment|// there is no join-value or join-key has all null elements
if|if
condition|(
name|rowContainer
operator|==
literal|null
operator|||
name|firstSetKey
operator|.
name|hasAnyNulls
argument_list|(
name|fieldCount
argument_list|,
name|nullsafes
argument_list|)
condition|)
block|{
if|if
condition|(
operator|!
name|noOuterJoin
condition|)
block|{
comment|// For Hybrid Grace Hash Join, during the 1st round processing,
comment|// we only keep the LEFT side if the row is not spilled
if|if
condition|(
operator|!
name|conf
operator|.
name|isHybridHashJoin
argument_list|()
operator|||
name|hybridMapJoinLeftover
operator|||
operator|(
operator|!
name|hybridMapJoinLeftover
operator|&&
name|joinResult
operator|!=
name|JoinUtil
operator|.
name|JoinResult
operator|.
name|SPILL
operator|)
condition|)
block|{
name|joinNeeded
operator|=
literal|true
expr_stmt|;
name|storage
index|[
name|pos
index|]
operator|=
name|dummyObjVectors
index|[
name|pos
index|]
expr_stmt|;
block|}
block|}
else|else
block|{
name|storage
index|[
name|pos
index|]
operator|=
name|emptyList
expr_stmt|;
block|}
block|}
else|else
block|{
name|joinNeeded
operator|=
literal|true
expr_stmt|;
name|storage
index|[
name|pos
index|]
operator|=
name|rowContainer
operator|.
name|copy
argument_list|()
expr_stmt|;
name|aliasFilterTags
index|[
name|pos
index|]
operator|=
name|rowContainer
operator|.
name|getAliasFilter
argument_list|()
expr_stmt|;
block|}
comment|// Spill the big table rows into appropriate partition:
comment|// When the JoinResult is SPILL, it means the corresponding small table row may have been
comment|// spilled to disk (at least the partition that holds this row is on disk). So we need to
comment|// postpone the join processing for this pair by also spilling this big table row.
if|if
condition|(
name|joinResult
operator|==
name|JoinUtil
operator|.
name|JoinResult
operator|.
name|SPILL
operator|&&
operator|!
name|bigTableRowSpilled
condition|)
block|{
comment|// For n-way join, only spill big table rows once
name|spillBigTableRow
argument_list|(
name|mapJoinTables
index|[
name|pos
index|]
argument_list|,
name|row
argument_list|)
expr_stmt|;
name|bigTableRowSpilled
operator|=
literal|true
expr_stmt|;
block|}
block|}
block|}
if|if
condition|(
name|joinNeeded
condition|)
block|{
name|List
argument_list|<
name|Object
argument_list|>
name|value
init|=
name|getFilteredValue
argument_list|(
name|alias
argument_list|,
name|row
argument_list|)
decl_stmt|;
comment|// Add the value to the ArrayList
name|storage
index|[
name|alias
index|]
operator|.
name|addRow
argument_list|(
name|value
argument_list|)
expr_stmt|;
comment|// generate the output records
name|checkAndGenObject
argument_list|()
expr_stmt|;
block|}
comment|// done with the row
name|storage
index|[
name|tag
index|]
operator|.
name|clearRows
argument_list|()
expr_stmt|;
for|for
control|(
name|byte
name|pos
init|=
literal|0
init|;
name|pos
operator|<
name|order
operator|.
name|length
condition|;
name|pos
operator|++
control|)
block|{
if|if
condition|(
name|pos
operator|!=
name|tag
condition|)
block|{
name|storage
index|[
name|pos
index|]
operator|=
literal|null
expr_stmt|;
block|}
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|String
name|msg
init|=
literal|"Unexpected exception from "
operator|+
name|this
operator|.
name|getClass
argument_list|()
operator|.
name|getSimpleName
argument_list|()
operator|+
literal|" : "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
decl_stmt|;
name|LOG
operator|.
name|error
argument_list|(
name|msg
argument_list|,
name|e
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|HiveException
argument_list|(
name|msg
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    * Postpone processing the big table row temporarily by spilling it to a row container    * @param hybridHtContainer Hybrid hashtable container    * @param row big table row    */
specifier|protected
name|void
name|spillBigTableRow
parameter_list|(
name|MapJoinTableContainer
name|hybridHtContainer
parameter_list|,
name|Object
name|row
parameter_list|)
throws|throws
name|HiveException
block|{
name|HybridHashTableContainer
name|ht
init|=
operator|(
name|HybridHashTableContainer
operator|)
name|hybridHtContainer
decl_stmt|;
name|int
name|partitionId
init|=
name|ht
operator|.
name|getToSpillPartitionId
argument_list|()
decl_stmt|;
name|HashPartition
name|hp
init|=
name|ht
operator|.
name|getHashPartitions
argument_list|()
index|[
name|partitionId
index|]
decl_stmt|;
name|ObjectContainer
name|bigTable
init|=
name|hp
operator|.
name|getMatchfileObjContainer
argument_list|()
decl_stmt|;
name|bigTable
operator|.
name|add
argument_list|(
name|row
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|closeOp
parameter_list|(
name|boolean
name|abort
parameter_list|)
throws|throws
name|HiveException
block|{
name|boolean
name|spilled
init|=
literal|false
decl_stmt|;
for|for
control|(
name|MapJoinTableContainer
name|container
range|:
name|mapJoinTables
control|)
block|{
if|if
condition|(
name|container
operator|!=
literal|null
condition|)
block|{
name|spilled
operator|=
name|spilled
operator|||
name|container
operator|.
name|hasSpill
argument_list|()
expr_stmt|;
name|container
operator|.
name|dumpMetrics
argument_list|()
expr_stmt|;
block|}
block|}
comment|// For Hybrid Grace Hash Join, we need to see if there is any spilled data to be processed next
if|if
condition|(
name|spilled
condition|)
block|{
if|if
condition|(
operator|!
name|abort
condition|)
block|{
if|if
condition|(
name|hashMapRowGetters
operator|==
literal|null
condition|)
block|{
name|hashMapRowGetters
operator|=
operator|new
name|ReusableGetAdaptor
index|[
name|mapJoinTables
operator|.
name|length
index|]
expr_stmt|;
block|}
name|int
name|numPartitions
init|=
literal|0
decl_stmt|;
comment|// Find out number of partitions for each small table (should be same across tables)
for|for
control|(
name|byte
name|pos
init|=
literal|0
init|;
name|pos
operator|<
name|mapJoinTables
operator|.
name|length
condition|;
name|pos
operator|++
control|)
block|{
if|if
condition|(
name|pos
operator|!=
name|conf
operator|.
name|getPosBigTable
argument_list|()
condition|)
block|{
name|firstSmallTable
operator|=
operator|(
name|HybridHashTableContainer
operator|)
name|mapJoinTables
index|[
name|pos
index|]
expr_stmt|;
name|numPartitions
operator|=
name|firstSmallTable
operator|.
name|getHashPartitions
argument_list|()
operator|.
name|length
expr_stmt|;
break|break;
block|}
block|}
assert|assert
name|numPartitions
operator|!=
literal|0
operator|:
literal|"Number of partitions must be greater than 0!"
assert|;
if|if
condition|(
name|firstSmallTable
operator|.
name|hasSpill
argument_list|()
condition|)
block|{
name|spilledMapJoinTables
operator|=
operator|new
name|MapJoinBytesTableContainer
index|[
name|mapJoinTables
operator|.
name|length
index|]
expr_stmt|;
name|hybridMapJoinLeftover
operator|=
literal|true
expr_stmt|;
comment|// Clear all in-memory partitions first
for|for
control|(
name|byte
name|pos
init|=
literal|0
init|;
name|pos
operator|<
name|mapJoinTables
operator|.
name|length
condition|;
name|pos
operator|++
control|)
block|{
name|MapJoinTableContainer
name|tableContainer
init|=
name|mapJoinTables
index|[
name|pos
index|]
decl_stmt|;
if|if
condition|(
name|tableContainer
operator|!=
literal|null
operator|&&
name|tableContainer
operator|instanceof
name|HybridHashTableContainer
condition|)
block|{
name|HybridHashTableContainer
name|hybridHtContainer
init|=
operator|(
name|HybridHashTableContainer
operator|)
name|tableContainer
decl_stmt|;
name|hybridHtContainer
operator|.
name|dumpStats
argument_list|()
expr_stmt|;
name|HashPartition
index|[]
name|hashPartitions
init|=
name|hybridHtContainer
operator|.
name|getHashPartitions
argument_list|()
decl_stmt|;
comment|// Clear all in memory partitions first
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|hashPartitions
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
if|if
condition|(
operator|!
name|hashPartitions
index|[
name|i
index|]
operator|.
name|isHashMapOnDisk
argument_list|()
condition|)
block|{
name|hybridHtContainer
operator|.
name|setTotalInMemRowCount
argument_list|(
name|hybridHtContainer
operator|.
name|getTotalInMemRowCount
argument_list|()
operator|-
name|hashPartitions
index|[
name|i
index|]
operator|.
name|getHashMapFromMemory
argument_list|()
operator|.
name|getNumValues
argument_list|()
argument_list|)
expr_stmt|;
name|hashPartitions
index|[
name|i
index|]
operator|.
name|getHashMapFromMemory
argument_list|()
operator|.
name|clear
argument_list|()
expr_stmt|;
block|}
block|}
assert|assert
name|hybridHtContainer
operator|.
name|getTotalInMemRowCount
argument_list|()
operator|==
literal|0
assert|;
block|}
block|}
comment|// Reprocess the spilled data
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|numPartitions
condition|;
name|i
operator|++
control|)
block|{
name|HashPartition
index|[]
name|hashPartitions
init|=
name|firstSmallTable
operator|.
name|getHashPartitions
argument_list|()
decl_stmt|;
if|if
condition|(
name|hashPartitions
index|[
name|i
index|]
operator|.
name|isHashMapOnDisk
argument_list|()
condition|)
block|{
try|try
block|{
name|continueProcess
argument_list|(
name|i
argument_list|)
expr_stmt|;
comment|// Re-process spilled data
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
name|e
argument_list|)
throw|;
block|}
for|for
control|(
name|byte
name|pos
init|=
literal|0
init|;
name|pos
operator|<
name|order
operator|.
name|length
condition|;
name|pos
operator|++
control|)
block|{
if|if
condition|(
name|pos
operator|!=
name|conf
operator|.
name|getPosBigTable
argument_list|()
condition|)
name|spilledMapJoinTables
index|[
name|pos
index|]
operator|=
literal|null
expr_stmt|;
block|}
block|}
block|}
block|}
block|}
if|if
condition|(
name|isLogInfoEnabled
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"spilled: "
operator|+
name|spilled
operator|+
literal|" abort: "
operator|+
name|abort
operator|+
literal|". Clearing spilled partitions."
argument_list|)
expr_stmt|;
block|}
comment|// spilled tables are loaded always (no sharing), so clear it
name|clearAllTableContainers
argument_list|()
expr_stmt|;
name|cache
operator|.
name|remove
argument_list|(
name|cacheKey
argument_list|)
expr_stmt|;
block|}
comment|// in mapreduce case, we need to always clear up as mapreduce doesn't have object registry.
if|if
condition|(
operator|(
name|this
operator|.
name|getExecContext
argument_list|()
operator|!=
literal|null
operator|)
operator|&&
operator|(
name|this
operator|.
name|getExecContext
argument_list|()
operator|.
name|getLocalWork
argument_list|()
operator|!=
literal|null
operator|)
operator|&&
operator|(
name|this
operator|.
name|getExecContext
argument_list|()
operator|.
name|getLocalWork
argument_list|()
operator|.
name|getInputFileChangeSensitive
argument_list|()
operator|)
operator|&&
operator|!
operator|(
name|HiveConf
operator|.
name|getVar
argument_list|(
name|hconf
argument_list|,
name|ConfVars
operator|.
name|HIVE_EXECUTION_ENGINE
argument_list|)
operator|.
name|equals
argument_list|(
literal|"spark"
argument_list|)
operator|&&
name|SparkUtilities
operator|.
name|isDedicatedCluster
argument_list|(
name|hconf
argument_list|)
operator|)
condition|)
block|{
if|if
condition|(
name|isLogInfoEnabled
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"MR: Clearing all map join table containers."
argument_list|)
expr_stmt|;
block|}
name|clearAllTableContainers
argument_list|()
expr_stmt|;
block|}
name|this
operator|.
name|loader
operator|=
literal|null
expr_stmt|;
name|super
operator|.
name|closeOp
argument_list|(
name|abort
argument_list|)
expr_stmt|;
block|}
specifier|private
name|void
name|clearAllTableContainers
parameter_list|()
block|{
if|if
condition|(
name|mapJoinTables
operator|!=
literal|null
condition|)
block|{
for|for
control|(
name|MapJoinTableContainer
name|tableContainer
range|:
name|mapJoinTables
control|)
block|{
if|if
condition|(
name|tableContainer
operator|!=
literal|null
condition|)
block|{
name|tableContainer
operator|.
name|clear
argument_list|()
expr_stmt|;
block|}
block|}
block|}
block|}
comment|/**    * Continue processing join between spilled hashtable(s) and spilled big table    * @param partitionId the partition number across all small tables to process    * @throws HiveException    * @throws IOException    * @throws SerDeException    */
specifier|private
name|void
name|continueProcess
parameter_list|(
name|int
name|partitionId
parameter_list|)
throws|throws
name|HiveException
throws|,
name|IOException
throws|,
name|SerDeException
throws|,
name|ClassNotFoundException
block|{
for|for
control|(
name|byte
name|pos
init|=
literal|0
init|;
name|pos
operator|<
name|mapJoinTables
operator|.
name|length
condition|;
name|pos
operator|++
control|)
block|{
if|if
condition|(
name|pos
operator|!=
name|conf
operator|.
name|getPosBigTable
argument_list|()
condition|)
block|{
name|reloadHashTable
argument_list|(
name|pos
argument_list|,
name|partitionId
argument_list|)
expr_stmt|;
block|}
block|}
name|reProcessBigTable
argument_list|(
name|partitionId
argument_list|)
expr_stmt|;
block|}
comment|/**    * Reload hashtable from the hash partition.    * It can have two steps:    * 1) Deserialize a serialized hash table, and    * 2) Merge every key/value pair from small table container into the hash table    * @param pos position of small table    * @param partitionId the partition of the small table to be reloaded from    * @throws IOException    * @throws HiveException    * @throws SerDeException    */
specifier|protected
name|void
name|reloadHashTable
parameter_list|(
name|byte
name|pos
parameter_list|,
name|int
name|partitionId
parameter_list|)
throws|throws
name|IOException
throws|,
name|HiveException
throws|,
name|SerDeException
throws|,
name|ClassNotFoundException
block|{
name|HybridHashTableContainer
name|container
init|=
operator|(
name|HybridHashTableContainer
operator|)
name|mapJoinTables
index|[
name|pos
index|]
decl_stmt|;
name|HashPartition
name|partition
init|=
name|container
operator|.
name|getHashPartitions
argument_list|()
index|[
name|partitionId
index|]
decl_stmt|;
comment|// Merge the sidefile into the newly created hash table
comment|// This is where the spilling may happen again
name|KeyValueContainer
name|kvContainer
init|=
name|partition
operator|.
name|getSidefileKVContainer
argument_list|()
decl_stmt|;
name|int
name|rowCount
init|=
name|kvContainer
operator|.
name|size
argument_list|()
decl_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Hybrid Grace Hash Join: Number of rows restored from KeyValueContainer: "
operator|+
name|kvContainer
operator|.
name|size
argument_list|()
argument_list|)
expr_stmt|;
comment|// Deserialize the on-disk hash table
comment|// We're sure this part is smaller than memory limit
if|if
condition|(
name|rowCount
operator|<=
literal|0
condition|)
block|{
name|rowCount
operator|=
literal|1024
operator|*
literal|1024
expr_stmt|;
comment|// Since rowCount is used later to instantiate a BytesBytesMultiHashMap
comment|// as the initialCapacity which cannot be 0, we provide a reasonable
comment|// positive number here
block|}
name|BytesBytesMultiHashMap
name|restoredHashMap
init|=
name|partition
operator|.
name|getHashMapFromDisk
argument_list|(
name|rowCount
argument_list|)
decl_stmt|;
name|rowCount
operator|+=
name|restoredHashMap
operator|.
name|getNumValues
argument_list|()
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Hybrid Grace Hash Join: Deserializing spilled hash partition..."
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Hybrid Grace Hash Join: Number of rows in hashmap: "
operator|+
name|rowCount
argument_list|)
expr_stmt|;
comment|// If based on the new key count, keyCount is smaller than a threshold,
comment|// then just load the entire restored hashmap into memory.
comment|// The size of deserialized partition shouldn't exceed half of memory limit
if|if
condition|(
name|rowCount
operator|*
name|container
operator|.
name|getTableRowSize
argument_list|()
operator|>=
name|container
operator|.
name|getMemoryThreshold
argument_list|()
operator|/
literal|2
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Hybrid Grace Hash Join: Hash table cannot be reloaded since it"
operator|+
literal|" will be greater than memory limit. Recursive spilling is currently not supported"
argument_list|)
expr_stmt|;
block|}
name|KeyValueHelper
name|writeHelper
init|=
name|container
operator|.
name|getWriteHelper
argument_list|()
decl_stmt|;
while|while
condition|(
name|kvContainer
operator|.
name|hasNext
argument_list|()
condition|)
block|{
name|ObjectPair
argument_list|<
name|HiveKey
argument_list|,
name|BytesWritable
argument_list|>
name|pair
init|=
name|kvContainer
operator|.
name|next
argument_list|()
decl_stmt|;
name|Writable
name|key
init|=
name|pair
operator|.
name|getFirst
argument_list|()
decl_stmt|;
name|Writable
name|val
init|=
name|pair
operator|.
name|getSecond
argument_list|()
decl_stmt|;
name|writeHelper
operator|.
name|setKeyValue
argument_list|(
name|key
argument_list|,
name|val
argument_list|)
expr_stmt|;
name|restoredHashMap
operator|.
name|put
argument_list|(
name|writeHelper
argument_list|,
operator|-
literal|1
argument_list|)
expr_stmt|;
block|}
name|container
operator|.
name|setTotalInMemRowCount
argument_list|(
name|container
operator|.
name|getTotalInMemRowCount
argument_list|()
operator|+
name|restoredHashMap
operator|.
name|getNumValues
argument_list|()
argument_list|)
expr_stmt|;
name|kvContainer
operator|.
name|clear
argument_list|()
expr_stmt|;
name|spilledMapJoinTables
index|[
name|pos
index|]
operator|=
operator|new
name|MapJoinBytesTableContainer
argument_list|(
name|restoredHashMap
argument_list|)
expr_stmt|;
name|spilledMapJoinTables
index|[
name|pos
index|]
operator|.
name|setInternalValueOi
argument_list|(
name|container
operator|.
name|getInternalValueOi
argument_list|()
argument_list|)
expr_stmt|;
name|spilledMapJoinTables
index|[
name|pos
index|]
operator|.
name|setSortableSortOrders
argument_list|(
name|container
operator|.
name|getSortableSortOrders
argument_list|()
argument_list|)
expr_stmt|;
block|}
comment|/**    * Iterate over the big table row container and feed process() with leftover rows    * @param partitionId the partition from which to take out spilled big table rows    * @throws HiveException    */
specifier|protected
name|void
name|reProcessBigTable
parameter_list|(
name|int
name|partitionId
parameter_list|)
throws|throws
name|HiveException
block|{
comment|// For binary join, firstSmallTable is the only small table; it has reference to spilled big
comment|// table rows;
comment|// For n-way join, since we only spill once, when processing the first small table, so only the
comment|// firstSmallTable has reference to the spilled big table rows.
name|HashPartition
name|partition
init|=
name|firstSmallTable
operator|.
name|getHashPartitions
argument_list|()
index|[
name|partitionId
index|]
decl_stmt|;
name|ObjectContainer
name|bigTable
init|=
name|partition
operator|.
name|getMatchfileObjContainer
argument_list|()
decl_stmt|;
while|while
condition|(
name|bigTable
operator|.
name|hasNext
argument_list|()
condition|)
block|{
name|Object
name|row
init|=
name|bigTable
operator|.
name|next
argument_list|()
decl_stmt|;
name|process
argument_list|(
name|row
argument_list|,
name|conf
operator|.
name|getPosBigTable
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|bigTable
operator|.
name|clear
argument_list|()
expr_stmt|;
block|}
comment|/**    * Implements the getName function for the Node Interface.    *    * @return the name of the operator    */
annotation|@
name|Override
specifier|public
name|String
name|getName
parameter_list|()
block|{
return|return
name|getOperatorName
argument_list|()
return|;
block|}
specifier|static
specifier|public
name|String
name|getOperatorName
parameter_list|()
block|{
return|return
literal|"MAPJOIN"
return|;
block|}
annotation|@
name|Override
specifier|public
name|OperatorType
name|getType
parameter_list|()
block|{
return|return
name|OperatorType
operator|.
name|MAPJOIN
return|;
block|}
specifier|protected
name|boolean
name|isInputFileChangeSensitive
parameter_list|(
name|ExecMapperContext
name|mapContext
parameter_list|)
block|{
return|return
operator|!
operator|(
name|mapContext
operator|==
literal|null
operator|||
name|mapContext
operator|.
name|getLocalWork
argument_list|()
operator|==
literal|null
operator|||
name|mapContext
operator|.
name|getLocalWork
argument_list|()
operator|.
name|getInputFileChangeSensitive
argument_list|()
operator|==
literal|false
operator|)
return|;
block|}
specifier|protected
name|boolean
name|canSkipReload
parameter_list|(
name|ExecMapperContext
name|mapContext
parameter_list|)
block|{
return|return
operator|(
name|this
operator|.
name|hashTblInitedOnce
operator|&&
operator|!
name|isInputFileChangeSensitive
argument_list|(
name|mapContext
argument_list|)
operator|)
return|;
block|}
comment|// If the loaded hash table is empty, for some conditions we can skip processing the big table rows.
specifier|protected
name|boolean
name|canSkipJoinProcessing
parameter_list|(
name|ExecMapperContext
name|mapContext
parameter_list|)
block|{
if|if
condition|(
operator|!
name|canSkipReload
argument_list|(
name|mapContext
argument_list|)
condition|)
block|{
return|return
literal|false
return|;
block|}
name|JoinCondDesc
index|[]
name|joinConds
init|=
name|getConf
argument_list|()
operator|.
name|getConds
argument_list|()
decl_stmt|;
if|if
condition|(
name|joinConds
operator|.
name|length
operator|>
literal|0
condition|)
block|{
for|for
control|(
name|JoinCondDesc
name|joinCond
range|:
name|joinConds
control|)
block|{
if|if
condition|(
name|joinCond
operator|.
name|getType
argument_list|()
operator|!=
name|JoinDesc
operator|.
name|INNER_JOIN
condition|)
block|{
return|return
literal|false
return|;
block|}
block|}
block|}
else|else
block|{
return|return
literal|false
return|;
block|}
name|boolean
name|skipJoinProcessing
init|=
literal|false
decl_stmt|;
for|for
control|(
name|int
name|idx
init|=
literal|0
init|;
name|idx
operator|<
name|mapJoinTables
operator|.
name|length
condition|;
operator|++
name|idx
control|)
block|{
if|if
condition|(
name|idx
operator|==
name|getConf
argument_list|()
operator|.
name|getPosBigTable
argument_list|()
condition|)
block|{
continue|continue;
block|}
name|MapJoinTableContainer
name|mapJoinTable
init|=
name|mapJoinTables
index|[
name|idx
index|]
decl_stmt|;
if|if
condition|(
name|mapJoinTable
operator|.
name|size
argument_list|()
operator|==
literal|0
condition|)
block|{
comment|// If any table is empty, an inner join involving the tables should yield 0 rows.
name|LOG
operator|.
name|info
argument_list|(
literal|"Hash table number "
operator|+
name|idx
operator|+
literal|" is empty"
argument_list|)
expr_stmt|;
name|skipJoinProcessing
operator|=
literal|true
expr_stmt|;
break|break;
block|}
block|}
return|return
name|skipJoinProcessing
return|;
block|}
block|}
end_class

end_unit

