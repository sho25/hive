begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/**  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|parse
package|;
end_package

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|base
operator|.
name|Function
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|collect
operator|.
name|Lists
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|primitives
operator|.
name|Ints
import|;
end_import

begin_import
import|import
name|org
operator|.
name|antlr
operator|.
name|runtime
operator|.
name|tree
operator|.
name|Tree
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileStatus
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|IMetaStoreClient
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Database
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|NotificationEvent
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|messaging
operator|.
name|EventUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|messaging
operator|.
name|MessageDeserializer
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|messaging
operator|.
name|MessageFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|messaging
operator|.
name|json
operator|.
name|JSONMessageFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|ErrorMsg
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|QueryState
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|ReplCopyTask
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|Task
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|TaskFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|Utilities
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|hooks
operator|.
name|ReadEntity
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|HiveException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|Partition
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|Table
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|CreateDatabaseDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|DDLWork
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|PlanUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|IOUtils
import|;
end_import

begin_import
import|import
name|javax
operator|.
name|annotation
operator|.
name|Nullable
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|DataOutputStream
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|FileNotFoundException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|Serializable
import|;
end_import

begin_import
import|import
name|java
operator|.
name|net
operator|.
name|URI
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Arrays
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Collections
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|LinkedHashMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|parse
operator|.
name|HiveParser
operator|.
name|*
import|;
end_import

begin_class
specifier|public
class|class
name|ReplicationSemanticAnalyzer
extends|extends
name|BaseSemanticAnalyzer
block|{
comment|// Database name or pattern
specifier|private
name|String
name|dbNameOrPattern
decl_stmt|;
comment|// Table name or pattern
specifier|private
name|String
name|tblNameOrPattern
decl_stmt|;
specifier|private
name|Long
name|eventFrom
decl_stmt|;
specifier|private
name|Long
name|eventTo
decl_stmt|;
specifier|private
name|Integer
name|batchSize
decl_stmt|;
comment|// Base path for REPL LOAD
specifier|private
name|String
name|path
decl_stmt|;
specifier|private
specifier|static
name|String
name|testInjectDumpDir
init|=
literal|null
decl_stmt|;
comment|// unit tests can overwrite this to affect default dump behaviour
specifier|private
specifier|static
specifier|final
name|String
name|dumpSchema
init|=
literal|"dump_dir,last_repl_id#string,string"
decl_stmt|;
specifier|public
name|ReplicationSemanticAnalyzer
parameter_list|(
name|QueryState
name|queryState
parameter_list|)
throws|throws
name|SemanticException
block|{
name|super
argument_list|(
name|queryState
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|analyzeInternal
parameter_list|(
name|ASTNode
name|ast
parameter_list|)
throws|throws
name|SemanticException
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"ReplicationSemanticAanalyzer: analyzeInternal"
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
name|ast
operator|.
name|getName
argument_list|()
operator|+
literal|":"
operator|+
name|ast
operator|.
name|getToken
argument_list|()
operator|.
name|getText
argument_list|()
operator|+
literal|"="
operator|+
name|ast
operator|.
name|getText
argument_list|()
argument_list|)
expr_stmt|;
switch|switch
condition|(
name|ast
operator|.
name|getToken
argument_list|()
operator|.
name|getType
argument_list|()
condition|)
block|{
case|case
name|TOK_REPL_DUMP
case|:
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"ReplicationSemanticAnalyzer: analyzeInternal: dump"
argument_list|)
expr_stmt|;
name|initReplDump
argument_list|(
name|ast
argument_list|)
expr_stmt|;
name|analyzeReplDump
argument_list|(
name|ast
argument_list|)
expr_stmt|;
break|break;
block|}
case|case
name|TOK_REPL_LOAD
case|:
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"ReplicationSemanticAnalyzer: analyzeInternal: load"
argument_list|)
expr_stmt|;
name|initReplLoad
argument_list|(
name|ast
argument_list|)
expr_stmt|;
name|analyzeReplLoad
argument_list|(
name|ast
argument_list|)
expr_stmt|;
break|break;
block|}
case|case
name|TOK_REPL_STATUS
case|:
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"ReplicationSemanticAnalyzer: analyzeInternal: status"
argument_list|)
expr_stmt|;
name|initReplStatus
argument_list|(
name|ast
argument_list|)
expr_stmt|;
name|analyzeReplStatus
argument_list|(
name|ast
argument_list|)
expr_stmt|;
break|break;
block|}
default|default:
block|{
throw|throw
operator|new
name|SemanticException
argument_list|(
literal|"Unexpected root token"
argument_list|)
throw|;
block|}
block|}
block|}
specifier|private
name|void
name|initReplDump
parameter_list|(
name|ASTNode
name|ast
parameter_list|)
block|{
name|int
name|numChildren
init|=
name|ast
operator|.
name|getChildCount
argument_list|()
decl_stmt|;
name|dbNameOrPattern
operator|=
name|PlanUtils
operator|.
name|stripQuotes
argument_list|(
name|ast
operator|.
name|getChild
argument_list|(
literal|0
argument_list|)
operator|.
name|getText
argument_list|()
argument_list|)
expr_stmt|;
comment|// skip the first node, which is always required
name|int
name|currNode
init|=
literal|1
decl_stmt|;
while|while
condition|(
name|currNode
operator|<
name|numChildren
condition|)
block|{
if|if
condition|(
name|ast
operator|.
name|getChild
argument_list|(
name|currNode
argument_list|)
operator|.
name|getType
argument_list|()
operator|!=
name|TOK_FROM
condition|)
block|{
comment|// optional tblName was specified.
name|tblNameOrPattern
operator|=
name|PlanUtils
operator|.
name|stripQuotes
argument_list|(
name|ast
operator|.
name|getChild
argument_list|(
name|currNode
argument_list|)
operator|.
name|getText
argument_list|()
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// TOK_FROM subtree
name|Tree
name|fromNode
init|=
name|ast
operator|.
name|getChild
argument_list|(
name|currNode
argument_list|)
decl_stmt|;
name|eventFrom
operator|=
name|Long
operator|.
name|parseLong
argument_list|(
name|PlanUtils
operator|.
name|stripQuotes
argument_list|(
name|fromNode
operator|.
name|getChild
argument_list|(
literal|0
argument_list|)
operator|.
name|getText
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
comment|// skip the first, which is always required
name|int
name|numChild
init|=
literal|1
decl_stmt|;
while|while
condition|(
name|numChild
operator|<
name|fromNode
operator|.
name|getChildCount
argument_list|()
condition|)
block|{
if|if
condition|(
name|fromNode
operator|.
name|getChild
argument_list|(
name|numChild
argument_list|)
operator|.
name|getType
argument_list|()
operator|==
name|TOK_TO
condition|)
block|{
name|eventTo
operator|=
name|Long
operator|.
name|parseLong
argument_list|(
name|PlanUtils
operator|.
name|stripQuotes
argument_list|(
name|fromNode
operator|.
name|getChild
argument_list|(
name|numChild
operator|+
literal|1
argument_list|)
operator|.
name|getText
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
comment|// skip the next child, since we already took care of it
name|numChild
operator|++
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|fromNode
operator|.
name|getChild
argument_list|(
name|numChild
argument_list|)
operator|.
name|getType
argument_list|()
operator|==
name|TOK_BATCH
condition|)
block|{
name|batchSize
operator|=
name|Integer
operator|.
name|parseInt
argument_list|(
name|PlanUtils
operator|.
name|stripQuotes
argument_list|(
name|fromNode
operator|.
name|getChild
argument_list|(
name|numChild
operator|+
literal|1
argument_list|)
operator|.
name|getText
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
comment|// skip the next child, since we already took care of it
name|numChild
operator|++
expr_stmt|;
block|}
comment|// move to the next child in FROM tree
name|numChild
operator|++
expr_stmt|;
block|}
comment|// FROM node is always the last
break|break;
block|}
comment|// move to the next root node
name|currNode
operator|++
expr_stmt|;
block|}
block|}
comment|// REPL DUMP
specifier|private
name|void
name|analyzeReplDump
parameter_list|(
name|ASTNode
name|ast
parameter_list|)
throws|throws
name|SemanticException
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"ReplicationSemanticAnalyzer.analyzeReplDump: "
operator|+
name|String
operator|.
name|valueOf
argument_list|(
name|dbNameOrPattern
argument_list|)
operator|+
literal|"."
operator|+
name|String
operator|.
name|valueOf
argument_list|(
name|tblNameOrPattern
argument_list|)
operator|+
literal|" from "
operator|+
name|String
operator|.
name|valueOf
argument_list|(
name|eventFrom
argument_list|)
operator|+
literal|" to "
operator|+
name|String
operator|.
name|valueOf
argument_list|(
name|eventTo
argument_list|)
operator|+
literal|" batchsize "
operator|+
name|String
operator|.
name|valueOf
argument_list|(
name|batchSize
argument_list|)
argument_list|)
expr_stmt|;
name|String
name|replRoot
init|=
name|conf
operator|.
name|getVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|REPLDIR
argument_list|)
decl_stmt|;
name|Path
name|dumpRoot
init|=
operator|new
name|Path
argument_list|(
name|replRoot
argument_list|,
name|getNextDumpDir
argument_list|()
argument_list|)
decl_stmt|;
name|Path
name|dumpMetadata
init|=
operator|new
name|Path
argument_list|(
name|dumpRoot
argument_list|,
literal|"_dumpmetadata"
argument_list|)
decl_stmt|;
name|String
name|lastReplId
decl_stmt|;
try|try
block|{
if|if
condition|(
name|eventFrom
operator|==
literal|null
condition|)
block|{
comment|// bootstrap case
name|String
name|bootDumpBeginReplId
init|=
name|String
operator|.
name|valueOf
argument_list|(
name|db
operator|.
name|getMSC
argument_list|()
operator|.
name|getCurrentNotificationEventId
argument_list|()
operator|.
name|getEventId
argument_list|()
argument_list|)
decl_stmt|;
for|for
control|(
name|String
name|dbName
range|:
name|matchesDb
argument_list|(
name|dbNameOrPattern
argument_list|)
control|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"ReplicationSemanticAnalyzer: analyzeReplDump dumping db: "
operator|+
name|dbName
argument_list|)
expr_stmt|;
name|Path
name|dbRoot
init|=
name|dumpDbMetadata
argument_list|(
name|dbName
argument_list|,
name|dumpRoot
argument_list|)
decl_stmt|;
for|for
control|(
name|String
name|tblName
range|:
name|matchesTbl
argument_list|(
name|dbName
argument_list|,
name|tblNameOrPattern
argument_list|)
control|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"ReplicationSemanticAnalyzer: analyzeReplDump dumping table: "
operator|+
name|tblName
operator|+
literal|" to db root "
operator|+
name|dbRoot
operator|.
name|toUri
argument_list|()
argument_list|)
expr_stmt|;
name|dumpTbl
argument_list|(
name|ast
argument_list|,
name|dbName
argument_list|,
name|tblName
argument_list|,
name|dbRoot
argument_list|)
expr_stmt|;
block|}
block|}
name|String
name|bootDumpEndReplId
init|=
name|String
operator|.
name|valueOf
argument_list|(
name|db
operator|.
name|getMSC
argument_list|()
operator|.
name|getCurrentNotificationEventId
argument_list|()
operator|.
name|getEventId
argument_list|()
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Bootstrap object dump phase took from {} to {}"
argument_list|,
name|bootDumpBeginReplId
argument_list|,
name|bootDumpEndReplId
argument_list|)
expr_stmt|;
comment|// Now that bootstrap has dumped all objects related, we have to account for the changes
comment|// that occurred while bootstrap was happening - i.e. we have to look through all events
comment|// during the bootstrap period and consolidate them with our dump.
name|IMetaStoreClient
operator|.
name|NotificationFilter
name|evFilter
init|=
name|EventUtils
operator|.
name|getDbTblNotificationFilter
argument_list|(
name|dbNameOrPattern
argument_list|,
name|tblNameOrPattern
argument_list|)
decl_stmt|;
name|EventUtils
operator|.
name|MSClientNotificationFetcher
name|evFetcher
init|=
operator|new
name|EventUtils
operator|.
name|MSClientNotificationFetcher
argument_list|(
name|db
operator|.
name|getMSC
argument_list|()
argument_list|)
decl_stmt|;
name|EventUtils
operator|.
name|NotificationEventIterator
name|evIter
init|=
operator|new
name|EventUtils
operator|.
name|NotificationEventIterator
argument_list|(
name|evFetcher
argument_list|,
name|Long
operator|.
name|valueOf
argument_list|(
name|bootDumpBeginReplId
argument_list|)
argument_list|,
name|Ints
operator|.
name|checkedCast
argument_list|(
name|Long
operator|.
name|valueOf
argument_list|(
name|bootDumpEndReplId
argument_list|)
operator|-
name|Long
operator|.
name|valueOf
argument_list|(
name|bootDumpBeginReplId
argument_list|)
operator|+
literal|1
argument_list|)
argument_list|,
name|evFilter
argument_list|)
decl_stmt|;
comment|// Now we consolidate all the events that happenned during the objdump into the objdump
while|while
condition|(
name|evIter
operator|.
name|hasNext
argument_list|()
condition|)
block|{
name|NotificationEvent
name|ev
init|=
name|evIter
operator|.
name|next
argument_list|()
decl_stmt|;
name|Path
name|evRoot
init|=
operator|new
name|Path
argument_list|(
name|dumpRoot
argument_list|,
name|String
operator|.
name|valueOf
argument_list|(
name|ev
operator|.
name|getEventId
argument_list|()
argument_list|)
argument_list|)
decl_stmt|;
comment|// FIXME : implement consolidateEvent(..) similar to dumpEvent(ev,evRoot)
block|}
name|LOG
operator|.
name|info
argument_list|(
literal|"Consolidation done, preparing to return {},{}"
argument_list|,
name|dumpRoot
operator|.
name|toUri
argument_list|()
argument_list|,
name|bootDumpEndReplId
argument_list|)
expr_stmt|;
comment|// Set the correct last repl id to return to the user
name|lastReplId
operator|=
name|bootDumpEndReplId
expr_stmt|;
block|}
else|else
block|{
comment|// get list of events matching dbPattern& tblPattern
comment|// go through each event, and dump out each event to a event-level dump dir inside dumproot
if|if
condition|(
name|eventTo
operator|==
literal|null
condition|)
block|{
name|eventTo
operator|=
name|db
operator|.
name|getMSC
argument_list|()
operator|.
name|getCurrentNotificationEventId
argument_list|()
operator|.
name|getEventId
argument_list|()
expr_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"eventTo not specified, using current event id : {}"
argument_list|,
name|eventTo
argument_list|)
expr_stmt|;
block|}
name|Integer
name|maxRange
init|=
name|Ints
operator|.
name|checkedCast
argument_list|(
name|eventTo
operator|-
name|eventFrom
operator|+
literal|1
argument_list|)
decl_stmt|;
name|batchSize
operator|=
literal|15
expr_stmt|;
if|if
condition|(
name|batchSize
operator|==
literal|null
condition|)
block|{
name|batchSize
operator|=
name|maxRange
expr_stmt|;
block|}
else|else
block|{
if|if
condition|(
name|batchSize
operator|>
name|maxRange
condition|)
block|{
name|batchSize
operator|=
name|maxRange
expr_stmt|;
block|}
block|}
name|IMetaStoreClient
operator|.
name|NotificationFilter
name|evFilter
init|=
name|EventUtils
operator|.
name|andFilter
argument_list|(
name|EventUtils
operator|.
name|getDbTblNotificationFilter
argument_list|(
name|dbNameOrPattern
argument_list|,
name|tblNameOrPattern
argument_list|)
argument_list|,
name|EventUtils
operator|.
name|getEventBoundaryFilter
argument_list|(
name|eventFrom
argument_list|,
name|eventTo
argument_list|)
argument_list|)
decl_stmt|;
name|EventUtils
operator|.
name|MSClientNotificationFetcher
name|evFetcher
init|=
operator|new
name|EventUtils
operator|.
name|MSClientNotificationFetcher
argument_list|(
name|db
operator|.
name|getMSC
argument_list|()
argument_list|)
decl_stmt|;
name|EventUtils
operator|.
name|NotificationEventIterator
name|evIter
init|=
operator|new
name|EventUtils
operator|.
name|NotificationEventIterator
argument_list|(
name|evFetcher
argument_list|,
name|eventFrom
argument_list|,
name|batchSize
argument_list|,
name|evFilter
argument_list|)
decl_stmt|;
while|while
condition|(
name|evIter
operator|.
name|hasNext
argument_list|()
condition|)
block|{
name|NotificationEvent
name|ev
init|=
name|evIter
operator|.
name|next
argument_list|()
decl_stmt|;
name|Path
name|evRoot
init|=
operator|new
name|Path
argument_list|(
name|dumpRoot
argument_list|,
name|String
operator|.
name|valueOf
argument_list|(
name|ev
operator|.
name|getEventId
argument_list|()
argument_list|)
argument_list|)
decl_stmt|;
name|dumpEvent
argument_list|(
name|ev
argument_list|,
name|evRoot
argument_list|)
expr_stmt|;
block|}
name|LOG
operator|.
name|info
argument_list|(
literal|"Done dumping events, preparing to return {},{}"
argument_list|,
name|dumpRoot
operator|.
name|toUri
argument_list|()
argument_list|,
name|eventTo
argument_list|)
expr_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|vals
decl_stmt|;
name|writeOutput
argument_list|(
name|Arrays
operator|.
name|asList
argument_list|(
literal|"event"
argument_list|,
name|String
operator|.
name|valueOf
argument_list|(
name|eventFrom
argument_list|)
argument_list|,
name|String
operator|.
name|valueOf
argument_list|(
name|eventTo
argument_list|)
argument_list|)
argument_list|,
name|dumpMetadata
argument_list|)
expr_stmt|;
comment|// Set the correct last repl id to return to the user
name|lastReplId
operator|=
name|String
operator|.
name|valueOf
argument_list|(
name|eventTo
argument_list|)
expr_stmt|;
block|}
name|prepareReturnValues
argument_list|(
name|Arrays
operator|.
name|asList
argument_list|(
name|dumpRoot
operator|.
name|toUri
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|,
name|lastReplId
argument_list|)
argument_list|,
name|dumpSchema
argument_list|)
expr_stmt|;
name|setFetchTask
argument_list|(
name|createFetchTask
argument_list|(
name|dumpSchema
argument_list|)
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
comment|// TODO : simple wrap& rethrow for now, clean up with error codes
name|LOG
operator|.
name|warn
argument_list|(
literal|"Error during analyzeReplDump"
argument_list|,
name|e
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|SemanticException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|private
name|void
name|dumpEvent
parameter_list|(
name|NotificationEvent
name|ev
parameter_list|,
name|Path
name|evRoot
parameter_list|)
throws|throws
name|Exception
block|{
name|long
name|evid
init|=
name|ev
operator|.
name|getEventId
argument_list|()
decl_stmt|;
name|String
name|evidStr
init|=
name|String
operator|.
name|valueOf
argument_list|(
name|evid
argument_list|)
decl_stmt|;
name|ReplicationSpec
name|replicationSpec
init|=
name|getNewReplicationSpec
argument_list|(
name|evidStr
argument_list|,
name|evidStr
argument_list|)
decl_stmt|;
name|MessageDeserializer
name|md
init|=
name|MessageFactory
operator|.
name|getInstance
argument_list|()
operator|.
name|getDeserializer
argument_list|()
decl_stmt|;
switch|switch
condition|(
name|ev
operator|.
name|getEventType
argument_list|()
condition|)
block|{
case|case
name|MessageFactory
operator|.
name|CREATE_TABLE_EVENT
case|:
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Processing#{} CREATE_TABLE message : {}"
argument_list|,
name|ev
operator|.
name|getEventId
argument_list|()
argument_list|,
name|ev
operator|.
name|getMessage
argument_list|()
argument_list|)
expr_stmt|;
comment|// FIXME : Current MessageFactory api is lacking,
comment|// and impl is in JSONMessageFactory instead. This needs to be
comment|// refactored correctly so we don't depend on a specific impl.
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Table
name|tobj
init|=
name|JSONMessageFactory
operator|.
name|getTableObj
argument_list|(
name|JSONMessageFactory
operator|.
name|getJsonTree
argument_list|(
name|ev
argument_list|)
argument_list|)
decl_stmt|;
if|if
condition|(
name|tobj
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Event#{} was a CREATE_TABLE_EVENT with no table listed"
argument_list|)
expr_stmt|;
break|break;
block|}
name|Table
name|qlMdTable
init|=
operator|new
name|Table
argument_list|(
name|tobj
argument_list|)
decl_stmt|;
name|Path
name|metaDataPath
init|=
operator|new
name|Path
argument_list|(
name|evRoot
argument_list|,
name|EximUtil
operator|.
name|METADATA_NAME
argument_list|)
decl_stmt|;
name|EximUtil
operator|.
name|createExportDump
argument_list|(
name|metaDataPath
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
argument_list|,
name|metaDataPath
argument_list|,
name|qlMdTable
argument_list|,
literal|null
argument_list|,
name|replicationSpec
argument_list|)
expr_stmt|;
comment|// FIXME : dump _files should happen at dbnotif time, doing it here is incorrect
comment|// we will, however, do so here, now, for dev/debug's sake.
name|Path
name|dataPath
init|=
operator|new
name|Path
argument_list|(
name|evRoot
argument_list|,
literal|"data"
argument_list|)
decl_stmt|;
name|rootTasks
operator|.
name|add
argument_list|(
name|ReplCopyTask
operator|.
name|getDumpCopyTask
argument_list|(
name|replicationSpec
argument_list|,
name|qlMdTable
operator|.
name|getPath
argument_list|()
argument_list|,
name|dataPath
argument_list|,
name|conf
argument_list|)
argument_list|)
expr_stmt|;
break|break;
block|}
case|case
name|MessageFactory
operator|.
name|ADD_PARTITION_EVENT
case|:
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Processing#{} ADD_PARTITION message : {}"
argument_list|,
name|ev
operator|.
name|getEventId
argument_list|()
argument_list|,
name|ev
operator|.
name|getMessage
argument_list|()
argument_list|)
expr_stmt|;
comment|// FIXME : Current MessageFactory api is lacking,
comment|// and impl is in JSONMessageFactory instead. This needs to be
comment|// refactored correctly so we don't depend on a specific impl.
name|List
argument_list|<
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
argument_list|>
name|ptnObjs
init|=
name|JSONMessageFactory
operator|.
name|getPartitionObjList
argument_list|(
name|JSONMessageFactory
operator|.
name|getJsonTree
argument_list|(
name|ev
argument_list|)
argument_list|)
decl_stmt|;
if|if
condition|(
operator|(
name|ptnObjs
operator|==
literal|null
operator|)
operator|||
operator|(
name|ptnObjs
operator|.
name|size
argument_list|()
operator|==
literal|0
operator|)
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Event#{} was an ADD_PTN_EVENT with no partitions"
argument_list|)
expr_stmt|;
break|break;
block|}
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Table
name|tobj
init|=
name|JSONMessageFactory
operator|.
name|getTableObj
argument_list|(
name|JSONMessageFactory
operator|.
name|getJsonTree
argument_list|(
name|ev
argument_list|)
argument_list|)
decl_stmt|;
if|if
condition|(
name|tobj
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Event#{} was a ADD_PTN_EVENT with no table listed"
argument_list|)
expr_stmt|;
break|break;
block|}
specifier|final
name|Table
name|qlMdTable
init|=
operator|new
name|Table
argument_list|(
name|tobj
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|Partition
argument_list|>
name|qlPtns
init|=
name|Lists
operator|.
name|transform
argument_list|(
name|ptnObjs
argument_list|,
operator|new
name|Function
argument_list|<
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
argument_list|,
name|Partition
argument_list|>
argument_list|()
block|{
annotation|@
name|Nullable
annotation|@
name|Override
specifier|public
name|Partition
name|apply
parameter_list|(
annotation|@
name|Nullable
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
name|input
parameter_list|)
block|{
if|if
condition|(
name|input
operator|==
literal|null
condition|)
block|{
return|return
literal|null
return|;
block|}
try|try
block|{
return|return
operator|new
name|Partition
argument_list|(
name|qlMdTable
argument_list|,
name|input
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|HiveException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
block|}
argument_list|)
decl_stmt|;
name|Path
name|metaDataPath
init|=
operator|new
name|Path
argument_list|(
name|evRoot
argument_list|,
name|EximUtil
operator|.
name|METADATA_NAME
argument_list|)
decl_stmt|;
name|EximUtil
operator|.
name|createExportDump
argument_list|(
name|metaDataPath
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
argument_list|,
name|metaDataPath
argument_list|,
name|qlMdTable
argument_list|,
name|qlPtns
argument_list|,
name|replicationSpec
argument_list|)
expr_stmt|;
comment|// FIXME : dump _files should ideally happen at dbnotif time, doing it here introduces
comment|// rubberbanding. But, till we have support for that, this is our closest equivalent
for|for
control|(
name|Partition
name|qlPtn
range|:
name|qlPtns
control|)
block|{
name|Path
name|ptnDataPath
init|=
operator|new
name|Path
argument_list|(
name|evRoot
argument_list|,
name|qlPtn
operator|.
name|getName
argument_list|()
argument_list|)
decl_stmt|;
name|rootTasks
operator|.
name|add
argument_list|(
name|ReplCopyTask
operator|.
name|getDumpCopyTask
argument_list|(
name|replicationSpec
argument_list|,
name|qlPtn
operator|.
name|getPartitionPath
argument_list|()
argument_list|,
name|ptnDataPath
argument_list|,
name|conf
argument_list|)
argument_list|)
expr_stmt|;
block|}
break|break;
block|}
default|default:
name|LOG
operator|.
name|info
argument_list|(
literal|"Skipping processing#{} message : {}"
argument_list|,
name|ev
operator|.
name|getEventId
argument_list|()
argument_list|,
name|ev
operator|.
name|getMessage
argument_list|()
argument_list|)
expr_stmt|;
comment|// TODO : handle other event types
break|break;
block|}
block|}
specifier|public
specifier|static
name|void
name|injectNextDumpDirForTest
parameter_list|(
name|String
name|dumpdir
parameter_list|)
block|{
name|testInjectDumpDir
operator|=
name|dumpdir
expr_stmt|;
block|}
name|String
name|getNextDumpDir
parameter_list|()
block|{
if|if
condition|(
name|conf
operator|.
name|getBoolVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_IN_TEST
argument_list|)
condition|)
block|{
comment|// make it easy to write .q unit tests, instead of unique id generation.
comment|// however, this does mean that in writing tests, we have to be aware that
comment|// repl dump will clash with prior dumps, and thus have to clean up properly.
if|if
condition|(
name|testInjectDumpDir
operator|==
literal|null
condition|)
block|{
return|return
literal|"next"
return|;
block|}
else|else
block|{
return|return
name|testInjectDumpDir
return|;
block|}
block|}
else|else
block|{
return|return
name|String
operator|.
name|valueOf
argument_list|(
name|System
operator|.
name|currentTimeMillis
argument_list|()
argument_list|)
return|;
comment|// TODO: time good enough for now - we'll likely improve this.
comment|// We may also work in something the equivalent of pid, thrid and move to nanos to ensure
comment|// uniqueness.
block|}
block|}
comment|/**    *    * @param dbName    * @param dumpRoot    * @return db dumped path    * @throws SemanticException    */
specifier|private
name|Path
name|dumpDbMetadata
parameter_list|(
name|String
name|dbName
parameter_list|,
name|Path
name|dumpRoot
parameter_list|)
throws|throws
name|SemanticException
block|{
name|Path
name|dbRoot
init|=
operator|new
name|Path
argument_list|(
name|dumpRoot
argument_list|,
name|dbName
argument_list|)
decl_stmt|;
try|try
block|{
comment|// TODO : instantiating FS objects are generally costly. Refactor
name|FileSystem
name|fs
init|=
name|dbRoot
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|Path
name|dumpPath
init|=
operator|new
name|Path
argument_list|(
name|dbRoot
argument_list|,
name|EximUtil
operator|.
name|METADATA_NAME
argument_list|)
decl_stmt|;
name|Database
name|dbObj
init|=
name|db
operator|.
name|getDatabase
argument_list|(
name|dbName
argument_list|)
decl_stmt|;
name|EximUtil
operator|.
name|createDbExportDump
argument_list|(
name|fs
argument_list|,
name|dumpPath
argument_list|,
name|dbObj
argument_list|,
name|getNewReplicationSpec
argument_list|()
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
comment|// TODO : simple wrap& rethrow for now, clean up with error codes
throw|throw
operator|new
name|SemanticException
argument_list|(
name|e
argument_list|)
throw|;
block|}
return|return
name|dbRoot
return|;
block|}
comment|/**    *    * @param ast    * @param dbName    * @param tblName    * @param dbRoot    * @return tbl dumped path    * @throws SemanticException    */
specifier|private
name|Path
name|dumpTbl
parameter_list|(
name|ASTNode
name|ast
parameter_list|,
name|String
name|dbName
parameter_list|,
name|String
name|tblName
parameter_list|,
name|Path
name|dbRoot
parameter_list|)
throws|throws
name|SemanticException
block|{
name|Path
name|tableRoot
init|=
operator|new
name|Path
argument_list|(
name|dbRoot
argument_list|,
name|tblName
argument_list|)
decl_stmt|;
try|try
block|{
name|URI
name|toURI
init|=
name|EximUtil
operator|.
name|getValidatedURI
argument_list|(
name|conf
argument_list|,
name|tableRoot
operator|.
name|toUri
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|)
decl_stmt|;
name|TableSpec
name|ts
init|=
operator|new
name|TableSpec
argument_list|(
name|db
argument_list|,
name|conf
argument_list|,
name|dbName
operator|+
literal|"."
operator|+
name|tblName
argument_list|,
literal|null
argument_list|)
decl_stmt|;
name|ExportSemanticAnalyzer
operator|.
name|prepareExport
argument_list|(
name|ast
argument_list|,
name|toURI
argument_list|,
name|ts
argument_list|,
name|getNewReplicationSpec
argument_list|()
argument_list|,
name|db
argument_list|,
name|conf
argument_list|,
name|ctx
argument_list|,
name|rootTasks
argument_list|,
name|inputs
argument_list|,
name|outputs
argument_list|,
name|LOG
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|HiveException
name|e
parameter_list|)
block|{
comment|// TODO : simple wrap& rethrow for now, clean up with error codes
throw|throw
operator|new
name|SemanticException
argument_list|(
name|e
argument_list|)
throw|;
block|}
return|return
name|tableRoot
return|;
block|}
comment|// REPL LOAD
specifier|private
name|void
name|initReplLoad
parameter_list|(
name|ASTNode
name|ast
parameter_list|)
block|{
name|int
name|numChildren
init|=
name|ast
operator|.
name|getChildCount
argument_list|()
decl_stmt|;
name|path
operator|=
name|PlanUtils
operator|.
name|stripQuotes
argument_list|(
name|ast
operator|.
name|getChild
argument_list|(
literal|0
argument_list|)
operator|.
name|getText
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|numChildren
operator|>
literal|1
condition|)
block|{
name|dbNameOrPattern
operator|=
name|PlanUtils
operator|.
name|stripQuotes
argument_list|(
name|ast
operator|.
name|getChild
argument_list|(
literal|1
argument_list|)
operator|.
name|getText
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|numChildren
operator|>
literal|2
condition|)
block|{
name|tblNameOrPattern
operator|=
name|PlanUtils
operator|.
name|stripQuotes
argument_list|(
name|ast
operator|.
name|getChild
argument_list|(
literal|2
argument_list|)
operator|.
name|getText
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
comment|/*    * Example dump dirs we need to be able to handle :    *    * for: hive.repl.rootdir = staging/ Then, repl dumps will be created in staging/<dumpdir>    *    * single-db-dump: staging/blah12345 blah12345/ default/ _metadata tbl1/ _metadata dt=20160907/    * _files tbl2/ tbl3/ unptn_tbl/ _metadata _files    *    * multi-db-dump: staging/bar12347 staging/ bar12347/ default/ ... sales/ ...    *    * single table-dump: staging/baz123 staging/ baz123/ _metadata dt=20150931/ _files    */
specifier|private
name|void
name|analyzeReplLoad
parameter_list|(
name|ASTNode
name|ast
parameter_list|)
throws|throws
name|SemanticException
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"ReplSemanticAnalyzer.analyzeReplLoad: "
operator|+
name|String
operator|.
name|valueOf
argument_list|(
name|dbNameOrPattern
argument_list|)
operator|+
literal|"."
operator|+
name|String
operator|.
name|valueOf
argument_list|(
name|tblNameOrPattern
argument_list|)
operator|+
literal|" from "
operator|+
name|String
operator|.
name|valueOf
argument_list|(
name|path
argument_list|)
argument_list|)
expr_stmt|;
comment|// for analyze repl load, we walk through the dir structure available in the path,
comment|// looking at each db, and then each table, and then setting up the appropriate
comment|// import job in its place.
try|try
block|{
name|Path
name|loadPath
init|=
operator|new
name|Path
argument_list|(
name|path
argument_list|)
decl_stmt|;
specifier|final
name|FileSystem
name|fs
init|=
name|loadPath
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|fs
operator|.
name|exists
argument_list|(
name|loadPath
argument_list|)
condition|)
block|{
comment|// supposed dump path does not exist.
throw|throw
operator|new
name|FileNotFoundException
argument_list|(
name|loadPath
operator|.
name|toUri
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|)
throw|;
block|}
comment|// Now, the dumped path can be one of three things:
comment|// a) It can be a db dump, in which case we expect a set of dirs, each with a
comment|// db name, and with a _metadata file in each, and table dirs inside that.
comment|// b) It can be a table dump dir, in which case we expect a _metadata dump of
comment|// a table in question in the dir, and individual ptn dir hierarchy.
comment|// c) A dump can be an event-level dump, which means we have several subdirs
comment|// each of which have the evid as the dir name, and each of which correspond
comment|// to a event-level dump. Currently, only CREATE_TABLE and ADD_PARTITION are
comment|// handled, so all of these dumps will be at a table/ptn level.
comment|// For incremental repl, eventually, we can have individual events which can
comment|// be other things like roles and fns as well.
name|boolean
name|evDump
init|=
literal|false
decl_stmt|;
name|Path
name|dumpMetadata
init|=
operator|new
name|Path
argument_list|(
name|loadPath
argument_list|,
literal|"_dumpmetadata"
argument_list|)
decl_stmt|;
comment|// TODO : only event dumps currently have _dumpmetadata - this might change. Generify.
if|if
condition|(
name|fs
operator|.
name|exists
argument_list|(
name|dumpMetadata
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"{} exists, this is a event dump"
argument_list|,
name|dumpMetadata
argument_list|)
expr_stmt|;
name|evDump
operator|=
literal|true
expr_stmt|;
block|}
else|else
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"{} does not exist, this is an object dump"
argument_list|,
name|dumpMetadata
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
operator|(
operator|!
name|evDump
operator|)
operator|&&
operator|(
name|tblNameOrPattern
operator|!=
literal|null
operator|)
operator|&&
operator|!
operator|(
name|tblNameOrPattern
operator|.
name|isEmpty
argument_list|()
operator|)
condition|)
block|{
comment|// not an event dump, and table name pattern specified, this has to be a tbl-level dump
name|analyzeTableLoad
argument_list|(
name|dbNameOrPattern
argument_list|,
name|tblNameOrPattern
argument_list|,
name|path
argument_list|,
literal|null
argument_list|)
expr_stmt|;
return|return;
block|}
name|FileStatus
index|[]
name|srcs
init|=
name|LoadSemanticAnalyzer
operator|.
name|matchFilesOrDir
argument_list|(
name|fs
argument_list|,
name|loadPath
argument_list|)
decl_stmt|;
if|if
condition|(
name|srcs
operator|==
literal|null
operator|||
operator|(
name|srcs
operator|.
name|length
operator|==
literal|0
operator|)
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Nothing to load at {}"
argument_list|,
name|loadPath
operator|.
name|toUri
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
return|return;
block|}
name|FileStatus
index|[]
name|dirsInLoadPath
init|=
name|fs
operator|.
name|listStatus
argument_list|(
name|loadPath
argument_list|,
name|EximUtil
operator|.
name|getDirectoryFilter
argument_list|(
name|fs
argument_list|)
argument_list|)
decl_stmt|;
if|if
condition|(
operator|(
name|dirsInLoadPath
operator|==
literal|null
operator|)
operator|||
operator|(
name|dirsInLoadPath
operator|.
name|length
operator|==
literal|0
operator|)
condition|)
block|{
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
literal|"No data to load in path "
operator|+
name|loadPath
operator|.
name|toUri
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|)
throw|;
block|}
if|if
condition|(
operator|!
name|evDump
condition|)
block|{
comment|// not an event dump, not a table dump - thus, a db dump
if|if
condition|(
operator|(
name|dbNameOrPattern
operator|!=
literal|null
operator|)
operator|&&
operator|(
name|dirsInLoadPath
operator|.
name|length
operator|>
literal|1
operator|)
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Found multiple dirs when we expected 1:"
argument_list|)
expr_stmt|;
for|for
control|(
name|FileStatus
name|d
range|:
name|dirsInLoadPath
control|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"> "
operator|+
name|d
operator|.
name|getPath
argument_list|()
operator|.
name|toUri
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
literal|"Multiple dirs in "
operator|+
name|loadPath
operator|.
name|toUri
argument_list|()
operator|.
name|toString
argument_list|()
operator|+
literal|" does not correspond to REPL LOAD expecting to load to a singular destination point."
argument_list|)
throw|;
block|}
for|for
control|(
name|FileStatus
name|dir
range|:
name|dirsInLoadPath
control|)
block|{
name|analyzeDatabaseLoad
argument_list|(
name|dbNameOrPattern
argument_list|,
name|fs
argument_list|,
name|dir
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
comment|// event dump, each subdir is an individual event dump.
for|for
control|(
name|FileStatus
name|dir
range|:
name|dirsInLoadPath
control|)
block|{
comment|// event loads will behave similar to table loads, with one crucial difference
comment|// precursor order is strict, and each event must be processed after the previous one.
name|LOG
operator|.
name|debug
argument_list|(
literal|"Loading event from {} to {}.{}"
argument_list|,
name|dir
operator|.
name|getPath
argument_list|()
operator|.
name|toUri
argument_list|()
argument_list|,
name|dbNameOrPattern
argument_list|,
name|tblNameOrPattern
argument_list|)
expr_stmt|;
name|analyzeTableLoad
argument_list|(
name|dbNameOrPattern
argument_list|,
name|tblNameOrPattern
argument_list|,
name|dir
operator|.
name|getPath
argument_list|()
operator|.
name|toUri
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|,
literal|null
argument_list|)
expr_stmt|;
comment|// FIXME: we should have a strict order of execution so that each event's tasks occur linearly
block|}
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
comment|// TODO : simple wrap& rethrow for now, clean up with error codes
throw|throw
operator|new
name|SemanticException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|private
name|void
name|analyzeEventLoad
parameter_list|(
name|String
name|dbNameOrPattern
parameter_list|,
name|String
name|tblNameOrPattern
parameter_list|,
name|FileSystem
name|fs
parameter_list|,
name|FileStatus
name|dir
parameter_list|)
throws|throws
name|SemanticException
block|{    }
specifier|private
name|void
name|analyzeDatabaseLoad
parameter_list|(
name|String
name|dbName
parameter_list|,
name|FileSystem
name|fs
parameter_list|,
name|FileStatus
name|dir
parameter_list|)
throws|throws
name|SemanticException
block|{
try|try
block|{
comment|// Path being passed to us is a db dump location. We go ahead and load as needed.
comment|// dbName might be null or empty, in which case we keep the original db name for the new
comment|// database creation
comment|// Two steps here - first, we read the _metadata file here, and create a CreateDatabaseDesc
comment|// associated with that
comment|// Then, we iterate over all subdirs, and create table imports for each.
name|EximUtil
operator|.
name|ReadMetaData
name|rv
init|=
operator|new
name|EximUtil
operator|.
name|ReadMetaData
argument_list|()
decl_stmt|;
try|try
block|{
name|rv
operator|=
name|EximUtil
operator|.
name|readMetaData
argument_list|(
name|fs
argument_list|,
operator|new
name|Path
argument_list|(
name|dir
operator|.
name|getPath
argument_list|()
argument_list|,
name|EximUtil
operator|.
name|METADATA_NAME
argument_list|)
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|SemanticException
argument_list|(
name|ErrorMsg
operator|.
name|INVALID_PATH
operator|.
name|getMsg
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
name|Database
name|dbObj
init|=
name|rv
operator|.
name|getDatabase
argument_list|()
decl_stmt|;
if|if
condition|(
name|dbObj
operator|==
literal|null
condition|)
block|{
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
literal|"_metadata file read did not contain a db object - invalid dump."
argument_list|)
throw|;
block|}
if|if
condition|(
operator|(
name|dbName
operator|==
literal|null
operator|)
operator|||
operator|(
name|dbName
operator|.
name|isEmpty
argument_list|()
operator|)
condition|)
block|{
comment|// We use dbName specified as long as it is not null/empty. If so, then we use the original
comment|// name
comment|// recorded in the thrift object.
name|dbName
operator|=
name|dbObj
operator|.
name|getName
argument_list|()
expr_stmt|;
block|}
name|CreateDatabaseDesc
name|createDbDesc
init|=
operator|new
name|CreateDatabaseDesc
argument_list|()
decl_stmt|;
name|createDbDesc
operator|.
name|setName
argument_list|(
name|dbName
argument_list|)
expr_stmt|;
name|createDbDesc
operator|.
name|setComment
argument_list|(
name|dbObj
operator|.
name|getDescription
argument_list|()
argument_list|)
expr_stmt|;
name|createDbDesc
operator|.
name|setDatabaseProperties
argument_list|(
name|dbObj
operator|.
name|getParameters
argument_list|()
argument_list|)
expr_stmt|;
comment|// note that we do not set location - for repl load, we want that auto-created.
name|createDbDesc
operator|.
name|setIfNotExists
argument_list|(
literal|false
argument_list|)
expr_stmt|;
comment|// If it exists, we want this to be an error condition. Repl Load is not intended to replace a
comment|// db.
comment|// TODO: we might revisit this in create-drop-recreate cases, needs some thinking on.
name|Task
argument_list|<
name|?
extends|extends
name|Serializable
argument_list|>
name|createDbTask
init|=
name|TaskFactory
operator|.
name|get
argument_list|(
operator|new
name|DDLWork
argument_list|(
name|inputs
argument_list|,
name|outputs
argument_list|,
name|createDbDesc
argument_list|)
argument_list|,
name|conf
argument_list|)
decl_stmt|;
name|rootTasks
operator|.
name|add
argument_list|(
name|createDbTask
argument_list|)
expr_stmt|;
name|FileStatus
index|[]
name|dirsInDbPath
init|=
name|fs
operator|.
name|listStatus
argument_list|(
name|dir
operator|.
name|getPath
argument_list|()
argument_list|,
name|EximUtil
operator|.
name|getDirectoryFilter
argument_list|(
name|fs
argument_list|)
argument_list|)
decl_stmt|;
for|for
control|(
name|FileStatus
name|tableDir
range|:
name|dirsInDbPath
control|)
block|{
name|analyzeTableLoad
argument_list|(
name|dbName
argument_list|,
literal|null
argument_list|,
name|tableDir
operator|.
name|getPath
argument_list|()
operator|.
name|toUri
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|,
name|createDbTask
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|SemanticException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|private
name|void
name|analyzeTableLoad
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|tblName
parameter_list|,
name|String
name|locn
parameter_list|,
name|Task
argument_list|<
name|?
extends|extends
name|Serializable
argument_list|>
name|precursor
parameter_list|)
throws|throws
name|SemanticException
block|{
comment|// Path being passed to us is a table dump location. We go ahead and load it in as needed.
comment|// If tblName is null, then we default to the table name specified in _metadata, which is good.
comment|// or are both specified, in which case, that's what we are intended to create the new table as.
if|if
condition|(
name|dbName
operator|==
literal|null
operator|||
name|dbName
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|SemanticException
argument_list|(
literal|"Database name cannot be null for a table load"
argument_list|)
throw|;
block|}
try|try
block|{
comment|// no location set on repl loads
name|boolean
name|isLocationSet
init|=
literal|false
decl_stmt|;
comment|// all repl imports are non-external
name|boolean
name|isExternalSet
init|=
literal|false
decl_stmt|;
comment|// bootstrap loads are not partition level
name|boolean
name|isPartSpecSet
init|=
literal|false
decl_stmt|;
comment|// repl loads are not partition level
name|LinkedHashMap
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|parsedPartSpec
init|=
literal|null
decl_stmt|;
comment|// no location for repl imports
name|String
name|parsedLocation
init|=
literal|null
decl_stmt|;
name|boolean
name|waitOnCreateDb
init|=
literal|false
decl_stmt|;
name|List
argument_list|<
name|Task
argument_list|<
name|?
extends|extends
name|Serializable
argument_list|>
argument_list|>
name|importTasks
init|=
literal|null
decl_stmt|;
if|if
condition|(
name|precursor
operator|==
literal|null
condition|)
block|{
name|importTasks
operator|=
name|rootTasks
expr_stmt|;
name|waitOnCreateDb
operator|=
literal|false
expr_stmt|;
block|}
else|else
block|{
name|importTasks
operator|=
operator|new
name|ArrayList
argument_list|<
name|Task
argument_list|<
name|?
extends|extends
name|Serializable
argument_list|>
argument_list|>
argument_list|()
expr_stmt|;
name|waitOnCreateDb
operator|=
literal|true
expr_stmt|;
block|}
name|EximUtil
operator|.
name|SemanticAnalyzerWrapperContext
name|x
init|=
operator|new
name|EximUtil
operator|.
name|SemanticAnalyzerWrapperContext
argument_list|(
name|conf
argument_list|,
name|db
argument_list|,
name|inputs
argument_list|,
name|outputs
argument_list|,
name|importTasks
argument_list|,
name|LOG
argument_list|,
name|ctx
argument_list|)
decl_stmt|;
name|ImportSemanticAnalyzer
operator|.
name|prepareImport
argument_list|(
name|isLocationSet
argument_list|,
name|isExternalSet
argument_list|,
name|isPartSpecSet
argument_list|,
name|waitOnCreateDb
argument_list|,
name|parsedLocation
argument_list|,
name|tblName
argument_list|,
name|dbName
argument_list|,
name|parsedPartSpec
argument_list|,
name|locn
argument_list|,
name|x
argument_list|)
expr_stmt|;
if|if
condition|(
name|precursor
operator|!=
literal|null
condition|)
block|{
for|for
control|(
name|Task
argument_list|<
name|?
extends|extends
name|Serializable
argument_list|>
name|t
range|:
name|importTasks
control|)
block|{
name|precursor
operator|.
name|addDependentTask
argument_list|(
name|t
argument_list|)
expr_stmt|;
block|}
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|SemanticException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
comment|// REPL STATUS
specifier|private
name|void
name|initReplStatus
parameter_list|(
name|ASTNode
name|ast
parameter_list|)
block|{
name|int
name|numChildren
init|=
name|ast
operator|.
name|getChildCount
argument_list|()
decl_stmt|;
name|dbNameOrPattern
operator|=
name|PlanUtils
operator|.
name|stripQuotes
argument_list|(
name|ast
operator|.
name|getChild
argument_list|(
literal|0
argument_list|)
operator|.
name|getText
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|numChildren
operator|>
literal|1
condition|)
block|{
name|tblNameOrPattern
operator|=
name|PlanUtils
operator|.
name|stripQuotes
argument_list|(
name|ast
operator|.
name|getChild
argument_list|(
literal|1
argument_list|)
operator|.
name|getText
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
specifier|private
name|void
name|analyzeReplStatus
parameter_list|(
name|ASTNode
name|ast
parameter_list|)
throws|throws
name|SemanticException
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"ReplicationSemanticAnalyzer.analyzeReplStatus: "
operator|+
name|String
operator|.
name|valueOf
argument_list|(
name|dbNameOrPattern
argument_list|)
operator|+
literal|"."
operator|+
name|String
operator|.
name|valueOf
argument_list|(
name|tblNameOrPattern
argument_list|)
argument_list|)
expr_stmt|;
name|String
name|replLastId
init|=
literal|null
decl_stmt|;
try|try
block|{
if|if
condition|(
name|tblNameOrPattern
operator|!=
literal|null
condition|)
block|{
comment|// Checking for status of table
name|Table
name|tbl
init|=
name|db
operator|.
name|getTable
argument_list|(
name|dbNameOrPattern
argument_list|,
name|tblNameOrPattern
argument_list|)
decl_stmt|;
if|if
condition|(
name|tbl
operator|!=
literal|null
condition|)
block|{
name|inputs
operator|.
name|add
argument_list|(
operator|new
name|ReadEntity
argument_list|(
name|tbl
argument_list|)
argument_list|)
expr_stmt|;
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|params
init|=
name|tbl
operator|.
name|getParameters
argument_list|()
decl_stmt|;
if|if
condition|(
name|params
operator|!=
literal|null
operator|&&
operator|(
name|params
operator|.
name|containsKey
argument_list|(
name|ReplicationSpec
operator|.
name|KEY
operator|.
name|CURR_STATE_ID
argument_list|)
operator|)
condition|)
block|{
name|replLastId
operator|=
name|params
operator|.
name|get
argument_list|(
name|ReplicationSpec
operator|.
name|KEY
operator|.
name|CURR_STATE_ID
argument_list|)
expr_stmt|;
block|}
block|}
block|}
else|else
block|{
comment|// Checking for status of a db
name|Database
name|database
init|=
name|db
operator|.
name|getDatabase
argument_list|(
name|dbNameOrPattern
argument_list|)
decl_stmt|;
if|if
condition|(
name|database
operator|!=
literal|null
condition|)
block|{
name|inputs
operator|.
name|add
argument_list|(
operator|new
name|ReadEntity
argument_list|(
name|database
argument_list|)
argument_list|)
expr_stmt|;
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|params
init|=
name|database
operator|.
name|getParameters
argument_list|()
decl_stmt|;
if|if
condition|(
name|params
operator|!=
literal|null
operator|&&
operator|(
name|params
operator|.
name|containsKey
argument_list|(
name|ReplicationSpec
operator|.
name|KEY
operator|.
name|CURR_STATE_ID
argument_list|)
operator|)
condition|)
block|{
name|replLastId
operator|=
name|params
operator|.
name|get
argument_list|(
name|ReplicationSpec
operator|.
name|KEY
operator|.
name|CURR_STATE_ID
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
catch|catch
parameter_list|(
name|HiveException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|SemanticException
argument_list|(
name|e
argument_list|)
throw|;
comment|// TODO : simple wrap& rethrow for now, clean up with error
comment|// codes
block|}
name|LOG
operator|.
name|debug
argument_list|(
literal|"RSTATUS: writing repl.last.id="
operator|+
name|String
operator|.
name|valueOf
argument_list|(
name|replLastId
argument_list|)
operator|+
literal|" out to "
operator|+
name|ctx
operator|.
name|getResFile
argument_list|()
argument_list|)
expr_stmt|;
name|prepareReturnValues
argument_list|(
name|Collections
operator|.
name|singletonList
argument_list|(
name|replLastId
argument_list|)
argument_list|,
literal|"last_repl_id#string"
argument_list|)
expr_stmt|;
block|}
specifier|private
name|void
name|prepareReturnValues
parameter_list|(
name|List
argument_list|<
name|String
argument_list|>
name|values
parameter_list|,
name|String
name|schema
parameter_list|)
throws|throws
name|SemanticException
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"prepareReturnValues : "
operator|+
name|schema
argument_list|)
expr_stmt|;
for|for
control|(
name|String
name|s
range|:
name|values
control|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"> "
operator|+
name|s
argument_list|)
expr_stmt|;
block|}
name|ctx
operator|.
name|setResFile
argument_list|(
name|ctx
operator|.
name|getLocalTmpPath
argument_list|()
argument_list|)
expr_stmt|;
name|writeOutput
argument_list|(
name|values
argument_list|,
name|ctx
operator|.
name|getResFile
argument_list|()
argument_list|)
expr_stmt|;
block|}
specifier|private
name|void
name|writeOutput
parameter_list|(
name|List
argument_list|<
name|String
argument_list|>
name|values
parameter_list|,
name|Path
name|outputFile
parameter_list|)
throws|throws
name|SemanticException
block|{
name|FileSystem
name|fs
init|=
literal|null
decl_stmt|;
name|DataOutputStream
name|outStream
init|=
literal|null
decl_stmt|;
try|try
block|{
name|fs
operator|=
name|outputFile
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
expr_stmt|;
name|outStream
operator|=
name|fs
operator|.
name|create
argument_list|(
name|outputFile
argument_list|)
expr_stmt|;
name|outStream
operator|.
name|writeBytes
argument_list|(
operator|(
name|values
operator|.
name|get
argument_list|(
literal|0
argument_list|)
operator|==
literal|null
condition|?
name|Utilities
operator|.
name|nullStringOutput
else|:
name|values
operator|.
name|get
argument_list|(
literal|0
argument_list|)
operator|)
argument_list|)
expr_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|1
init|;
name|i
operator|<
name|values
operator|.
name|size
argument_list|()
condition|;
name|i
operator|++
control|)
block|{
name|outStream
operator|.
name|write
argument_list|(
name|Utilities
operator|.
name|tabCode
argument_list|)
expr_stmt|;
name|outStream
operator|.
name|writeBytes
argument_list|(
operator|(
name|values
operator|.
name|get
argument_list|(
name|i
argument_list|)
operator|==
literal|null
condition|?
name|Utilities
operator|.
name|nullStringOutput
else|:
name|values
operator|.
name|get
argument_list|(
name|i
argument_list|)
operator|)
argument_list|)
expr_stmt|;
block|}
name|outStream
operator|.
name|write
argument_list|(
name|Utilities
operator|.
name|newLineCode
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|SemanticException
argument_list|(
name|e
argument_list|)
throw|;
block|}
finally|finally
block|{
name|IOUtils
operator|.
name|closeStream
argument_list|(
name|outStream
argument_list|)
expr_stmt|;
block|}
block|}
specifier|private
name|ReplicationSpec
name|getNewReplicationSpec
parameter_list|()
throws|throws
name|SemanticException
block|{
try|try
block|{
name|ReplicationSpec
name|rspec
init|=
name|getNewReplicationSpec
argument_list|(
literal|"replv2"
argument_list|,
literal|"will-be-set"
argument_list|)
decl_stmt|;
name|rspec
operator|.
name|setCurrentReplicationState
argument_list|(
name|String
operator|.
name|valueOf
argument_list|(
name|db
operator|.
name|getMSC
argument_list|()
operator|.
name|getCurrentNotificationEventId
argument_list|()
operator|.
name|getEventId
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
return|return
name|rspec
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|SemanticException
argument_list|(
name|e
argument_list|)
throw|;
comment|// TODO : simple wrap& rethrow for now, clean up with error codes
block|}
block|}
specifier|private
name|ReplicationSpec
name|getNewReplicationSpec
parameter_list|(
name|String
name|evState
parameter_list|,
name|String
name|objState
parameter_list|)
throws|throws
name|SemanticException
block|{
return|return
operator|new
name|ReplicationSpec
argument_list|(
literal|true
argument_list|,
literal|false
argument_list|,
name|evState
argument_list|,
name|objState
argument_list|,
literal|false
argument_list|,
literal|true
argument_list|)
return|;
block|}
specifier|private
name|Iterable
argument_list|<
name|?
extends|extends
name|String
argument_list|>
name|matchesTbl
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|tblPattern
parameter_list|)
throws|throws
name|HiveException
block|{
if|if
condition|(
name|tblPattern
operator|==
literal|null
condition|)
block|{
return|return
name|db
operator|.
name|getAllTables
argument_list|(
name|dbName
argument_list|)
return|;
block|}
else|else
block|{
return|return
name|db
operator|.
name|getTablesByPattern
argument_list|(
name|dbName
argument_list|,
name|tblPattern
argument_list|)
return|;
block|}
block|}
specifier|private
name|Iterable
argument_list|<
name|?
extends|extends
name|String
argument_list|>
name|matchesDb
parameter_list|(
name|String
name|dbPattern
parameter_list|)
throws|throws
name|HiveException
block|{
if|if
condition|(
name|dbPattern
operator|==
literal|null
condition|)
block|{
return|return
name|db
operator|.
name|getAllDatabases
argument_list|()
return|;
block|}
else|else
block|{
return|return
name|db
operator|.
name|getDatabasesByPattern
argument_list|(
name|dbPattern
argument_list|)
return|;
block|}
block|}
block|}
end_class

end_unit

