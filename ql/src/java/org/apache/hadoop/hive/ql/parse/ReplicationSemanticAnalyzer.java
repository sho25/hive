begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/**  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|parse
package|;
end_package

begin_import
import|import
name|org
operator|.
name|antlr
operator|.
name|runtime
operator|.
name|tree
operator|.
name|Tree
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileStatus
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|IMetaStoreClient
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Database
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|NotificationEvent
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|messaging
operator|.
name|AddPartitionMessage
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|messaging
operator|.
name|CreateTableMessage
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|messaging
operator|.
name|EventUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|messaging
operator|.
name|InsertMessage
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|messaging
operator|.
name|MessageDeserializer
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|messaging
operator|.
name|MessageFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|ErrorMsg
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|QueryState
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|Task
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|TaskFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|Utilities
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|hooks
operator|.
name|ReadEntity
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|HiveException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|Table
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|CreateDatabaseDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|DDLWork
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|PlanUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|IOUtils
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|DataOutputStream
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|FileNotFoundException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|Serializable
import|;
end_import

begin_import
import|import
name|java
operator|.
name|net
operator|.
name|URI
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Arrays
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Collections
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|LinkedHashMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|parse
operator|.
name|HiveParser
operator|.
name|*
import|;
end_import

begin_class
specifier|public
class|class
name|ReplicationSemanticAnalyzer
extends|extends
name|BaseSemanticAnalyzer
block|{
comment|// Database name or pattern
specifier|private
name|String
name|dbNameOrPattern
decl_stmt|;
comment|// Table name or pattern
specifier|private
name|String
name|tblNameOrPattern
decl_stmt|;
specifier|private
name|Integer
name|eventFrom
decl_stmt|;
specifier|private
name|Integer
name|eventTo
decl_stmt|;
specifier|private
name|Integer
name|batchSize
decl_stmt|;
comment|// Base path for REPL LOAD
specifier|private
name|String
name|path
decl_stmt|;
specifier|public
name|ReplicationSemanticAnalyzer
parameter_list|(
name|QueryState
name|queryState
parameter_list|)
throws|throws
name|SemanticException
block|{
name|super
argument_list|(
name|queryState
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|analyzeInternal
parameter_list|(
name|ASTNode
name|ast
parameter_list|)
throws|throws
name|SemanticException
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"ReplicationSemanticAanalyzer: analyzeInternal"
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
name|ast
operator|.
name|getName
argument_list|()
operator|+
literal|":"
operator|+
name|ast
operator|.
name|getToken
argument_list|()
operator|.
name|getText
argument_list|()
operator|+
literal|"="
operator|+
name|ast
operator|.
name|getText
argument_list|()
argument_list|)
expr_stmt|;
switch|switch
condition|(
name|ast
operator|.
name|getToken
argument_list|()
operator|.
name|getType
argument_list|()
condition|)
block|{
case|case
name|TOK_REPL_DUMP
case|:
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"ReplicationSemanticAnalyzer: analyzeInternal: dump"
argument_list|)
expr_stmt|;
name|initReplDump
argument_list|(
name|ast
argument_list|)
expr_stmt|;
name|analyzeReplDump
argument_list|(
name|ast
argument_list|)
expr_stmt|;
break|break;
block|}
case|case
name|TOK_REPL_LOAD
case|:
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"ReplicationSemanticAnalyzer: analyzeInternal: load"
argument_list|)
expr_stmt|;
name|initReplLoad
argument_list|(
name|ast
argument_list|)
expr_stmt|;
name|analyzeReplLoad
argument_list|(
name|ast
argument_list|)
expr_stmt|;
break|break;
block|}
case|case
name|TOK_REPL_STATUS
case|:
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"ReplicationSemanticAnalyzer: analyzeInternal: status"
argument_list|)
expr_stmt|;
name|initReplStatus
argument_list|(
name|ast
argument_list|)
expr_stmt|;
name|analyzeReplStatus
argument_list|(
name|ast
argument_list|)
expr_stmt|;
break|break;
block|}
default|default:
block|{
throw|throw
operator|new
name|SemanticException
argument_list|(
literal|"Unexpected root token"
argument_list|)
throw|;
block|}
block|}
block|}
specifier|private
name|void
name|initReplDump
parameter_list|(
name|ASTNode
name|ast
parameter_list|)
block|{
name|int
name|numChildren
init|=
name|ast
operator|.
name|getChildCount
argument_list|()
decl_stmt|;
name|dbNameOrPattern
operator|=
name|PlanUtils
operator|.
name|stripQuotes
argument_list|(
name|ast
operator|.
name|getChild
argument_list|(
literal|0
argument_list|)
operator|.
name|getText
argument_list|()
argument_list|)
expr_stmt|;
comment|// skip the first node, which is always required
name|int
name|currNode
init|=
literal|1
decl_stmt|;
while|while
condition|(
name|currNode
operator|<
name|numChildren
condition|)
block|{
if|if
condition|(
name|ast
operator|.
name|getChild
argument_list|(
name|currNode
argument_list|)
operator|.
name|getType
argument_list|()
operator|!=
name|TOK_FROM
condition|)
block|{
comment|// optional tblName was specified.
name|tblNameOrPattern
operator|=
name|PlanUtils
operator|.
name|stripQuotes
argument_list|(
name|ast
operator|.
name|getChild
argument_list|(
name|currNode
argument_list|)
operator|.
name|getText
argument_list|()
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// TOK_FROM subtree
name|Tree
name|fromNode
init|=
name|ast
operator|.
name|getChild
argument_list|(
name|currNode
argument_list|)
decl_stmt|;
name|eventFrom
operator|=
name|Integer
operator|.
name|parseInt
argument_list|(
name|PlanUtils
operator|.
name|stripQuotes
argument_list|(
name|fromNode
operator|.
name|getChild
argument_list|(
literal|0
argument_list|)
operator|.
name|getText
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
comment|// skip the first, which is always required
name|int
name|numChild
init|=
literal|1
decl_stmt|;
while|while
condition|(
name|numChild
operator|<
name|fromNode
operator|.
name|getChildCount
argument_list|()
condition|)
block|{
if|if
condition|(
name|fromNode
operator|.
name|getChild
argument_list|(
name|numChild
argument_list|)
operator|.
name|getType
argument_list|()
operator|==
name|TOK_TO
condition|)
block|{
name|eventTo
operator|=
name|Integer
operator|.
name|parseInt
argument_list|(
name|PlanUtils
operator|.
name|stripQuotes
argument_list|(
name|fromNode
operator|.
name|getChild
argument_list|(
name|numChild
operator|+
literal|1
argument_list|)
operator|.
name|getText
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
comment|// skip the next child, since we already took care of it
name|numChild
operator|++
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|fromNode
operator|.
name|getChild
argument_list|(
name|numChild
argument_list|)
operator|.
name|getType
argument_list|()
operator|==
name|TOK_BATCH
condition|)
block|{
name|batchSize
operator|=
name|Integer
operator|.
name|parseInt
argument_list|(
name|PlanUtils
operator|.
name|stripQuotes
argument_list|(
name|fromNode
operator|.
name|getChild
argument_list|(
name|numChild
operator|+
literal|1
argument_list|)
operator|.
name|getText
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
comment|// skip the next child, since we already took care of it
name|numChild
operator|++
expr_stmt|;
block|}
comment|// move to the next child in FROM tree
name|numChild
operator|++
expr_stmt|;
block|}
comment|// FROM node is always the last
break|break;
block|}
comment|// move to the next root node
name|currNode
operator|++
expr_stmt|;
block|}
block|}
comment|// REPL DUMP
specifier|private
name|void
name|analyzeReplDump
parameter_list|(
name|ASTNode
name|ast
parameter_list|)
throws|throws
name|SemanticException
block|{
comment|// FIXME: support non-bootstrap: use eventFrom/eventTo/batchSize
name|LOG
operator|.
name|debug
argument_list|(
literal|"ReplicationSemanticAnalyzer.analyzeReplDump: "
operator|+
name|String
operator|.
name|valueOf
argument_list|(
name|dbNameOrPattern
argument_list|)
operator|+
literal|"."
operator|+
name|String
operator|.
name|valueOf
argument_list|(
name|tblNameOrPattern
argument_list|)
operator|+
literal|" from "
operator|+
name|String
operator|.
name|valueOf
argument_list|(
name|eventFrom
argument_list|)
operator|+
literal|" to "
operator|+
name|String
operator|.
name|valueOf
argument_list|(
name|eventTo
argument_list|)
operator|+
literal|" batchsize "
operator|+
name|String
operator|.
name|valueOf
argument_list|(
name|batchSize
argument_list|)
argument_list|)
expr_stmt|;
name|String
name|replRoot
init|=
name|conf
operator|.
name|getVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|REPLDIR
argument_list|)
decl_stmt|;
name|Path
name|dumpRoot
init|=
operator|new
name|Path
argument_list|(
name|replRoot
argument_list|,
name|getNextDumpDir
argument_list|()
argument_list|)
decl_stmt|;
try|try
block|{
for|for
control|(
name|String
name|dbName
range|:
name|matchesDb
argument_list|(
name|dbNameOrPattern
argument_list|)
control|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"ReplicationSemanticAnalyzer: analyzeReplDump dumping db: "
operator|+
name|dbName
argument_list|)
expr_stmt|;
name|Path
name|dbRoot
init|=
name|dumpDbMetadata
argument_list|(
name|dbName
argument_list|,
name|dumpRoot
argument_list|)
decl_stmt|;
for|for
control|(
name|String
name|tblName
range|:
name|matchesTbl
argument_list|(
name|dbName
argument_list|,
name|tblNameOrPattern
argument_list|)
control|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"ReplicationSemanticAnalyzer: analyzeReplDump dumping table: "
operator|+
name|tblName
operator|+
literal|" to db root "
operator|+
name|dbRoot
operator|.
name|toUri
argument_list|()
argument_list|)
expr_stmt|;
name|dumpTbl
argument_list|(
name|ast
argument_list|,
name|dbName
argument_list|,
name|tblName
argument_list|,
name|dbRoot
argument_list|)
expr_stmt|;
block|}
block|}
name|String
name|currentReplId
init|=
name|String
operator|.
name|valueOf
argument_list|(
name|db
operator|.
name|getMSC
argument_list|()
operator|.
name|getCurrentNotificationEventId
argument_list|()
operator|.
name|getEventId
argument_list|()
argument_list|)
decl_stmt|;
name|prepareReturnValues
argument_list|(
name|Arrays
operator|.
name|asList
argument_list|(
name|dumpRoot
operator|.
name|toUri
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|,
name|currentReplId
argument_list|)
argument_list|,
literal|"dump_dir,last_repl_id#string,string"
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
comment|// TODO : simple wrap& rethrow for now, clean up with error codes
throw|throw
operator|new
name|SemanticException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
name|String
name|getNextDumpDir
parameter_list|()
block|{
if|if
condition|(
name|conf
operator|.
name|getBoolVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_IN_TEST
argument_list|)
condition|)
block|{
return|return
literal|"next"
return|;
comment|// make it easy to write unit tests, instead of unique id generation.
comment|// however, this does mean that in writing tests, we have to be aware that
comment|// repl dump will clash with prior dumps, and thus have to clean up properly.
block|}
else|else
block|{
return|return
name|String
operator|.
name|valueOf
argument_list|(
name|System
operator|.
name|currentTimeMillis
argument_list|()
argument_list|)
return|;
comment|// TODO: time good enough for now - we'll likely improve this.
comment|// We may also work in something the equivalent of pid, thrid and move to nanos to ensure
comment|// uniqueness.
block|}
block|}
comment|/**    *    * @param dbName    * @param dumpRoot    * @return db dumped path    * @throws SemanticException    */
specifier|private
name|Path
name|dumpDbMetadata
parameter_list|(
name|String
name|dbName
parameter_list|,
name|Path
name|dumpRoot
parameter_list|)
throws|throws
name|SemanticException
block|{
name|Path
name|dbRoot
init|=
operator|new
name|Path
argument_list|(
name|dumpRoot
argument_list|,
name|dbName
argument_list|)
decl_stmt|;
try|try
block|{
comment|// TODO : instantiating FS objects are generally costly. Refactor
name|FileSystem
name|fs
init|=
name|dbRoot
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|Path
name|dumpPath
init|=
operator|new
name|Path
argument_list|(
name|dbRoot
argument_list|,
name|EximUtil
operator|.
name|METADATA_NAME
argument_list|)
decl_stmt|;
name|Database
name|dbObj
init|=
name|db
operator|.
name|getDatabase
argument_list|(
name|dbName
argument_list|)
decl_stmt|;
name|EximUtil
operator|.
name|createDbExportDump
argument_list|(
name|fs
argument_list|,
name|dumpPath
argument_list|,
name|dbObj
argument_list|,
name|getNewReplicationSpec
argument_list|()
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
comment|// TODO : simple wrap& rethrow for now, clean up with error codes
throw|throw
operator|new
name|SemanticException
argument_list|(
name|e
argument_list|)
throw|;
block|}
return|return
name|dbRoot
return|;
block|}
comment|/**    *    * @param ast    * @param dbName    * @param tblName    * @param dbRoot    * @return tbl dumped path    * @throws SemanticException    */
specifier|private
name|Path
name|dumpTbl
parameter_list|(
name|ASTNode
name|ast
parameter_list|,
name|String
name|dbName
parameter_list|,
name|String
name|tblName
parameter_list|,
name|Path
name|dbRoot
parameter_list|)
throws|throws
name|SemanticException
block|{
name|Path
name|tableRoot
init|=
operator|new
name|Path
argument_list|(
name|dbRoot
argument_list|,
name|tblName
argument_list|)
decl_stmt|;
try|try
block|{
name|URI
name|toURI
init|=
name|EximUtil
operator|.
name|getValidatedURI
argument_list|(
name|conf
argument_list|,
name|tableRoot
operator|.
name|toUri
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|)
decl_stmt|;
name|TableSpec
name|ts
init|=
operator|new
name|TableSpec
argument_list|(
name|db
argument_list|,
name|conf
argument_list|,
name|dbName
operator|+
literal|"."
operator|+
name|tblName
argument_list|,
literal|null
argument_list|)
decl_stmt|;
name|ExportSemanticAnalyzer
operator|.
name|prepareExport
argument_list|(
name|ast
argument_list|,
name|toURI
argument_list|,
name|ts
argument_list|,
name|getNewReplicationSpec
argument_list|()
argument_list|,
name|db
argument_list|,
name|conf
argument_list|,
name|ctx
argument_list|,
name|rootTasks
argument_list|,
name|inputs
argument_list|,
name|outputs
argument_list|,
name|LOG
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|HiveException
name|e
parameter_list|)
block|{
comment|// TODO : simple wrap& rethrow for now, clean up with error codes
throw|throw
operator|new
name|SemanticException
argument_list|(
name|e
argument_list|)
throw|;
block|}
return|return
name|tableRoot
return|;
block|}
comment|// REPL LOAD
specifier|private
name|void
name|initReplLoad
parameter_list|(
name|ASTNode
name|ast
parameter_list|)
block|{
name|int
name|numChildren
init|=
name|ast
operator|.
name|getChildCount
argument_list|()
decl_stmt|;
name|path
operator|=
name|PlanUtils
operator|.
name|stripQuotes
argument_list|(
name|ast
operator|.
name|getChild
argument_list|(
literal|0
argument_list|)
operator|.
name|getText
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|numChildren
operator|>
literal|1
condition|)
block|{
name|dbNameOrPattern
operator|=
name|PlanUtils
operator|.
name|stripQuotes
argument_list|(
name|ast
operator|.
name|getChild
argument_list|(
literal|1
argument_list|)
operator|.
name|getText
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|numChildren
operator|>
literal|2
condition|)
block|{
name|tblNameOrPattern
operator|=
name|PlanUtils
operator|.
name|stripQuotes
argument_list|(
name|ast
operator|.
name|getChild
argument_list|(
literal|2
argument_list|)
operator|.
name|getText
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
comment|/*    * Example dump dirs we need to be able to handle :    *    * for: hive.repl.rootdir = staging/ Then, repl dumps will be created in staging/<dumpdir>    *    * single-db-dump: staging/blah12345 blah12345/ default/ _metadata tbl1/ _metadata dt=20160907/    * _files tbl2/ tbl3/ unptn_tbl/ _metadata _files    *    * multi-db-dump: staging/bar12347 staging/ bar12347/ default/ ... sales/ ...    *    * single table-dump: staging/baz123 staging/ baz123/ _metadata dt=20150931/ _files    */
specifier|private
name|void
name|analyzeReplLoad
parameter_list|(
name|ASTNode
name|ast
parameter_list|)
throws|throws
name|SemanticException
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"ReplSemanticAnalyzer.analyzeReplLoad: "
operator|+
name|String
operator|.
name|valueOf
argument_list|(
name|dbNameOrPattern
argument_list|)
operator|+
literal|"."
operator|+
name|String
operator|.
name|valueOf
argument_list|(
name|tblNameOrPattern
argument_list|)
operator|+
literal|" from "
operator|+
name|String
operator|.
name|valueOf
argument_list|(
name|path
argument_list|)
argument_list|)
expr_stmt|;
comment|// for analyze repl load, we walk through the dir structure available in the path,
comment|// looking at each db, and then each table, and then setting up the appropriate
comment|// import job in its place.
comment|// FIXME : handle non-bootstrap cases.
comment|// We look at the path, and go through each subdir.
comment|// Each subdir corresponds to a database.
comment|// For each subdir, there is a _metadata file which allows us to re-impress the db object
comment|// After each db object is loaded appropriately, iterate through the sub-table dirs, and pretend
comment|// that we had an IMPORT on each of them, into this db.
try|try
block|{
name|Path
name|loadPath
init|=
operator|new
name|Path
argument_list|(
name|path
argument_list|)
decl_stmt|;
specifier|final
name|FileSystem
name|fs
init|=
name|loadPath
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|fs
operator|.
name|exists
argument_list|(
name|loadPath
argument_list|)
condition|)
block|{
comment|// supposed dump path does not exist.
throw|throw
operator|new
name|FileNotFoundException
argument_list|(
name|loadPath
operator|.
name|toUri
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|)
throw|;
block|}
comment|// Now, the dumped path can be one of two things:
comment|// a) It can be a db dump, in which case we expect a set of dirs, each with a
comment|// db name, and with a _metadata file in each, and table dirs inside that.
comment|// b) It can be a table dump dir, in which case we expect a _metadata dump of
comment|// a table in question in the dir, and individual ptn dir hierarchy.
comment|// Once we expand this into doing incremental repl, we can have individual events which can
comment|// be other things like roles and fns as well. Also, if tblname is specified, we're guaranteed
comment|// that this is a tbl-level dump, and it is an error condition if we find anything else. Also,
comment|// if dbname is specified, we expect exactly one db dumped, and having more is an error
comment|// condition.
if|if
condition|(
operator|(
name|tblNameOrPattern
operator|!=
literal|null
operator|)
operator|&&
operator|!
operator|(
name|tblNameOrPattern
operator|.
name|isEmpty
argument_list|()
operator|)
condition|)
block|{
name|analyzeTableLoad
argument_list|(
name|dbNameOrPattern
argument_list|,
name|tblNameOrPattern
argument_list|,
name|path
argument_list|,
literal|null
argument_list|)
expr_stmt|;
return|return;
block|}
name|FileStatus
index|[]
name|srcs
init|=
name|LoadSemanticAnalyzer
operator|.
name|matchFilesOrDir
argument_list|(
name|fs
argument_list|,
name|loadPath
argument_list|)
decl_stmt|;
if|if
condition|(
name|srcs
operator|==
literal|null
operator|||
operator|(
name|srcs
operator|.
name|length
operator|==
literal|0
operator|)
condition|)
block|{
throw|throw
operator|new
name|FileNotFoundException
argument_list|(
name|loadPath
operator|.
name|toUri
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|)
throw|;
block|}
name|FileStatus
index|[]
name|dirsInLoadPath
init|=
name|fs
operator|.
name|listStatus
argument_list|(
name|loadPath
argument_list|,
name|EximUtil
operator|.
name|getDirectoryFilter
argument_list|(
name|fs
argument_list|)
argument_list|)
decl_stmt|;
if|if
condition|(
operator|(
name|dirsInLoadPath
operator|==
literal|null
operator|)
operator|||
operator|(
name|dirsInLoadPath
operator|.
name|length
operator|==
literal|0
operator|)
condition|)
block|{
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
literal|"No data to load in path "
operator|+
name|loadPath
operator|.
name|toUri
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|)
throw|;
block|}
if|if
condition|(
operator|(
name|dbNameOrPattern
operator|!=
literal|null
operator|)
operator|&&
operator|(
name|dirsInLoadPath
operator|.
name|length
operator|>
literal|1
operator|)
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Found multiple dirs when we expected 1:"
argument_list|)
expr_stmt|;
for|for
control|(
name|FileStatus
name|d
range|:
name|dirsInLoadPath
control|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"> "
operator|+
name|d
operator|.
name|getPath
argument_list|()
operator|.
name|toUri
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
literal|"Multiple dirs in "
operator|+
name|loadPath
operator|.
name|toUri
argument_list|()
operator|.
name|toString
argument_list|()
operator|+
literal|" does not correspond to REPL LOAD expecting to load to a singular destination point."
argument_list|)
throw|;
block|}
for|for
control|(
name|FileStatus
name|dir
range|:
name|dirsInLoadPath
control|)
block|{
name|analyzeDatabaseLoad
argument_list|(
name|dbNameOrPattern
argument_list|,
name|fs
argument_list|,
name|dir
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
comment|// TODO : simple wrap& rethrow for now, clean up with error codes
throw|throw
operator|new
name|SemanticException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|private
name|void
name|analyzeDatabaseLoad
parameter_list|(
name|String
name|dbName
parameter_list|,
name|FileSystem
name|fs
parameter_list|,
name|FileStatus
name|dir
parameter_list|)
throws|throws
name|SemanticException
block|{
try|try
block|{
comment|// Path being passed to us is a db dump location. We go ahead and load as needed.
comment|// dbName might be null or empty, in which case we keep the original db name for the new
comment|// database creation
comment|// Two steps here - first, we read the _metadata file here, and create a CreateDatabaseDesc
comment|// associated with that
comment|// Then, we iterate over all subdirs, and create table imports for each.
name|EximUtil
operator|.
name|ReadMetaData
name|rv
init|=
operator|new
name|EximUtil
operator|.
name|ReadMetaData
argument_list|()
decl_stmt|;
try|try
block|{
name|rv
operator|=
name|EximUtil
operator|.
name|readMetaData
argument_list|(
name|fs
argument_list|,
operator|new
name|Path
argument_list|(
name|dir
operator|.
name|getPath
argument_list|()
argument_list|,
name|EximUtil
operator|.
name|METADATA_NAME
argument_list|)
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|SemanticException
argument_list|(
name|ErrorMsg
operator|.
name|INVALID_PATH
operator|.
name|getMsg
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
name|Database
name|dbObj
init|=
name|rv
operator|.
name|getDatabase
argument_list|()
decl_stmt|;
if|if
condition|(
name|dbObj
operator|==
literal|null
condition|)
block|{
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
literal|"_metadata file read did not contain a db object - invalid dump."
argument_list|)
throw|;
block|}
if|if
condition|(
operator|(
name|dbName
operator|==
literal|null
operator|)
operator|||
operator|(
name|dbName
operator|.
name|isEmpty
argument_list|()
operator|)
condition|)
block|{
comment|// We use dbName specified as long as it is not null/empty. If so, then we use the original
comment|// name
comment|// recorded in the thrift object.
name|dbName
operator|=
name|dbObj
operator|.
name|getName
argument_list|()
expr_stmt|;
block|}
name|CreateDatabaseDesc
name|createDbDesc
init|=
operator|new
name|CreateDatabaseDesc
argument_list|()
decl_stmt|;
name|createDbDesc
operator|.
name|setName
argument_list|(
name|dbName
argument_list|)
expr_stmt|;
name|createDbDesc
operator|.
name|setComment
argument_list|(
name|dbObj
operator|.
name|getDescription
argument_list|()
argument_list|)
expr_stmt|;
name|createDbDesc
operator|.
name|setDatabaseProperties
argument_list|(
name|dbObj
operator|.
name|getParameters
argument_list|()
argument_list|)
expr_stmt|;
comment|// note that we do not set location - for repl load, we want that auto-created.
name|createDbDesc
operator|.
name|setIfNotExists
argument_list|(
literal|false
argument_list|)
expr_stmt|;
comment|// If it exists, we want this to be an error condition. Repl Load is not intended to replace a
comment|// db.
comment|// TODO: we might revisit this in create-drop-recreate cases, needs some thinking on.
name|Task
argument_list|<
name|?
extends|extends
name|Serializable
argument_list|>
name|createDbTask
init|=
name|TaskFactory
operator|.
name|get
argument_list|(
operator|new
name|DDLWork
argument_list|(
name|inputs
argument_list|,
name|outputs
argument_list|,
name|createDbDesc
argument_list|)
argument_list|,
name|conf
argument_list|)
decl_stmt|;
name|rootTasks
operator|.
name|add
argument_list|(
name|createDbTask
argument_list|)
expr_stmt|;
name|FileStatus
index|[]
name|dirsInDbPath
init|=
name|fs
operator|.
name|listStatus
argument_list|(
name|dir
operator|.
name|getPath
argument_list|()
argument_list|,
name|EximUtil
operator|.
name|getDirectoryFilter
argument_list|(
name|fs
argument_list|)
argument_list|)
decl_stmt|;
for|for
control|(
name|FileStatus
name|tableDir
range|:
name|dirsInDbPath
control|)
block|{
name|analyzeTableLoad
argument_list|(
name|dbName
argument_list|,
literal|null
argument_list|,
name|tableDir
operator|.
name|getPath
argument_list|()
operator|.
name|toUri
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|,
name|createDbTask
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|SemanticException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|private
name|void
name|analyzeTableLoad
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|tblName
parameter_list|,
name|String
name|locn
parameter_list|,
name|Task
argument_list|<
name|?
extends|extends
name|Serializable
argument_list|>
name|precursor
parameter_list|)
throws|throws
name|SemanticException
block|{
comment|// Path being passed to us is a table dump location. We go ahead and load it in as needed.
comment|// If tblName is null, then we default to the table name specified in _metadata, which is good.
comment|// or are both specified, in which case, that's what we are intended to create the new table as.
if|if
condition|(
name|dbName
operator|==
literal|null
operator|||
name|dbName
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|SemanticException
argument_list|(
literal|"Database name cannot be null for a table load"
argument_list|)
throw|;
block|}
try|try
block|{
comment|// no location set on repl loads
name|boolean
name|isLocationSet
init|=
literal|false
decl_stmt|;
comment|// all repl imports are non-external
name|boolean
name|isExternalSet
init|=
literal|false
decl_stmt|;
comment|// bootstrap loads are not partition level
name|boolean
name|isPartSpecSet
init|=
literal|false
decl_stmt|;
comment|// repl loads are not partition level
name|LinkedHashMap
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|parsedPartSpec
init|=
literal|null
decl_stmt|;
comment|// no location for repl imports
name|String
name|parsedLocation
init|=
literal|null
decl_stmt|;
name|boolean
name|waitOnCreateDb
init|=
literal|false
decl_stmt|;
name|List
argument_list|<
name|Task
argument_list|<
name|?
extends|extends
name|Serializable
argument_list|>
argument_list|>
name|importTasks
init|=
literal|null
decl_stmt|;
if|if
condition|(
name|precursor
operator|==
literal|null
condition|)
block|{
name|importTasks
operator|=
name|rootTasks
expr_stmt|;
name|waitOnCreateDb
operator|=
literal|false
expr_stmt|;
block|}
else|else
block|{
name|importTasks
operator|=
operator|new
name|ArrayList
argument_list|<
name|Task
argument_list|<
name|?
extends|extends
name|Serializable
argument_list|>
argument_list|>
argument_list|()
expr_stmt|;
name|waitOnCreateDb
operator|=
literal|true
expr_stmt|;
block|}
name|EximUtil
operator|.
name|SemanticAnalyzerWrapperContext
name|x
init|=
operator|new
name|EximUtil
operator|.
name|SemanticAnalyzerWrapperContext
argument_list|(
name|conf
argument_list|,
name|db
argument_list|,
name|inputs
argument_list|,
name|outputs
argument_list|,
name|importTasks
argument_list|,
name|LOG
argument_list|,
name|ctx
argument_list|)
decl_stmt|;
name|ImportSemanticAnalyzer
operator|.
name|prepareImport
argument_list|(
name|isLocationSet
argument_list|,
name|isExternalSet
argument_list|,
name|isPartSpecSet
argument_list|,
name|waitOnCreateDb
argument_list|,
name|parsedLocation
argument_list|,
name|tblName
argument_list|,
name|dbName
argument_list|,
name|parsedPartSpec
argument_list|,
name|locn
argument_list|,
name|x
argument_list|)
expr_stmt|;
if|if
condition|(
name|precursor
operator|!=
literal|null
condition|)
block|{
for|for
control|(
name|Task
argument_list|<
name|?
extends|extends
name|Serializable
argument_list|>
name|t
range|:
name|importTasks
control|)
block|{
name|precursor
operator|.
name|addDependentTask
argument_list|(
name|t
argument_list|)
expr_stmt|;
block|}
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|SemanticException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
comment|// REPL STATUS
specifier|private
name|void
name|initReplStatus
parameter_list|(
name|ASTNode
name|ast
parameter_list|)
block|{
name|int
name|numChildren
init|=
name|ast
operator|.
name|getChildCount
argument_list|()
decl_stmt|;
name|dbNameOrPattern
operator|=
name|PlanUtils
operator|.
name|stripQuotes
argument_list|(
name|ast
operator|.
name|getChild
argument_list|(
literal|0
argument_list|)
operator|.
name|getText
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|numChildren
operator|>
literal|1
condition|)
block|{
name|tblNameOrPattern
operator|=
name|PlanUtils
operator|.
name|stripQuotes
argument_list|(
name|ast
operator|.
name|getChild
argument_list|(
literal|1
argument_list|)
operator|.
name|getText
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
specifier|private
name|void
name|analyzeReplStatus
parameter_list|(
name|ASTNode
name|ast
parameter_list|)
throws|throws
name|SemanticException
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"ReplicationSemanticAnalyzer.analyzeReplStatus: "
operator|+
name|String
operator|.
name|valueOf
argument_list|(
name|dbNameOrPattern
argument_list|)
operator|+
literal|"."
operator|+
name|String
operator|.
name|valueOf
argument_list|(
name|tblNameOrPattern
argument_list|)
argument_list|)
expr_stmt|;
name|String
name|replLastId
init|=
literal|null
decl_stmt|;
try|try
block|{
if|if
condition|(
name|tblNameOrPattern
operator|!=
literal|null
condition|)
block|{
comment|// Checking for status of table
name|Table
name|tbl
init|=
name|db
operator|.
name|getTable
argument_list|(
name|dbNameOrPattern
argument_list|,
name|tblNameOrPattern
argument_list|)
decl_stmt|;
if|if
condition|(
name|tbl
operator|!=
literal|null
condition|)
block|{
name|inputs
operator|.
name|add
argument_list|(
operator|new
name|ReadEntity
argument_list|(
name|tbl
argument_list|)
argument_list|)
expr_stmt|;
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|params
init|=
name|tbl
operator|.
name|getParameters
argument_list|()
decl_stmt|;
if|if
condition|(
name|params
operator|!=
literal|null
operator|&&
operator|(
name|params
operator|.
name|containsKey
argument_list|(
name|ReplicationSpec
operator|.
name|KEY
operator|.
name|CURR_STATE_ID
argument_list|)
operator|)
condition|)
block|{
name|replLastId
operator|=
name|params
operator|.
name|get
argument_list|(
name|ReplicationSpec
operator|.
name|KEY
operator|.
name|CURR_STATE_ID
argument_list|)
expr_stmt|;
block|}
block|}
block|}
else|else
block|{
comment|// Checking for status of a db
name|Database
name|database
init|=
name|db
operator|.
name|getDatabase
argument_list|(
name|dbNameOrPattern
argument_list|)
decl_stmt|;
if|if
condition|(
name|database
operator|!=
literal|null
condition|)
block|{
name|inputs
operator|.
name|add
argument_list|(
operator|new
name|ReadEntity
argument_list|(
name|database
argument_list|)
argument_list|)
expr_stmt|;
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|params
init|=
name|database
operator|.
name|getParameters
argument_list|()
decl_stmt|;
if|if
condition|(
name|params
operator|!=
literal|null
operator|&&
operator|(
name|params
operator|.
name|containsKey
argument_list|(
name|ReplicationSpec
operator|.
name|KEY
operator|.
name|CURR_STATE_ID
argument_list|)
operator|)
condition|)
block|{
name|replLastId
operator|=
name|params
operator|.
name|get
argument_list|(
name|ReplicationSpec
operator|.
name|KEY
operator|.
name|CURR_STATE_ID
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
catch|catch
parameter_list|(
name|HiveException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|SemanticException
argument_list|(
name|e
argument_list|)
throw|;
comment|// TODO : simple wrap& rethrow for now, clean up with error
comment|// codes
block|}
name|LOG
operator|.
name|debug
argument_list|(
literal|"RSTATUS: writing repl.last.id="
operator|+
name|String
operator|.
name|valueOf
argument_list|(
name|replLastId
argument_list|)
operator|+
literal|" out to "
operator|+
name|ctx
operator|.
name|getResFile
argument_list|()
argument_list|)
expr_stmt|;
name|prepareReturnValues
argument_list|(
name|Collections
operator|.
name|singletonList
argument_list|(
name|replLastId
argument_list|)
argument_list|,
literal|"last_repl_id#string"
argument_list|)
expr_stmt|;
block|}
specifier|private
name|void
name|prepareReturnValues
parameter_list|(
name|List
argument_list|<
name|String
argument_list|>
name|values
parameter_list|,
name|String
name|schema
parameter_list|)
throws|throws
name|SemanticException
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"prepareReturnValues : "
operator|+
name|schema
argument_list|)
expr_stmt|;
for|for
control|(
name|String
name|s
range|:
name|values
control|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"> "
operator|+
name|s
argument_list|)
expr_stmt|;
block|}
name|ctx
operator|.
name|setResFile
argument_list|(
name|ctx
operator|.
name|getLocalTmpPath
argument_list|()
argument_list|)
expr_stmt|;
comment|// FIXME : this should not accessible by the user if we write to it from the frontend.
comment|// Thus, we should Desc/Work this, otherwise there is a security issue here.
comment|// Note: if we don't call ctx.setResFile, we get a NPE from the following code section
comment|// If we do call it, then FetchWork thinks that the "table" here winds up thinking that
comment|// this is a partitioned dir, which does not work. Thus, this does not work.
name|writeOutput
argument_list|(
name|values
argument_list|)
expr_stmt|;
block|}
specifier|private
name|void
name|writeOutput
parameter_list|(
name|List
argument_list|<
name|String
argument_list|>
name|values
parameter_list|)
throws|throws
name|SemanticException
block|{
name|Path
name|outputFile
init|=
name|ctx
operator|.
name|getResFile
argument_list|()
decl_stmt|;
name|FileSystem
name|fs
init|=
literal|null
decl_stmt|;
name|DataOutputStream
name|outStream
init|=
literal|null
decl_stmt|;
try|try
block|{
name|fs
operator|=
name|outputFile
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
expr_stmt|;
name|outStream
operator|=
name|fs
operator|.
name|create
argument_list|(
name|outputFile
argument_list|)
expr_stmt|;
name|outStream
operator|.
name|writeBytes
argument_list|(
operator|(
name|values
operator|.
name|get
argument_list|(
literal|0
argument_list|)
operator|==
literal|null
condition|?
name|Utilities
operator|.
name|nullStringOutput
else|:
name|values
operator|.
name|get
argument_list|(
literal|0
argument_list|)
operator|)
argument_list|)
expr_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|1
init|;
name|i
operator|<
name|values
operator|.
name|size
argument_list|()
condition|;
name|i
operator|++
control|)
block|{
name|outStream
operator|.
name|write
argument_list|(
name|Utilities
operator|.
name|ctrlaCode
argument_list|)
expr_stmt|;
name|outStream
operator|.
name|writeBytes
argument_list|(
operator|(
name|values
operator|.
name|get
argument_list|(
literal|1
argument_list|)
operator|==
literal|null
condition|?
name|Utilities
operator|.
name|nullStringOutput
else|:
name|values
operator|.
name|get
argument_list|(
literal|1
argument_list|)
operator|)
argument_list|)
expr_stmt|;
block|}
name|outStream
operator|.
name|write
argument_list|(
name|Utilities
operator|.
name|newLineCode
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|SemanticException
argument_list|(
name|e
argument_list|)
throw|;
comment|// TODO : simple wrap& rethrow for now, clean up with error
comment|// codes
block|}
finally|finally
block|{
name|IOUtils
operator|.
name|closeStream
argument_list|(
name|outStream
argument_list|)
expr_stmt|;
comment|// TODO : we have other closes here, and in ReplCopyTask -
comment|// replace with this
block|}
block|}
specifier|private
name|ReplicationSpec
name|getNewReplicationSpec
parameter_list|()
throws|throws
name|SemanticException
block|{
try|try
block|{
name|ReplicationSpec
name|replicationSpec
init|=
operator|new
name|ReplicationSpec
argument_list|(
literal|true
argument_list|,
literal|false
argument_list|,
literal|"replv2"
argument_list|,
literal|"will-be-set"
argument_list|,
literal|false
argument_list|,
literal|true
argument_list|)
decl_stmt|;
name|replicationSpec
operator|.
name|setCurrentReplicationState
argument_list|(
name|String
operator|.
name|valueOf
argument_list|(
name|db
operator|.
name|getMSC
argument_list|()
operator|.
name|getCurrentNotificationEventId
argument_list|()
operator|.
name|getEventId
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
return|return
name|replicationSpec
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|SemanticException
argument_list|(
name|e
argument_list|)
throw|;
comment|// TODO : simple wrap& rethrow for now, clean up with error
comment|// codes
block|}
block|}
specifier|private
name|Iterable
argument_list|<
name|?
extends|extends
name|String
argument_list|>
name|matchesTbl
parameter_list|(
name|String
name|dbName
parameter_list|,
name|String
name|tblPattern
parameter_list|)
throws|throws
name|HiveException
block|{
if|if
condition|(
name|tblPattern
operator|==
literal|null
condition|)
block|{
return|return
name|db
operator|.
name|getAllTables
argument_list|(
name|dbName
argument_list|)
return|;
block|}
else|else
block|{
return|return
name|db
operator|.
name|getTablesByPattern
argument_list|(
name|dbName
argument_list|,
name|tblPattern
argument_list|)
return|;
block|}
block|}
specifier|private
name|Iterable
argument_list|<
name|?
extends|extends
name|String
argument_list|>
name|matchesDb
parameter_list|(
name|String
name|dbPattern
parameter_list|)
throws|throws
name|HiveException
block|{
if|if
condition|(
name|dbPattern
operator|==
literal|null
condition|)
block|{
return|return
name|db
operator|.
name|getAllDatabases
argument_list|()
return|;
block|}
else|else
block|{
return|return
name|db
operator|.
name|getDatabasesByPattern
argument_list|(
name|dbPattern
argument_list|)
return|;
block|}
block|}
block|}
end_class

end_unit

