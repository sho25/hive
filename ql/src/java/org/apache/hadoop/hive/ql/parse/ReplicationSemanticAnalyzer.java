begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/*  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|parse
package|;
end_package

begin_import
import|import
name|org
operator|.
name|antlr
operator|.
name|runtime
operator|.
name|tree
operator|.
name|Tree
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|lang3
operator|.
name|StringUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|ValidTxnList
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|repl
operator|.
name|ReplScope
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|ReplChangeManager
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|Warehouse
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Database
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|ErrorMsg
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|QueryState
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|Task
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|TaskFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|repl
operator|.
name|ReplDumpWork
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|repl
operator|.
name|ReplExternalTables
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|repl
operator|.
name|ReplLoadWork
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|hooks
operator|.
name|ReadEntity
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|Hive
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|HiveException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|parse
operator|.
name|repl
operator|.
name|dump
operator|.
name|Utils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|parse
operator|.
name|repl
operator|.
name|load
operator|.
name|DumpMetaData
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|PlanUtils
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|FileNotFoundException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|net
operator|.
name|URI
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Collections
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|repl
operator|.
name|ReplExternalTables
operator|.
name|Reader
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|repl
operator|.
name|ExternalTableCopyTaskBuilder
operator|.
name|DirCopyWork
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEQUERYID
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
operator|.
name|ConfVars
operator|.
name|REPL_DUMP_METADATA_ONLY
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
operator|.
name|ConfVars
operator|.
name|REPL_ENABLE_MOVE_OPTIMIZATION
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
operator|.
name|ConfVars
operator|.
name|REPL_MOVE_OPTIMIZED_FILE_SCHEMES
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|parse
operator|.
name|HiveParser
operator|.
name|TOK_DBNAME
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|parse
operator|.
name|HiveParser
operator|.
name|TOK_FROM
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|parse
operator|.
name|HiveParser
operator|.
name|TOK_LIMIT
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|parse
operator|.
name|HiveParser
operator|.
name|TOK_REPLACE
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|parse
operator|.
name|HiveParser
operator|.
name|TOK_REPL_CONFIG
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|parse
operator|.
name|HiveParser
operator|.
name|TOK_REPL_DUMP
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|parse
operator|.
name|HiveParser
operator|.
name|TOK_REPL_LOAD
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|parse
operator|.
name|HiveParser
operator|.
name|TOK_REPL_STATUS
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|parse
operator|.
name|HiveParser
operator|.
name|TOK_REPL_TABLES
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|parse
operator|.
name|HiveParser
operator|.
name|TOK_TO
import|;
end_import

begin_class
specifier|public
class|class
name|ReplicationSemanticAnalyzer
extends|extends
name|BaseSemanticAnalyzer
block|{
comment|// Replication Scope
specifier|private
name|ReplScope
name|replScope
init|=
operator|new
name|ReplScope
argument_list|()
decl_stmt|;
specifier|private
name|ReplScope
name|oldReplScope
init|=
literal|null
decl_stmt|;
specifier|private
name|Long
name|eventFrom
decl_stmt|;
specifier|private
name|Long
name|eventTo
decl_stmt|;
specifier|private
name|Integer
name|maxEventLimit
decl_stmt|;
comment|// Base path for REPL LOAD
specifier|private
name|String
name|path
decl_stmt|;
comment|// Added conf member to set the REPL command specific config entries without affecting the configs
comment|// of any other queries running in the session
specifier|private
name|HiveConf
name|conf
decl_stmt|;
comment|// By default, this will be same as that of super class BaseSemanticAnalyzer. But need to obtain again
comment|// if the Hive configs are received from WITH clause in REPL LOAD or REPL STATUS commands.
specifier|private
name|Hive
name|db
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|String
name|dumpSchema
init|=
literal|"dump_dir,last_repl_id#string,string"
decl_stmt|;
name|ReplicationSemanticAnalyzer
parameter_list|(
name|QueryState
name|queryState
parameter_list|)
throws|throws
name|SemanticException
block|{
name|super
argument_list|(
name|queryState
argument_list|)
expr_stmt|;
name|this
operator|.
name|db
operator|=
name|super
operator|.
name|db
expr_stmt|;
name|this
operator|.
name|conf
operator|=
operator|new
name|HiveConf
argument_list|(
name|super
operator|.
name|conf
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|analyzeInternal
parameter_list|(
name|ASTNode
name|ast
parameter_list|)
throws|throws
name|SemanticException
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"ReplicationSemanticAanalyzer: analyzeInternal"
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
name|ast
operator|.
name|getName
argument_list|()
operator|+
literal|":"
operator|+
name|ast
operator|.
name|getToken
argument_list|()
operator|.
name|getText
argument_list|()
operator|+
literal|"="
operator|+
name|ast
operator|.
name|getText
argument_list|()
argument_list|)
expr_stmt|;
comment|// Some of the txn related configs were not set when ReplicationSemanticAnalyzer.conf was initialized.
comment|// It should be set first.
name|setTxnConfigs
argument_list|()
expr_stmt|;
switch|switch
condition|(
name|ast
operator|.
name|getToken
argument_list|()
operator|.
name|getType
argument_list|()
condition|)
block|{
case|case
name|TOK_REPL_DUMP
case|:
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"ReplicationSemanticAnalyzer: analyzeInternal: dump"
argument_list|)
expr_stmt|;
name|analyzeReplDump
argument_list|(
name|ast
argument_list|)
expr_stmt|;
break|break;
block|}
case|case
name|TOK_REPL_LOAD
case|:
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"ReplicationSemanticAnalyzer: analyzeInternal: load"
argument_list|)
expr_stmt|;
name|analyzeReplLoad
argument_list|(
name|ast
argument_list|)
expr_stmt|;
break|break;
block|}
case|case
name|TOK_REPL_STATUS
case|:
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"ReplicationSemanticAnalyzer: analyzeInternal: status"
argument_list|)
expr_stmt|;
name|analyzeReplStatus
argument_list|(
name|ast
argument_list|)
expr_stmt|;
break|break;
block|}
default|default:
block|{
throw|throw
operator|new
name|SemanticException
argument_list|(
literal|"Unexpected root token"
argument_list|)
throw|;
block|}
block|}
block|}
specifier|private
name|void
name|setTxnConfigs
parameter_list|()
block|{
name|String
name|validTxnList
init|=
name|queryState
operator|.
name|getConf
argument_list|()
operator|.
name|get
argument_list|(
name|ValidTxnList
operator|.
name|VALID_TXNS_KEY
argument_list|)
decl_stmt|;
if|if
condition|(
name|validTxnList
operator|!=
literal|null
condition|)
block|{
name|conf
operator|.
name|set
argument_list|(
name|ValidTxnList
operator|.
name|VALID_TXNS_KEY
argument_list|,
name|validTxnList
argument_list|)
expr_stmt|;
block|}
block|}
specifier|private
name|void
name|setReplDumpTablesList
parameter_list|(
name|Tree
name|replTablesNode
parameter_list|,
name|ReplScope
name|replScope
parameter_list|)
throws|throws
name|HiveException
block|{
name|int
name|childCount
init|=
name|replTablesNode
operator|.
name|getChildCount
argument_list|()
decl_stmt|;
assert|assert
operator|(
name|childCount
operator|<=
literal|2
operator|)
assert|;
comment|// Traverse the children which can be either just include tables list or both include
comment|// and exclude tables lists.
name|String
name|replScopeType
init|=
operator|(
name|replScope
operator|==
name|this
operator|.
name|replScope
operator|)
condition|?
literal|"Current"
else|:
literal|"Old"
decl_stmt|;
for|for
control|(
name|int
name|listIdx
init|=
literal|0
init|;
name|listIdx
operator|<
name|childCount
condition|;
name|listIdx
operator|++
control|)
block|{
name|String
name|tableList
init|=
name|unescapeSQLString
argument_list|(
name|replTablesNode
operator|.
name|getChild
argument_list|(
name|listIdx
argument_list|)
operator|.
name|getText
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|tableList
operator|==
literal|null
operator|||
name|tableList
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
throw|throw
operator|new
name|SemanticException
argument_list|(
name|ErrorMsg
operator|.
name|REPL_INVALID_DB_OR_TABLE_PATTERN
argument_list|)
throw|;
block|}
if|if
condition|(
name|listIdx
operator|==
literal|0
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"{} ReplScope: Set Included Tables List: {}"
argument_list|,
name|replScopeType
argument_list|,
name|tableList
argument_list|)
expr_stmt|;
name|replScope
operator|.
name|setIncludedTablePatterns
argument_list|(
name|tableList
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"{} ReplScope: Set Excluded Tables List: {}"
argument_list|,
name|replScopeType
argument_list|,
name|tableList
argument_list|)
expr_stmt|;
name|replScope
operator|.
name|setExcludedTablePatterns
argument_list|(
name|tableList
argument_list|)
expr_stmt|;
block|}
block|}
block|}
specifier|private
name|void
name|setOldReplPolicy
parameter_list|(
name|Tree
name|oldReplPolicyTree
parameter_list|)
throws|throws
name|HiveException
block|{
name|oldReplScope
operator|=
operator|new
name|ReplScope
argument_list|()
expr_stmt|;
name|int
name|childCount
init|=
name|oldReplPolicyTree
operator|.
name|getChildCount
argument_list|()
decl_stmt|;
comment|// First child is DB name and optional second child is tables list.
assert|assert
operator|(
name|childCount
operator|<=
literal|2
operator|)
assert|;
comment|// First child is always the DB name. So set it.
name|oldReplScope
operator|.
name|setDbName
argument_list|(
name|oldReplPolicyTree
operator|.
name|getChild
argument_list|(
literal|0
argument_list|)
operator|.
name|getText
argument_list|()
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Old ReplScope: Set DB Name: {}"
argument_list|,
name|oldReplScope
operator|.
name|getDbName
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|oldReplScope
operator|.
name|getDbName
argument_list|()
operator|.
name|equalsIgnoreCase
argument_list|(
name|replScope
operator|.
name|getDbName
argument_list|()
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"DB name {} cannot be replaced to {} in the replication policy."
argument_list|,
name|oldReplScope
operator|.
name|getDbName
argument_list|()
argument_list|,
name|replScope
operator|.
name|getDbName
argument_list|()
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|SemanticException
argument_list|(
literal|"DB name cannot be replaced in the replication policy."
argument_list|)
throw|;
block|}
comment|// If the old policy is just<db_name>, then tables list won't be there.
if|if
condition|(
name|childCount
operator|<=
literal|1
condition|)
block|{
return|return;
block|}
comment|// Traverse the children which can be either just include tables list or both include
comment|// and exclude tables lists.
name|Tree
name|oldPolicyTablesListNode
init|=
name|oldReplPolicyTree
operator|.
name|getChild
argument_list|(
literal|1
argument_list|)
decl_stmt|;
assert|assert
operator|(
name|oldPolicyTablesListNode
operator|.
name|getType
argument_list|()
operator|==
name|TOK_REPL_TABLES
operator|)
assert|;
name|setReplDumpTablesList
argument_list|(
name|oldPolicyTablesListNode
argument_list|,
name|oldReplScope
argument_list|)
expr_stmt|;
block|}
specifier|private
name|void
name|initReplDump
parameter_list|(
name|ASTNode
name|ast
parameter_list|)
throws|throws
name|HiveException
block|{
name|int
name|numChildren
init|=
name|ast
operator|.
name|getChildCount
argument_list|()
decl_stmt|;
name|boolean
name|isMetaDataOnly
init|=
literal|false
decl_stmt|;
name|String
name|dbNameOrPattern
init|=
name|PlanUtils
operator|.
name|stripQuotes
argument_list|(
name|ast
operator|.
name|getChild
argument_list|(
literal|0
argument_list|)
operator|.
name|getText
argument_list|()
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Current ReplScope: Set DB Name: {}"
argument_list|,
name|dbNameOrPattern
argument_list|)
expr_stmt|;
name|replScope
operator|.
name|setDbName
argument_list|(
name|dbNameOrPattern
argument_list|)
expr_stmt|;
comment|// Skip the first node, which is always required
name|int
name|childIdx
init|=
literal|1
decl_stmt|;
while|while
condition|(
name|childIdx
operator|<
name|numChildren
condition|)
block|{
name|Tree
name|currNode
init|=
name|ast
operator|.
name|getChild
argument_list|(
name|childIdx
argument_list|)
decl_stmt|;
switch|switch
condition|(
name|currNode
operator|.
name|getType
argument_list|()
condition|)
block|{
case|case
name|TOK_REPL_CONFIG
case|:
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|replConfigs
init|=
name|getProps
argument_list|(
operator|(
name|ASTNode
operator|)
name|currNode
operator|.
name|getChild
argument_list|(
literal|0
argument_list|)
argument_list|)
decl_stmt|;
if|if
condition|(
literal|null
operator|!=
name|replConfigs
condition|)
block|{
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|config
range|:
name|replConfigs
operator|.
name|entrySet
argument_list|()
control|)
block|{
name|conf
operator|.
name|set
argument_list|(
name|config
operator|.
name|getKey
argument_list|()
argument_list|,
name|config
operator|.
name|getValue
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|isMetaDataOnly
operator|=
name|HiveConf
operator|.
name|getBoolVar
argument_list|(
name|conf
argument_list|,
name|REPL_DUMP_METADATA_ONLY
argument_list|)
expr_stmt|;
block|}
break|break;
case|case
name|TOK_REPL_TABLES
case|:
name|setReplDumpTablesList
argument_list|(
name|currNode
argument_list|,
name|replScope
argument_list|)
expr_stmt|;
break|break;
case|case
name|TOK_REPLACE
case|:
name|setOldReplPolicy
argument_list|(
name|currNode
argument_list|)
expr_stmt|;
break|break;
case|case
name|TOK_FROM
case|:
comment|// TOK_FROM subtree
name|Tree
name|fromNode
init|=
name|currNode
decl_stmt|;
name|eventFrom
operator|=
name|Long
operator|.
name|parseLong
argument_list|(
name|PlanUtils
operator|.
name|stripQuotes
argument_list|(
name|fromNode
operator|.
name|getChild
argument_list|(
literal|0
argument_list|)
operator|.
name|getText
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
comment|// Skip the first, which is always required
name|int
name|fromChildIdx
init|=
literal|1
decl_stmt|;
while|while
condition|(
name|fromChildIdx
operator|<
name|fromNode
operator|.
name|getChildCount
argument_list|()
condition|)
block|{
if|if
condition|(
name|fromNode
operator|.
name|getChild
argument_list|(
name|fromChildIdx
argument_list|)
operator|.
name|getType
argument_list|()
operator|==
name|TOK_TO
condition|)
block|{
name|eventTo
operator|=
name|Long
operator|.
name|parseLong
argument_list|(
name|PlanUtils
operator|.
name|stripQuotes
argument_list|(
name|fromNode
operator|.
name|getChild
argument_list|(
name|fromChildIdx
operator|+
literal|1
argument_list|)
operator|.
name|getText
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
comment|// Skip the next child, since we already took care of it
name|fromChildIdx
operator|++
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|fromNode
operator|.
name|getChild
argument_list|(
name|fromChildIdx
argument_list|)
operator|.
name|getType
argument_list|()
operator|==
name|TOK_LIMIT
condition|)
block|{
name|maxEventLimit
operator|=
name|Integer
operator|.
name|parseInt
argument_list|(
name|PlanUtils
operator|.
name|stripQuotes
argument_list|(
name|fromNode
operator|.
name|getChild
argument_list|(
name|fromChildIdx
operator|+
literal|1
argument_list|)
operator|.
name|getText
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
comment|// Skip the next child, since we already took care of it
name|fromChildIdx
operator|++
expr_stmt|;
block|}
comment|// move to the next child in FROM tree
name|fromChildIdx
operator|++
expr_stmt|;
block|}
break|break;
default|default:
throw|throw
operator|new
name|SemanticException
argument_list|(
literal|"Unrecognized token "
operator|+
name|currNode
operator|.
name|getType
argument_list|()
operator|+
literal|" in REPL DUMP statement."
argument_list|)
throw|;
block|}
comment|// Move to the next root node
name|childIdx
operator|++
expr_stmt|;
block|}
for|for
control|(
name|String
name|dbName
range|:
name|Utils
operator|.
name|matchesDb
argument_list|(
name|db
argument_list|,
name|dbNameOrPattern
argument_list|)
control|)
block|{
name|Database
name|database
init|=
name|db
operator|.
name|getDatabase
argument_list|(
name|dbName
argument_list|)
decl_stmt|;
if|if
condition|(
name|database
operator|!=
literal|null
condition|)
block|{
if|if
condition|(
operator|!
name|isMetaDataOnly
operator|&&
operator|!
name|ReplChangeManager
operator|.
name|isSourceOfReplication
argument_list|(
name|database
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Cannot dump database "
operator|+
name|dbNameOrPattern
operator|+
literal|" as it is not a source of replication (repl.source.for)"
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|SemanticException
argument_list|(
name|ErrorMsg
operator|.
name|REPL_DATABASE_IS_NOT_SOURCE_OF_REPLICATION
operator|.
name|getMsg
argument_list|()
argument_list|)
throw|;
block|}
block|}
else|else
block|{
throw|throw
operator|new
name|SemanticException
argument_list|(
literal|"Cannot dump database "
operator|+
name|dbNameOrPattern
operator|+
literal|" as it does not exist"
argument_list|)
throw|;
block|}
block|}
block|}
comment|// REPL DUMP
specifier|private
name|void
name|analyzeReplDump
parameter_list|(
name|ASTNode
name|ast
parameter_list|)
throws|throws
name|SemanticException
block|{
try|try
block|{
name|initReplDump
argument_list|(
name|ast
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|HiveException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|SemanticException
argument_list|(
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
try|try
block|{
name|ctx
operator|.
name|setResFile
argument_list|(
name|ctx
operator|.
name|getLocalTmpPath
argument_list|()
argument_list|)
expr_stmt|;
name|Task
argument_list|<
name|ReplDumpWork
argument_list|>
name|replDumpWorkTask
init|=
name|TaskFactory
operator|.
name|get
argument_list|(
operator|new
name|ReplDumpWork
argument_list|(
name|replScope
argument_list|,
name|oldReplScope
argument_list|,
name|eventFrom
argument_list|,
name|eventTo
argument_list|,
name|ErrorMsg
operator|.
name|INVALID_PATH
operator|.
name|getMsg
argument_list|(
name|ast
argument_list|)
argument_list|,
name|maxEventLimit
argument_list|,
name|ctx
operator|.
name|getResFile
argument_list|()
operator|.
name|toUri
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|)
argument_list|,
name|conf
argument_list|)
decl_stmt|;
name|rootTasks
operator|.
name|add
argument_list|(
name|replDumpWorkTask
argument_list|)
expr_stmt|;
for|for
control|(
name|String
name|dbName
range|:
name|Utils
operator|.
name|matchesDb
argument_list|(
name|db
argument_list|,
name|replScope
operator|.
name|getDbName
argument_list|()
argument_list|)
control|)
block|{
if|if
condition|(
operator|!
name|replScope
operator|.
name|includeAllTables
argument_list|()
condition|)
block|{
for|for
control|(
name|String
name|tblName
range|:
name|Utils
operator|.
name|matchesTbl
argument_list|(
name|db
argument_list|,
name|dbName
argument_list|,
name|replScope
argument_list|)
control|)
block|{
name|inputs
operator|.
name|add
argument_list|(
operator|new
name|ReadEntity
argument_list|(
name|db
operator|.
name|getTable
argument_list|(
name|dbName
argument_list|,
name|tblName
argument_list|)
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
name|inputs
operator|.
name|add
argument_list|(
operator|new
name|ReadEntity
argument_list|(
name|db
operator|.
name|getDatabase
argument_list|(
name|dbName
argument_list|)
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
name|setFetchTask
argument_list|(
name|createFetchTask
argument_list|(
name|dumpSchema
argument_list|)
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
comment|// TODO : simple wrap& rethrow for now, clean up with error codes
name|LOG
operator|.
name|warn
argument_list|(
literal|"Error during analyzeReplDump"
argument_list|,
name|e
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|SemanticException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|private
name|boolean
name|ifEnableMoveOptimization
parameter_list|(
name|Path
name|filePath
parameter_list|,
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configuration
name|conf
parameter_list|)
throws|throws
name|Exception
block|{
if|if
condition|(
name|filePath
operator|==
literal|null
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"filePath cannot be null"
argument_list|)
throw|;
block|}
name|URI
name|uri
init|=
name|filePath
operator|.
name|toUri
argument_list|()
decl_stmt|;
name|String
name|scheme
init|=
name|uri
operator|.
name|getScheme
argument_list|()
decl_stmt|;
name|scheme
operator|=
name|StringUtils
operator|.
name|isBlank
argument_list|(
name|scheme
argument_list|)
condition|?
name|FileSystem
operator|.
name|get
argument_list|(
name|uri
argument_list|,
name|conf
argument_list|)
operator|.
name|getScheme
argument_list|()
else|:
name|scheme
expr_stmt|;
if|if
condition|(
name|StringUtils
operator|.
name|isBlank
argument_list|(
name|scheme
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|HiveException
argument_list|(
literal|"Cannot get valid scheme for "
operator|+
name|filePath
argument_list|)
throw|;
block|}
name|LOG
operator|.
name|info
argument_list|(
literal|"scheme is "
operator|+
name|scheme
argument_list|)
expr_stmt|;
name|String
index|[]
name|schmeList
init|=
name|conf
operator|.
name|get
argument_list|(
name|REPL_MOVE_OPTIMIZED_FILE_SCHEMES
operator|.
name|varname
argument_list|)
operator|.
name|toLowerCase
argument_list|()
operator|.
name|split
argument_list|(
literal|","
argument_list|)
decl_stmt|;
for|for
control|(
name|String
name|schemeIter
range|:
name|schmeList
control|)
block|{
if|if
condition|(
name|schemeIter
operator|.
name|trim
argument_list|()
operator|.
name|equalsIgnoreCase
argument_list|(
name|scheme
operator|.
name|trim
argument_list|()
argument_list|)
condition|)
block|{
return|return
literal|true
return|;
block|}
block|}
return|return
literal|false
return|;
block|}
comment|// REPL LOAD
specifier|private
name|void
name|initReplLoad
parameter_list|(
name|ASTNode
name|ast
parameter_list|)
throws|throws
name|SemanticException
block|{
name|path
operator|=
name|PlanUtils
operator|.
name|stripQuotes
argument_list|(
name|ast
operator|.
name|getChild
argument_list|(
literal|0
argument_list|)
operator|.
name|getText
argument_list|()
argument_list|)
expr_stmt|;
name|int
name|numChildren
init|=
name|ast
operator|.
name|getChildCount
argument_list|()
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|1
init|;
name|i
operator|<
name|numChildren
condition|;
name|i
operator|++
control|)
block|{
name|ASTNode
name|childNode
init|=
operator|(
name|ASTNode
operator|)
name|ast
operator|.
name|getChild
argument_list|(
name|i
argument_list|)
decl_stmt|;
switch|switch
condition|(
name|childNode
operator|.
name|getToken
argument_list|()
operator|.
name|getType
argument_list|()
condition|)
block|{
case|case
name|TOK_DBNAME
case|:
name|replScope
operator|.
name|setDbName
argument_list|(
name|PlanUtils
operator|.
name|stripQuotes
argument_list|(
name|childNode
operator|.
name|getChild
argument_list|(
literal|0
argument_list|)
operator|.
name|getText
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
break|break;
case|case
name|TOK_REPL_CONFIG
case|:
name|setConfigs
argument_list|(
operator|(
name|ASTNode
operator|)
name|childNode
operator|.
name|getChild
argument_list|(
literal|0
argument_list|)
argument_list|)
expr_stmt|;
break|break;
default|default:
throw|throw
operator|new
name|SemanticException
argument_list|(
literal|"Unrecognized token in REPL LOAD statement."
argument_list|)
throw|;
block|}
block|}
block|}
comment|/*    * Example dump dirs we need to be able to handle :    *    * for: hive.repl.rootdir = staging/    * Then, repl dumps will be created in staging/<dumpdir>    *    * single-db-dump: staging/blah12345 will contain a db dir for the db specified    *  blah12345/    *   default/    *    _metadata    *    tbl1/    *      _metadata    *      dt=20160907/    *        _files    *    tbl2/    *    tbl3/    *    unptn_tbl/    *      _metadata    *      _files    *    * multi-db-dump: staging/bar12347 will contain dirs for each db covered    * staging/    *  bar12347/    *   default/    *     ...    *   sales/    *     ...    *    * single table-dump: staging/baz123 will contain a table object dump inside    * staging/    *  baz123/    *    _metadata    *    dt=20150931/    *      _files    *    * incremental dump : staging/blue123 will contain dirs for each event inside.    * staging/    *  blue123/    *    34/    *    35/    *    36/    */
specifier|private
name|void
name|analyzeReplLoad
parameter_list|(
name|ASTNode
name|ast
parameter_list|)
throws|throws
name|SemanticException
block|{
name|initReplLoad
argument_list|(
name|ast
argument_list|)
expr_stmt|;
comment|// For analyze repl load, we walk through the dir structure available in the path,
comment|// looking at each db, and then each table, and then setting up the appropriate
comment|// import job in its place.
try|try
block|{
assert|assert
operator|(
name|path
operator|!=
literal|null
operator|)
assert|;
name|Path
name|loadPath
init|=
operator|new
name|Path
argument_list|(
name|path
argument_list|)
decl_stmt|;
specifier|final
name|FileSystem
name|fs
init|=
name|loadPath
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
comment|// Make fully qualified path for further use.
name|loadPath
operator|=
name|fs
operator|.
name|makeQualified
argument_list|(
name|loadPath
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|fs
operator|.
name|exists
argument_list|(
name|loadPath
argument_list|)
condition|)
block|{
comment|// supposed dump path does not exist.
name|LOG
operator|.
name|error
argument_list|(
literal|"File not found "
operator|+
name|loadPath
operator|.
name|toUri
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|FileNotFoundException
argument_list|(
name|ErrorMsg
operator|.
name|REPL_LOAD_PATH_NOT_FOUND
operator|.
name|getMsg
argument_list|()
argument_list|)
throw|;
block|}
comment|// Ths config is set to make sure that in case of s3 replication, move is skipped.
try|try
block|{
name|Warehouse
name|wh
init|=
operator|new
name|Warehouse
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|Path
name|filePath
init|=
name|wh
operator|.
name|getWhRoot
argument_list|()
decl_stmt|;
if|if
condition|(
name|ifEnableMoveOptimization
argument_list|(
name|filePath
argument_list|,
name|conf
argument_list|)
condition|)
block|{
name|conf
operator|.
name|setBoolVar
argument_list|(
name|REPL_ENABLE_MOVE_OPTIMIZATION
argument_list|,
literal|true
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|" Set move optimization to true for warehouse "
operator|+
name|filePath
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|SemanticException
argument_list|(
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
comment|// Now, the dumped path can be one of three things:
comment|// a) It can be a db dump, in which case we expect a set of dirs, each with a
comment|// db name, and with a _metadata file in each, and table dirs inside that.
comment|// b) It can be a table dump dir, in which case we expect a _metadata dump of
comment|// a table in question in the dir, and individual ptn dir hierarchy.
comment|// c) A dump can be an incremental dump, which means we have several subdirs
comment|// each of which have the evid as the dir name, and each of which correspond
comment|// to a event-level dump. Currently, only CREATE_TABLE and ADD_PARTITION are
comment|// handled, so all of these dumps will be at a table/ptn level.
comment|// For incremental repl, we will have individual events which can
comment|// be other things like roles and fns as well.
comment|// At this point, all dump dirs should contain a _dumpmetadata file that
comment|// tells us what is inside that dumpdir.
name|DumpMetaData
name|dmd
init|=
operator|new
name|DumpMetaData
argument_list|(
name|loadPath
argument_list|,
name|conf
argument_list|)
decl_stmt|;
name|boolean
name|evDump
init|=
literal|false
decl_stmt|;
comment|// we will decide what hdfs locations needs to be copied over here as well.
if|if
condition|(
name|dmd
operator|.
name|isIncrementalDump
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"{} contains an incremental dump"
argument_list|,
name|loadPath
argument_list|)
expr_stmt|;
name|evDump
operator|=
literal|true
expr_stmt|;
block|}
else|else
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"{} contains an bootstrap dump"
argument_list|,
name|loadPath
argument_list|)
expr_stmt|;
block|}
name|ReplLoadWork
name|replLoadWork
init|=
operator|new
name|ReplLoadWork
argument_list|(
name|conf
argument_list|,
name|loadPath
operator|.
name|toString
argument_list|()
argument_list|,
name|replScope
operator|.
name|getDbName
argument_list|()
argument_list|,
name|dmd
operator|.
name|getReplScope
argument_list|()
argument_list|,
name|queryState
operator|.
name|getLineageState
argument_list|()
argument_list|,
name|evDump
argument_list|,
name|dmd
operator|.
name|getEventTo
argument_list|()
argument_list|,
name|dirLocationsToCopy
argument_list|(
name|loadPath
argument_list|,
name|evDump
argument_list|)
argument_list|)
decl_stmt|;
name|rootTasks
operator|.
name|add
argument_list|(
name|TaskFactory
operator|.
name|get
argument_list|(
name|replLoadWork
argument_list|,
name|conf
argument_list|)
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
comment|// TODO : simple wrap& rethrow for now, clean up with error codes
throw|throw
operator|new
name|SemanticException
argument_list|(
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
specifier|private
name|List
argument_list|<
name|DirCopyWork
argument_list|>
name|dirLocationsToCopy
parameter_list|(
name|Path
name|loadPath
parameter_list|,
name|boolean
name|isIncrementalPhase
parameter_list|)
throws|throws
name|HiveException
throws|,
name|IOException
block|{
name|List
argument_list|<
name|DirCopyWork
argument_list|>
name|list
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|()
decl_stmt|;
name|String
name|baseDir
init|=
name|conf
operator|.
name|get
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|REPL_EXTERNAL_TABLE_BASE_DIR
operator|.
name|varname
argument_list|)
decl_stmt|;
comment|// this is done to remove any scheme related information that will be present in the base path
comment|// specifically when we are replicating to cloud storage
name|Path
name|basePath
init|=
operator|new
name|Path
argument_list|(
name|baseDir
argument_list|)
decl_stmt|;
for|for
control|(
name|String
name|location
range|:
operator|new
name|Reader
argument_list|(
name|conf
argument_list|,
name|loadPath
argument_list|,
name|isIncrementalPhase
argument_list|)
operator|.
name|sourceLocationsToCopy
argument_list|()
control|)
block|{
name|Path
name|sourcePath
init|=
operator|new
name|Path
argument_list|(
name|location
argument_list|)
decl_stmt|;
name|Path
name|targetPath
init|=
name|ReplExternalTables
operator|.
name|externalTableDataPath
argument_list|(
name|conf
argument_list|,
name|basePath
argument_list|,
name|sourcePath
argument_list|)
decl_stmt|;
name|list
operator|.
name|add
argument_list|(
operator|new
name|DirCopyWork
argument_list|(
name|sourcePath
argument_list|,
name|targetPath
argument_list|)
argument_list|)
expr_stmt|;
block|}
return|return
name|list
return|;
block|}
specifier|private
name|void
name|setConfigs
parameter_list|(
name|ASTNode
name|node
parameter_list|)
throws|throws
name|SemanticException
block|{
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|replConfigs
init|=
name|getProps
argument_list|(
name|node
argument_list|)
decl_stmt|;
if|if
condition|(
literal|null
operator|!=
name|replConfigs
condition|)
block|{
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|config
range|:
name|replConfigs
operator|.
name|entrySet
argument_list|()
control|)
block|{
name|String
name|key
init|=
name|config
operator|.
name|getKey
argument_list|()
decl_stmt|;
comment|// don't set the query id in the config
if|if
condition|(
name|key
operator|.
name|equalsIgnoreCase
argument_list|(
name|HIVEQUERYID
operator|.
name|varname
argument_list|)
condition|)
block|{
name|String
name|queryTag
init|=
name|config
operator|.
name|getValue
argument_list|()
decl_stmt|;
if|if
condition|(
operator|!
name|StringUtils
operator|.
name|isEmpty
argument_list|(
name|queryTag
argument_list|)
condition|)
block|{
name|QueryState
operator|.
name|setApplicationTag
argument_list|(
name|conf
argument_list|,
name|queryTag
argument_list|)
expr_stmt|;
block|}
name|queryState
operator|.
name|setQueryTag
argument_list|(
name|queryTag
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|conf
operator|.
name|set
argument_list|(
name|key
argument_list|,
name|config
operator|.
name|getValue
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
comment|// As hive conf is changed, need to get the Hive DB again with it.
try|try
block|{
name|db
operator|=
name|Hive
operator|.
name|get
argument_list|(
name|conf
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|HiveException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|SemanticException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
block|}
comment|// REPL STATUS
specifier|private
name|void
name|initReplStatus
parameter_list|(
name|ASTNode
name|ast
parameter_list|)
throws|throws
name|SemanticException
block|{
name|replScope
operator|.
name|setDbName
argument_list|(
name|PlanUtils
operator|.
name|stripQuotes
argument_list|(
name|ast
operator|.
name|getChild
argument_list|(
literal|0
argument_list|)
operator|.
name|getText
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
name|int
name|numChildren
init|=
name|ast
operator|.
name|getChildCount
argument_list|()
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|1
init|;
name|i
operator|<
name|numChildren
condition|;
name|i
operator|++
control|)
block|{
name|ASTNode
name|childNode
init|=
operator|(
name|ASTNode
operator|)
name|ast
operator|.
name|getChild
argument_list|(
name|i
argument_list|)
decl_stmt|;
if|if
condition|(
name|childNode
operator|.
name|getToken
argument_list|()
operator|.
name|getType
argument_list|()
operator|==
name|TOK_REPL_CONFIG
condition|)
block|{
name|setConfigs
argument_list|(
operator|(
name|ASTNode
operator|)
name|childNode
operator|.
name|getChild
argument_list|(
literal|0
argument_list|)
argument_list|)
expr_stmt|;
block|}
else|else
block|{
throw|throw
operator|new
name|SemanticException
argument_list|(
literal|"Unrecognized token in REPL STATUS statement."
argument_list|)
throw|;
block|}
block|}
block|}
specifier|private
name|void
name|analyzeReplStatus
parameter_list|(
name|ASTNode
name|ast
parameter_list|)
throws|throws
name|SemanticException
block|{
name|initReplStatus
argument_list|(
name|ast
argument_list|)
expr_stmt|;
name|String
name|dbNameOrPattern
init|=
name|replScope
operator|.
name|getDbName
argument_list|()
decl_stmt|;
name|String
name|replLastId
init|=
literal|null
decl_stmt|;
try|try
block|{
comment|// Checking for status of a db
name|Database
name|database
init|=
name|db
operator|.
name|getDatabase
argument_list|(
name|dbNameOrPattern
argument_list|)
decl_stmt|;
if|if
condition|(
name|database
operator|!=
literal|null
condition|)
block|{
name|inputs
operator|.
name|add
argument_list|(
operator|new
name|ReadEntity
argument_list|(
name|database
argument_list|)
argument_list|)
expr_stmt|;
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|params
init|=
name|database
operator|.
name|getParameters
argument_list|()
decl_stmt|;
if|if
condition|(
name|params
operator|!=
literal|null
operator|&&
operator|(
name|params
operator|.
name|containsKey
argument_list|(
name|ReplicationSpec
operator|.
name|KEY
operator|.
name|CURR_STATE_ID
operator|.
name|toString
argument_list|()
argument_list|)
operator|)
condition|)
block|{
name|replLastId
operator|=
name|params
operator|.
name|get
argument_list|(
name|ReplicationSpec
operator|.
name|KEY
operator|.
name|CURR_STATE_ID
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
block|}
catch|catch
parameter_list|(
name|HiveException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|SemanticException
argument_list|(
name|e
argument_list|)
throw|;
comment|// TODO : simple wrap& rethrow for now, clean up with error
comment|// codes
block|}
name|prepareReturnValues
argument_list|(
name|Collections
operator|.
name|singletonList
argument_list|(
name|replLastId
argument_list|)
argument_list|,
literal|"last_repl_id#string"
argument_list|)
expr_stmt|;
name|setFetchTask
argument_list|(
name|createFetchTask
argument_list|(
literal|"last_repl_id#string"
argument_list|)
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"ReplicationSemanticAnalyzer.analyzeReplStatus: writing repl.last.id={} out to {}"
argument_list|,
name|replLastId
argument_list|,
name|ctx
operator|.
name|getResFile
argument_list|()
argument_list|,
name|conf
argument_list|)
expr_stmt|;
block|}
specifier|private
name|void
name|prepareReturnValues
parameter_list|(
name|List
argument_list|<
name|String
argument_list|>
name|values
parameter_list|,
name|String
name|schema
parameter_list|)
throws|throws
name|SemanticException
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"prepareReturnValues : "
operator|+
name|schema
argument_list|)
expr_stmt|;
for|for
control|(
name|String
name|s
range|:
name|values
control|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"> "
operator|+
name|s
argument_list|)
expr_stmt|;
block|}
name|ctx
operator|.
name|setResFile
argument_list|(
name|ctx
operator|.
name|getLocalTmpPath
argument_list|()
argument_list|)
expr_stmt|;
name|Utils
operator|.
name|writeOutput
argument_list|(
name|Collections
operator|.
name|singletonList
argument_list|(
name|values
argument_list|)
argument_list|,
name|ctx
operator|.
name|getResFile
argument_list|()
argument_list|,
name|conf
argument_list|)
expr_stmt|;
block|}
block|}
end_class

end_unit

