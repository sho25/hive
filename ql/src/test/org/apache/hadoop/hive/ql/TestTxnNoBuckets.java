begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/*  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
package|;
end_package

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|IMetaStoreClient
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|ColumnStatisticsObj
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|ShowCompactRequest
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|ShowCompactResponse
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|conf
operator|.
name|MetastoreConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|txn
operator|.
name|TxnStore
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|txn
operator|.
name|TxnUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|AcidUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|BucketCodec
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|Hive
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|processors
operator|.
name|CommandProcessorResponse
import|;
end_import

begin_import
import|import
name|org
operator|.
name|junit
operator|.
name|Assert
import|;
end_import

begin_import
import|import
name|org
operator|.
name|junit
operator|.
name|Rule
import|;
end_import

begin_import
import|import
name|org
operator|.
name|junit
operator|.
name|Test
import|;
end_import

begin_import
import|import
name|org
operator|.
name|junit
operator|.
name|rules
operator|.
name|TestName
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|Logger
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|LoggerFactory
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|File
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashSet
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Set
import|;
end_import

begin_class
specifier|public
class|class
name|TestTxnNoBuckets
extends|extends
name|TxnCommandsBaseForTests
block|{
specifier|static
specifier|final
specifier|private
name|Logger
name|LOG
init|=
name|LoggerFactory
operator|.
name|getLogger
argument_list|(
name|TestTxnNoBuckets
operator|.
name|class
argument_list|)
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|String
name|TEST_DATA_DIR
init|=
operator|new
name|File
argument_list|(
name|System
operator|.
name|getProperty
argument_list|(
literal|"java.io.tmpdir"
argument_list|)
operator|+
name|File
operator|.
name|separator
operator|+
name|TestTxnNoBuckets
operator|.
name|class
operator|.
name|getCanonicalName
argument_list|()
operator|+
literal|"-"
operator|+
name|System
operator|.
name|currentTimeMillis
argument_list|()
argument_list|)
operator|.
name|getPath
argument_list|()
operator|.
name|replaceAll
argument_list|(
literal|"\\\\"
argument_list|,
literal|"/"
argument_list|)
decl_stmt|;
annotation|@
name|Rule
specifier|public
name|TestName
name|testName
init|=
operator|new
name|TestName
argument_list|()
decl_stmt|;
annotation|@
name|Override
name|String
name|getTestDataDir
parameter_list|()
block|{
return|return
name|TEST_DATA_DIR
return|;
block|}
specifier|private
name|boolean
name|shouldVectorize
parameter_list|()
block|{
return|return
name|hiveConf
operator|.
name|getBoolVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_VECTORIZATION_ENABLED
argument_list|)
return|;
block|}
comment|/**    * Tests that Acid can work with un-bucketed tables.    */
annotation|@
name|Test
specifier|public
name|void
name|testNoBuckets
parameter_list|()
throws|throws
name|Exception
block|{
name|int
index|[]
index|[]
name|sourceVals1
init|=
block|{
block|{
literal|0
block|,
literal|0
block|,
literal|0
block|}
block|,
block|{
literal|3
block|,
literal|3
block|,
literal|3
block|}
block|}
decl_stmt|;
name|int
index|[]
index|[]
name|sourceVals2
init|=
block|{
block|{
literal|1
block|,
literal|1
block|,
literal|1
block|}
block|,
block|{
literal|2
block|,
literal|2
block|,
literal|2
block|}
block|}
decl_stmt|;
name|runStatementOnDriver
argument_list|(
literal|"drop table if exists tmp"
argument_list|)
expr_stmt|;
name|runStatementOnDriver
argument_list|(
literal|"create table tmp (c1 integer, c2 integer, c3 integer) stored as orc tblproperties('transactional'='false')"
argument_list|)
expr_stmt|;
name|runStatementOnDriver
argument_list|(
literal|"insert into tmp "
operator|+
name|makeValuesClause
argument_list|(
name|sourceVals1
argument_list|)
argument_list|)
expr_stmt|;
name|runStatementOnDriver
argument_list|(
literal|"insert into tmp "
operator|+
name|makeValuesClause
argument_list|(
name|sourceVals2
argument_list|)
argument_list|)
expr_stmt|;
name|runStatementOnDriver
argument_list|(
literal|"drop table if exists nobuckets"
argument_list|)
expr_stmt|;
name|runStatementOnDriver
argument_list|(
literal|"create table nobuckets (c1 integer, c2 integer, c3 integer) stored "
operator|+
literal|"as orc tblproperties('transactional'='true', 'transactional_properties'='default')"
argument_list|)
expr_stmt|;
name|String
name|stmt
init|=
literal|"insert into nobuckets select * from tmp"
decl_stmt|;
name|runStatementOnDriver
argument_list|(
name|stmt
argument_list|)
expr_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|rs
init|=
name|runStatementOnDriver
argument_list|(
literal|"select ROW__ID, c1, c2, c3, INPUT__FILE__NAME from nobuckets order by ROW__ID"
argument_list|)
decl_stmt|;
name|Assert
operator|.
name|assertEquals
argument_list|(
literal|""
argument_list|,
literal|4
argument_list|,
name|rs
operator|.
name|size
argument_list|()
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|warn
argument_list|(
literal|"after insert"
argument_list|)
expr_stmt|;
for|for
control|(
name|String
name|s
range|:
name|rs
control|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
name|s
argument_list|)
expr_stmt|;
block|}
comment|/**the insert creates 2 output files (presumably because there are 2 input files)      * The number in the file name is writerId.  This is the number encoded in ROW__ID.bucketId -      * see {@link org.apache.hadoop.hive.ql.io.BucketCodec}*/
name|Assert
operator|.
name|assertTrue
argument_list|(
name|rs
operator|.
name|get
argument_list|(
literal|0
argument_list|)
argument_list|,
name|rs
operator|.
name|get
argument_list|(
literal|0
argument_list|)
operator|.
name|startsWith
argument_list|(
literal|"{\"writeid\":1,\"bucketid\":536870912,\"rowid\":0}\t0\t0\t0\t"
argument_list|)
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertTrue
argument_list|(
name|rs
operator|.
name|get
argument_list|(
literal|0
argument_list|)
argument_list|,
name|rs
operator|.
name|get
argument_list|(
literal|0
argument_list|)
operator|.
name|endsWith
argument_list|(
literal|"nobuckets/delta_0000001_0000001_0000/bucket_00000"
argument_list|)
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertTrue
argument_list|(
name|rs
operator|.
name|get
argument_list|(
literal|1
argument_list|)
argument_list|,
name|rs
operator|.
name|get
argument_list|(
literal|1
argument_list|)
operator|.
name|startsWith
argument_list|(
literal|"{\"writeid\":1,\"bucketid\":536870912,\"rowid\":1}\t3\t3\t3\t"
argument_list|)
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertTrue
argument_list|(
name|rs
operator|.
name|get
argument_list|(
literal|1
argument_list|)
argument_list|,
name|rs
operator|.
name|get
argument_list|(
literal|1
argument_list|)
operator|.
name|endsWith
argument_list|(
literal|"nobuckets/delta_0000001_0000001_0000/bucket_00000"
argument_list|)
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertTrue
argument_list|(
name|rs
operator|.
name|get
argument_list|(
literal|2
argument_list|)
argument_list|,
name|rs
operator|.
name|get
argument_list|(
literal|2
argument_list|)
operator|.
name|startsWith
argument_list|(
literal|"{\"writeid\":1,\"bucketid\":536936448,\"rowid\":0}\t1\t1\t1\t"
argument_list|)
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertTrue
argument_list|(
name|rs
operator|.
name|get
argument_list|(
literal|2
argument_list|)
argument_list|,
name|rs
operator|.
name|get
argument_list|(
literal|2
argument_list|)
operator|.
name|endsWith
argument_list|(
literal|"nobuckets/delta_0000001_0000001_0000/bucket_00001"
argument_list|)
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertTrue
argument_list|(
name|rs
operator|.
name|get
argument_list|(
literal|3
argument_list|)
argument_list|,
name|rs
operator|.
name|get
argument_list|(
literal|3
argument_list|)
operator|.
name|startsWith
argument_list|(
literal|"{\"writeid\":1,\"bucketid\":536936448,\"rowid\":1}\t2\t2\t2\t"
argument_list|)
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertTrue
argument_list|(
name|rs
operator|.
name|get
argument_list|(
literal|3
argument_list|)
argument_list|,
name|rs
operator|.
name|get
argument_list|(
literal|3
argument_list|)
operator|.
name|endsWith
argument_list|(
literal|"nobuckets/delta_0000001_0000001_0000/bucket_00001"
argument_list|)
argument_list|)
expr_stmt|;
name|runStatementOnDriver
argument_list|(
literal|"update nobuckets set c3 = 17 where c3 in(0,1)"
argument_list|)
expr_stmt|;
name|rs
operator|=
name|runStatementOnDriver
argument_list|(
literal|"select ROW__ID, c1, c2, c3, INPUT__FILE__NAME from nobuckets order by INPUT__FILE__NAME, ROW__ID"
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|warn
argument_list|(
literal|"after update"
argument_list|)
expr_stmt|;
for|for
control|(
name|String
name|s
range|:
name|rs
control|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
name|s
argument_list|)
expr_stmt|;
block|}
name|Assert
operator|.
name|assertTrue
argument_list|(
name|rs
operator|.
name|get
argument_list|(
literal|0
argument_list|)
argument_list|,
name|rs
operator|.
name|get
argument_list|(
literal|0
argument_list|)
operator|.
name|startsWith
argument_list|(
literal|"{\"writeid\":1,\"bucketid\":536870912,\"rowid\":1}\t3\t3\t3\t"
argument_list|)
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertTrue
argument_list|(
name|rs
operator|.
name|get
argument_list|(
literal|0
argument_list|)
argument_list|,
name|rs
operator|.
name|get
argument_list|(
literal|0
argument_list|)
operator|.
name|endsWith
argument_list|(
literal|"nobuckets/delta_0000001_0000001_0000/bucket_00000"
argument_list|)
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertTrue
argument_list|(
name|rs
operator|.
name|get
argument_list|(
literal|1
argument_list|)
argument_list|,
name|rs
operator|.
name|get
argument_list|(
literal|1
argument_list|)
operator|.
name|startsWith
argument_list|(
literal|"{\"writeid\":1,\"bucketid\":536936448,\"rowid\":1}\t2\t2\t2\t"
argument_list|)
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertTrue
argument_list|(
name|rs
operator|.
name|get
argument_list|(
literal|1
argument_list|)
argument_list|,
name|rs
operator|.
name|get
argument_list|(
literal|1
argument_list|)
operator|.
name|endsWith
argument_list|(
literal|"nobuckets/delta_0000001_0000001_0000/bucket_00001"
argument_list|)
argument_list|)
expr_stmt|;
comment|//so update has 1 writer which creates bucket0 where both new rows land
name|Assert
operator|.
name|assertTrue
argument_list|(
name|rs
operator|.
name|get
argument_list|(
literal|2
argument_list|)
argument_list|,
name|rs
operator|.
name|get
argument_list|(
literal|2
argument_list|)
operator|.
name|startsWith
argument_list|(
literal|"{\"writeid\":2,\"bucketid\":536870912,\"rowid\":0}\t0\t0\t17\t"
argument_list|)
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertTrue
argument_list|(
name|rs
operator|.
name|get
argument_list|(
literal|2
argument_list|)
argument_list|,
name|rs
operator|.
name|get
argument_list|(
literal|2
argument_list|)
operator|.
name|endsWith
argument_list|(
literal|"nobuckets/delta_0000002_0000002_0000/bucket_00000"
argument_list|)
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertTrue
argument_list|(
name|rs
operator|.
name|get
argument_list|(
literal|3
argument_list|)
argument_list|,
name|rs
operator|.
name|get
argument_list|(
literal|3
argument_list|)
operator|.
name|startsWith
argument_list|(
literal|"{\"writeid\":2,\"bucketid\":536870912,\"rowid\":1}\t1\t1\t17\t"
argument_list|)
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertTrue
argument_list|(
name|rs
operator|.
name|get
argument_list|(
literal|3
argument_list|)
argument_list|,
name|rs
operator|.
name|get
argument_list|(
literal|3
argument_list|)
operator|.
name|endsWith
argument_list|(
literal|"nobuckets/delta_0000002_0000002_0000/bucket_00000"
argument_list|)
argument_list|)
expr_stmt|;
name|Set
argument_list|<
name|String
argument_list|>
name|expectedFiles
init|=
operator|new
name|HashSet
argument_list|<>
argument_list|()
decl_stmt|;
comment|//both delete events land in a single bucket0.  Each has a different ROW__ID.bucketId value (even writerId in it is different)
name|expectedFiles
operator|.
name|add
argument_list|(
literal|"ts/delete_delta_0000002_0000002_0000/bucket_00000"
argument_list|)
expr_stmt|;
name|expectedFiles
operator|.
name|add
argument_list|(
literal|"nobuckets/delta_0000001_0000001_0000/bucket_00000"
argument_list|)
expr_stmt|;
name|expectedFiles
operator|.
name|add
argument_list|(
literal|"nobuckets/delta_0000001_0000001_0000/bucket_00001"
argument_list|)
expr_stmt|;
name|expectedFiles
operator|.
name|add
argument_list|(
literal|"nobuckets/delta_0000002_0000002_0000/bucket_00000"
argument_list|)
expr_stmt|;
comment|//check that we get the right files on disk
name|assertExpectedFileSet
argument_list|(
name|expectedFiles
argument_list|,
name|getWarehouseDir
argument_list|()
operator|+
literal|"/nobuckets"
argument_list|)
expr_stmt|;
comment|//todo: it would be nice to check the contents of the files... could use orc.FileDump - it has
comment|// methods to print to a supplied stream but those are package private
name|runStatementOnDriver
argument_list|(
literal|"alter table nobuckets compact 'major'"
argument_list|)
expr_stmt|;
name|TestTxnCommands2
operator|.
name|runWorker
argument_list|(
name|hiveConf
argument_list|)
expr_stmt|;
name|rs
operator|=
name|runStatementOnDriver
argument_list|(
literal|"select ROW__ID, c1, c2, c3, INPUT__FILE__NAME from nobuckets order by INPUT__FILE__NAME, ROW__ID"
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|warn
argument_list|(
literal|"after major compact"
argument_list|)
expr_stmt|;
for|for
control|(
name|String
name|s
range|:
name|rs
control|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
name|s
argument_list|)
expr_stmt|;
block|}
comment|/* ├── base_0000002 │   ├── bucket_00000 │   └── bucket_00001 ├── delete_delta_0000002_0000002_0000 │   └── bucket_00000 ├── delta_0000001_0000001_0000 │   ├── bucket_00000 │   └── bucket_00001 └── delta_0000002_0000002_0000     └── bucket_00000     */
name|Assert
operator|.
name|assertTrue
argument_list|(
name|rs
operator|.
name|get
argument_list|(
literal|0
argument_list|)
argument_list|,
name|rs
operator|.
name|get
argument_list|(
literal|0
argument_list|)
operator|.
name|startsWith
argument_list|(
literal|"{\"writeid\":1,\"bucketid\":536870912,\"rowid\":1}\t3\t3\t3\t"
argument_list|)
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertTrue
argument_list|(
name|rs
operator|.
name|get
argument_list|(
literal|0
argument_list|)
argument_list|,
name|rs
operator|.
name|get
argument_list|(
literal|0
argument_list|)
operator|.
name|endsWith
argument_list|(
literal|"nobuckets/base_0000002/bucket_00000"
argument_list|)
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertTrue
argument_list|(
name|rs
operator|.
name|get
argument_list|(
literal|1
argument_list|)
argument_list|,
name|rs
operator|.
name|get
argument_list|(
literal|1
argument_list|)
operator|.
name|startsWith
argument_list|(
literal|"{\"writeid\":2,\"bucketid\":536870912,\"rowid\":0}\t0\t0\t17\t"
argument_list|)
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertTrue
argument_list|(
name|rs
operator|.
name|get
argument_list|(
literal|1
argument_list|)
argument_list|,
name|rs
operator|.
name|get
argument_list|(
literal|1
argument_list|)
operator|.
name|endsWith
argument_list|(
literal|"nobuckets/base_0000002/bucket_00000"
argument_list|)
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertTrue
argument_list|(
name|rs
operator|.
name|get
argument_list|(
literal|2
argument_list|)
argument_list|,
name|rs
operator|.
name|get
argument_list|(
literal|2
argument_list|)
operator|.
name|startsWith
argument_list|(
literal|"{\"writeid\":2,\"bucketid\":536870912,\"rowid\":1}\t1\t1\t17\t"
argument_list|)
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertTrue
argument_list|(
name|rs
operator|.
name|get
argument_list|(
literal|2
argument_list|)
argument_list|,
name|rs
operator|.
name|get
argument_list|(
literal|2
argument_list|)
operator|.
name|endsWith
argument_list|(
literal|"nobuckets/base_0000002/bucket_00000"
argument_list|)
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertTrue
argument_list|(
name|rs
operator|.
name|get
argument_list|(
literal|3
argument_list|)
argument_list|,
name|rs
operator|.
name|get
argument_list|(
literal|3
argument_list|)
operator|.
name|startsWith
argument_list|(
literal|"{\"writeid\":1,\"bucketid\":536936448,\"rowid\":1}\t2\t2\t2\t"
argument_list|)
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertTrue
argument_list|(
name|rs
operator|.
name|get
argument_list|(
literal|3
argument_list|)
argument_list|,
name|rs
operator|.
name|get
argument_list|(
literal|3
argument_list|)
operator|.
name|endsWith
argument_list|(
literal|"nobuckets/base_0000002/bucket_00001"
argument_list|)
argument_list|)
expr_stmt|;
name|expectedFiles
operator|.
name|clear
argument_list|()
expr_stmt|;
name|expectedFiles
operator|.
name|add
argument_list|(
literal|"delete_delta_0000002_0000002_0000/bucket_00000"
argument_list|)
expr_stmt|;
name|expectedFiles
operator|.
name|add
argument_list|(
literal|"uckets/delta_0000001_0000001_0000/bucket_00000"
argument_list|)
expr_stmt|;
name|expectedFiles
operator|.
name|add
argument_list|(
literal|"uckets/delta_0000001_0000001_0000/bucket_00001"
argument_list|)
expr_stmt|;
name|expectedFiles
operator|.
name|add
argument_list|(
literal|"uckets/delta_0000002_0000002_0000/bucket_00000"
argument_list|)
expr_stmt|;
name|expectedFiles
operator|.
name|add
argument_list|(
literal|"/warehouse/nobuckets/base_0000002/bucket_00000"
argument_list|)
expr_stmt|;
name|expectedFiles
operator|.
name|add
argument_list|(
literal|"/warehouse/nobuckets/base_0000002/bucket_00001"
argument_list|)
expr_stmt|;
name|assertExpectedFileSet
argument_list|(
name|expectedFiles
argument_list|,
name|getWarehouseDir
argument_list|()
operator|+
literal|"/nobuckets"
argument_list|)
expr_stmt|;
name|TestTxnCommands2
operator|.
name|runCleaner
argument_list|(
name|hiveConf
argument_list|)
expr_stmt|;
name|rs
operator|=
name|runStatementOnDriver
argument_list|(
literal|"select c1, c2, c3 from nobuckets order by c1, c2, c3"
argument_list|)
expr_stmt|;
name|int
index|[]
index|[]
name|result
init|=
block|{
block|{
literal|0
block|,
literal|0
block|,
literal|17
block|}
block|,
block|{
literal|1
block|,
literal|1
block|,
literal|17
block|}
block|,
block|{
literal|2
block|,
literal|2
block|,
literal|2
block|}
block|,
block|{
literal|3
block|,
literal|3
block|,
literal|3
block|}
block|}
decl_stmt|;
name|Assert
operator|.
name|assertEquals
argument_list|(
literal|"Unexpected result after clean"
argument_list|,
name|stringifyValues
argument_list|(
name|result
argument_list|)
argument_list|,
name|rs
argument_list|)
expr_stmt|;
name|expectedFiles
operator|.
name|clear
argument_list|()
expr_stmt|;
name|expectedFiles
operator|.
name|add
argument_list|(
literal|"nobuckets/base_0000002/bucket_00000"
argument_list|)
expr_stmt|;
name|expectedFiles
operator|.
name|add
argument_list|(
literal|"nobuckets/base_0000002/bucket_00001"
argument_list|)
expr_stmt|;
name|assertExpectedFileSet
argument_list|(
name|expectedFiles
argument_list|,
name|getWarehouseDir
argument_list|()
operator|+
literal|"/nobuckets"
argument_list|)
expr_stmt|;
block|}
comment|/**    * See CTAS tests in TestAcidOnTez    */
annotation|@
name|Test
specifier|public
name|void
name|testCTAS
parameter_list|()
throws|throws
name|Exception
block|{
name|runStatementOnDriver
argument_list|(
literal|"drop table if exists myctas"
argument_list|)
expr_stmt|;
name|int
index|[]
index|[]
name|values
init|=
block|{
block|{
literal|1
block|,
literal|2
block|}
block|,
block|{
literal|3
block|,
literal|4
block|}
block|}
decl_stmt|;
name|runStatementOnDriver
argument_list|(
literal|"insert into "
operator|+
name|Table
operator|.
name|NONACIDORCTBL
operator|+
name|makeValuesClause
argument_list|(
name|values
argument_list|)
argument_list|)
expr_stmt|;
name|runStatementOnDriver
argument_list|(
literal|"create table myctas stored as ORC TBLPROPERTIES ('transactional"
operator|+
literal|"'='true', 'transactional_properties'='default') as select a, b from "
operator|+
name|Table
operator|.
name|NONACIDORCTBL
argument_list|)
expr_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|rs
init|=
name|runStatementOnDriver
argument_list|(
literal|"select ROW__ID, a, b, INPUT__FILE__NAME from myctas order by ROW__ID"
argument_list|)
decl_stmt|;
name|String
name|expected
index|[]
index|[]
init|=
block|{
block|{
literal|"{\"writeid\":1,\"bucketid\":536870912,\"rowid\":0}\t3\t4"
block|,
literal|"warehouse/myctas/delta_0000001_0000001_0000/bucket_00000"
block|}
block|,
block|{
literal|"{\"writeid\":1,\"bucketid\":536936448,\"rowid\":0}\t1\t2"
block|,
literal|"warehouse/myctas/delta_0000001_0000001_0000/bucket_00001"
block|}
block|,     }
decl_stmt|;
name|checkExpected
argument_list|(
name|rs
argument_list|,
name|expected
argument_list|,
literal|"Unexpected row count after ctas from non acid table"
argument_list|)
expr_stmt|;
name|runStatementOnDriver
argument_list|(
literal|"insert into "
operator|+
name|Table
operator|.
name|ACIDTBL
operator|+
name|makeValuesClause
argument_list|(
name|values
argument_list|)
argument_list|)
expr_stmt|;
comment|//todo: try this with acid default - it seem making table acid in listener is too late
name|runStatementOnDriver
argument_list|(
literal|"create table myctas2 stored as ORC TBLPROPERTIES ('transactional"
operator|+
literal|"'='true', 'transactional_properties'='default') as select a, b from "
operator|+
name|Table
operator|.
name|ACIDTBL
argument_list|)
expr_stmt|;
name|rs
operator|=
name|runStatementOnDriver
argument_list|(
literal|"select ROW__ID, a, b, INPUT__FILE__NAME from myctas2 order by ROW__ID"
argument_list|)
expr_stmt|;
name|String
name|expected2
index|[]
index|[]
init|=
block|{
block|{
literal|"{\"writeid\":1,\"bucketid\":536870912,\"rowid\":0}\t1\t2"
block|,
literal|"warehouse/myctas2/delta_0000001_0000001_0000/bucket_00000"
block|}
block|,
block|{
literal|"{\"writeid\":1,\"bucketid\":536936448,\"rowid\":0}\t3\t4"
block|,
literal|"warehouse/myctas2/delta_0000001_0000001_0000/bucket_00001"
block|}
block|}
decl_stmt|;
name|checkExpected
argument_list|(
name|rs
argument_list|,
name|expected2
argument_list|,
literal|"Unexpected row count after ctas from acid table"
argument_list|)
expr_stmt|;
name|runStatementOnDriver
argument_list|(
literal|"create table myctas3 stored as ORC TBLPROPERTIES ('transactional"
operator|+
literal|"'='true', 'transactional_properties'='default') as select a, b from "
operator|+
name|Table
operator|.
name|NONACIDORCTBL
operator|+
literal|" union all select a, b from "
operator|+
name|Table
operator|.
name|ACIDTBL
argument_list|)
expr_stmt|;
name|rs
operator|=
name|runStatementOnDriver
argument_list|(
literal|"select ROW__ID, a, b, INPUT__FILE__NAME from myctas3 order by ROW__ID"
argument_list|)
expr_stmt|;
name|String
name|expected3
index|[]
index|[]
init|=
block|{
block|{
literal|"{\"writeid\":1,\"bucketid\":536870912,\"rowid\":0}\t1\t2"
block|,
literal|"warehouse/myctas3/delta_0000001_0000001_0000/bucket_00000"
block|}
block|,
block|{
literal|"{\"writeid\":1,\"bucketid\":536936448,\"rowid\":0}\t3\t4"
block|,
literal|"warehouse/myctas3/delta_0000001_0000001_0000/bucket_00001"
block|}
block|,
block|{
literal|"{\"writeid\":1,\"bucketid\":537001984,\"rowid\":0}\t3\t4"
block|,
literal|"warehouse/myctas3/delta_0000001_0000001_0000/bucket_00002"
block|}
block|,
block|{
literal|"{\"writeid\":1,\"bucketid\":537067520,\"rowid\":0}\t1\t2"
block|,
literal|"warehouse/myctas3/delta_0000001_0000001_0000/bucket_00003"
block|}
block|,     }
decl_stmt|;
name|checkExpected
argument_list|(
name|rs
argument_list|,
name|expected3
argument_list|,
literal|"Unexpected row count after ctas from union all query"
argument_list|)
expr_stmt|;
name|runStatementOnDriver
argument_list|(
literal|"create table myctas4 stored as ORC TBLPROPERTIES ('transactional"
operator|+
literal|"'='true', 'transactional_properties'='default') as select a, b from "
operator|+
name|Table
operator|.
name|NONACIDORCTBL
operator|+
literal|" union distinct select a, b from "
operator|+
name|Table
operator|.
name|ACIDTBL
argument_list|)
expr_stmt|;
name|rs
operator|=
name|runStatementOnDriver
argument_list|(
literal|"select ROW__ID, a, b, INPUT__FILE__NAME from myctas4 order by ROW__ID"
argument_list|)
expr_stmt|;
name|String
name|expected4
index|[]
index|[]
init|=
block|{
block|{
literal|"{\"writeid\":1,\"bucketid\":536870912,\"rowid\":0}\t1\t2"
block|,
literal|"/delta_0000001_0000001_0000/bucket_00000"
block|}
block|,
block|{
literal|"{\"writeid\":1,\"bucketid\":536870912,\"rowid\":1}\t3\t4"
block|,
literal|"/delta_0000001_0000001_0000/bucket_00000"
block|}
block|,     }
decl_stmt|;
name|checkExpected
argument_list|(
name|rs
argument_list|,
name|expected4
argument_list|,
literal|"Unexpected row count after ctas from union distinct query"
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Test
specifier|public
name|void
name|testCtasEmpty
parameter_list|()
throws|throws
name|Exception
block|{
name|MetastoreConf
operator|.
name|setBoolVar
argument_list|(
name|hiveConf
argument_list|,
name|MetastoreConf
operator|.
name|ConfVars
operator|.
name|CREATE_TABLES_AS_ACID
argument_list|,
literal|true
argument_list|)
expr_stmt|;
name|runStatementOnDriver
argument_list|(
literal|"drop table if exists myctas"
argument_list|)
expr_stmt|;
name|runStatementOnDriver
argument_list|(
literal|"create table myctas stored as ORC as"
operator|+
literal|" select a, b from "
operator|+
name|Table
operator|.
name|NONACIDORCTBL
argument_list|)
expr_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|rs
init|=
name|runStatementOnDriver
argument_list|(
literal|"select ROW__ID, a, b, INPUT__FILE__NAME"
operator|+
literal|" from myctas order by ROW__ID"
argument_list|)
decl_stmt|;
block|}
comment|/**    * Insert into unbucketed acid table from union all query    * Union All is flattened so nested subdirs are created and acid move drops them since    * delta dirs have unique names    */
annotation|@
name|Test
specifier|public
name|void
name|testInsertToAcidWithUnionRemove
parameter_list|()
throws|throws
name|Exception
block|{
name|hiveConf
operator|.
name|setBoolVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_OPTIMIZE_UNION_REMOVE
argument_list|,
literal|true
argument_list|)
expr_stmt|;
name|hiveConf
operator|.
name|setVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEFETCHTASKCONVERSION
argument_list|,
literal|"none"
argument_list|)
expr_stmt|;
name|d
operator|.
name|close
argument_list|()
expr_stmt|;
name|d
operator|=
operator|new
name|Driver
argument_list|(
name|hiveConf
argument_list|)
expr_stmt|;
name|int
index|[]
index|[]
name|values
init|=
block|{
block|{
literal|1
block|,
literal|2
block|}
block|,
block|{
literal|3
block|,
literal|4
block|}
block|,
block|{
literal|5
block|,
literal|6
block|}
block|,
block|{
literal|7
block|,
literal|8
block|}
block|,
block|{
literal|9
block|,
literal|10
block|}
block|}
decl_stmt|;
name|runStatementOnDriver
argument_list|(
literal|"insert into "
operator|+
name|TxnCommandsBaseForTests
operator|.
name|Table
operator|.
name|ACIDTBL
operator|+
name|makeValuesClause
argument_list|(
name|values
argument_list|)
argument_list|)
expr_stmt|;
comment|//HIVE-17138: this creates 1 delta_0000013_0000013_0000/bucket_00001
name|runStatementOnDriver
argument_list|(
literal|"drop table if exists T"
argument_list|)
expr_stmt|;
name|runStatementOnDriver
argument_list|(
literal|"create table T (a int, b int) stored as ORC  TBLPROPERTIES ('transactional'='true')"
argument_list|)
expr_stmt|;
comment|/*     So Union All removal kicks in and we get 3 subdirs in staging. ekoifman:apache-hive-3.0.0-SNAPSHOT-bin ekoifman$ tree /Users/ekoifman/dev/hiverwgit/ql/target/tmp/org.apache.hadoop.hive.ql.TestTxnNoBuckets-1505516390532/warehouse/t/.hive-staging_hive_2017-09-15_16-05-06_895_1123322677843388168-1/ └── -ext-10000     ├── HIVE_UNION_SUBDIR_19     │   └── 000000_0     │       ├── _orc_acid_version     │       └── delta_0000016_0000016_0001     ├── HIVE_UNION_SUBDIR_20     │   └── 000000_0     │       ├── _orc_acid_version     │       └── delta_0000016_0000016_0002     └── HIVE_UNION_SUBDIR_21         └── 000000_0             ├── _orc_acid_version             └── delta_0000016_0000016_0003*/
name|runStatementOnDriver
argument_list|(
literal|"insert into T(a,b) select a, b from "
operator|+
name|TxnCommandsBaseForTests
operator|.
name|Table
operator|.
name|ACIDTBL
operator|+
literal|" where a between 1 and 3 group by a, b union all select a, b from "
operator|+
name|TxnCommandsBaseForTests
operator|.
name|Table
operator|.
name|ACIDTBL
operator|+
literal|" where a between 5 and 7 union all select a, b from "
operator|+
name|TxnCommandsBaseForTests
operator|.
name|Table
operator|.
name|ACIDTBL
operator|+
literal|" where a>= 9"
argument_list|)
expr_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|rs
init|=
name|runStatementOnDriver
argument_list|(
literal|"select ROW__ID, a, b, INPUT__FILE__NAME from T order by ROW__ID"
argument_list|)
decl_stmt|;
name|String
name|expected
index|[]
index|[]
init|=
block|{
block|{
literal|"{\"writeid\":1,\"bucketid\":536870913,\"rowid\":0}\t1\t2"
block|,
literal|"/delta_0000001_0000001_0001/bucket_00000"
block|}
block|,
block|{
literal|"{\"writeid\":1,\"bucketid\":536870913,\"rowid\":1}\t3\t4"
block|,
literal|"/delta_0000001_0000001_0001/bucket_00000"
block|}
block|,
block|{
literal|"{\"writeid\":1,\"bucketid\":536870914,\"rowid\":0}\t5\t6"
block|,
literal|"/delta_0000001_0000001_0002/bucket_00000"
block|}
block|,
block|{
literal|"{\"writeid\":1,\"bucketid\":536870915,\"rowid\":0}\t9\t10"
block|,
literal|"/delta_0000001_0000001_0003/bucket_00000"
block|}
block|,
block|{
literal|"{\"writeid\":1,\"bucketid\":536936450,\"rowid\":0}\t7\t8"
block|,
literal|"/delta_0000001_0000001_0002/bucket_00001"
block|}
block|,     }
decl_stmt|;
name|checkExpected
argument_list|(
name|rs
argument_list|,
name|expected
argument_list|,
literal|"Unexpected row count after ctas"
argument_list|)
expr_stmt|;
block|}
comment|/**    * The idea here is to create a non acid table that was written by multiple writers, i.e.    * unbucketed table that has 000000_0& 000001_0, for example.    * Also, checks that we can handle a case when data files can be at multiple levels (subdirs)    * of the table.    */
annotation|@
name|Test
specifier|public
name|void
name|testToAcidConversionMultiBucket
parameter_list|()
throws|throws
name|Exception
block|{
comment|//need to disable these so that automatic merge doesn't merge the files
name|hiveConf
operator|.
name|setBoolVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEMERGEMAPFILES
argument_list|,
literal|false
argument_list|)
expr_stmt|;
name|hiveConf
operator|.
name|setBoolVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEMERGEMAPREDFILES
argument_list|,
literal|false
argument_list|)
expr_stmt|;
name|hiveConf
operator|.
name|setBoolVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEMERGETEZFILES
argument_list|,
literal|false
argument_list|)
expr_stmt|;
name|d
operator|.
name|close
argument_list|()
expr_stmt|;
name|d
operator|=
operator|new
name|Driver
argument_list|(
name|hiveConf
argument_list|)
expr_stmt|;
name|int
index|[]
index|[]
name|values
init|=
block|{
block|{
literal|1
block|,
literal|2
block|}
block|,
block|{
literal|2
block|,
literal|4
block|}
block|,
block|{
literal|5
block|,
literal|6
block|}
block|,
block|{
literal|6
block|,
literal|8
block|}
block|,
block|{
literal|9
block|,
literal|10
block|}
block|}
decl_stmt|;
name|runStatementOnDriver
argument_list|(
literal|"insert into "
operator|+
name|Table
operator|.
name|ACIDTBL
operator|+
name|makeValuesClause
argument_list|(
name|values
argument_list|)
argument_list|)
expr_stmt|;
name|runStatementOnDriver
argument_list|(
literal|"drop table if exists T"
argument_list|)
expr_stmt|;
name|runStatementOnDriver
argument_list|(
literal|"create table T (a int, b int) stored as ORC  TBLPROPERTIES ('transactional'='false')"
argument_list|)
expr_stmt|;
name|runStatementOnDriver
argument_list|(
literal|"insert into T(a,b) select a, b from "
operator|+
name|Table
operator|.
name|ACIDTBL
operator|+
literal|" where a between 1 and 3 group by a, b union all select a, b from "
operator|+
name|Table
operator|.
name|ACIDTBL
operator|+
literal|" where a between 5 and 7 union all select a, b from "
operator|+
name|Table
operator|.
name|ACIDTBL
operator|+
literal|" where a>= 9"
argument_list|)
expr_stmt|;
name|runStatementOnDriver
argument_list|(
literal|"insert into T values(12,12)"
argument_list|)
expr_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|rs
init|=
name|runStatementOnDriver
argument_list|(
literal|"select a, b, INPUT__FILE__NAME from T order by a, b, INPUT__FILE__NAME"
argument_list|)
decl_stmt|;
comment|//previous insert+union creates 3 data files (0-3)
comment|//insert (12,12) creates 000000_0_copy_1
name|String
name|expected
index|[]
index|[]
init|=
block|{
block|{
literal|"1\t2"
block|,
literal|"warehouse/t/000002_0"
block|}
block|,
block|{
literal|"2\t4"
block|,
literal|"warehouse/t/000002_0"
block|}
block|,
block|{
literal|"5\t6"
block|,
literal|"warehouse/t/000000_0"
block|}
block|,
block|{
literal|"6\t8"
block|,
literal|"warehouse/t/000001_0"
block|}
block|,
block|{
literal|"9\t10"
block|,
literal|"warehouse/t/000000_0"
block|}
block|,
block|{
literal|"12\t12"
block|,
literal|"warehouse/t/000000_0_copy_1"
block|}
block|}
decl_stmt|;
name|checkExpected
argument_list|(
name|rs
argument_list|,
name|expected
argument_list|,
literal|"before converting to acid"
argument_list|)
expr_stmt|;
comment|//now do Insert from Union here to create data files in sub dirs
name|hiveConf
operator|.
name|setBoolVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_OPTIMIZE_UNION_REMOVE
argument_list|,
literal|true
argument_list|)
expr_stmt|;
name|hiveConf
operator|.
name|setVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEFETCHTASKCONVERSION
argument_list|,
literal|"none"
argument_list|)
expr_stmt|;
name|d
operator|.
name|close
argument_list|()
expr_stmt|;
name|d
operator|=
operator|new
name|Driver
argument_list|(
name|hiveConf
argument_list|)
expr_stmt|;
name|runStatementOnDriver
argument_list|(
literal|"insert into T(a,b) select a * 10, b * 10 from "
operator|+
name|Table
operator|.
name|ACIDTBL
operator|+
literal|" where a between 1 and 3 group by a, b union all select a * 10, b * 10 from "
operator|+
name|Table
operator|.
name|ACIDTBL
operator|+
literal|" where a between 5 and 7"
argument_list|)
expr_stmt|;
comment|//now we have a table with data files at multiple different levels.
name|String
name|expected1
index|[]
index|[]
init|=
block|{
block|{
literal|"1\t2"
block|,
literal|"warehouse/t/000002_0"
block|}
block|,
block|{
literal|"2\t4"
block|,
literal|"warehouse/t/000002_0"
block|}
block|,
block|{
literal|"5\t6"
block|,
literal|"warehouse/t/000000_0"
block|}
block|,
block|{
literal|"6\t8"
block|,
literal|"warehouse/t/000001_0"
block|}
block|,
block|{
literal|"9\t10"
block|,
literal|"warehouse/t/000000_0"
block|}
block|,
block|{
literal|"10\t20"
block|,
literal|"warehouse/t/HIVE_UNION_SUBDIR_15/000000_0"
block|}
block|,
block|{
literal|"12\t12"
block|,
literal|"warehouse/t/000000_0_copy_1"
block|}
block|,
block|{
literal|"20\t40"
block|,
literal|"warehouse/t/HIVE_UNION_SUBDIR_15/000000_0"
block|}
block|,
block|{
literal|"50\t60"
block|,
literal|"warehouse/t/HIVE_UNION_SUBDIR_16/000000_0"
block|}
block|,
block|{
literal|"60\t80"
block|,
literal|"warehouse/t/HIVE_UNION_SUBDIR_16/000001_0"
block|}
block|}
decl_stmt|;
name|rs
operator|=
name|runStatementOnDriver
argument_list|(
literal|"select a, b, INPUT__FILE__NAME from T order by a, b, INPUT__FILE__NAME"
argument_list|)
expr_stmt|;
name|checkExpected
argument_list|(
name|rs
argument_list|,
name|expected1
argument_list|,
literal|"before converting to acid (with multi level data layout)"
argument_list|)
expr_stmt|;
comment|//make it an Acid table and make sure we assign ROW__IDs correctly
name|runStatementOnDriver
argument_list|(
literal|"alter table T SET TBLPROPERTIES ('transactional'='true')"
argument_list|)
expr_stmt|;
name|rs
operator|=
name|runStatementOnDriver
argument_list|(
literal|"select ROW__ID, a, b, INPUT__FILE__NAME from T order by a, b, INPUT__FILE__NAME"
argument_list|)
expr_stmt|;
comment|/**     now that T is Acid, data for each writerId is treated like a logical bucket (though T is not     bucketed), so rowid are assigned per logical bucket (e.g. 000000_0 + 000000_0_copy_1 + subdirs).      {@link AcidUtils.Directory#getOriginalFiles()} ensures consistent ordering of files within      logical bucket (tranche)      */
name|String
name|expected2
index|[]
index|[]
init|=
block|{
block|{
literal|"{\"writeid\":0,\"bucketid\":537001984,\"rowid\":0}\t1\t2"
block|,
literal|"warehouse/t/000002_0"
block|}
block|,
block|{
literal|"{\"writeid\":0,\"bucketid\":537001984,\"rowid\":1}\t2\t4"
block|,
literal|"warehouse/t/000002_0"
block|}
block|,
block|{
literal|"{\"writeid\":0,\"bucketid\":536870912,\"rowid\":1}\t5\t6"
block|,
literal|"warehouse/t/000000_0"
block|}
block|,
block|{
literal|"{\"writeid\":0,\"bucketid\":536936448,\"rowid\":0}\t6\t8"
block|,
literal|"warehouse/t/000001_0"
block|}
block|,
block|{
literal|"{\"writeid\":0,\"bucketid\":536870912,\"rowid\":0}\t9\t10"
block|,
literal|"warehouse/t/000000_0"
block|}
block|,
block|{
literal|"{\"writeid\":0,\"bucketid\":536870912,\"rowid\":3}\t10\t20"
block|,
literal|"warehouse/t/HIVE_UNION_SUBDIR_15/000000_0"
block|}
block|,
block|{
literal|"{\"writeid\":0,\"bucketid\":536870912,\"rowid\":2}\t12\t12"
block|,
literal|"warehouse/t/000000_0_copy_1"
block|}
block|,
block|{
literal|"{\"writeid\":0,\"bucketid\":536870912,\"rowid\":4}\t20\t40"
block|,
literal|"warehouse/t/HIVE_UNION_SUBDIR_15/000000_0"
block|}
block|,
block|{
literal|"{\"writeid\":0,\"bucketid\":536870912,\"rowid\":5}\t50\t60"
block|,
literal|"warehouse/t/HIVE_UNION_SUBDIR_16/000000_0"
block|}
block|,
block|{
literal|"{\"writeid\":0,\"bucketid\":536936448,\"rowid\":1}\t60\t80"
block|,
literal|"warehouse/t/HIVE_UNION_SUBDIR_16/000001_0"
block|}
block|,     }
decl_stmt|;
name|checkExpected
argument_list|(
name|rs
argument_list|,
name|expected2
argument_list|,
literal|"after converting to acid (no compaction)"
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertEquals
argument_list|(
literal|0
argument_list|,
name|BucketCodec
operator|.
name|determineVersion
argument_list|(
literal|536870912
argument_list|)
operator|.
name|decodeWriterId
argument_list|(
literal|536870912
argument_list|)
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertEquals
argument_list|(
literal|2
argument_list|,
name|BucketCodec
operator|.
name|determineVersion
argument_list|(
literal|537001984
argument_list|)
operator|.
name|decodeWriterId
argument_list|(
literal|537001984
argument_list|)
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertEquals
argument_list|(
literal|1
argument_list|,
name|BucketCodec
operator|.
name|determineVersion
argument_list|(
literal|536936448
argument_list|)
operator|.
name|decodeWriterId
argument_list|(
literal|536936448
argument_list|)
argument_list|)
expr_stmt|;
name|assertVectorized
argument_list|(
name|shouldVectorize
argument_list|()
argument_list|,
literal|"update T set b = 88 where b = 80"
argument_list|)
expr_stmt|;
name|runStatementOnDriver
argument_list|(
literal|"update T set b = 88 where b = 80"
argument_list|)
expr_stmt|;
name|assertVectorized
argument_list|(
name|shouldVectorize
argument_list|()
argument_list|,
literal|"delete from T where b = 8"
argument_list|)
expr_stmt|;
name|runStatementOnDriver
argument_list|(
literal|"delete from T where b = 8"
argument_list|)
expr_stmt|;
name|String
name|expected3
index|[]
index|[]
init|=
block|{
block|{
literal|"{\"writeid\":0,\"bucketid\":537001984,\"rowid\":0}\t1\t2"
block|,
literal|"warehouse/t/000002_0"
block|}
block|,
block|{
literal|"{\"writeid\":0,\"bucketid\":537001984,\"rowid\":1}\t2\t4"
block|,
literal|"warehouse/t/000002_0"
block|}
block|,
block|{
literal|"{\"writeid\":0,\"bucketid\":536870912,\"rowid\":1}\t5\t6"
block|,
literal|"warehouse/t/000000_0"
block|}
block|,
block|{
literal|"{\"writeid\":0,\"bucketid\":536870912,\"rowid\":0}\t9\t10"
block|,
literal|"warehouse/t/000000_0"
block|}
block|,
block|{
literal|"{\"writeid\":0,\"bucketid\":536870912,\"rowid\":3}\t10\t20"
block|,
literal|"warehouse/t/HIVE_UNION_SUBDIR_15/000000_0"
block|}
block|,
block|{
literal|"{\"writeid\":0,\"bucketid\":536870912,\"rowid\":2}\t12\t12"
block|,
literal|"warehouse/t/000000_0_copy_1"
block|}
block|,
block|{
literal|"{\"writeid\":0,\"bucketid\":536870912,\"rowid\":4}\t20\t40"
block|,
literal|"warehouse/t/HIVE_UNION_SUBDIR_15/000000_0"
block|}
block|,
block|{
literal|"{\"writeid\":0,\"bucketid\":536870912,\"rowid\":5}\t50\t60"
block|,
literal|"warehouse/t/HIVE_UNION_SUBDIR_16/000000_0"
block|}
block|,
block|{
literal|"{\"writeid\":1,\"bucketid\":536870912,\"rowid\":0}\t60\t88"
block|,
literal|"warehouse/t/delta_0000001_0000001_0000/bucket_00000"
block|}
block|,     }
decl_stmt|;
name|rs
operator|=
name|runStatementOnDriver
argument_list|(
literal|"select ROW__ID, a, b, INPUT__FILE__NAME from T order by a, b, INPUT__FILE__NAME"
argument_list|)
expr_stmt|;
name|checkExpected
argument_list|(
name|rs
argument_list|,
name|expected3
argument_list|,
literal|"after converting to acid (no compaction with updates)"
argument_list|)
expr_stmt|;
comment|//major compaction + check data + files
name|runStatementOnDriver
argument_list|(
literal|"alter table T compact 'major'"
argument_list|)
expr_stmt|;
name|TestTxnCommands2
operator|.
name|runWorker
argument_list|(
name|hiveConf
argument_list|)
expr_stmt|;
name|rs
operator|=
name|runStatementOnDriver
argument_list|(
literal|"select ROW__ID, a, b, INPUT__FILE__NAME from T order by a, b, INPUT__FILE__NAME"
argument_list|)
expr_stmt|;
comment|/*Compaction preserves location of rows wrt buckets/tranches (for now)*/
name|String
name|expected4
index|[]
index|[]
init|=
block|{
block|{
literal|"{\"writeid\":0,\"bucketid\":537001984,\"rowid\":0}\t1\t2"
block|,
literal|"warehouse/t/base_0000002/bucket_00002"
block|}
block|,
block|{
literal|"{\"writeid\":0,\"bucketid\":537001984,\"rowid\":1}\t2\t4"
block|,
literal|"warehouse/t/base_0000002/bucket_00002"
block|}
block|,
block|{
literal|"{\"writeid\":0,\"bucketid\":536870912,\"rowid\":1}\t5\t6"
block|,
literal|"warehouse/t/base_0000002/bucket_00000"
block|}
block|,
block|{
literal|"{\"writeid\":0,\"bucketid\":536870912,\"rowid\":0}\t9\t10"
block|,
literal|"warehouse/t/base_0000002/bucket_00000"
block|}
block|,
block|{
literal|"{\"writeid\":0,\"bucketid\":536870912,\"rowid\":3}\t10\t20"
block|,
literal|"warehouse/t/base_0000002/bucket_00000"
block|}
block|,
block|{
literal|"{\"writeid\":0,\"bucketid\":536870912,\"rowid\":2}\t12\t12"
block|,
literal|"warehouse/t/base_0000002/bucket_00000"
block|}
block|,
block|{
literal|"{\"writeid\":0,\"bucketid\":536870912,\"rowid\":4}\t20\t40"
block|,
literal|"warehouse/t/base_0000002/bucket_00000"
block|}
block|,
block|{
literal|"{\"writeid\":0,\"bucketid\":536870912,\"rowid\":5}\t50\t60"
block|,
literal|"warehouse/t/base_0000002/bucket_00000"
block|}
block|,
block|{
literal|"{\"writeid\":1,\"bucketid\":536870912,\"rowid\":0}\t60\t88"
block|,
literal|"warehouse/t/base_0000002/bucket_00000"
block|}
block|,     }
decl_stmt|;
name|checkExpected
argument_list|(
name|rs
argument_list|,
name|expected4
argument_list|,
literal|"after major compact"
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Test
specifier|public
name|void
name|testInsertFromUnion
parameter_list|()
throws|throws
name|Exception
block|{
name|int
index|[]
index|[]
name|values
init|=
block|{
block|{
literal|1
block|,
literal|2
block|}
block|,
block|{
literal|2
block|,
literal|4
block|}
block|,
block|{
literal|5
block|,
literal|6
block|}
block|,
block|{
literal|6
block|,
literal|8
block|}
block|,
block|{
literal|9
block|,
literal|10
block|}
block|}
decl_stmt|;
name|runStatementOnDriver
argument_list|(
literal|"insert into "
operator|+
name|Table
operator|.
name|NONACIDNONBUCKET
operator|+
name|makeValuesClause
argument_list|(
name|values
argument_list|)
argument_list|)
expr_stmt|;
name|runStatementOnDriver
argument_list|(
literal|"drop table if exists T"
argument_list|)
expr_stmt|;
name|runStatementOnDriver
argument_list|(
literal|"create table T (a int, b int) stored as ORC  TBLPROPERTIES ('transactional'='true')"
argument_list|)
expr_stmt|;
name|runStatementOnDriver
argument_list|(
literal|"insert into T(a,b) select a, b from "
operator|+
name|Table
operator|.
name|NONACIDNONBUCKET
operator|+
literal|" where a between 1 and 3 group by a, b union all select a, b from "
operator|+
name|Table
operator|.
name|NONACIDNONBUCKET
operator|+
literal|" where a between 5 and 7 union all select a, b from "
operator|+
name|Table
operator|.
name|NONACIDNONBUCKET
operator|+
literal|" where a>= 9"
argument_list|)
expr_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|rs
init|=
name|runStatementOnDriver
argument_list|(
literal|"select ROW__ID, a, b, INPUT__FILE__NAME from T order by a, b, INPUT__FILE__NAME"
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|warn
argument_list|(
literal|"before converting to acid"
argument_list|)
expr_stmt|;
for|for
control|(
name|String
name|s
range|:
name|rs
control|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
name|s
argument_list|)
expr_stmt|;
block|}
comment|/*     The number of writers seems to be based on number of MR jobs for the src query.  todo check number of FileSinks     warehouse/t/.hive-staging_hive_2017-09-13_08-59-28_141_6304543600372946004-1/-ext-10000/000000_0/delta_0000001_0000001_0000/bucket_00000 [length: 648]     {"operation":0,"originalTransaction":1,"bucket":536870912,"rowId":0,"currentTransaction":1,"row":{"_col0":1,"_col1":2}}     {"operation":0,"originalTransaction":1,"bucket":536870912,"rowId":1,"currentTransaction":1,"row":{"_col0":2,"_col1":4}}     ________________________________________________________________________________________________________________________     warehouse/t/.hive-staging_hive_2017-09-13_08-59-28_141_6304543600372946004-1/-ext-10000/000001_0/delta_0000001_0000001_0000/bucket_00001 [length: 658]     {"operation":0,"originalTransaction":1,"bucket":536936448,"rowId":0,"currentTransaction":1,"row":{"_col0":5,"_col1":6}}     {"operation":0,"originalTransaction":1,"bucket":536936448,"rowId":1,"currentTransaction":1,"row":{"_col0":6,"_col1":8}}     {"operation":0,"originalTransaction":1,"bucket":536936448,"rowId":2,"currentTransaction":1,"row":{"_col0":9,"_col1":10}}     */
name|rs
operator|=
name|runStatementOnDriver
argument_list|(
literal|"select a, b from T order by a, b"
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertEquals
argument_list|(
name|stringifyValues
argument_list|(
name|values
argument_list|)
argument_list|,
name|rs
argument_list|)
expr_stmt|;
name|rs
operator|=
name|runStatementOnDriver
argument_list|(
literal|"select ROW__ID from T group by ROW__ID having count(*)> 1"
argument_list|)
expr_stmt|;
if|if
condition|(
name|rs
operator|.
name|size
argument_list|()
operator|>
literal|0
condition|)
block|{
name|Assert
operator|.
name|assertEquals
argument_list|(
literal|"Duplicate ROW__IDs: "
operator|+
name|rs
operator|.
name|get
argument_list|(
literal|0
argument_list|)
argument_list|,
literal|0
argument_list|,
name|rs
operator|.
name|size
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**    * see HIVE-16177    * See also {@link TestTxnCommands2#testNonAcidToAcidConversion02()}    */
annotation|@
name|Test
specifier|public
name|void
name|testToAcidConversion02
parameter_list|()
throws|throws
name|Exception
block|{
comment|//create 2 rows in a file 00000_0
name|runStatementOnDriver
argument_list|(
literal|"insert into "
operator|+
name|Table
operator|.
name|NONACIDNONBUCKET
operator|+
literal|"(a,b) values(1,2),(1,3)"
argument_list|)
expr_stmt|;
comment|//create 4 rows in a file 000000_0_copy_1
name|runStatementOnDriver
argument_list|(
literal|"insert into "
operator|+
name|Table
operator|.
name|NONACIDNONBUCKET
operator|+
literal|"(a,b) values(0,12),(0,13),(1,4),(1,5)"
argument_list|)
expr_stmt|;
comment|//create 1 row in a file 000000_0_copy_2
name|runStatementOnDriver
argument_list|(
literal|"insert into "
operator|+
name|Table
operator|.
name|NONACIDNONBUCKET
operator|+
literal|"(a,b) values(1,6)"
argument_list|)
expr_stmt|;
comment|//convert the table to Acid  //todo: remove trans_prop after HIVE-17089
name|runStatementOnDriver
argument_list|(
literal|"alter table "
operator|+
name|Table
operator|.
name|NONACIDNONBUCKET
operator|+
literal|" SET TBLPROPERTIES ('transactional'='true', 'transactional_properties'='default')"
argument_list|)
expr_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|rs
init|=
name|runStatementOnDriver
argument_list|(
literal|"select ROW__ID, a, b, INPUT__FILE__NAME from "
operator|+
name|Table
operator|.
name|NONACIDNONBUCKET
operator|+
literal|" order by ROW__ID"
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|warn
argument_list|(
literal|"before acid ops (after convert)"
argument_list|)
expr_stmt|;
for|for
control|(
name|String
name|s
range|:
name|rs
control|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
name|s
argument_list|)
expr_stmt|;
block|}
comment|//create a some of delta directories
name|runStatementOnDriver
argument_list|(
literal|"insert into "
operator|+
name|Table
operator|.
name|NONACIDNONBUCKET
operator|+
literal|"(a,b) values(0,15),(1,16)"
argument_list|)
expr_stmt|;
name|runStatementOnDriver
argument_list|(
literal|"update "
operator|+
name|Table
operator|.
name|NONACIDNONBUCKET
operator|+
literal|" set b = 120 where a = 0 and b = 12"
argument_list|)
expr_stmt|;
name|runStatementOnDriver
argument_list|(
literal|"insert into "
operator|+
name|Table
operator|.
name|NONACIDNONBUCKET
operator|+
literal|"(a,b) values(0,17)"
argument_list|)
expr_stmt|;
name|runStatementOnDriver
argument_list|(
literal|"delete from "
operator|+
name|Table
operator|.
name|NONACIDNONBUCKET
operator|+
literal|" where a = 1 and b = 3"
argument_list|)
expr_stmt|;
name|rs
operator|=
name|runStatementOnDriver
argument_list|(
literal|"select ROW__ID, a, b, INPUT__FILE__NAME from "
operator|+
name|Table
operator|.
name|NONACIDNONBUCKET
operator|+
literal|" order by a,b"
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|warn
argument_list|(
literal|"before compact"
argument_list|)
expr_stmt|;
for|for
control|(
name|String
name|s
range|:
name|rs
control|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
name|s
argument_list|)
expr_stmt|;
block|}
name|Assert
operator|.
name|assertEquals
argument_list|(
literal|0
argument_list|,
name|BucketCodec
operator|.
name|determineVersion
argument_list|(
literal|536870912
argument_list|)
operator|.
name|decodeWriterId
argument_list|(
literal|536870912
argument_list|)
argument_list|)
expr_stmt|;
comment|/*      * All ROW__IDs are unique on read after conversion to acid      * ROW__IDs are exactly the same before and after compaction      * Also check the file name (only) after compaction for completeness      */
name|String
index|[]
index|[]
name|expected
init|=
block|{
block|{
literal|"{\"writeid\":0,\"bucketid\":536870912,\"rowid\":3}\t0\t13"
block|,
literal|"bucket_00000"
block|,
literal|"000000_0_copy_1"
block|}
block|,
block|{
literal|"{\"writeid\":1,\"bucketid\":536870912,\"rowid\":0}\t0\t15"
block|,
literal|"bucket_00000"
block|,
literal|"bucket_00000"
block|}
block|,
block|{
literal|"{\"writeid\":3,\"bucketid\":536870912,\"rowid\":0}\t0\t17"
block|,
literal|"bucket_00000"
block|,
literal|"bucket_00000"
block|}
block|,
block|{
literal|"{\"writeid\":2,\"bucketid\":536870912,\"rowid\":0}\t0\t120"
block|,
literal|"bucket_00000"
block|,
literal|"bucket_00000"
block|}
block|,
block|{
literal|"{\"writeid\":0,\"bucketid\":536870912,\"rowid\":0}\t1\t2"
block|,
literal|"bucket_00000"
block|,
literal|"000000_0"
block|}
block|,
block|{
literal|"{\"writeid\":0,\"bucketid\":536870912,\"rowid\":4}\t1\t4"
block|,
literal|"bucket_00000"
block|,
literal|"000000_0_copy_1"
block|}
block|,
block|{
literal|"{\"writeid\":0,\"bucketid\":536870912,\"rowid\":5}\t1\t5"
block|,
literal|"bucket_00000"
block|,
literal|"000000_0_copy_1"
block|}
block|,
block|{
literal|"{\"writeid\":0,\"bucketid\":536870912,\"rowid\":6}\t1\t6"
block|,
literal|"bucket_00000"
block|,
literal|"000000_0_copy_2"
block|}
block|,
block|{
literal|"{\"writeid\":1,\"bucketid\":536870912,\"rowid\":1}\t1\t16"
block|,
literal|"bucket_00000"
block|,
literal|"bucket_00000"
block|}
block|}
decl_stmt|;
name|Assert
operator|.
name|assertEquals
argument_list|(
literal|"Unexpected row count before compaction"
argument_list|,
name|expected
operator|.
name|length
argument_list|,
name|rs
operator|.
name|size
argument_list|()
argument_list|)
expr_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|expected
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|Assert
operator|.
name|assertTrue
argument_list|(
literal|"Actual line "
operator|+
name|i
operator|+
literal|" bc: "
operator|+
name|rs
operator|.
name|get
argument_list|(
name|i
argument_list|)
argument_list|,
name|rs
operator|.
name|get
argument_list|(
name|i
argument_list|)
operator|.
name|startsWith
argument_list|(
name|expected
index|[
name|i
index|]
index|[
literal|0
index|]
argument_list|)
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertTrue
argument_list|(
literal|"Actual line(file) "
operator|+
name|i
operator|+
literal|" bc: "
operator|+
name|rs
operator|.
name|get
argument_list|(
name|i
argument_list|)
argument_list|,
name|rs
operator|.
name|get
argument_list|(
name|i
argument_list|)
operator|.
name|endsWith
argument_list|(
name|expected
index|[
name|i
index|]
index|[
literal|2
index|]
argument_list|)
argument_list|)
expr_stmt|;
block|}
comment|//run Compaction
name|runStatementOnDriver
argument_list|(
literal|"alter table "
operator|+
name|Table
operator|.
name|NONACIDNONBUCKET
operator|+
literal|" compact 'major'"
argument_list|)
expr_stmt|;
name|TestTxnCommands2
operator|.
name|runWorker
argument_list|(
name|hiveConf
argument_list|)
expr_stmt|;
comment|/*     nonacidnonbucket/     ├── 000000_0     ├── 000000_0_copy_1     ├── 000000_0_copy_2     ├── base_0000004     │   └── bucket_00000     ├── delete_delta_0000002_0000002_0000     │   └── bucket_00000     ├── delete_delta_0000004_0000004_0000     │   └── bucket_00000     ├── delta_0000001_0000001_0000     │   └── bucket_00000     ├── delta_0000002_0000002_0000     │   └── bucket_00000     └── delta_0000003_0000003_0000         └── bucket_00000      6 directories, 9 files     */
name|rs
operator|=
name|runStatementOnDriver
argument_list|(
literal|"select ROW__ID, a, b, INPUT__FILE__NAME from "
operator|+
name|Table
operator|.
name|NONACIDNONBUCKET
operator|+
literal|" order by a,b"
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|warn
argument_list|(
literal|"after compact"
argument_list|)
expr_stmt|;
for|for
control|(
name|String
name|s
range|:
name|rs
control|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
name|s
argument_list|)
expr_stmt|;
block|}
name|Assert
operator|.
name|assertEquals
argument_list|(
literal|"Unexpected row count after compaction"
argument_list|,
name|expected
operator|.
name|length
argument_list|,
name|rs
operator|.
name|size
argument_list|()
argument_list|)
expr_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|expected
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|Assert
operator|.
name|assertTrue
argument_list|(
literal|"Actual line "
operator|+
name|i
operator|+
literal|" ac: "
operator|+
name|rs
operator|.
name|get
argument_list|(
name|i
argument_list|)
argument_list|,
name|rs
operator|.
name|get
argument_list|(
name|i
argument_list|)
operator|.
name|startsWith
argument_list|(
name|expected
index|[
name|i
index|]
index|[
literal|0
index|]
argument_list|)
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertTrue
argument_list|(
literal|"Actual line(file) "
operator|+
name|i
operator|+
literal|" ac: "
operator|+
name|rs
operator|.
name|get
argument_list|(
name|i
argument_list|)
argument_list|,
name|rs
operator|.
name|get
argument_list|(
name|i
argument_list|)
operator|.
name|endsWith
argument_list|(
name|expected
index|[
name|i
index|]
index|[
literal|1
index|]
argument_list|)
argument_list|)
expr_stmt|;
block|}
comment|//make sure they are the same before and after compaction
block|}
comment|/**    * Currently CTAS doesn't support bucketed tables.  Correspondingly Acid only supports CTAS for    * unbucketed tables.  This test is here to make sure that if CTAS is made to support unbucketed    * tables, that it raises a red flag for Acid.    */
annotation|@
name|Test
specifier|public
name|void
name|testCtasBucketed
parameter_list|()
throws|throws
name|Exception
block|{
name|runStatementOnDriver
argument_list|(
literal|"insert into "
operator|+
name|Table
operator|.
name|NONACIDNONBUCKET
operator|+
literal|"(a,b) values(1,2),(1,3)"
argument_list|)
expr_stmt|;
name|CommandProcessorResponse
name|cpr
init|=
name|runStatementOnDriverNegative
argument_list|(
literal|"create table myctas "
operator|+
literal|"clustered by (a) into 2 buckets stored as ORC TBLPROPERTIES ('transactional'='true') as "
operator|+
literal|"select a, b from "
operator|+
name|Table
operator|.
name|NONACIDORCTBL
argument_list|)
decl_stmt|;
name|int
name|j
init|=
name|ErrorMsg
operator|.
name|CTAS_PARCOL_COEXISTENCE
operator|.
name|getErrorCode
argument_list|()
decl_stmt|;
comment|//this code doesn't propagate
comment|//    Assert.assertEquals("Wrong msg", ErrorMsg.CTAS_PARCOL_COEXISTENCE.getErrorCode(), cpr.getErrorCode());
name|Assert
operator|.
name|assertTrue
argument_list|(
name|cpr
operator|.
name|getErrorMessage
argument_list|()
operator|.
name|contains
argument_list|(
literal|"CREATE-TABLE-AS-SELECT does not support"
argument_list|)
argument_list|)
expr_stmt|;
block|}
comment|/**    * Currently CTAS doesn't support partitioned tables.  Correspondingly Acid only supports CTAS for    * un-partitioned tables.  This test is here to make sure that if CTAS is made to support    * un-partitioned tables, that it raises a red flag for Acid.    */
annotation|@
name|Test
specifier|public
name|void
name|testCtasPartitioned
parameter_list|()
throws|throws
name|Exception
block|{
name|runStatementOnDriver
argument_list|(
literal|"insert into "
operator|+
name|Table
operator|.
name|NONACIDNONBUCKET
operator|+
literal|"(a,b) values(1,2),(1,3)"
argument_list|)
expr_stmt|;
name|CommandProcessorResponse
name|cpr
init|=
name|runStatementOnDriverNegative
argument_list|(
literal|"create table myctas partitioned "
operator|+
literal|"by (b int) stored as "
operator|+
literal|"ORC TBLPROPERTIES ('transactional'='true') as select a, b from "
operator|+
name|Table
operator|.
name|NONACIDORCTBL
argument_list|)
decl_stmt|;
name|int
name|j
init|=
name|ErrorMsg
operator|.
name|CTAS_PARCOL_COEXISTENCE
operator|.
name|getErrorCode
argument_list|()
decl_stmt|;
comment|//this code doesn't propagate
name|Assert
operator|.
name|assertTrue
argument_list|(
name|cpr
operator|.
name|getErrorMessage
argument_list|()
operator|.
name|contains
argument_list|(
literal|"CREATE-TABLE-AS-SELECT does not support "
operator|+
literal|"partitioning in the target table"
argument_list|)
argument_list|)
expr_stmt|;
block|}
comment|/**    * Tests to check that we are able to use vectorized acid reader,    * VectorizedOrcAcidRowBatchReader, when reading "original" files,    * i.e. those that were written before the table was converted to acid.    * See also acid_vectorization_original*.q    */
annotation|@
name|Test
specifier|public
name|void
name|testNonAcidToAcidVectorzied
parameter_list|()
throws|throws
name|Exception
block|{
name|hiveConf
operator|.
name|setBoolVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_VECTORIZATION_ENABLED
argument_list|,
literal|true
argument_list|)
expr_stmt|;
name|hiveConf
operator|.
name|setVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEFETCHTASKCONVERSION
argument_list|,
literal|"none"
argument_list|)
expr_stmt|;
comment|//this enables vectorization of ROW__ID
name|hiveConf
operator|.
name|setBoolVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_VECTORIZATION_ROW_IDENTIFIER_ENABLED
argument_list|,
literal|true
argument_list|)
expr_stmt|;
comment|//HIVE-12631
name|runStatementOnDriver
argument_list|(
literal|"drop table if exists T"
argument_list|)
expr_stmt|;
name|runStatementOnDriver
argument_list|(
literal|"create table T(a int, b int) stored as orc tblproperties('transactional'='false')"
argument_list|)
expr_stmt|;
name|int
index|[]
index|[]
name|values
init|=
block|{
block|{
literal|1
block|,
literal|2
block|}
block|,
block|{
literal|2
block|,
literal|4
block|}
block|,
block|{
literal|5
block|,
literal|6
block|}
block|,
block|{
literal|6
block|,
literal|8
block|}
block|,
block|{
literal|9
block|,
literal|10
block|}
block|}
decl_stmt|;
name|runStatementOnDriver
argument_list|(
literal|"insert into T(a, b) "
operator|+
name|makeValuesClause
argument_list|(
name|values
argument_list|)
argument_list|)
expr_stmt|;
comment|//, 'transactional_properties'='default'
name|runStatementOnDriver
argument_list|(
literal|"alter table T SET TBLPROPERTIES ('transactional'='true')"
argument_list|)
expr_stmt|;
comment|//Execution mode: vectorized
comment|//this uses VectorizedOrcAcidRowBatchReader
name|String
name|query
init|=
literal|"select a from T where b> 6 order by a"
decl_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|rs
init|=
name|runStatementOnDriver
argument_list|(
name|query
argument_list|)
decl_stmt|;
name|String
index|[]
index|[]
name|expected
init|=
block|{
block|{
literal|"6"
block|,
literal|""
block|}
block|,
block|{
literal|"9"
block|,
literal|""
block|}
block|,     }
decl_stmt|;
name|checkExpected
argument_list|(
name|rs
argument_list|,
name|expected
argument_list|,
literal|"After conversion"
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertEquals
argument_list|(
name|Integer
operator|.
name|toString
argument_list|(
literal|6
argument_list|)
argument_list|,
name|rs
operator|.
name|get
argument_list|(
literal|0
argument_list|)
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertEquals
argument_list|(
name|Integer
operator|.
name|toString
argument_list|(
literal|9
argument_list|)
argument_list|,
name|rs
operator|.
name|get
argument_list|(
literal|1
argument_list|)
argument_list|)
expr_stmt|;
name|assertVectorized
argument_list|(
name|shouldVectorize
argument_list|()
argument_list|,
name|query
argument_list|)
expr_stmt|;
comment|//why isn't PPD working.... - it is working but storage layer doesn't do row level filtering; only row group level
comment|//this uses VectorizedOrcAcidRowBatchReader
name|query
operator|=
literal|"select ROW__ID, a from T where b> 6 order by a"
expr_stmt|;
name|rs
operator|=
name|runStatementOnDriver
argument_list|(
name|query
argument_list|)
expr_stmt|;
name|String
index|[]
index|[]
name|expected1
init|=
block|{
block|{
literal|"{\"writeid\":0,\"bucketid\":536870912,\"rowid\":3}"
block|,
literal|"6"
block|}
block|,
block|{
literal|"{\"writeid\":0,\"bucketid\":536870912,\"rowid\":4}"
block|,
literal|"9"
block|}
block|}
decl_stmt|;
name|checkExpected
argument_list|(
name|rs
argument_list|,
name|expected1
argument_list|,
literal|"After conversion with VC1"
argument_list|)
expr_stmt|;
name|assertVectorized
argument_list|(
name|shouldVectorize
argument_list|()
argument_list|,
name|query
argument_list|)
expr_stmt|;
comment|//this uses VectorizedOrcAcidRowBatchReader
name|query
operator|=
literal|"select ROW__ID, a from T where b> 0 order by a"
expr_stmt|;
name|rs
operator|=
name|runStatementOnDriver
argument_list|(
name|query
argument_list|)
expr_stmt|;
name|String
index|[]
index|[]
name|expected2
init|=
block|{
block|{
literal|"{\"writeid\":0,\"bucketid\":536870912,\"rowid\":0}"
block|,
literal|"1"
block|}
block|,
block|{
literal|"{\"writeid\":0,\"bucketid\":536870912,\"rowid\":1}"
block|,
literal|"2"
block|}
block|,
block|{
literal|"{\"writeid\":0,\"bucketid\":536870912,\"rowid\":2}"
block|,
literal|"5"
block|}
block|,
block|{
literal|"{\"writeid\":0,\"bucketid\":536870912,\"rowid\":3}"
block|,
literal|"6"
block|}
block|,
block|{
literal|"{\"writeid\":0,\"bucketid\":536870912,\"rowid\":4}"
block|,
literal|"9"
block|}
block|}
decl_stmt|;
name|checkExpected
argument_list|(
name|rs
argument_list|,
name|expected2
argument_list|,
literal|"After conversion with VC2"
argument_list|)
expr_stmt|;
name|assertVectorized
argument_list|(
name|shouldVectorize
argument_list|()
argument_list|,
name|query
argument_list|)
expr_stmt|;
comment|//doesn't vectorize (uses neither of the Vectorzied Acid readers)
name|query
operator|=
literal|"select ROW__ID, a, INPUT__FILE__NAME from T where b> 6 order by a"
expr_stmt|;
name|rs
operator|=
name|runStatementOnDriver
argument_list|(
name|query
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertEquals
argument_list|(
literal|""
argument_list|,
literal|2
argument_list|,
name|rs
operator|.
name|size
argument_list|()
argument_list|)
expr_stmt|;
name|String
index|[]
index|[]
name|expected3
init|=
block|{
block|{
literal|"{\"writeid\":0,\"bucketid\":536870912,\"rowid\":3}\t6"
block|,
literal|"warehouse/t/000000_0"
block|}
block|,
block|{
literal|"{\"writeid\":0,\"bucketid\":536870912,\"rowid\":4}\t9"
block|,
literal|"warehouse/t/000000_0"
block|}
block|}
decl_stmt|;
name|checkExpected
argument_list|(
name|rs
argument_list|,
name|expected3
argument_list|,
literal|"After non-vectorized read"
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertEquals
argument_list|(
literal|0
argument_list|,
name|BucketCodec
operator|.
name|determineVersion
argument_list|(
literal|536870912
argument_list|)
operator|.
name|decodeWriterId
argument_list|(
literal|536870912
argument_list|)
argument_list|)
expr_stmt|;
comment|//vectorized because there is INPUT__FILE__NAME
name|assertVectorized
argument_list|(
literal|false
argument_list|,
name|query
argument_list|)
expr_stmt|;
name|runStatementOnDriver
argument_list|(
literal|"update T set b = 17 where a = 1"
argument_list|)
expr_stmt|;
comment|//this should use VectorizedOrcAcidRowReader
name|query
operator|=
literal|"select ROW__ID, b from T where b> 0 order by a"
expr_stmt|;
name|rs
operator|=
name|runStatementOnDriver
argument_list|(
name|query
argument_list|)
expr_stmt|;
name|String
index|[]
index|[]
name|expected4
init|=
block|{
block|{
literal|"{\"writeid\":1,\"bucketid\":536870912,\"rowid\":0}"
block|,
literal|"17"
block|}
block|,
block|{
literal|"{\"writeid\":0,\"bucketid\":536870912,\"rowid\":1}"
block|,
literal|"4"
block|}
block|,
block|{
literal|"{\"writeid\":0,\"bucketid\":536870912,\"rowid\":2}"
block|,
literal|"6"
block|}
block|,
block|{
literal|"{\"writeid\":0,\"bucketid\":536870912,\"rowid\":3}"
block|,
literal|"8"
block|}
block|,
block|{
literal|"{\"writeid\":0,\"bucketid\":536870912,\"rowid\":4}"
block|,
literal|"10"
block|}
block|}
decl_stmt|;
name|checkExpected
argument_list|(
name|rs
argument_list|,
name|expected4
argument_list|,
literal|"After conversion with VC4"
argument_list|)
expr_stmt|;
name|assertVectorized
argument_list|(
name|shouldVectorize
argument_list|()
argument_list|,
name|query
argument_list|)
expr_stmt|;
name|runStatementOnDriver
argument_list|(
literal|"alter table T compact 'major'"
argument_list|)
expr_stmt|;
name|TestTxnCommands2
operator|.
name|runWorker
argument_list|(
name|hiveConf
argument_list|)
expr_stmt|;
name|TxnStore
name|txnHandler
init|=
name|TxnUtils
operator|.
name|getTxnStore
argument_list|(
name|hiveConf
argument_list|)
decl_stmt|;
name|ShowCompactResponse
name|resp
init|=
name|txnHandler
operator|.
name|showCompact
argument_list|(
operator|new
name|ShowCompactRequest
argument_list|()
argument_list|)
decl_stmt|;
name|Assert
operator|.
name|assertEquals
argument_list|(
literal|"Unexpected number of compactions in history"
argument_list|,
literal|1
argument_list|,
name|resp
operator|.
name|getCompactsSize
argument_list|()
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertEquals
argument_list|(
literal|"Unexpected 0 compaction state"
argument_list|,
name|TxnStore
operator|.
name|CLEANING_RESPONSE
argument_list|,
name|resp
operator|.
name|getCompacts
argument_list|()
operator|.
name|get
argument_list|(
literal|0
argument_list|)
operator|.
name|getState
argument_list|()
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertTrue
argument_list|(
name|resp
operator|.
name|getCompacts
argument_list|()
operator|.
name|get
argument_list|(
literal|0
argument_list|)
operator|.
name|getHadoopJobId
argument_list|()
operator|.
name|startsWith
argument_list|(
literal|"job_local"
argument_list|)
argument_list|)
expr_stmt|;
comment|//this should not vectorize at all
name|query
operator|=
literal|"select ROW__ID, a, b, INPUT__FILE__NAME from T where b> 0 order by a, b"
expr_stmt|;
name|rs
operator|=
name|runStatementOnDriver
argument_list|(
name|query
argument_list|)
expr_stmt|;
name|String
index|[]
index|[]
name|expected5
init|=
block|{
comment|//the row__ids are the same after compaction
block|{
literal|"{\"writeid\":1,\"bucketid\":536870912,\"rowid\":0}\t1\t17"
block|,
literal|"warehouse/t/base_0000001/bucket_00000"
block|}
block|,
block|{
literal|"{\"writeid\":0,\"bucketid\":536870912,\"rowid\":1}\t2\t4"
block|,
literal|"warehouse/t/base_0000001/bucket_00000"
block|}
block|,
block|{
literal|"{\"writeid\":0,\"bucketid\":536870912,\"rowid\":2}\t5\t6"
block|,
literal|"warehouse/t/base_0000001/bucket_00000"
block|}
block|,
block|{
literal|"{\"writeid\":0,\"bucketid\":536870912,\"rowid\":3}\t6\t8"
block|,
literal|"warehouse/t/base_0000001/bucket_00000"
block|}
block|,
block|{
literal|"{\"writeid\":0,\"bucketid\":536870912,\"rowid\":4}\t9\t10"
block|,
literal|"warehouse/t/base_0000001/bucket_00000"
block|}
block|}
decl_stmt|;
name|checkExpected
argument_list|(
name|rs
argument_list|,
name|expected5
argument_list|,
literal|"After major compaction"
argument_list|)
expr_stmt|;
comment|//vectorized because there is INPUT__FILE__NAME
name|assertVectorized
argument_list|(
literal|false
argument_list|,
name|query
argument_list|)
expr_stmt|;
block|}
specifier|private
name|void
name|checkExpected
parameter_list|(
name|List
argument_list|<
name|String
argument_list|>
name|rs
parameter_list|,
name|String
index|[]
index|[]
name|expected
parameter_list|,
name|String
name|msg
parameter_list|)
block|{
name|super
operator|.
name|checkExpected
argument_list|(
name|rs
argument_list|,
name|expected
argument_list|,
name|msg
argument_list|,
name|LOG
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
comment|/**    * HIVE-17900    */
annotation|@
name|Test
specifier|public
name|void
name|testCompactStatsGather
parameter_list|()
throws|throws
name|Exception
block|{
name|hiveConf
operator|.
name|setVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|DYNAMICPARTITIONINGMODE
argument_list|,
literal|"nonstrict"
argument_list|)
expr_stmt|;
name|runStatementOnDriver
argument_list|(
literal|"drop table if exists T"
argument_list|)
expr_stmt|;
name|runStatementOnDriver
argument_list|(
literal|"create table T(a int, b int) partitioned by (p int, q int) "
operator|+
literal|"stored as orc TBLPROPERTIES ('transactional'='true')"
argument_list|)
expr_stmt|;
name|int
index|[]
index|[]
name|targetVals
init|=
block|{
block|{
literal|4
block|,
literal|1
block|,
literal|1
block|}
block|,
block|{
literal|4
block|,
literal|2
block|,
literal|2
block|}
block|,
block|{
literal|4
block|,
literal|3
block|,
literal|1
block|}
block|,
block|{
literal|4
block|,
literal|4
block|,
literal|2
block|}
block|}
decl_stmt|;
comment|//we only recompute stats after major compact if they existed before
name|runStatementOnDriver
argument_list|(
literal|"insert into T partition(p=1,q) "
operator|+
name|makeValuesClause
argument_list|(
name|targetVals
argument_list|)
argument_list|)
expr_stmt|;
name|runStatementOnDriver
argument_list|(
literal|"analyze table T  partition(p=1) compute statistics for columns"
argument_list|)
expr_stmt|;
name|IMetaStoreClient
name|hms
init|=
name|Hive
operator|.
name|get
argument_list|()
operator|.
name|getMSC
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|partNames
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|()
decl_stmt|;
name|partNames
operator|.
name|add
argument_list|(
literal|"p=1/q=2"
argument_list|)
expr_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|colNames
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|()
decl_stmt|;
name|colNames
operator|.
name|add
argument_list|(
literal|"a"
argument_list|)
expr_stmt|;
name|Map
argument_list|<
name|String
argument_list|,
name|List
argument_list|<
name|ColumnStatisticsObj
argument_list|>
argument_list|>
name|map
init|=
name|hms
operator|.
name|getPartitionColumnStatistics
argument_list|(
literal|"default"
argument_list|,
literal|"T"
argument_list|,
name|partNames
argument_list|,
name|colNames
argument_list|)
decl_stmt|;
name|Assert
operator|.
name|assertEquals
argument_list|(
literal|4
argument_list|,
name|map
operator|.
name|get
argument_list|(
name|partNames
operator|.
name|get
argument_list|(
literal|0
argument_list|)
argument_list|)
operator|.
name|get
argument_list|(
literal|0
argument_list|)
operator|.
name|getStatsData
argument_list|()
operator|.
name|getLongStats
argument_list|()
operator|.
name|getHighValue
argument_list|()
argument_list|)
expr_stmt|;
name|int
index|[]
index|[]
name|targetVals2
init|=
block|{
block|{
literal|5
block|,
literal|1
block|,
literal|1
block|}
block|,
block|{
literal|5
block|,
literal|2
block|,
literal|2
block|}
block|,
block|{
literal|5
block|,
literal|3
block|,
literal|1
block|}
block|,
block|{
literal|5
block|,
literal|4
block|,
literal|2
block|}
block|}
decl_stmt|;
name|runStatementOnDriver
argument_list|(
literal|"insert into T partition(p=1,q) "
operator|+
name|makeValuesClause
argument_list|(
name|targetVals2
argument_list|)
argument_list|)
expr_stmt|;
name|String
name|query
init|=
literal|"select ROW__ID, p, q, a, b, INPUT__FILE__NAME from T order by p, q, a, b"
decl_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|rs
init|=
name|runStatementOnDriver
argument_list|(
name|query
argument_list|)
decl_stmt|;
name|String
index|[]
index|[]
name|expected
init|=
block|{
block|{
literal|"{\"writeid\":1,\"bucketid\":536870912,\"rowid\":0}\t1\t1\t4\t1"
block|,
literal|"t/p=1/q=1/delta_0000001_0000001_0000/bucket_00000"
block|}
block|,
block|{
literal|"{\"writeid\":1,\"bucketid\":536870912,\"rowid\":1}\t1\t1\t4\t3"
block|,
literal|"t/p=1/q=1/delta_0000001_0000001_0000/bucket_00000"
block|}
block|,
block|{
literal|"{\"writeid\":2,\"bucketid\":536870912,\"rowid\":0}\t1\t1\t5\t1"
block|,
literal|"t/p=1/q=1/delta_0000002_0000002_0000/bucket_00000"
block|}
block|,
block|{
literal|"{\"writeid\":2,\"bucketid\":536870912,\"rowid\":1}\t1\t1\t5\t3"
block|,
literal|"t/p=1/q=1/delta_0000002_0000002_0000/bucket_00000"
block|}
block|,
block|{
literal|"{\"writeid\":1,\"bucketid\":536870912,\"rowid\":0}\t1\t2\t4\t2"
block|,
literal|"t/p=1/q=2/delta_0000001_0000001_0000/bucket_00000"
block|}
block|,
block|{
literal|"{\"writeid\":1,\"bucketid\":536870912,\"rowid\":1}\t1\t2\t4\t4"
block|,
literal|"t/p=1/q=2/delta_0000001_0000001_0000/bucket_00000"
block|}
block|,
block|{
literal|"{\"writeid\":2,\"bucketid\":536870912,\"rowid\":0}\t1\t2\t5\t2"
block|,
literal|"t/p=1/q=2/delta_0000002_0000002_0000/bucket_00000"
block|}
block|,
block|{
literal|"{\"writeid\":2,\"bucketid\":536870912,\"rowid\":1}\t1\t2\t5\t4"
block|,
literal|"t/p=1/q=2/delta_0000002_0000002_0000/bucket_00000"
block|}
block|}
decl_stmt|;
name|checkExpected
argument_list|(
name|rs
argument_list|,
name|expected
argument_list|,
literal|"insert data"
argument_list|)
expr_stmt|;
comment|//run major compaction
name|runStatementOnDriver
argument_list|(
literal|"alter table T partition(p=1,q=2) compact 'major'"
argument_list|)
expr_stmt|;
name|TestTxnCommands2
operator|.
name|runWorker
argument_list|(
name|hiveConf
argument_list|)
expr_stmt|;
name|query
operator|=
literal|"select ROW__ID, p, q, a, b, INPUT__FILE__NAME from T order by p, q, a, b"
expr_stmt|;
name|rs
operator|=
name|runStatementOnDriver
argument_list|(
name|query
argument_list|)
expr_stmt|;
name|String
index|[]
index|[]
name|expected2
init|=
block|{
block|{
literal|"{\"writeid\":1,\"bucketid\":536870912,\"rowid\":0}\t1\t1\t4\t1"
block|,
literal|"t/p=1/q=1/delta_0000001_0000001_0000/bucket_00000"
block|}
block|,
block|{
literal|"{\"writeid\":1,\"bucketid\":536870912,\"rowid\":1}\t1\t1\t4\t3"
block|,
literal|"t/p=1/q=1/delta_0000001_0000001_0000/bucket_00000"
block|}
block|,
block|{
literal|"{\"writeid\":2,\"bucketid\":536870912,\"rowid\":0}\t1\t1\t5\t1"
block|,
literal|"t/p=1/q=1/delta_0000002_0000002_0000/bucket_00000"
block|}
block|,
block|{
literal|"{\"writeid\":2,\"bucketid\":536870912,\"rowid\":1}\t1\t1\t5\t3"
block|,
literal|"t/p=1/q=1/delta_0000002_0000002_0000/bucket_00000"
block|}
block|,
block|{
literal|"{\"writeid\":1,\"bucketid\":536870912,\"rowid\":0}\t1\t2\t4\t2"
block|,
literal|"t/p=1/q=2/base_0000002/bucket_00000"
block|}
block|,
block|{
literal|"{\"writeid\":1,\"bucketid\":536870912,\"rowid\":1}\t1\t2\t4\t4"
block|,
literal|"t/p=1/q=2/base_0000002/bucket_00000"
block|}
block|,
block|{
literal|"{\"writeid\":2,\"bucketid\":536870912,\"rowid\":0}\t1\t2\t5\t2"
block|,
literal|"t/p=1/q=2/base_0000002/bucket_00000"
block|}
block|,
block|{
literal|"{\"writeid\":2,\"bucketid\":536870912,\"rowid\":1}\t1\t2\t5\t4"
block|,
literal|"t/p=1/q=2/base_0000002/bucket_00000"
block|}
block|}
decl_stmt|;
name|checkExpected
argument_list|(
name|rs
argument_list|,
name|expected2
argument_list|,
literal|"after major compaction"
argument_list|)
expr_stmt|;
comment|//check status of compaction job
name|TxnStore
name|txnHandler
init|=
name|TxnUtils
operator|.
name|getTxnStore
argument_list|(
name|hiveConf
argument_list|)
decl_stmt|;
name|ShowCompactResponse
name|resp
init|=
name|txnHandler
operator|.
name|showCompact
argument_list|(
operator|new
name|ShowCompactRequest
argument_list|()
argument_list|)
decl_stmt|;
name|Assert
operator|.
name|assertEquals
argument_list|(
literal|"Unexpected number of compactions in history"
argument_list|,
literal|1
argument_list|,
name|resp
operator|.
name|getCompactsSize
argument_list|()
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertEquals
argument_list|(
literal|"Unexpected 0 compaction state"
argument_list|,
name|TxnStore
operator|.
name|CLEANING_RESPONSE
argument_list|,
name|resp
operator|.
name|getCompacts
argument_list|()
operator|.
name|get
argument_list|(
literal|0
argument_list|)
operator|.
name|getState
argument_list|()
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertTrue
argument_list|(
name|resp
operator|.
name|getCompacts
argument_list|()
operator|.
name|get
argument_list|(
literal|0
argument_list|)
operator|.
name|getHadoopJobId
argument_list|()
operator|.
name|startsWith
argument_list|(
literal|"job_local"
argument_list|)
argument_list|)
expr_stmt|;
comment|//now check that stats were updated
name|map
operator|=
name|hms
operator|.
name|getPartitionColumnStatistics
argument_list|(
literal|"default"
argument_list|,
literal|"T"
argument_list|,
name|partNames
argument_list|,
name|colNames
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertEquals
argument_list|(
literal|""
argument_list|,
literal|5
argument_list|,
name|map
operator|.
name|get
argument_list|(
name|partNames
operator|.
name|get
argument_list|(
literal|0
argument_list|)
argument_list|)
operator|.
name|get
argument_list|(
literal|0
argument_list|)
operator|.
name|getStatsData
argument_list|()
operator|.
name|getLongStats
argument_list|()
operator|.
name|getHighValue
argument_list|()
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Test
specifier|public
name|void
name|testDefault
parameter_list|()
throws|throws
name|Exception
block|{
name|hiveConf
operator|.
name|set
argument_list|(
name|MetastoreConf
operator|.
name|ConfVars
operator|.
name|CREATE_TABLES_AS_ACID
operator|.
name|getVarname
argument_list|()
argument_list|,
literal|"true"
argument_list|)
expr_stmt|;
name|runStatementOnDriver
argument_list|(
literal|"drop table if exists T"
argument_list|)
expr_stmt|;
name|runStatementOnDriver
argument_list|(
literal|"create table T (a int, b int) stored as orc"
argument_list|)
expr_stmt|;
name|runStatementOnDriver
argument_list|(
literal|"insert into T values(1,2),(3,4)"
argument_list|)
expr_stmt|;
name|String
name|query
init|=
literal|"select ROW__ID, a, b, INPUT__FILE__NAME from T order by a, b"
decl_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|rs
init|=
name|runStatementOnDriver
argument_list|(
name|query
argument_list|)
decl_stmt|;
name|String
index|[]
index|[]
name|expected
init|=
block|{
comment|//this proves data is written in Acid layout so T was made Acid
block|{
literal|"{\"writeid\":1,\"bucketid\":536870912,\"rowid\":0}\t1\t2"
block|,
literal|"t/delta_0000001_0000001_0000/bucket_00000"
block|}
block|,
block|{
literal|"{\"writeid\":1,\"bucketid\":536870912,\"rowid\":1}\t3\t4"
block|,
literal|"t/delta_0000001_0000001_0000/bucket_00000"
block|}
block|}
decl_stmt|;
name|checkExpected
argument_list|(
name|rs
argument_list|,
name|expected
argument_list|,
literal|"insert data"
argument_list|)
expr_stmt|;
block|}
comment|/**    * see HIVE-18429    */
annotation|@
name|Test
specifier|public
name|void
name|testEmptyCompactionResult
parameter_list|()
throws|throws
name|Exception
block|{
name|hiveConf
operator|.
name|set
argument_list|(
name|MetastoreConf
operator|.
name|ConfVars
operator|.
name|CREATE_TABLES_AS_ACID
operator|.
name|getVarname
argument_list|()
argument_list|,
literal|"true"
argument_list|)
expr_stmt|;
name|runStatementOnDriver
argument_list|(
literal|"drop table if exists T"
argument_list|)
expr_stmt|;
name|runStatementOnDriver
argument_list|(
literal|"create table T (a int, b int) stored as orc"
argument_list|)
expr_stmt|;
name|int
index|[]
index|[]
name|data
init|=
block|{
block|{
literal|1
block|,
literal|2
block|}
block|,
block|{
literal|3
block|,
literal|4
block|}
block|}
decl_stmt|;
name|runStatementOnDriver
argument_list|(
literal|"insert into T"
operator|+
name|makeValuesClause
argument_list|(
name|data
argument_list|)
argument_list|)
expr_stmt|;
name|runStatementOnDriver
argument_list|(
literal|"insert into T"
operator|+
name|makeValuesClause
argument_list|(
name|data
argument_list|)
argument_list|)
expr_stmt|;
comment|//delete the bucket files so now we have empty delta dirs
name|List
argument_list|<
name|String
argument_list|>
name|rs
init|=
name|runStatementOnDriver
argument_list|(
literal|"select distinct INPUT__FILE__NAME from T"
argument_list|)
decl_stmt|;
name|FileSystem
name|fs
init|=
name|FileSystem
operator|.
name|get
argument_list|(
name|hiveConf
argument_list|)
decl_stmt|;
for|for
control|(
name|String
name|path
range|:
name|rs
control|)
block|{
name|fs
operator|.
name|delete
argument_list|(
operator|new
name|Path
argument_list|(
name|path
argument_list|)
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
name|runStatementOnDriver
argument_list|(
literal|"alter table T compact 'major'"
argument_list|)
expr_stmt|;
name|TestTxnCommands2
operator|.
name|runWorker
argument_list|(
name|hiveConf
argument_list|)
expr_stmt|;
comment|//check status of compaction job
name|TxnStore
name|txnHandler
init|=
name|TxnUtils
operator|.
name|getTxnStore
argument_list|(
name|hiveConf
argument_list|)
decl_stmt|;
name|ShowCompactResponse
name|resp
init|=
name|txnHandler
operator|.
name|showCompact
argument_list|(
operator|new
name|ShowCompactRequest
argument_list|()
argument_list|)
decl_stmt|;
name|Assert
operator|.
name|assertEquals
argument_list|(
literal|"Unexpected number of compactions in history"
argument_list|,
literal|1
argument_list|,
name|resp
operator|.
name|getCompactsSize
argument_list|()
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertEquals
argument_list|(
literal|"Unexpected 0 compaction state"
argument_list|,
name|TxnStore
operator|.
name|CLEANING_RESPONSE
argument_list|,
name|resp
operator|.
name|getCompacts
argument_list|()
operator|.
name|get
argument_list|(
literal|0
argument_list|)
operator|.
name|getState
argument_list|()
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertTrue
argument_list|(
name|resp
operator|.
name|getCompacts
argument_list|()
operator|.
name|get
argument_list|(
literal|0
argument_list|)
operator|.
name|getHadoopJobId
argument_list|()
operator|.
name|startsWith
argument_list|(
literal|"job_local"
argument_list|)
argument_list|)
expr_stmt|;
comment|//now run another compaction make sure empty dirs don't cause issues
name|runStatementOnDriver
argument_list|(
literal|"insert into T"
operator|+
name|makeValuesClause
argument_list|(
name|data
argument_list|)
argument_list|)
expr_stmt|;
name|runStatementOnDriver
argument_list|(
literal|"alter table T compact 'major'"
argument_list|)
expr_stmt|;
name|TestTxnCommands2
operator|.
name|runWorker
argument_list|(
name|hiveConf
argument_list|)
expr_stmt|;
comment|//check status of compaction job
name|resp
operator|=
name|txnHandler
operator|.
name|showCompact
argument_list|(
operator|new
name|ShowCompactRequest
argument_list|()
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertEquals
argument_list|(
literal|"Unexpected number of compactions in history"
argument_list|,
literal|2
argument_list|,
name|resp
operator|.
name|getCompactsSize
argument_list|()
argument_list|)
expr_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
literal|2
condition|;
name|i
operator|++
control|)
block|{
name|Assert
operator|.
name|assertEquals
argument_list|(
literal|"Unexpected 0 compaction state"
argument_list|,
name|TxnStore
operator|.
name|CLEANING_RESPONSE
argument_list|,
name|resp
operator|.
name|getCompacts
argument_list|()
operator|.
name|get
argument_list|(
name|i
argument_list|)
operator|.
name|getState
argument_list|()
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertTrue
argument_list|(
name|resp
operator|.
name|getCompacts
argument_list|()
operator|.
name|get
argument_list|(
name|i
argument_list|)
operator|.
name|getHadoopJobId
argument_list|()
operator|.
name|startsWith
argument_list|(
literal|"job_local"
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|rs
operator|=
name|runStatementOnDriver
argument_list|(
literal|"select a, b from T order by a, b"
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertEquals
argument_list|(
name|stringifyValues
argument_list|(
name|data
argument_list|)
argument_list|,
name|rs
argument_list|)
expr_stmt|;
block|}
block|}
end_class

end_unit

