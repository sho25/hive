begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/*  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|upgrade
operator|.
name|acid
package|;
end_package

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|annotations
operator|.
name|VisibleForTesting
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|cli
operator|.
name|CommandLine
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|cli
operator|.
name|CommandLineParser
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|cli
operator|.
name|GnuParser
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|cli
operator|.
name|HelpFormatter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|cli
operator|.
name|Option
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|cli
operator|.
name|Options
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|cli
operator|.
name|ParseException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|lang3
operator|.
name|exception
operator|.
name|ExceptionUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|ContentSummary
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileStatus
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|PathFilter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|ValidTxnList
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|HiveMetaHook
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|HiveMetaHookLoader
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|HiveMetaStoreClient
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|IMetaStoreClient
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|RetryingMetaStoreClient
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|Warehouse
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|CompactionResponse
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|MetaException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|ShowCompactResponse
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|ShowCompactResponseElement
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Table
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|hive_metastoreConstants
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|txn
operator|.
name|TxnStore
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|txn
operator|.
name|TxnUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|AcidUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|Hive
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|HiveException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|orc
operator|.
name|OrcFile
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|orc
operator|.
name|Reader
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde
operator|.
name|serdeConstants
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|shims
operator|.
name|HadoopShims
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|security
operator|.
name|AccessControlException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|security
operator|.
name|UserGroupInformation
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hive
operator|.
name|common
operator|.
name|util
operator|.
name|HiveVersionInfo
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|thrift
operator|.
name|TException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|Logger
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|LoggerFactory
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|FileWriter
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|PrintWriter
import|;
end_import

begin_import
import|import
name|java
operator|.
name|nio
operator|.
name|ByteBuffer
import|;
end_import

begin_import
import|import
name|java
operator|.
name|nio
operator|.
name|charset
operator|.
name|CharacterCodingException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|nio
operator|.
name|charset
operator|.
name|Charset
import|;
end_import

begin_import
import|import
name|java
operator|.
name|nio
operator|.
name|charset
operator|.
name|CharsetDecoder
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Collections
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashSet
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Set
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|parse
operator|.
name|BaseSemanticAnalyzer
operator|.
name|escapeSQLString
import|;
end_import

begin_comment
comment|/**  * This utility is designed to help with upgrading Hive 2.x to Hive 3.0.  On-disk layout for  * transactional tables has changed in 3.0 and require pre-processing before upgrade to ensure  * they are readable by Hive 3.0.  Some transactional tables (identified by this utility) require  * Major compaction to be run on them before upgrading to 3.0.  Once this compaction starts, no  * more update/delete/merge statements may be executed on these tables until upgrade is finished.  *  * Additionally, a new type of transactional tables was added in 3.0 - insert-only tables.  These  * tables support ACID semantics and work with any Input/OutputFormat.  Any Managed tables may  * be made insert-only transactional table. These tables don't support Update/Delete/Merge commands.  *  * Note that depending on the number of tables/partitions and amount of data in them compactions  * may take a significant amount of time and resources.  The script output by this utility includes  * some heuristics that may help estimate the time required.  If no script is produced, no action  * is needed.  For compactions to run an instance of standalone Hive Metastore must be running.  * Please make sure hive.compactor.worker.threads is sufficiently high - this specifies the limit  * of concurrent compactions that may be run.  Each compaction job is a Map-Reduce job.  * hive.compactor.job.queue may be used to set a Yarn queue ame where all compaction jobs will be  * submitted.  *  * "execute" option may be supplied to have the utility automatically execute the  * equivalent of the generated commands  *  * "location" option may be supplied followed by a path to set the location for the generated  * scripts.  *  * Random:  * This utility connects to the Metastore via API.  It may be necessary to set  * -Djavax.security.auth.useSubjectCredsOnly=false in Kerberized environment if errors like  * "org.ietf.jgss.GSSException: No valid credentials provided (  *    Mechanism level: Failed to find any Kerberos tgt)"  * show up after kinit.  *  * See also org.apache.hadoop.hive.ql.util.UpgradeTool in Hive 3.x  */
end_comment

begin_class
specifier|public
class|class
name|PreUpgradeTool
block|{
specifier|private
specifier|static
specifier|final
name|Logger
name|LOG
init|=
name|LoggerFactory
operator|.
name|getLogger
argument_list|(
name|PreUpgradeTool
operator|.
name|class
argument_list|)
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|int
name|PARTITION_BATCH_SIZE
init|=
literal|10000
decl_stmt|;
specifier|private
specifier|final
name|Options
name|cmdLineOptions
init|=
operator|new
name|Options
argument_list|()
decl_stmt|;
specifier|public
specifier|static
name|void
name|main
parameter_list|(
name|String
index|[]
name|args
parameter_list|)
throws|throws
name|Exception
block|{
name|PreUpgradeTool
name|tool
init|=
operator|new
name|PreUpgradeTool
argument_list|()
decl_stmt|;
name|tool
operator|.
name|init
argument_list|()
expr_stmt|;
name|CommandLineParser
name|parser
init|=
operator|new
name|GnuParser
argument_list|()
decl_stmt|;
name|CommandLine
name|line
decl_stmt|;
name|String
name|outputDir
init|=
literal|"."
decl_stmt|;
name|boolean
name|execute
init|=
literal|false
decl_stmt|;
try|try
block|{
name|line
operator|=
name|parser
operator|.
name|parse
argument_list|(
name|tool
operator|.
name|cmdLineOptions
argument_list|,
name|args
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|ParseException
name|e
parameter_list|)
block|{
name|System
operator|.
name|err
operator|.
name|println
argument_list|(
literal|"PreUpgradeTool: Parsing failed.  Reason: "
operator|+
name|e
operator|.
name|getLocalizedMessage
argument_list|()
argument_list|)
expr_stmt|;
name|printAndExit
argument_list|(
name|tool
argument_list|)
expr_stmt|;
return|return;
block|}
if|if
condition|(
name|line
operator|.
name|hasOption
argument_list|(
literal|"help"
argument_list|)
condition|)
block|{
name|HelpFormatter
name|formatter
init|=
operator|new
name|HelpFormatter
argument_list|()
decl_stmt|;
name|formatter
operator|.
name|printHelp
argument_list|(
literal|"upgrade-acid"
argument_list|,
name|tool
operator|.
name|cmdLineOptions
argument_list|)
expr_stmt|;
return|return;
block|}
if|if
condition|(
name|line
operator|.
name|hasOption
argument_list|(
literal|"location"
argument_list|)
condition|)
block|{
name|outputDir
operator|=
name|line
operator|.
name|getOptionValue
argument_list|(
literal|"location"
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|line
operator|.
name|hasOption
argument_list|(
literal|"execute"
argument_list|)
condition|)
block|{
name|execute
operator|=
literal|true
expr_stmt|;
block|}
name|LOG
operator|.
name|info
argument_list|(
literal|"Starting with execute="
operator|+
name|execute
operator|+
literal|", location="
operator|+
name|outputDir
argument_list|)
expr_stmt|;
try|try
block|{
name|String
name|hiveVer
init|=
name|HiveVersionInfo
operator|.
name|getShortVersion
argument_list|()
decl_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Using Hive Version: "
operator|+
name|HiveVersionInfo
operator|.
name|getVersion
argument_list|()
operator|+
literal|" build: "
operator|+
name|HiveVersionInfo
operator|.
name|getBuildVersion
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
operator|!
name|hiveVer
operator|.
name|startsWith
argument_list|(
literal|"2."
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|IllegalStateException
argument_list|(
literal|"preUpgrade requires Hive 2.x.  Actual: "
operator|+
name|hiveVer
argument_list|)
throw|;
block|}
name|tool
operator|.
name|prepareAcidUpgradeInternal
argument_list|(
name|outputDir
argument_list|,
name|execute
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|ex
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"PreUpgradeTool failed"
argument_list|,
name|ex
argument_list|)
expr_stmt|;
throw|throw
name|ex
throw|;
block|}
block|}
specifier|private
specifier|static
name|void
name|printAndExit
parameter_list|(
name|PreUpgradeTool
name|tool
parameter_list|)
block|{
name|HelpFormatter
name|formatter
init|=
operator|new
name|HelpFormatter
argument_list|()
decl_stmt|;
name|formatter
operator|.
name|printHelp
argument_list|(
literal|"upgrade-acid"
argument_list|,
name|tool
operator|.
name|cmdLineOptions
argument_list|)
expr_stmt|;
name|System
operator|.
name|exit
argument_list|(
literal|1
argument_list|)
expr_stmt|;
block|}
specifier|private
name|void
name|init
parameter_list|()
block|{
try|try
block|{
name|cmdLineOptions
operator|.
name|addOption
argument_list|(
operator|new
name|Option
argument_list|(
literal|"help"
argument_list|,
literal|"Generates a script to execute on 2.x"
operator|+
literal|" cluster.  This requires 2.x binaries on the classpath and hive-site.xml."
argument_list|)
argument_list|)
expr_stmt|;
name|Option
name|exec
init|=
operator|new
name|Option
argument_list|(
literal|"execute"
argument_list|,
literal|"Executes commands equivalent to generated scrips"
argument_list|)
decl_stmt|;
name|exec
operator|.
name|setOptionalArg
argument_list|(
literal|true
argument_list|)
expr_stmt|;
name|cmdLineOptions
operator|.
name|addOption
argument_list|(
name|exec
argument_list|)
expr_stmt|;
name|cmdLineOptions
operator|.
name|addOption
argument_list|(
operator|new
name|Option
argument_list|(
literal|"location"
argument_list|,
literal|true
argument_list|,
literal|"Location to write scripts to. Default is CWD."
argument_list|)
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|ex
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"init()"
argument_list|,
name|ex
argument_list|)
expr_stmt|;
throw|throw
name|ex
throw|;
block|}
block|}
specifier|private
specifier|static
name|HiveMetaHookLoader
name|getHookLoader
parameter_list|()
block|{
return|return
operator|new
name|HiveMetaHookLoader
argument_list|()
block|{
annotation|@
name|Override
specifier|public
name|HiveMetaHook
name|getHook
parameter_list|(
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Table
name|tbl
parameter_list|)
block|{
return|return
literal|null
return|;
block|}
block|}
return|;
block|}
specifier|private
specifier|static
name|IMetaStoreClient
name|getHMS
parameter_list|(
name|HiveConf
name|conf
parameter_list|)
block|{
name|UserGroupInformation
name|loggedInUser
init|=
literal|null
decl_stmt|;
try|try
block|{
name|loggedInUser
operator|=
name|UserGroupInformation
operator|.
name|getLoginUser
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Unable to get logged in user via UGI. err: {}"
argument_list|,
name|e
operator|.
name|getMessage
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|boolean
name|secureMode
init|=
name|loggedInUser
operator|!=
literal|null
operator|&&
name|loggedInUser
operator|.
name|hasKerberosCredentials
argument_list|()
decl_stmt|;
if|if
condition|(
name|secureMode
condition|)
block|{
name|conf
operator|.
name|setBoolVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|METASTORE_USE_THRIFT_SASL
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
try|try
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Creating metastore client for {}"
argument_list|,
literal|"PreUpgradeTool"
argument_list|)
expr_stmt|;
comment|/* I'd rather call return RetryingMetaStoreClient.getProxy(conf, true)       which calls HiveMetaStoreClient(HiveConf, Boolean) which exists in        (at least) 2.1.0.2.6.5.0-292 and later but not in 2.1.0.2.6.0.3-8 (the HDP 2.6 release)        i.e. RetryingMetaStoreClient.getProxy(conf, true) is broken in 2.6.0*/
return|return
name|RetryingMetaStoreClient
operator|.
name|getProxy
argument_list|(
name|conf
argument_list|,
operator|new
name|Class
index|[]
block|{
name|HiveConf
operator|.
name|class
block|,
name|HiveMetaHookLoader
operator|.
name|class
block|,
name|Boolean
operator|.
name|class
block|}
argument_list|,
operator|new
name|Object
index|[]
block|{
name|conf
block|,
name|getHookLoader
argument_list|()
block|,
name|Boolean
operator|.
name|TRUE
block|}
argument_list|,
literal|null
argument_list|,
name|HiveMetaStoreClient
operator|.
name|class
operator|.
name|getName
argument_list|()
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|MetaException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
literal|"Error connecting to Hive Metastore URI: "
operator|+
name|conf
operator|.
name|getVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|METASTOREURIS
argument_list|)
operator|+
literal|". "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    * todo: change script comments to a preamble instead of a footer    */
specifier|private
name|void
name|prepareAcidUpgradeInternal
parameter_list|(
name|String
name|scriptLocation
parameter_list|,
name|boolean
name|execute
parameter_list|)
throws|throws
name|HiveException
throws|,
name|TException
throws|,
name|IOException
block|{
name|HiveConf
name|conf
init|=
name|hiveConf
operator|!=
literal|null
condition|?
name|hiveConf
else|:
operator|new
name|HiveConf
argument_list|()
decl_stmt|;
name|boolean
name|isAcidEnabled
init|=
name|isAcidEnabled
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|IMetaStoreClient
name|hms
init|=
name|getHMS
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Looking for databases"
argument_list|)
expr_stmt|;
name|String
name|exceptionMsg
init|=
literal|null
decl_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|databases
decl_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|compactions
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|()
decl_stmt|;
specifier|final
name|CompactionMetaInfo
name|compactionMetaInfo
init|=
operator|new
name|CompactionMetaInfo
argument_list|()
decl_stmt|;
name|ValidTxnList
name|txns
init|=
literal|null
decl_stmt|;
name|Hive
name|db
init|=
literal|null
decl_stmt|;
try|try
block|{
name|databases
operator|=
name|hms
operator|.
name|getAllDatabases
argument_list|()
expr_stmt|;
comment|//TException
name|LOG
operator|.
name|debug
argument_list|(
literal|"Found "
operator|+
name|databases
operator|.
name|size
argument_list|()
operator|+
literal|" databases to process"
argument_list|)
expr_stmt|;
if|if
condition|(
name|execute
condition|)
block|{
name|db
operator|=
name|Hive
operator|.
name|get
argument_list|(
name|conf
argument_list|)
expr_stmt|;
block|}
for|for
control|(
name|String
name|dbName
range|:
name|databases
control|)
block|{
try|try
block|{
name|List
argument_list|<
name|String
argument_list|>
name|tables
init|=
name|hms
operator|.
name|getAllTables
argument_list|(
name|dbName
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"found "
operator|+
name|tables
operator|.
name|size
argument_list|()
operator|+
literal|" tables in "
operator|+
name|dbName
argument_list|)
expr_stmt|;
for|for
control|(
name|String
name|tableName
range|:
name|tables
control|)
block|{
try|try
block|{
name|Table
name|t
init|=
name|hms
operator|.
name|getTable
argument_list|(
name|dbName
argument_list|,
name|tableName
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"processing table "
operator|+
name|Warehouse
operator|.
name|getQualifiedName
argument_list|(
name|t
argument_list|)
argument_list|)
expr_stmt|;
if|if
condition|(
name|isAcidEnabled
condition|)
block|{
comment|//if acid is off, there can't be any acid tables - nothing to compact
if|if
condition|(
name|txns
operator|==
literal|null
condition|)
block|{
comment|/*            This API changed from 2.x to 3.0.  so this won't even compile with 3.0            but it doesn't need to since we only run this preUpgrade           */
name|TxnStore
name|txnHandler
init|=
name|TxnUtils
operator|.
name|getTxnStore
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|txns
operator|=
name|TxnUtils
operator|.
name|createValidCompactTxnList
argument_list|(
name|txnHandler
operator|.
name|getOpenTxnsInfo
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|List
argument_list|<
name|String
argument_list|>
name|compactionCommands
init|=
name|getCompactionCommands
argument_list|(
name|t
argument_list|,
name|conf
argument_list|,
name|hms
argument_list|,
name|compactionMetaInfo
argument_list|,
name|execute
argument_list|,
name|db
argument_list|,
name|txns
argument_list|)
decl_stmt|;
name|compactions
operator|.
name|addAll
argument_list|(
name|compactionCommands
argument_list|)
expr_stmt|;
block|}
comment|/*todo: handle renaming files somewhere*/
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
if|if
condition|(
name|isAccessControlException
argument_list|(
name|e
argument_list|)
condition|)
block|{
comment|// this could be external table with 0 permission for hive user
name|exceptionMsg
operator|=
literal|"Unable to access "
operator|+
name|dbName
operator|+
literal|"."
operator|+
name|tableName
operator|+
literal|". Pre-upgrade tool requires read-access "
operator|+
literal|"to databases and tables to determine if a table has to be compacted. "
operator|+
literal|"Set "
operator|+
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_METASTORE_AUTHORIZATION_AUTH_READS
operator|.
name|varname
operator|+
literal|" config to "
operator|+
literal|"false to allow read-access to databases and tables and retry the pre-upgrade tool again.."
expr_stmt|;
block|}
throw|throw
name|e
throw|;
block|}
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
if|if
condition|(
name|exceptionMsg
operator|==
literal|null
operator|&&
name|isAccessControlException
argument_list|(
name|e
argument_list|)
condition|)
block|{
comment|// we may not have access to read all tables from this db
name|exceptionMsg
operator|=
literal|"Unable to access "
operator|+
name|dbName
operator|+
literal|". Pre-upgrade tool requires read-access "
operator|+
literal|"to databases and tables to determine if a table has to be compacted. "
operator|+
literal|"Set "
operator|+
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_METASTORE_AUTHORIZATION_AUTH_READS
operator|.
name|varname
operator|+
literal|" config to "
operator|+
literal|"false to allow read-access to databases and tables and retry the pre-upgrade tool again.."
expr_stmt|;
block|}
throw|throw
name|e
throw|;
block|}
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
if|if
condition|(
name|exceptionMsg
operator|==
literal|null
operator|&&
name|isAccessControlException
argument_list|(
name|e
argument_list|)
condition|)
block|{
name|exceptionMsg
operator|=
literal|"Unable to get databases. Pre-upgrade tool requires read-access "
operator|+
literal|"to databases and tables to determine if a table has to be compacted. "
operator|+
literal|"Set "
operator|+
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_METASTORE_AUTHORIZATION_AUTH_READS
operator|.
name|varname
operator|+
literal|" config to "
operator|+
literal|"false to allow read-access to databases and tables and retry the pre-upgrade tool again.."
expr_stmt|;
block|}
throw|throw
operator|new
name|HiveException
argument_list|(
name|exceptionMsg
argument_list|,
name|e
argument_list|)
throw|;
block|}
name|makeCompactionScript
argument_list|(
name|compactions
argument_list|,
name|scriptLocation
argument_list|,
name|compactionMetaInfo
argument_list|)
expr_stmt|;
if|if
condition|(
name|execute
condition|)
block|{
while|while
condition|(
name|compactionMetaInfo
operator|.
name|compactionIds
operator|.
name|size
argument_list|()
operator|>
literal|0
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Will wait for "
operator|+
name|compactionMetaInfo
operator|.
name|compactionIds
operator|.
name|size
argument_list|()
operator|+
literal|" compactions to complete"
argument_list|)
expr_stmt|;
name|ShowCompactResponse
name|resp
init|=
name|db
operator|.
name|showCompactions
argument_list|()
decl_stmt|;
for|for
control|(
name|ShowCompactResponseElement
name|e
range|:
name|resp
operator|.
name|getCompacts
argument_list|()
control|)
block|{
specifier|final
name|String
name|state
init|=
name|e
operator|.
name|getState
argument_list|()
decl_stmt|;
name|boolean
name|removed
decl_stmt|;
switch|switch
condition|(
name|state
condition|)
block|{
case|case
name|TxnStore
operator|.
name|CLEANING_RESPONSE
case|:
case|case
name|TxnStore
operator|.
name|SUCCEEDED_RESPONSE
case|:
name|removed
operator|=
name|compactionMetaInfo
operator|.
name|compactionIds
operator|.
name|remove
argument_list|(
name|e
operator|.
name|getId
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|removed
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Required compaction succeeded: "
operator|+
name|e
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
break|break;
case|case
name|TxnStore
operator|.
name|ATTEMPTED_RESPONSE
case|:
case|case
name|TxnStore
operator|.
name|FAILED_RESPONSE
case|:
name|removed
operator|=
name|compactionMetaInfo
operator|.
name|compactionIds
operator|.
name|remove
argument_list|(
name|e
operator|.
name|getId
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|removed
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Required compaction failed: "
operator|+
name|e
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
break|break;
case|case
name|TxnStore
operator|.
name|INITIATED_RESPONSE
case|:
comment|//may flood the log
comment|//LOG.debug("Still waiting  on: " + e.toString());
break|break;
case|case
name|TxnStore
operator|.
name|WORKING_RESPONSE
case|:
name|LOG
operator|.
name|debug
argument_list|(
literal|"Still working on: "
operator|+
name|e
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
break|break;
default|default:
comment|//shouldn't be any others
name|LOG
operator|.
name|error
argument_list|(
literal|"Unexpected state for : "
operator|+
name|e
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
name|compactionMetaInfo
operator|.
name|compactionIds
operator|.
name|size
argument_list|()
operator|>
literal|0
condition|)
block|{
try|try
block|{
if|if
condition|(
name|callback
operator|!=
literal|null
condition|)
block|{
name|callback
operator|.
name|onWaitForCompaction
argument_list|()
expr_stmt|;
block|}
name|Thread
operator|.
name|sleep
argument_list|(
name|pollIntervalMs
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|InterruptedException
name|ex
parameter_list|)
block|{
empty_stmt|;
comment|//this only responds to ^C
block|}
block|}
block|}
block|}
block|}
specifier|private
name|boolean
name|isAccessControlException
parameter_list|(
specifier|final
name|Exception
name|e
parameter_list|)
block|{
comment|// hadoop security AccessControlException
if|if
condition|(
operator|(
name|e
operator|instanceof
name|MetaException
operator|&&
name|e
operator|.
name|getCause
argument_list|()
operator|instanceof
name|AccessControlException
operator|)
operator|||
name|ExceptionUtils
operator|.
name|getRootCause
argument_list|(
name|e
argument_list|)
operator|instanceof
name|AccessControlException
condition|)
block|{
return|return
literal|true
return|;
block|}
comment|// java security AccessControlException
if|if
condition|(
operator|(
name|e
operator|instanceof
name|MetaException
operator|&&
name|e
operator|.
name|getCause
argument_list|()
operator|instanceof
name|java
operator|.
name|security
operator|.
name|AccessControlException
operator|)
operator|||
name|ExceptionUtils
operator|.
name|getRootCause
argument_list|(
name|e
argument_list|)
operator|instanceof
name|java
operator|.
name|security
operator|.
name|AccessControlException
condition|)
block|{
return|return
literal|true
return|;
block|}
comment|// metastore in some cases sets the AccessControlException as message instead of wrapping the exception
return|return
name|e
operator|instanceof
name|MetaException
operator|&&
name|e
operator|.
name|getMessage
argument_list|()
operator|.
name|startsWith
argument_list|(
literal|"java.security.AccessControlException: Permission denied"
argument_list|)
return|;
block|}
comment|/**    * Generates a set compaction commands to run on pre Hive 3 cluster    */
specifier|private
specifier|static
name|void
name|makeCompactionScript
parameter_list|(
name|List
argument_list|<
name|String
argument_list|>
name|commands
parameter_list|,
name|String
name|scriptLocation
parameter_list|,
name|CompactionMetaInfo
name|compactionMetaInfo
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|commands
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"No compaction is necessary"
argument_list|)
expr_stmt|;
return|return;
block|}
name|String
name|fileName
init|=
literal|"compacts_"
operator|+
name|System
operator|.
name|currentTimeMillis
argument_list|()
operator|+
literal|".sql"
decl_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Writing compaction commands to "
operator|+
name|fileName
argument_list|)
expr_stmt|;
try|try
init|(
name|PrintWriter
name|pw
init|=
name|createScript
argument_list|(
name|commands
argument_list|,
name|fileName
argument_list|,
name|scriptLocation
argument_list|)
init|)
block|{
comment|//add post script
name|pw
operator|.
name|println
argument_list|(
literal|"-- Generated total of "
operator|+
name|commands
operator|.
name|size
argument_list|()
operator|+
literal|" compaction commands"
argument_list|)
expr_stmt|;
if|if
condition|(
name|compactionMetaInfo
operator|.
name|numberOfBytes
operator|<
name|Math
operator|.
name|pow
argument_list|(
literal|2
argument_list|,
literal|20
argument_list|)
condition|)
block|{
comment|//to see it working in UTs
name|pw
operator|.
name|println
argument_list|(
literal|"-- The total volume of data to be compacted is "
operator|+
name|String
operator|.
name|format
argument_list|(
literal|"%.6fMB"
argument_list|,
name|compactionMetaInfo
operator|.
name|numberOfBytes
operator|/
name|Math
operator|.
name|pow
argument_list|(
literal|2
argument_list|,
literal|20
argument_list|)
argument_list|)
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|pw
operator|.
name|println
argument_list|(
literal|"-- The total volume of data to be compacted is "
operator|+
name|String
operator|.
name|format
argument_list|(
literal|"%.3fGB"
argument_list|,
name|compactionMetaInfo
operator|.
name|numberOfBytes
operator|/
name|Math
operator|.
name|pow
argument_list|(
literal|2
argument_list|,
literal|30
argument_list|)
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|pw
operator|.
name|println
argument_list|()
expr_stmt|;
comment|//todo: should be at the top of the file...
name|pw
operator|.
name|println
argument_list|(
literal|"-- Please note that compaction may be a heavyweight and time consuming process.\n"
operator|+
literal|"-- Submitting all of these commands will enqueue them to a scheduling queue from\n"
operator|+
literal|"-- which they will be picked up by compactor Workers.  The max number of\n"
operator|+
literal|"-- concurrent Workers is controlled by hive.compactor.worker.threads configured\n"
operator|+
literal|"-- for the standalone metastore process.  Compaction itself is a Map-Reduce job\n"
operator|+
literal|"-- which is submitted to the YARN queue identified by hive.compactor.job.queue\n"
operator|+
literal|"-- property if defined or 'default' if not defined.  It's advisable to set the\n"
operator|+
literal|"-- capacity of this queue appropriately"
argument_list|)
expr_stmt|;
block|}
block|}
specifier|private
specifier|static
name|PrintWriter
name|createScript
parameter_list|(
name|List
argument_list|<
name|String
argument_list|>
name|commands
parameter_list|,
name|String
name|fileName
parameter_list|,
name|String
name|scriptLocation
parameter_list|)
throws|throws
name|IOException
block|{
name|FileWriter
name|fw
init|=
operator|new
name|FileWriter
argument_list|(
name|scriptLocation
operator|+
literal|"/"
operator|+
name|fileName
argument_list|)
decl_stmt|;
name|PrintWriter
name|pw
init|=
operator|new
name|PrintWriter
argument_list|(
name|fw
argument_list|)
decl_stmt|;
for|for
control|(
name|String
name|cmd
range|:
name|commands
control|)
block|{
name|pw
operator|.
name|println
argument_list|(
name|cmd
operator|+
literal|";"
argument_list|)
expr_stmt|;
block|}
return|return
name|pw
return|;
block|}
comment|/**    * @return any compaction commands to run for {@code Table t}    */
specifier|private
specifier|static
name|List
argument_list|<
name|String
argument_list|>
name|getCompactionCommands
parameter_list|(
name|Table
name|t
parameter_list|,
name|HiveConf
name|conf
parameter_list|,
name|IMetaStoreClient
name|hms
parameter_list|,
name|CompactionMetaInfo
name|compactionMetaInfo
parameter_list|,
name|boolean
name|execute
parameter_list|,
name|Hive
name|db
parameter_list|,
name|ValidTxnList
name|txns
parameter_list|)
throws|throws
name|IOException
throws|,
name|TException
throws|,
name|HiveException
block|{
if|if
condition|(
operator|!
name|isFullAcidTable
argument_list|(
name|t
argument_list|)
condition|)
block|{
return|return
name|Collections
operator|.
name|emptyList
argument_list|()
return|;
block|}
if|if
condition|(
name|t
operator|.
name|getPartitionKeysSize
argument_list|()
operator|<=
literal|0
condition|)
block|{
comment|//not partitioned
if|if
condition|(
operator|!
name|needsCompaction
argument_list|(
operator|new
name|Path
argument_list|(
name|t
operator|.
name|getSd
argument_list|()
operator|.
name|getLocation
argument_list|()
argument_list|)
argument_list|,
name|conf
argument_list|,
name|compactionMetaInfo
argument_list|,
name|txns
argument_list|)
condition|)
block|{
return|return
name|Collections
operator|.
name|emptyList
argument_list|()
return|;
block|}
name|List
argument_list|<
name|String
argument_list|>
name|cmds
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|()
decl_stmt|;
name|cmds
operator|.
name|add
argument_list|(
name|getCompactionCommand
argument_list|(
name|t
argument_list|,
literal|null
argument_list|)
argument_list|)
expr_stmt|;
if|if
condition|(
name|execute
condition|)
block|{
name|scheduleCompaction
argument_list|(
name|t
argument_list|,
literal|null
argument_list|,
name|db
argument_list|,
name|compactionMetaInfo
argument_list|)
expr_stmt|;
block|}
return|return
name|cmds
return|;
block|}
name|List
argument_list|<
name|String
argument_list|>
name|partNames
init|=
name|hms
operator|.
name|listPartitionNames
argument_list|(
name|t
operator|.
name|getDbName
argument_list|()
argument_list|,
name|t
operator|.
name|getTableName
argument_list|()
argument_list|,
operator|(
name|short
operator|)
operator|-
literal|1
argument_list|)
decl_stmt|;
name|int
name|batchSize
init|=
name|PARTITION_BATCH_SIZE
decl_stmt|;
name|int
name|numWholeBatches
init|=
name|partNames
operator|.
name|size
argument_list|()
operator|/
name|batchSize
decl_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|compactionCommands
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|()
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|numWholeBatches
condition|;
name|i
operator|++
control|)
block|{
name|List
argument_list|<
name|Partition
argument_list|>
name|partitionList
init|=
name|hms
operator|.
name|getPartitionsByNames
argument_list|(
name|t
operator|.
name|getDbName
argument_list|()
argument_list|,
name|t
operator|.
name|getTableName
argument_list|()
argument_list|,
name|partNames
operator|.
name|subList
argument_list|(
name|i
operator|*
name|batchSize
argument_list|,
operator|(
name|i
operator|+
literal|1
operator|)
operator|*
name|batchSize
argument_list|)
argument_list|)
decl_stmt|;
name|getCompactionCommands
argument_list|(
name|t
argument_list|,
name|partitionList
argument_list|,
name|db
argument_list|,
name|execute
argument_list|,
name|compactionCommands
argument_list|,
name|compactionMetaInfo
argument_list|,
name|conf
argument_list|,
name|txns
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|numWholeBatches
operator|*
name|batchSize
operator|<
name|partNames
operator|.
name|size
argument_list|()
condition|)
block|{
comment|//last partial batch
name|List
argument_list|<
name|Partition
argument_list|>
name|partitionList
init|=
name|hms
operator|.
name|getPartitionsByNames
argument_list|(
name|t
operator|.
name|getDbName
argument_list|()
argument_list|,
name|t
operator|.
name|getTableName
argument_list|()
argument_list|,
name|partNames
operator|.
name|subList
argument_list|(
name|numWholeBatches
operator|*
name|batchSize
argument_list|,
name|partNames
operator|.
name|size
argument_list|()
argument_list|)
argument_list|)
decl_stmt|;
name|getCompactionCommands
argument_list|(
name|t
argument_list|,
name|partitionList
argument_list|,
name|db
argument_list|,
name|execute
argument_list|,
name|compactionCommands
argument_list|,
name|compactionMetaInfo
argument_list|,
name|conf
argument_list|,
name|txns
argument_list|)
expr_stmt|;
block|}
return|return
name|compactionCommands
return|;
block|}
specifier|private
specifier|static
name|void
name|getCompactionCommands
parameter_list|(
name|Table
name|t
parameter_list|,
name|List
argument_list|<
name|Partition
argument_list|>
name|partitionList
parameter_list|,
name|Hive
name|db
parameter_list|,
name|boolean
name|execute
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|compactionCommands
parameter_list|,
name|CompactionMetaInfo
name|compactionMetaInfo
parameter_list|,
name|HiveConf
name|conf
parameter_list|,
name|ValidTxnList
name|txns
parameter_list|)
throws|throws
name|IOException
throws|,
name|TException
throws|,
name|HiveException
block|{
for|for
control|(
name|Partition
name|p
range|:
name|partitionList
control|)
block|{
if|if
condition|(
name|needsCompaction
argument_list|(
operator|new
name|Path
argument_list|(
name|p
operator|.
name|getSd
argument_list|()
operator|.
name|getLocation
argument_list|()
argument_list|)
argument_list|,
name|conf
argument_list|,
name|compactionMetaInfo
argument_list|,
name|txns
argument_list|)
condition|)
block|{
name|compactionCommands
operator|.
name|add
argument_list|(
name|getCompactionCommand
argument_list|(
name|t
argument_list|,
name|p
argument_list|)
argument_list|)
expr_stmt|;
if|if
condition|(
name|execute
condition|)
block|{
name|scheduleCompaction
argument_list|(
name|t
argument_list|,
name|p
argument_list|,
name|db
argument_list|,
name|compactionMetaInfo
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
specifier|private
specifier|static
name|void
name|scheduleCompaction
parameter_list|(
name|Table
name|t
parameter_list|,
name|Partition
name|p
parameter_list|,
name|Hive
name|db
parameter_list|,
name|CompactionMetaInfo
name|compactionMetaInfo
parameter_list|)
throws|throws
name|HiveException
throws|,
name|MetaException
block|{
name|String
name|partName
init|=
name|p
operator|==
literal|null
condition|?
literal|null
else|:
name|Warehouse
operator|.
name|makePartName
argument_list|(
name|t
operator|.
name|getPartitionKeys
argument_list|()
argument_list|,
name|p
operator|.
name|getValues
argument_list|()
argument_list|)
decl_stmt|;
name|CompactionResponse
name|resp
init|=
comment|//this gives an easy way to get at compaction ID so we can only wait for those this
comment|//utility started
name|db
operator|.
name|compact2
argument_list|(
name|t
operator|.
name|getDbName
argument_list|()
argument_list|,
name|t
operator|.
name|getTableName
argument_list|()
argument_list|,
name|partName
argument_list|,
literal|"major"
argument_list|,
literal|null
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|resp
operator|.
name|isAccepted
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
name|Warehouse
operator|.
name|getQualifiedName
argument_list|(
name|t
argument_list|)
operator|+
operator|(
name|p
operator|==
literal|null
condition|?
literal|""
else|:
literal|"/"
operator|+
name|partName
operator|)
operator|+
literal|" is already being compacted with id="
operator|+
name|resp
operator|.
name|getId
argument_list|()
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Scheduled compaction for "
operator|+
name|Warehouse
operator|.
name|getQualifiedName
argument_list|(
name|t
argument_list|)
operator|+
operator|(
name|p
operator|==
literal|null
condition|?
literal|""
else|:
literal|"/"
operator|+
name|partName
operator|)
operator|+
literal|" with id="
operator|+
name|resp
operator|.
name|getId
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|compactionMetaInfo
operator|.
name|compactionIds
operator|.
name|add
argument_list|(
name|resp
operator|.
name|getId
argument_list|()
argument_list|)
expr_stmt|;
block|}
comment|/**    *    * @param location - path to a partition (or table if not partitioned) dir    */
specifier|private
specifier|static
name|boolean
name|needsCompaction
parameter_list|(
name|Path
name|location
parameter_list|,
name|HiveConf
name|conf
parameter_list|,
name|CompactionMetaInfo
name|compactionMetaInfo
parameter_list|,
name|ValidTxnList
name|txns
parameter_list|)
throws|throws
name|IOException
block|{
name|FileSystem
name|fs
init|=
name|location
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|FileStatus
index|[]
name|deltas
init|=
name|fs
operator|.
name|listStatus
argument_list|(
name|location
argument_list|,
operator|new
name|PathFilter
argument_list|()
block|{
annotation|@
name|Override
specifier|public
name|boolean
name|accept
parameter_list|(
name|Path
name|path
parameter_list|)
block|{
comment|//checking for delete_delta is only so that this functionality can be exercised by code 3.0
comment|//which cannot produce any deltas with mix of update/insert events
return|return
name|path
operator|.
name|getName
argument_list|()
operator|.
name|startsWith
argument_list|(
literal|"delta_"
argument_list|)
operator|||
name|path
operator|.
name|getName
argument_list|()
operator|.
name|startsWith
argument_list|(
literal|"delete_delta_"
argument_list|)
return|;
block|}
block|}
argument_list|)
decl_stmt|;
if|if
condition|(
name|deltas
operator|==
literal|null
operator|||
name|deltas
operator|.
name|length
operator|==
literal|0
condition|)
block|{
comment|//base_n cannot contain update/delete.  Original files are all 'insert' and we need to compact
comment|//only if there are update/delete events.
return|return
literal|false
return|;
block|}
comment|/*getAcidState() is smart not to return any deltas in current if there is a base that covers     * them, i.e. if they were compacted but not yet cleaned.  This means re-checking if     * compaction is needed should cheap(er)*/
name|AcidUtils
operator|.
name|Directory
name|dir
init|=
name|AcidUtils
operator|.
name|getAcidState
argument_list|(
name|location
argument_list|,
name|conf
argument_list|,
name|txns
argument_list|)
decl_stmt|;
name|deltaLoop
label|:
for|for
control|(
name|AcidUtils
operator|.
name|ParsedDelta
name|delta
range|:
name|dir
operator|.
name|getCurrentDirectories
argument_list|()
control|)
block|{
name|FileStatus
index|[]
name|buckets
init|=
name|fs
operator|.
name|listStatus
argument_list|(
name|delta
operator|.
name|getPath
argument_list|()
argument_list|,
operator|new
name|PathFilter
argument_list|()
block|{
annotation|@
name|Override
specifier|public
name|boolean
name|accept
parameter_list|(
name|Path
name|path
parameter_list|)
block|{
comment|//since this is inside a delta dir created by Hive 2.x or earlier it can only contain
comment|//bucket_x or bucket_x__flush_length
return|return
name|path
operator|.
name|getName
argument_list|()
operator|.
name|startsWith
argument_list|(
literal|"bucket_"
argument_list|)
return|;
block|}
block|}
argument_list|)
decl_stmt|;
for|for
control|(
name|FileStatus
name|bucket
range|:
name|buckets
control|)
block|{
if|if
condition|(
name|bucket
operator|.
name|getPath
argument_list|()
operator|.
name|getName
argument_list|()
operator|.
name|endsWith
argument_list|(
literal|"_flush_length"
argument_list|)
condition|)
block|{
comment|//streaming ingest dir - cannot have update/delete events
continue|continue
name|deltaLoop
continue|;
block|}
if|if
condition|(
name|needsCompaction
argument_list|(
name|bucket
argument_list|,
name|fs
argument_list|)
condition|)
block|{
comment|//found delete events - this 'location' needs compacting
name|compactionMetaInfo
operator|.
name|numberOfBytes
operator|+=
name|getDataSize
argument_list|(
name|location
argument_list|,
name|conf
argument_list|)
expr_stmt|;
comment|//if there are un-compacted original files, they will be included in compaction, so
comment|//count at the size for 'cost' estimation later
for|for
control|(
name|HadoopShims
operator|.
name|HdfsFileStatusWithId
name|origFile
range|:
name|dir
operator|.
name|getOriginalFiles
argument_list|()
control|)
block|{
name|FileStatus
name|fileStatus
init|=
name|origFile
operator|.
name|getFileStatus
argument_list|()
decl_stmt|;
if|if
condition|(
name|fileStatus
operator|!=
literal|null
condition|)
block|{
name|compactionMetaInfo
operator|.
name|numberOfBytes
operator|+=
name|fileStatus
operator|.
name|getLen
argument_list|()
expr_stmt|;
block|}
block|}
return|return
literal|true
return|;
block|}
block|}
block|}
return|return
literal|false
return|;
block|}
comment|/**    * @param location - path to a partition (or table if not partitioned) dir    */
specifier|private
specifier|static
name|long
name|getDataSize
parameter_list|(
name|Path
name|location
parameter_list|,
name|HiveConf
name|conf
parameter_list|)
throws|throws
name|IOException
block|{
name|FileSystem
name|fs
init|=
name|location
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|ContentSummary
name|cs
init|=
name|fs
operator|.
name|getContentSummary
argument_list|(
name|location
argument_list|)
decl_stmt|;
return|return
name|cs
operator|.
name|getLength
argument_list|()
return|;
block|}
specifier|private
specifier|static
specifier|final
name|Charset
name|utf8
init|=
name|Charset
operator|.
name|forName
argument_list|(
literal|"UTF-8"
argument_list|)
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|CharsetDecoder
name|utf8Decoder
init|=
name|utf8
operator|.
name|newDecoder
argument_list|()
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|String
name|ACID_STATS
init|=
literal|"hive.acid.stats"
decl_stmt|;
specifier|private
specifier|static
name|boolean
name|needsCompaction
parameter_list|(
name|FileStatus
name|bucket
parameter_list|,
name|FileSystem
name|fs
parameter_list|)
throws|throws
name|IOException
block|{
comment|//create reader, look at footer
comment|//no need to check side file since it can only be in a streaming ingest delta
name|Reader
name|orcReader
init|=
name|OrcFile
operator|.
name|createReader
argument_list|(
name|bucket
operator|.
name|getPath
argument_list|()
argument_list|,
name|OrcFile
operator|.
name|readerOptions
argument_list|(
name|fs
operator|.
name|getConf
argument_list|()
argument_list|)
operator|.
name|filesystem
argument_list|(
name|fs
argument_list|)
argument_list|)
decl_stmt|;
if|if
condition|(
name|orcReader
operator|.
name|hasMetadataValue
argument_list|(
name|ACID_STATS
argument_list|)
condition|)
block|{
try|try
block|{
name|ByteBuffer
name|val
init|=
name|orcReader
operator|.
name|getMetadataValue
argument_list|(
name|ACID_STATS
argument_list|)
operator|.
name|duplicate
argument_list|()
decl_stmt|;
name|String
name|acidStats
init|=
name|utf8Decoder
operator|.
name|decode
argument_list|(
name|val
argument_list|)
operator|.
name|toString
argument_list|()
decl_stmt|;
name|String
index|[]
name|parts
init|=
name|acidStats
operator|.
name|split
argument_list|(
literal|","
argument_list|)
decl_stmt|;
name|long
name|updates
init|=
name|Long
operator|.
name|parseLong
argument_list|(
name|parts
index|[
literal|1
index|]
argument_list|)
decl_stmt|;
name|long
name|deletes
init|=
name|Long
operator|.
name|parseLong
argument_list|(
name|parts
index|[
literal|2
index|]
argument_list|)
decl_stmt|;
return|return
name|deletes
operator|>
literal|0
operator|||
name|updates
operator|>
literal|0
return|;
block|}
catch|catch
parameter_list|(
name|CharacterCodingException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
literal|"Bad string encoding for "
operator|+
name|ACID_STATS
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
else|else
block|{
throw|throw
operator|new
name|IllegalStateException
argument_list|(
literal|"AcidStats missing in "
operator|+
name|bucket
operator|.
name|getPath
argument_list|()
argument_list|)
throw|;
block|}
block|}
specifier|private
specifier|static
name|String
name|getCompactionCommand
parameter_list|(
name|Table
name|t
parameter_list|,
name|Partition
name|p
parameter_list|)
block|{
name|StringBuilder
name|sb
init|=
operator|new
name|StringBuilder
argument_list|(
literal|"ALTER TABLE "
argument_list|)
operator|.
name|append
argument_list|(
name|Warehouse
operator|.
name|getQualifiedName
argument_list|(
name|t
argument_list|)
argument_list|)
decl_stmt|;
if|if
condition|(
name|t
operator|.
name|getPartitionKeysSize
argument_list|()
operator|>
literal|0
condition|)
block|{
assert|assert
name|p
operator|!=
literal|null
operator|:
literal|"must supply partition for partitioned table "
operator|+
name|Warehouse
operator|.
name|getQualifiedName
argument_list|(
name|t
argument_list|)
assert|;
name|sb
operator|.
name|append
argument_list|(
literal|" PARTITION("
argument_list|)
expr_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|t
operator|.
name|getPartitionKeysSize
argument_list|()
condition|;
name|i
operator|++
control|)
block|{
name|sb
operator|.
name|append
argument_list|(
name|t
operator|.
name|getPartitionKeys
argument_list|()
operator|.
name|get
argument_list|(
name|i
argument_list|)
operator|.
name|getName
argument_list|()
argument_list|)
operator|.
name|append
argument_list|(
literal|'='
argument_list|)
operator|.
name|append
argument_list|(
name|genPartValueString
argument_list|(
name|t
operator|.
name|getPartitionKeys
argument_list|()
operator|.
name|get
argument_list|(
name|i
argument_list|)
operator|.
name|getType
argument_list|()
argument_list|,
name|p
operator|.
name|getValues
argument_list|()
operator|.
name|get
argument_list|(
name|i
argument_list|)
argument_list|)
argument_list|)
operator|.
name|append
argument_list|(
literal|","
argument_list|)
expr_stmt|;
block|}
name|sb
operator|.
name|setCharAt
argument_list|(
name|sb
operator|.
name|length
argument_list|()
operator|-
literal|1
argument_list|,
literal|')'
argument_list|)
expr_stmt|;
comment|//replace trailing ','
block|}
return|return
name|sb
operator|.
name|append
argument_list|(
literal|" COMPACT 'major'"
argument_list|)
operator|.
name|toString
argument_list|()
return|;
block|}
comment|/**    * This is copy-pasted from {@link org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer},    * which can't be refactored since this is linked against Hive 2.x    */
specifier|private
specifier|static
name|String
name|genPartValueString
parameter_list|(
name|String
name|partColType
parameter_list|,
name|String
name|partVal
parameter_list|)
block|{
name|String
name|returnVal
init|=
name|partVal
decl_stmt|;
if|if
condition|(
name|partColType
operator|.
name|equals
argument_list|(
name|serdeConstants
operator|.
name|STRING_TYPE_NAME
argument_list|)
operator|||
name|partColType
operator|.
name|contains
argument_list|(
name|serdeConstants
operator|.
name|VARCHAR_TYPE_NAME
argument_list|)
operator|||
name|partColType
operator|.
name|contains
argument_list|(
name|serdeConstants
operator|.
name|CHAR_TYPE_NAME
argument_list|)
condition|)
block|{
name|returnVal
operator|=
literal|"'"
operator|+
name|escapeSQLString
argument_list|(
name|partVal
argument_list|)
operator|+
literal|"'"
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|partColType
operator|.
name|equals
argument_list|(
name|serdeConstants
operator|.
name|TINYINT_TYPE_NAME
argument_list|)
condition|)
block|{
name|returnVal
operator|=
name|partVal
operator|+
literal|"Y"
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|partColType
operator|.
name|equals
argument_list|(
name|serdeConstants
operator|.
name|SMALLINT_TYPE_NAME
argument_list|)
condition|)
block|{
name|returnVal
operator|=
name|partVal
operator|+
literal|"S"
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|partColType
operator|.
name|equals
argument_list|(
name|serdeConstants
operator|.
name|INT_TYPE_NAME
argument_list|)
condition|)
block|{
name|returnVal
operator|=
name|partVal
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|partColType
operator|.
name|equals
argument_list|(
name|serdeConstants
operator|.
name|BIGINT_TYPE_NAME
argument_list|)
condition|)
block|{
name|returnVal
operator|=
name|partVal
operator|+
literal|"L"
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|partColType
operator|.
name|contains
argument_list|(
name|serdeConstants
operator|.
name|DECIMAL_TYPE_NAME
argument_list|)
condition|)
block|{
name|returnVal
operator|=
name|partVal
operator|+
literal|"BD"
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|partColType
operator|.
name|equals
argument_list|(
name|serdeConstants
operator|.
name|DATE_TYPE_NAME
argument_list|)
operator|||
name|partColType
operator|.
name|equals
argument_list|(
name|serdeConstants
operator|.
name|TIMESTAMP_TYPE_NAME
argument_list|)
condition|)
block|{
name|returnVal
operator|=
name|partColType
operator|+
literal|" '"
operator|+
name|escapeSQLString
argument_list|(
name|partVal
argument_list|)
operator|+
literal|"'"
expr_stmt|;
block|}
else|else
block|{
comment|//for other usually not used types, just quote the value
name|returnVal
operator|=
literal|"'"
operator|+
name|escapeSQLString
argument_list|(
name|partVal
argument_list|)
operator|+
literal|"'"
expr_stmt|;
block|}
return|return
name|returnVal
return|;
block|}
specifier|private
specifier|static
name|boolean
name|isFullAcidTable
parameter_list|(
name|Table
name|t
parameter_list|)
block|{
if|if
condition|(
name|t
operator|.
name|getParametersSize
argument_list|()
operator|<=
literal|0
condition|)
block|{
comment|//cannot be acid
return|return
literal|false
return|;
block|}
name|String
name|transacationalValue
init|=
name|t
operator|.
name|getParameters
argument_list|()
operator|.
name|get
argument_list|(
name|hive_metastoreConstants
operator|.
name|TABLE_IS_TRANSACTIONAL
argument_list|)
decl_stmt|;
if|if
condition|(
name|transacationalValue
operator|!=
literal|null
operator|&&
literal|"true"
operator|.
name|equalsIgnoreCase
argument_list|(
name|transacationalValue
argument_list|)
condition|)
block|{
name|System
operator|.
name|out
operator|.
name|println
argument_list|(
literal|"Found Acid table: "
operator|+
name|Warehouse
operator|.
name|getQualifiedName
argument_list|(
name|t
argument_list|)
argument_list|)
expr_stmt|;
return|return
literal|true
return|;
block|}
return|return
literal|false
return|;
block|}
specifier|private
specifier|static
name|boolean
name|isAcidEnabled
parameter_list|(
name|HiveConf
name|hiveConf
parameter_list|)
block|{
name|String
name|txnMgr
init|=
name|hiveConf
operator|.
name|getVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_TXN_MANAGER
argument_list|)
decl_stmt|;
name|boolean
name|concurrency
init|=
name|hiveConf
operator|.
name|getBoolVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_SUPPORT_CONCURRENCY
argument_list|)
decl_stmt|;
name|String
name|dbTxnMgr
init|=
literal|"org.apache.hadoop.hive.ql.lockmgr.DbTxnManager"
decl_stmt|;
return|return
name|txnMgr
operator|.
name|equals
argument_list|(
name|dbTxnMgr
argument_list|)
operator|&&
name|concurrency
return|;
block|}
specifier|private
specifier|static
class|class
name|CompactionMetaInfo
block|{
comment|/**      * total number of bytes to be compacted across all compaction commands      */
name|long
name|numberOfBytes
decl_stmt|;
comment|/**      * IDs of compactions launched by this utility      */
name|Set
argument_list|<
name|Long
argument_list|>
name|compactionIds
init|=
operator|new
name|HashSet
argument_list|<>
argument_list|()
decl_stmt|;
block|}
annotation|@
name|VisibleForTesting
specifier|static
specifier|abstract
class|class
name|Callback
block|{
comment|/**      * This is a hack enable Unit testing.  Derby can't handle multiple concurrent threads but      * somehow Compactor needs to run to test "execute" mode.  This callback can be used      * to run Worker.  For TESTING ONLY.      */
name|void
name|onWaitForCompaction
parameter_list|()
throws|throws
name|MetaException
block|{}
block|}
annotation|@
name|VisibleForTesting
specifier|static
name|Callback
name|callback
decl_stmt|;
annotation|@
name|VisibleForTesting
specifier|static
name|int
name|pollIntervalMs
init|=
literal|1000
operator|*
literal|30
decl_stmt|;
comment|/**    * can set it from tests to test when config needs something other than default values    */
annotation|@
name|VisibleForTesting
specifier|static
name|HiveConf
name|hiveConf
init|=
literal|null
decl_stmt|;
block|}
end_class

end_unit

