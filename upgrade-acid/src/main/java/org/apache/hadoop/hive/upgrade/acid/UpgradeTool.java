begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/*  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|upgrade
operator|.
name|acid
package|;
end_package

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|annotations
operator|.
name|VisibleForTesting
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|cli
operator|.
name|CommandLine
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|cli
operator|.
name|CommandLineParser
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|cli
operator|.
name|GnuParser
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|cli
operator|.
name|HelpFormatter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|cli
operator|.
name|Option
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|cli
operator|.
name|Options
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|cli
operator|.
name|ParseException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|ContentSummary
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileStatus
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|PathFilter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|ValidTxnList
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|HiveMetaStoreClient
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|TableType
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|Warehouse
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|CompactionResponse
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|InvalidOperationException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|MetaException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Partition
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|ShowCompactResponse
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|ShowCompactResponseElement
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|StorageDescriptor
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Table
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|hive_metastoreConstants
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|txn
operator|.
name|TxnStore
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|txn
operator|.
name|TxnUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|io
operator|.
name|AcidUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|Hive
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|HiveException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde
operator|.
name|serdeConstants
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|shims
operator|.
name|HadoopShims
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hive
operator|.
name|common
operator|.
name|util
operator|.
name|HiveVersionInfo
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|orc
operator|.
name|OrcFile
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|orc
operator|.
name|Reader
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|orc
operator|.
name|impl
operator|.
name|AcidStats
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|orc
operator|.
name|impl
operator|.
name|OrcAcidUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|thrift
operator|.
name|TException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|Logger
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|LoggerFactory
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|FileWriter
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|PrintWriter
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Collections
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashSet
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Set
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|parse
operator|.
name|BaseSemanticAnalyzer
operator|.
name|escapeSQLString
import|;
end_import

begin_comment
comment|/**  * This utility is designed to help with upgrading to Hive 3.0.  On-disk layout for transactional  * tables has changed in 3.0 and require pre-processing before upgrade to ensure they are readable  * by Hive 3.0.  Some transactional tables (identified by this utility) require Major compaction  * to be run on them before upgrading to 3.0.  Once this compaction starts, no more  * update/delete/merge statements may be executed on these tables until upgrade is finished.  *  * Additionally, a new type of transactional tables was added in 3.0 - insert-only tables.  These  * tables support ACID semantics and work with any Input/OutputFormat.  Any Managed tables may  * be made insert-only transactional table. These tables don't support Update/Delete/Merge commands.  *  * This utility works in 2 modes: preUpgrade and postUpgrade.  * In preUpgrade mode it has to have 2.x Hive jars on the classpath.  It will perform analysis on  * existing transactional tables, determine which require compaction and generate a set of SQL  * commands to launch all of these compactions.  *  * Note that depending on the number of tables/partitions and amount of data in them compactions  * may take a significant amount of time and resources.  The script output by this utility includes  * some heuristics that may help estimate the time required.  If no script is produced, no action  * is needed.  For compactions to run an instance of standalone Hive Metastore must be running.  * Please make sure hive.compactor.worker.threads is sufficiently high - this specifies the limit  * of concurrent compactions that may be run.  Each compaction job is a Map-Reduce job.  * hive.compactor.job.queue may be used to set a Yarn queue ame where all compaction jobs will be  * submitted.  *  * In postUpgrade mode, Hive 3.0 jars/hive-site.xml should be on the classpath. This utility will  * find all the tables that may be made transactional (with ful CRUD support) and generate  * Alter Table commands to do so.  It will also find all tables that may not support full CRUD  * but can be made insert-only transactional tables and generate corresponding Alter Table commands.  *  * TODO: rename files  *  * "execute" option may be supplied in both modes to have the utility automatically execute the  * equivalent of the generated commands  *  * "location" option may be supplied followed by a path to set the location for the generated  * scripts.  */
end_comment

begin_class
specifier|public
class|class
name|UpgradeTool
block|{
specifier|private
specifier|static
specifier|final
name|Logger
name|LOG
init|=
name|LoggerFactory
operator|.
name|getLogger
argument_list|(
name|UpgradeTool
operator|.
name|class
argument_list|)
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|int
name|PARTITION_BATCH_SIZE
init|=
literal|10000
decl_stmt|;
specifier|private
specifier|final
name|Options
name|cmdLineOptions
init|=
operator|new
name|Options
argument_list|()
decl_stmt|;
specifier|public
specifier|static
name|void
name|main
parameter_list|(
name|String
index|[]
name|args
parameter_list|)
throws|throws
name|Exception
block|{
name|UpgradeTool
name|tool
init|=
operator|new
name|UpgradeTool
argument_list|()
decl_stmt|;
name|tool
operator|.
name|init
argument_list|()
expr_stmt|;
name|CommandLineParser
name|parser
init|=
operator|new
name|GnuParser
argument_list|()
decl_stmt|;
name|CommandLine
name|line
decl_stmt|;
name|String
name|outputDir
init|=
literal|"."
decl_stmt|;
name|boolean
name|preUpgrade
init|=
literal|false
decl_stmt|,
name|postUpgrade
init|=
literal|false
decl_stmt|,
name|execute
init|=
literal|false
decl_stmt|,
name|nonBlocking
init|=
literal|false
decl_stmt|;
try|try
block|{
name|line
operator|=
name|parser
operator|.
name|parse
argument_list|(
name|tool
operator|.
name|cmdLineOptions
argument_list|,
name|args
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|ParseException
name|e
parameter_list|)
block|{
name|System
operator|.
name|err
operator|.
name|println
argument_list|(
literal|"UpgradeTool: Parsing failed.  Reason: "
operator|+
name|e
operator|.
name|getLocalizedMessage
argument_list|()
argument_list|)
expr_stmt|;
name|printAndExit
argument_list|(
name|tool
argument_list|)
expr_stmt|;
return|return;
block|}
if|if
condition|(
name|line
operator|.
name|hasOption
argument_list|(
literal|"help"
argument_list|)
condition|)
block|{
name|HelpFormatter
name|formatter
init|=
operator|new
name|HelpFormatter
argument_list|()
decl_stmt|;
name|formatter
operator|.
name|printHelp
argument_list|(
literal|"upgrade-acid"
argument_list|,
name|tool
operator|.
name|cmdLineOptions
argument_list|)
expr_stmt|;
return|return;
block|}
if|if
condition|(
name|line
operator|.
name|hasOption
argument_list|(
literal|"location"
argument_list|)
condition|)
block|{
name|outputDir
operator|=
name|line
operator|.
name|getOptionValue
argument_list|(
literal|"location"
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|line
operator|.
name|hasOption
argument_list|(
literal|"execute"
argument_list|)
condition|)
block|{
name|execute
operator|=
literal|true
expr_stmt|;
block|}
if|if
condition|(
name|line
operator|.
name|hasOption
argument_list|(
literal|"preUpgrade"
argument_list|)
condition|)
block|{
name|preUpgrade
operator|=
literal|true
expr_stmt|;
block|}
if|if
condition|(
name|line
operator|.
name|hasOption
argument_list|(
literal|"postUpgrade"
argument_list|)
condition|)
block|{
name|postUpgrade
operator|=
literal|true
expr_stmt|;
block|}
name|LOG
operator|.
name|info
argument_list|(
literal|"Starting with preUpgrade="
operator|+
name|preUpgrade
operator|+
literal|", postUpgrade="
operator|+
name|postUpgrade
operator|+
literal|", execute="
operator|+
name|execute
operator|+
literal|", location="
operator|+
name|outputDir
argument_list|)
expr_stmt|;
if|if
condition|(
name|preUpgrade
operator|&&
name|postUpgrade
condition|)
block|{
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
literal|"Cannot specify both preUpgrade and postUpgrade"
argument_list|)
throw|;
block|}
try|try
block|{
name|String
name|hiveVer
init|=
name|HiveVersionInfo
operator|.
name|getShortVersion
argument_list|()
decl_stmt|;
if|if
condition|(
name|preUpgrade
condition|)
block|{
if|if
condition|(
operator|!
name|hiveVer
operator|.
name|startsWith
argument_list|(
literal|"2."
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|IllegalStateException
argument_list|(
literal|"preUpgrade requires Hive 2.x.  Actual: "
operator|+
name|hiveVer
argument_list|)
throw|;
block|}
block|}
if|if
condition|(
name|postUpgrade
operator|&&
name|execute
operator|&&
operator|!
name|isTestMode
condition|)
block|{
if|if
condition|(
operator|!
name|hiveVer
operator|.
name|startsWith
argument_list|(
literal|"3."
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|IllegalStateException
argument_list|(
literal|"postUpgrade w/execute requires Hive 3.x.  Actual: "
operator|+
name|hiveVer
argument_list|)
throw|;
block|}
block|}
name|tool
operator|.
name|prepareAcidUpgradeInternal
argument_list|(
name|outputDir
argument_list|,
name|preUpgrade
argument_list|,
name|postUpgrade
argument_list|,
name|execute
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|ex
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"UpgradeTool failed"
argument_list|,
name|ex
argument_list|)
expr_stmt|;
throw|throw
name|ex
throw|;
block|}
block|}
specifier|private
specifier|static
name|void
name|printAndExit
parameter_list|(
name|UpgradeTool
name|tool
parameter_list|)
block|{
name|HelpFormatter
name|formatter
init|=
operator|new
name|HelpFormatter
argument_list|()
decl_stmt|;
name|formatter
operator|.
name|printHelp
argument_list|(
literal|"upgrade-acid"
argument_list|,
name|tool
operator|.
name|cmdLineOptions
argument_list|)
expr_stmt|;
name|System
operator|.
name|exit
argument_list|(
literal|1
argument_list|)
expr_stmt|;
block|}
specifier|private
name|void
name|init
parameter_list|()
block|{
try|try
block|{
name|cmdLineOptions
operator|.
name|addOption
argument_list|(
operator|new
name|Option
argument_list|(
literal|"help"
argument_list|,
literal|"print this message"
argument_list|)
argument_list|)
expr_stmt|;
name|cmdLineOptions
operator|.
name|addOption
argument_list|(
operator|new
name|Option
argument_list|(
literal|"preUpgrade"
argument_list|,
literal|"Generates a script to execute on 2.x cluster.  This requires 2.x binaries"
operator|+
literal|" on the classpath and hive-site.xml."
argument_list|)
argument_list|)
expr_stmt|;
name|cmdLineOptions
operator|.
name|addOption
argument_list|(
operator|new
name|Option
argument_list|(
literal|"postUpgrade"
argument_list|,
literal|"Generates a script to execute on 3.x cluster.  This requires 3.x binaries"
operator|+
literal|" on the classpath and hive-site.xml."
argument_list|)
argument_list|)
expr_stmt|;
name|Option
name|exec
init|=
operator|new
name|Option
argument_list|(
literal|"execute"
argument_list|,
literal|"Executes commands equivalent to generated scrips"
argument_list|)
decl_stmt|;
name|exec
operator|.
name|setOptionalArg
argument_list|(
literal|true
argument_list|)
expr_stmt|;
name|cmdLineOptions
operator|.
name|addOption
argument_list|(
name|exec
argument_list|)
expr_stmt|;
name|cmdLineOptions
operator|.
name|addOption
argument_list|(
operator|new
name|Option
argument_list|(
literal|"location"
argument_list|,
literal|true
argument_list|,
literal|"Location to write scripts to. Default is CWD."
argument_list|)
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|ex
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"init()"
argument_list|,
name|ex
argument_list|)
expr_stmt|;
throw|throw
name|ex
throw|;
block|}
block|}
comment|/**    * todo: this should accept a file of table names to exclude from non-acid to acid conversion    * todo: change script comments to a preamble instead of a footer    *    * how does rename script work?  "hadoop fs -mv oldname newname"    * and what what about S3?    * How does this actually get executed?    * all other actions are done via embedded JDBC    *    *    */
specifier|private
name|void
name|prepareAcidUpgradeInternal
parameter_list|(
name|String
name|scriptLocation
parameter_list|,
name|boolean
name|preUpgrade
parameter_list|,
name|boolean
name|postUpgrade
parameter_list|,
name|boolean
name|execute
parameter_list|)
throws|throws
name|HiveException
throws|,
name|TException
throws|,
name|IOException
block|{
name|HiveConf
name|conf
init|=
name|hiveConf
operator|!=
literal|null
condition|?
name|hiveConf
else|:
operator|new
name|HiveConf
argument_list|()
decl_stmt|;
name|boolean
name|isAcidEnabled
init|=
name|isAcidEnabled
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|HiveMetaStoreClient
name|hms
init|=
operator|new
name|HiveMetaStoreClient
argument_list|(
name|conf
argument_list|)
decl_stmt|;
comment|//MetaException
name|LOG
operator|.
name|debug
argument_list|(
literal|"Looking for databases"
argument_list|)
expr_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|databases
init|=
name|hms
operator|.
name|getAllDatabases
argument_list|()
decl_stmt|;
comment|//TException
name|LOG
operator|.
name|debug
argument_list|(
literal|"Found "
operator|+
name|databases
operator|.
name|size
argument_list|()
operator|+
literal|" databases to process"
argument_list|)
expr_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|compactions
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|convertToAcid
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|convertToMM
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|()
decl_stmt|;
specifier|final
name|CompactionMetaInfo
name|compactionMetaInfo
init|=
operator|new
name|CompactionMetaInfo
argument_list|()
decl_stmt|;
name|ValidTxnList
name|txns
init|=
literal|null
decl_stmt|;
name|Hive
name|db
init|=
literal|null
decl_stmt|;
if|if
condition|(
name|execute
condition|)
block|{
name|db
operator|=
name|Hive
operator|.
name|get
argument_list|(
name|conf
argument_list|)
expr_stmt|;
block|}
for|for
control|(
name|String
name|dbName
range|:
name|databases
control|)
block|{
name|List
argument_list|<
name|String
argument_list|>
name|tables
init|=
name|hms
operator|.
name|getAllTables
argument_list|(
name|dbName
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"found "
operator|+
name|tables
operator|.
name|size
argument_list|()
operator|+
literal|" tables in "
operator|+
name|dbName
argument_list|)
expr_stmt|;
for|for
control|(
name|String
name|tableName
range|:
name|tables
control|)
block|{
name|Table
name|t
init|=
name|hms
operator|.
name|getTable
argument_list|(
name|dbName
argument_list|,
name|tableName
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"processing table "
operator|+
name|Warehouse
operator|.
name|getQualifiedName
argument_list|(
name|t
argument_list|)
argument_list|)
expr_stmt|;
if|if
condition|(
name|preUpgrade
operator|&&
name|isAcidEnabled
condition|)
block|{
comment|//if acid is off, there can't be any acid tables - nothing to compact
if|if
condition|(
name|execute
operator|&&
name|txns
operator|==
literal|null
condition|)
block|{
comment|/*            This API changed from 2.x to 3.0.  so this won't even compile with 3.0            but it doesn't need to since we only run this preUpgrade           */
name|TxnStore
name|txnHandler
init|=
name|TxnUtils
operator|.
name|getTxnStore
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|txns
operator|=
name|TxnUtils
operator|.
name|createValidCompactTxnList
argument_list|(
name|txnHandler
operator|.
name|getOpenTxnsInfo
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|List
argument_list|<
name|String
argument_list|>
name|compactionCommands
init|=
name|getCompactionCommands
argument_list|(
name|t
argument_list|,
name|conf
argument_list|,
name|hms
argument_list|,
name|compactionMetaInfo
argument_list|,
name|execute
argument_list|,
name|db
argument_list|,
name|txns
argument_list|)
decl_stmt|;
name|compactions
operator|.
name|addAll
argument_list|(
name|compactionCommands
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|postUpgrade
operator|&&
name|isAcidEnabled
condition|)
block|{
comment|//if acid is off post upgrade, you can't make any tables acid - will throw
name|processConversion
argument_list|(
name|t
argument_list|,
name|convertToAcid
argument_list|,
name|convertToMM
argument_list|,
name|hms
argument_list|,
name|db
argument_list|,
name|execute
argument_list|)
expr_stmt|;
block|}
comment|/*todo: handle renaming files somewhere*/
block|}
block|}
name|makeCompactionScript
argument_list|(
name|compactions
argument_list|,
name|scriptLocation
argument_list|,
name|compactionMetaInfo
argument_list|)
expr_stmt|;
name|makeConvertTableScript
argument_list|(
name|convertToAcid
argument_list|,
name|convertToMM
argument_list|,
name|scriptLocation
argument_list|)
expr_stmt|;
name|makeRenameFileScript
argument_list|(
name|scriptLocation
argument_list|)
expr_stmt|;
comment|//todo: is this pre or post upgrade?
comment|//todo: can different tables be in different FileSystems?
if|if
condition|(
name|preUpgrade
operator|&&
name|execute
condition|)
block|{
while|while
condition|(
name|compactionMetaInfo
operator|.
name|compactionIds
operator|.
name|size
argument_list|()
operator|>
literal|0
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Will wait for "
operator|+
name|compactionMetaInfo
operator|.
name|compactionIds
operator|.
name|size
argument_list|()
operator|+
literal|" compactions to complete"
argument_list|)
expr_stmt|;
name|ShowCompactResponse
name|resp
init|=
name|db
operator|.
name|showCompactions
argument_list|()
decl_stmt|;
for|for
control|(
name|ShowCompactResponseElement
name|e
range|:
name|resp
operator|.
name|getCompacts
argument_list|()
control|)
block|{
specifier|final
name|String
name|state
init|=
name|e
operator|.
name|getState
argument_list|()
decl_stmt|;
name|boolean
name|removed
decl_stmt|;
switch|switch
condition|(
name|state
condition|)
block|{
case|case
name|TxnStore
operator|.
name|CLEANING_RESPONSE
case|:
case|case
name|TxnStore
operator|.
name|SUCCEEDED_RESPONSE
case|:
name|removed
operator|=
name|compactionMetaInfo
operator|.
name|compactionIds
operator|.
name|remove
argument_list|(
name|e
operator|.
name|getId
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|removed
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Required compaction succeeded: "
operator|+
name|e
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
break|break;
case|case
name|TxnStore
operator|.
name|ATTEMPTED_RESPONSE
case|:
case|case
name|TxnStore
operator|.
name|FAILED_RESPONSE
case|:
name|removed
operator|=
name|compactionMetaInfo
operator|.
name|compactionIds
operator|.
name|remove
argument_list|(
name|e
operator|.
name|getId
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|removed
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Required compaction failed: "
operator|+
name|e
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
break|break;
case|case
name|TxnStore
operator|.
name|INITIATED_RESPONSE
case|:
comment|//may flood the log
comment|//LOG.debug("Still waiting  on: " + e.toString());
break|break;
case|case
name|TxnStore
operator|.
name|WORKING_RESPONSE
case|:
name|LOG
operator|.
name|debug
argument_list|(
literal|"Still working on: "
operator|+
name|e
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
break|break;
default|default:
comment|//shouldn't be any others
name|LOG
operator|.
name|error
argument_list|(
literal|"Unexpected state for : "
operator|+
name|e
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
name|compactionMetaInfo
operator|.
name|compactionIds
operator|.
name|size
argument_list|()
operator|>
literal|0
condition|)
block|{
try|try
block|{
if|if
condition|(
name|callback
operator|!=
literal|null
condition|)
block|{
name|callback
operator|.
name|onWaitForCompaction
argument_list|()
expr_stmt|;
block|}
name|Thread
operator|.
name|sleep
argument_list|(
name|pollIntervalMs
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|InterruptedException
name|ex
parameter_list|)
block|{
empty_stmt|;
comment|//this only responds to ^C
block|}
block|}
block|}
block|}
block|}
comment|/**    * Actualy makes the table transactional    */
specifier|private
specifier|static
name|void
name|alterTable
parameter_list|(
name|Table
name|t
parameter_list|,
name|Hive
name|db
parameter_list|,
name|boolean
name|isMM
parameter_list|)
throws|throws
name|HiveException
throws|,
name|InvalidOperationException
block|{
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|Table
name|metaTable
init|=
comment|//clone to make sure new prop doesn't leak
operator|new
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|Table
argument_list|(
name|t
operator|.
name|deepCopy
argument_list|()
argument_list|)
decl_stmt|;
name|metaTable
operator|.
name|getParameters
argument_list|()
operator|.
name|put
argument_list|(
name|hive_metastoreConstants
operator|.
name|TABLE_IS_TRANSACTIONAL
argument_list|,
literal|"true"
argument_list|)
expr_stmt|;
if|if
condition|(
name|isMM
condition|)
block|{
name|metaTable
operator|.
name|getParameters
argument_list|()
operator|.
name|put
argument_list|(
name|hive_metastoreConstants
operator|.
name|TABLE_TRANSACTIONAL_PROPERTIES
argument_list|,
literal|"insert_only"
argument_list|)
expr_stmt|;
block|}
name|db
operator|.
name|alterTable
argument_list|(
name|Warehouse
operator|.
name|getQualifiedName
argument_list|(
name|t
argument_list|)
argument_list|,
name|metaTable
argument_list|,
literal|false
argument_list|,
literal|null
argument_list|)
expr_stmt|;
block|}
comment|/**    * todo: handle exclusion list    * Figures out which tables to make Acid, MM and (optionally, performs the operation)    */
specifier|private
specifier|static
name|void
name|processConversion
parameter_list|(
name|Table
name|t
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|convertToAcid
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|convertToMM
parameter_list|,
name|HiveMetaStoreClient
name|hms
parameter_list|,
name|Hive
name|db
parameter_list|,
name|boolean
name|execute
parameter_list|)
throws|throws
name|TException
throws|,
name|HiveException
block|{
if|if
condition|(
name|isFullAcidTable
argument_list|(
name|t
argument_list|)
condition|)
block|{
return|return;
block|}
if|if
condition|(
operator|!
name|TableType
operator|.
name|MANAGED_TABLE
operator|.
name|name
argument_list|()
operator|.
name|equalsIgnoreCase
argument_list|(
name|t
operator|.
name|getTableType
argument_list|()
argument_list|)
condition|)
block|{
return|return;
block|}
name|String
name|fullTableName
init|=
name|Warehouse
operator|.
name|getQualifiedName
argument_list|(
name|t
argument_list|)
decl_stmt|;
if|if
condition|(
name|t
operator|.
name|getPartitionKeysSize
argument_list|()
operator|<=
literal|0
condition|)
block|{
if|if
condition|(
name|canBeMadeAcid
argument_list|(
name|fullTableName
argument_list|,
name|t
operator|.
name|getSd
argument_list|()
argument_list|)
condition|)
block|{
name|convertToAcid
operator|.
name|add
argument_list|(
literal|"ALTER TABLE "
operator|+
name|Warehouse
operator|.
name|getQualifiedName
argument_list|(
name|t
argument_list|)
operator|+
literal|" SET TBLPROPERTIES ("
operator|+
literal|"'transactional'='true')"
argument_list|)
expr_stmt|;
if|if
condition|(
name|execute
condition|)
block|{
name|alterTable
argument_list|(
name|t
argument_list|,
name|db
argument_list|,
literal|false
argument_list|)
expr_stmt|;
block|}
block|}
else|else
block|{
name|convertToMM
operator|.
name|add
argument_list|(
literal|"ALTER TABLE "
operator|+
name|Warehouse
operator|.
name|getQualifiedName
argument_list|(
name|t
argument_list|)
operator|+
literal|" SET TBLPROPERTIES ("
operator|+
literal|"'transactional'='true', 'transactional_properties'='insert_only')"
argument_list|)
expr_stmt|;
if|if
condition|(
name|execute
condition|)
block|{
name|alterTable
argument_list|(
name|t
argument_list|,
name|db
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
block|}
block|}
else|else
block|{
comment|/*         each Partition may have different I/O Format so have to check them all before deciding to         make a full CRUD table.         Run in batches to prevent OOM        */
name|List
argument_list|<
name|String
argument_list|>
name|partNames
init|=
name|hms
operator|.
name|listPartitionNames
argument_list|(
name|t
operator|.
name|getDbName
argument_list|()
argument_list|,
name|t
operator|.
name|getTableName
argument_list|()
argument_list|,
operator|(
name|short
operator|)
operator|-
literal|1
argument_list|)
decl_stmt|;
name|int
name|batchSize
init|=
name|PARTITION_BATCH_SIZE
decl_stmt|;
name|int
name|numWholeBatches
init|=
name|partNames
operator|.
name|size
argument_list|()
operator|/
name|batchSize
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|numWholeBatches
condition|;
name|i
operator|++
control|)
block|{
name|List
argument_list|<
name|Partition
argument_list|>
name|partitionList
init|=
name|hms
operator|.
name|getPartitionsByNames
argument_list|(
name|t
operator|.
name|getDbName
argument_list|()
argument_list|,
name|t
operator|.
name|getTableName
argument_list|()
argument_list|,
name|partNames
operator|.
name|subList
argument_list|(
name|i
operator|*
name|batchSize
argument_list|,
operator|(
name|i
operator|+
literal|1
operator|)
operator|*
name|batchSize
argument_list|)
argument_list|)
decl_stmt|;
if|if
condition|(
name|alterTable
argument_list|(
name|fullTableName
argument_list|,
name|partitionList
argument_list|,
name|convertToMM
argument_list|,
name|t
argument_list|,
name|db
argument_list|,
name|execute
argument_list|)
condition|)
block|{
return|return;
block|}
block|}
if|if
condition|(
name|numWholeBatches
operator|*
name|batchSize
operator|<
name|partNames
operator|.
name|size
argument_list|()
condition|)
block|{
comment|//last partial batch
name|List
argument_list|<
name|Partition
argument_list|>
name|partitionList
init|=
name|hms
operator|.
name|getPartitionsByNames
argument_list|(
name|t
operator|.
name|getDbName
argument_list|()
argument_list|,
name|t
operator|.
name|getTableName
argument_list|()
argument_list|,
name|partNames
operator|.
name|subList
argument_list|(
name|numWholeBatches
operator|*
name|batchSize
argument_list|,
name|partNames
operator|.
name|size
argument_list|()
argument_list|)
argument_list|)
decl_stmt|;
if|if
condition|(
name|alterTable
argument_list|(
name|fullTableName
argument_list|,
name|partitionList
argument_list|,
name|convertToMM
argument_list|,
name|t
argument_list|,
name|db
argument_list|,
name|execute
argument_list|)
condition|)
block|{
return|return;
block|}
block|}
comment|//if here checked all parts and they are Acid compatible - make it acid
name|convertToAcid
operator|.
name|add
argument_list|(
literal|"ALTER TABLE "
operator|+
name|Warehouse
operator|.
name|getQualifiedName
argument_list|(
name|t
argument_list|)
operator|+
literal|" SET TBLPROPERTIES ("
operator|+
literal|"'transactional'='true')"
argument_list|)
expr_stmt|;
if|if
condition|(
name|execute
condition|)
block|{
name|alterTable
argument_list|(
name|t
argument_list|,
name|db
argument_list|,
literal|false
argument_list|)
expr_stmt|;
block|}
block|}
block|}
comment|/**    * @return true if table was converted/command generated    */
specifier|private
specifier|static
name|boolean
name|alterTable
parameter_list|(
name|String
name|fullTableName
parameter_list|,
name|List
argument_list|<
name|Partition
argument_list|>
name|partitionList
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|convertToMM
parameter_list|,
name|Table
name|t
parameter_list|,
name|Hive
name|db
parameter_list|,
name|boolean
name|execute
parameter_list|)
throws|throws
name|InvalidOperationException
throws|,
name|HiveException
block|{
for|for
control|(
name|Partition
name|p
range|:
name|partitionList
control|)
block|{
if|if
condition|(
operator|!
name|canBeMadeAcid
argument_list|(
name|fullTableName
argument_list|,
name|p
operator|.
name|getSd
argument_list|()
argument_list|)
condition|)
block|{
name|convertToMM
operator|.
name|add
argument_list|(
literal|"ALTER TABLE "
operator|+
name|Warehouse
operator|.
name|getQualifiedName
argument_list|(
name|t
argument_list|)
operator|+
literal|" SET TBLPROPERTIES ("
operator|+
literal|"'transactional'='true', 'transactional_properties'='insert_only')"
argument_list|)
expr_stmt|;
if|if
condition|(
name|execute
condition|)
block|{
name|alterTable
argument_list|(
name|t
argument_list|,
name|db
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
return|return
literal|true
return|;
block|}
block|}
return|return
literal|false
return|;
block|}
specifier|private
specifier|static
name|boolean
name|canBeMadeAcid
parameter_list|(
name|String
name|fullTableName
parameter_list|,
name|StorageDescriptor
name|sd
parameter_list|)
block|{
return|return
name|isAcidInputOutputFormat
argument_list|(
name|fullTableName
argument_list|,
name|sd
argument_list|)
operator|&&
name|sd
operator|.
name|getSortColsSize
argument_list|()
operator|<=
literal|0
return|;
block|}
specifier|private
specifier|static
name|boolean
name|isAcidInputOutputFormat
parameter_list|(
name|String
name|fullTableName
parameter_list|,
name|StorageDescriptor
name|sd
parameter_list|)
block|{
try|try
block|{
name|Class
name|inputFormatClass
init|=
name|sd
operator|.
name|getInputFormat
argument_list|()
operator|==
literal|null
condition|?
literal|null
else|:
name|Class
operator|.
name|forName
argument_list|(
name|sd
operator|.
name|getInputFormat
argument_list|()
argument_list|)
decl_stmt|;
name|Class
name|outputFormatClass
init|=
name|sd
operator|.
name|getOutputFormat
argument_list|()
operator|==
literal|null
condition|?
literal|null
else|:
name|Class
operator|.
name|forName
argument_list|(
name|sd
operator|.
name|getOutputFormat
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|inputFormatClass
operator|!=
literal|null
operator|&&
name|outputFormatClass
operator|!=
literal|null
operator|&&
name|Class
operator|.
name|forName
argument_list|(
literal|"org.apache.hadoop.hive.ql.io.AcidInputFormat"
argument_list|)
operator|.
name|isAssignableFrom
argument_list|(
name|inputFormatClass
argument_list|)
operator|&&
name|Class
operator|.
name|forName
argument_list|(
literal|"org.apache.hadoop.hive.ql.io.AcidOutputFormat"
argument_list|)
operator|.
name|isAssignableFrom
argument_list|(
name|outputFormatClass
argument_list|)
condition|)
block|{
return|return
literal|true
return|;
block|}
block|}
catch|catch
parameter_list|(
name|ClassNotFoundException
name|e
parameter_list|)
block|{
comment|//if a table is using some custom I/O format and it's not in the classpath, we won't mark
comment|//the table for Acid, but today (Hive 3.1 and earlier) OrcInput/OutputFormat is the only
comment|//Acid format
name|LOG
operator|.
name|error
argument_list|(
literal|"Could not determine if "
operator|+
name|fullTableName
operator|+
literal|" can be made Acid due to: "
operator|+
name|e
operator|.
name|getMessage
argument_list|()
argument_list|,
name|e
argument_list|)
expr_stmt|;
return|return
literal|false
return|;
block|}
return|return
literal|false
return|;
block|}
comment|/**    * Generates a set compaction commands to run on pre Hive 3 cluster    */
specifier|private
specifier|static
name|void
name|makeCompactionScript
parameter_list|(
name|List
argument_list|<
name|String
argument_list|>
name|commands
parameter_list|,
name|String
name|scriptLocation
parameter_list|,
name|CompactionMetaInfo
name|compactionMetaInfo
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|commands
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"No compaction is necessary"
argument_list|)
expr_stmt|;
return|return;
block|}
name|String
name|fileName
init|=
literal|"compacts_"
operator|+
name|System
operator|.
name|currentTimeMillis
argument_list|()
operator|+
literal|".sql"
decl_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Writing compaction commands to "
operator|+
name|fileName
argument_list|)
expr_stmt|;
try|try
init|(
name|PrintWriter
name|pw
init|=
name|createScript
argument_list|(
name|commands
argument_list|,
name|fileName
argument_list|,
name|scriptLocation
argument_list|)
init|)
block|{
comment|//add post script
name|pw
operator|.
name|println
argument_list|(
literal|"-- Generated total of "
operator|+
name|commands
operator|.
name|size
argument_list|()
operator|+
literal|" compaction commands"
argument_list|)
expr_stmt|;
if|if
condition|(
name|compactionMetaInfo
operator|.
name|numberOfBytes
operator|<
name|Math
operator|.
name|pow
argument_list|(
literal|2
argument_list|,
literal|20
argument_list|)
condition|)
block|{
comment|//to see it working in UTs
name|pw
operator|.
name|println
argument_list|(
literal|"-- The total volume of data to be compacted is "
operator|+
name|String
operator|.
name|format
argument_list|(
literal|"%.6fMB"
argument_list|,
name|compactionMetaInfo
operator|.
name|numberOfBytes
operator|/
name|Math
operator|.
name|pow
argument_list|(
literal|2
argument_list|,
literal|20
argument_list|)
argument_list|)
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|pw
operator|.
name|println
argument_list|(
literal|"-- The total volume of data to be compacted is "
operator|+
name|String
operator|.
name|format
argument_list|(
literal|"%.3fGB"
argument_list|,
name|compactionMetaInfo
operator|.
name|numberOfBytes
operator|/
name|Math
operator|.
name|pow
argument_list|(
literal|2
argument_list|,
literal|30
argument_list|)
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|pw
operator|.
name|println
argument_list|()
expr_stmt|;
comment|//todo: should be at the top of the file...
name|pw
operator|.
name|println
argument_list|(
literal|"-- Please note that compaction may be a heavyweight and time consuming process.\n"
operator|+
literal|"-- Submitting all of these commands will enqueue them to a scheduling queue from\n"
operator|+
literal|"-- which they will be picked up by compactor Workers.  The max number of\n"
operator|+
literal|"-- concurrent Workers is controlled by hive.compactor.worker.threads configured\n"
operator|+
literal|"-- for the standalone metastore process.  Compaction itself is a Map-Reduce job\n"
operator|+
literal|"-- which is submitted to the YARN queue identified by hive.compactor.job.queue\n"
operator|+
literal|"-- property if defined or 'default' if not defined.  It's advisable to set the\n"
operator|+
literal|"-- capacity of this queue appropriately"
argument_list|)
expr_stmt|;
block|}
block|}
specifier|private
specifier|static
name|void
name|makeConvertTableScript
parameter_list|(
name|List
argument_list|<
name|String
argument_list|>
name|alterTableAcid
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|alterTableMm
parameter_list|,
name|String
name|scriptLocation
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|alterTableAcid
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"No acid conversion is necessary"
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|String
name|fileName
init|=
literal|"convertToAcid_"
operator|+
name|System
operator|.
name|currentTimeMillis
argument_list|()
operator|+
literal|".sql"
decl_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Writing CRUD conversion commands to "
operator|+
name|fileName
argument_list|)
expr_stmt|;
try|try
init|(
name|PrintWriter
name|pw
init|=
name|createScript
argument_list|(
name|alterTableAcid
argument_list|,
name|fileName
argument_list|,
name|scriptLocation
argument_list|)
init|)
block|{
comment|//todo: fix this - it has to run in 3.0 since tables may be unbucketed
name|pw
operator|.
name|println
argument_list|(
literal|"-- These commands may be executed by Hive 1.x later"
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
name|alterTableMm
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"No managed table conversion is necessary"
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|String
name|fileName
init|=
literal|"convertToMM_"
operator|+
name|System
operator|.
name|currentTimeMillis
argument_list|()
operator|+
literal|".sql"
decl_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Writing managed table conversion commands to "
operator|+
name|fileName
argument_list|)
expr_stmt|;
try|try
init|(
name|PrintWriter
name|pw
init|=
name|createScript
argument_list|(
name|alterTableMm
argument_list|,
name|fileName
argument_list|,
name|scriptLocation
argument_list|)
init|)
block|{
name|pw
operator|.
name|println
argument_list|(
literal|"-- These commands must be executed by Hive 3.0 or later"
argument_list|)
expr_stmt|;
block|}
block|}
block|}
specifier|private
specifier|static
name|PrintWriter
name|createScript
parameter_list|(
name|List
argument_list|<
name|String
argument_list|>
name|commands
parameter_list|,
name|String
name|fileName
parameter_list|,
name|String
name|scriptLocation
parameter_list|)
throws|throws
name|IOException
block|{
name|FileWriter
name|fw
init|=
operator|new
name|FileWriter
argument_list|(
name|scriptLocation
operator|+
literal|"/"
operator|+
name|fileName
argument_list|)
decl_stmt|;
name|PrintWriter
name|pw
init|=
operator|new
name|PrintWriter
argument_list|(
name|fw
argument_list|)
decl_stmt|;
for|for
control|(
name|String
name|cmd
range|:
name|commands
control|)
block|{
name|pw
operator|.
name|println
argument_list|(
name|cmd
operator|+
literal|";"
argument_list|)
expr_stmt|;
block|}
return|return
name|pw
return|;
block|}
specifier|private
specifier|static
name|void
name|makeRenameFileScript
parameter_list|(
name|String
name|scriptLocation
parameter_list|)
throws|throws
name|IOException
block|{
name|List
argument_list|<
name|String
argument_list|>
name|commands
init|=
name|Collections
operator|.
name|emptyList
argument_list|()
decl_stmt|;
if|if
condition|(
name|commands
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"No file renaming is necessary"
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|String
name|fileName
init|=
literal|"normalizeFileNames_"
operator|+
name|System
operator|.
name|currentTimeMillis
argument_list|()
operator|+
literal|".sh"
decl_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Writing file renaming commands to "
operator|+
name|fileName
argument_list|)
expr_stmt|;
name|PrintWriter
name|pw
init|=
name|createScript
argument_list|(
name|commands
argument_list|,
name|fileName
argument_list|,
name|scriptLocation
argument_list|)
decl_stmt|;
name|pw
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
comment|/**    * @return any compaction commands to run for {@code Table t}    */
specifier|private
specifier|static
name|List
argument_list|<
name|String
argument_list|>
name|getCompactionCommands
parameter_list|(
name|Table
name|t
parameter_list|,
name|HiveConf
name|conf
parameter_list|,
name|HiveMetaStoreClient
name|hms
parameter_list|,
name|CompactionMetaInfo
name|compactionMetaInfo
parameter_list|,
name|boolean
name|execute
parameter_list|,
name|Hive
name|db
parameter_list|,
name|ValidTxnList
name|txns
parameter_list|)
throws|throws
name|IOException
throws|,
name|TException
throws|,
name|HiveException
block|{
if|if
condition|(
operator|!
name|isFullAcidTable
argument_list|(
name|t
argument_list|)
condition|)
block|{
return|return
name|Collections
operator|.
name|emptyList
argument_list|()
return|;
block|}
if|if
condition|(
name|t
operator|.
name|getPartitionKeysSize
argument_list|()
operator|<=
literal|0
condition|)
block|{
comment|//not partitioned
if|if
condition|(
operator|!
name|needsCompaction
argument_list|(
operator|new
name|Path
argument_list|(
name|t
operator|.
name|getSd
argument_list|()
operator|.
name|getLocation
argument_list|()
argument_list|)
argument_list|,
name|conf
argument_list|,
name|compactionMetaInfo
argument_list|,
name|txns
argument_list|)
condition|)
block|{
return|return
name|Collections
operator|.
name|emptyList
argument_list|()
return|;
block|}
name|List
argument_list|<
name|String
argument_list|>
name|cmds
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|()
decl_stmt|;
name|cmds
operator|.
name|add
argument_list|(
name|getCompactionCommand
argument_list|(
name|t
argument_list|,
literal|null
argument_list|)
argument_list|)
expr_stmt|;
if|if
condition|(
name|execute
condition|)
block|{
name|scheduleCompaction
argument_list|(
name|t
argument_list|,
literal|null
argument_list|,
name|db
argument_list|,
name|compactionMetaInfo
argument_list|)
expr_stmt|;
block|}
return|return
name|cmds
return|;
block|}
name|List
argument_list|<
name|String
argument_list|>
name|partNames
init|=
name|hms
operator|.
name|listPartitionNames
argument_list|(
name|t
operator|.
name|getDbName
argument_list|()
argument_list|,
name|t
operator|.
name|getTableName
argument_list|()
argument_list|,
operator|(
name|short
operator|)
operator|-
literal|1
argument_list|)
decl_stmt|;
name|int
name|batchSize
init|=
name|PARTITION_BATCH_SIZE
decl_stmt|;
name|int
name|numWholeBatches
init|=
name|partNames
operator|.
name|size
argument_list|()
operator|/
name|batchSize
decl_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|compactionCommands
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|()
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|numWholeBatches
condition|;
name|i
operator|++
control|)
block|{
name|List
argument_list|<
name|Partition
argument_list|>
name|partitionList
init|=
name|hms
operator|.
name|getPartitionsByNames
argument_list|(
name|t
operator|.
name|getDbName
argument_list|()
argument_list|,
name|t
operator|.
name|getTableName
argument_list|()
argument_list|,
name|partNames
operator|.
name|subList
argument_list|(
name|i
operator|*
name|batchSize
argument_list|,
operator|(
name|i
operator|+
literal|1
operator|)
operator|*
name|batchSize
argument_list|)
argument_list|)
decl_stmt|;
name|getCompactionCommands
argument_list|(
name|t
argument_list|,
name|partitionList
argument_list|,
name|db
argument_list|,
name|execute
argument_list|,
name|compactionCommands
argument_list|,
name|compactionMetaInfo
argument_list|,
name|conf
argument_list|,
name|txns
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|numWholeBatches
operator|*
name|batchSize
operator|<
name|partNames
operator|.
name|size
argument_list|()
condition|)
block|{
comment|//last partial batch
name|List
argument_list|<
name|Partition
argument_list|>
name|partitionList
init|=
name|hms
operator|.
name|getPartitionsByNames
argument_list|(
name|t
operator|.
name|getDbName
argument_list|()
argument_list|,
name|t
operator|.
name|getTableName
argument_list|()
argument_list|,
name|partNames
operator|.
name|subList
argument_list|(
name|numWholeBatches
operator|*
name|batchSize
argument_list|,
name|partNames
operator|.
name|size
argument_list|()
argument_list|)
argument_list|)
decl_stmt|;
name|getCompactionCommands
argument_list|(
name|t
argument_list|,
name|partitionList
argument_list|,
name|db
argument_list|,
name|execute
argument_list|,
name|compactionCommands
argument_list|,
name|compactionMetaInfo
argument_list|,
name|conf
argument_list|,
name|txns
argument_list|)
expr_stmt|;
block|}
return|return
name|compactionCommands
return|;
block|}
specifier|private
specifier|static
name|void
name|getCompactionCommands
parameter_list|(
name|Table
name|t
parameter_list|,
name|List
argument_list|<
name|Partition
argument_list|>
name|partitionList
parameter_list|,
name|Hive
name|db
parameter_list|,
name|boolean
name|execute
parameter_list|,
name|List
argument_list|<
name|String
argument_list|>
name|compactionCommands
parameter_list|,
name|CompactionMetaInfo
name|compactionMetaInfo
parameter_list|,
name|HiveConf
name|conf
parameter_list|,
name|ValidTxnList
name|txns
parameter_list|)
throws|throws
name|IOException
throws|,
name|TException
throws|,
name|HiveException
block|{
for|for
control|(
name|Partition
name|p
range|:
name|partitionList
control|)
block|{
if|if
condition|(
name|needsCompaction
argument_list|(
operator|new
name|Path
argument_list|(
name|p
operator|.
name|getSd
argument_list|()
operator|.
name|getLocation
argument_list|()
argument_list|)
argument_list|,
name|conf
argument_list|,
name|compactionMetaInfo
argument_list|,
name|txns
argument_list|)
condition|)
block|{
name|compactionCommands
operator|.
name|add
argument_list|(
name|getCompactionCommand
argument_list|(
name|t
argument_list|,
name|p
argument_list|)
argument_list|)
expr_stmt|;
if|if
condition|(
name|execute
condition|)
block|{
name|scheduleCompaction
argument_list|(
name|t
argument_list|,
name|p
argument_list|,
name|db
argument_list|,
name|compactionMetaInfo
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
specifier|private
specifier|static
name|void
name|scheduleCompaction
parameter_list|(
name|Table
name|t
parameter_list|,
name|Partition
name|p
parameter_list|,
name|Hive
name|db
parameter_list|,
name|CompactionMetaInfo
name|compactionMetaInfo
parameter_list|)
throws|throws
name|HiveException
throws|,
name|MetaException
block|{
name|String
name|partName
init|=
name|p
operator|==
literal|null
condition|?
literal|null
else|:
name|Warehouse
operator|.
name|makePartName
argument_list|(
name|t
operator|.
name|getPartitionKeys
argument_list|()
argument_list|,
name|p
operator|.
name|getValues
argument_list|()
argument_list|)
decl_stmt|;
name|CompactionResponse
name|resp
init|=
comment|//this gives an easy way to get at compaction ID so we can only wait for those this
comment|//utility started
name|db
operator|.
name|compact2
argument_list|(
name|t
operator|.
name|getDbName
argument_list|()
argument_list|,
name|t
operator|.
name|getTableName
argument_list|()
argument_list|,
name|partName
argument_list|,
literal|"major"
argument_list|,
literal|null
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|resp
operator|.
name|isAccepted
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
name|Warehouse
operator|.
name|getQualifiedName
argument_list|(
name|t
argument_list|)
operator|+
operator|(
name|p
operator|==
literal|null
condition|?
literal|""
else|:
literal|"/"
operator|+
name|partName
operator|)
operator|+
literal|" is already being compacted with id="
operator|+
name|resp
operator|.
name|getId
argument_list|()
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Scheduled compaction for "
operator|+
name|Warehouse
operator|.
name|getQualifiedName
argument_list|(
name|t
argument_list|)
operator|+
operator|(
name|p
operator|==
literal|null
condition|?
literal|""
else|:
literal|"/"
operator|+
name|partName
operator|)
operator|+
literal|" with id="
operator|+
name|resp
operator|.
name|getId
argument_list|()
argument_list|)
expr_stmt|;
block|}
name|compactionMetaInfo
operator|.
name|compactionIds
operator|.
name|add
argument_list|(
name|resp
operator|.
name|getId
argument_list|()
argument_list|)
expr_stmt|;
block|}
comment|/**    *    * @param location - path to a partition (or table if not partitioned) dir    */
specifier|private
specifier|static
name|boolean
name|needsCompaction2
parameter_list|(
name|Path
name|location
parameter_list|,
name|HiveConf
name|conf
parameter_list|,
name|CompactionMetaInfo
name|compactionMetaInfo
parameter_list|)
throws|throws
name|IOException
block|{
name|FileSystem
name|fs
init|=
name|location
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|FileStatus
index|[]
name|deltas
init|=
name|fs
operator|.
name|listStatus
argument_list|(
name|location
argument_list|,
operator|new
name|PathFilter
argument_list|()
block|{
annotation|@
name|Override
specifier|public
name|boolean
name|accept
parameter_list|(
name|Path
name|path
parameter_list|)
block|{
comment|//checking for delete_delta is only so that this functionality can be exercised by code 3.0
comment|//which cannot produce any deltas with mix of update/insert events
return|return
name|path
operator|.
name|getName
argument_list|()
operator|.
name|startsWith
argument_list|(
literal|"delta_"
argument_list|)
operator|||
name|path
operator|.
name|getName
argument_list|()
operator|.
name|startsWith
argument_list|(
literal|"delete_delta_"
argument_list|)
return|;
block|}
block|}
argument_list|)
decl_stmt|;
if|if
condition|(
name|deltas
operator|==
literal|null
operator|||
name|deltas
operator|.
name|length
operator|==
literal|0
condition|)
block|{
comment|//base_n cannot contain update/delete.  Original files are all 'insert' and we need to compact
comment|//only if there are update/delete events.
return|return
literal|false
return|;
block|}
name|deltaLoop
label|:
for|for
control|(
name|FileStatus
name|delta
range|:
name|deltas
control|)
block|{
if|if
condition|(
operator|!
name|delta
operator|.
name|isDirectory
argument_list|()
condition|)
block|{
comment|//should never happen - just in case
continue|continue;
block|}
name|FileStatus
index|[]
name|buckets
init|=
name|fs
operator|.
name|listStatus
argument_list|(
name|delta
operator|.
name|getPath
argument_list|()
argument_list|,
operator|new
name|PathFilter
argument_list|()
block|{
annotation|@
name|Override
specifier|public
name|boolean
name|accept
parameter_list|(
name|Path
name|path
parameter_list|)
block|{
comment|//since this is inside a delta dir created by Hive 2.x or earlier it can only contain
comment|//bucket_x or bucket_x__flush_length
return|return
name|path
operator|.
name|getName
argument_list|()
operator|.
name|startsWith
argument_list|(
literal|"bucket_"
argument_list|)
return|;
block|}
block|}
argument_list|)
decl_stmt|;
for|for
control|(
name|FileStatus
name|bucket
range|:
name|buckets
control|)
block|{
if|if
condition|(
name|bucket
operator|.
name|getPath
argument_list|()
operator|.
name|getName
argument_list|()
operator|.
name|endsWith
argument_list|(
literal|"_flush_length"
argument_list|)
condition|)
block|{
comment|//streaming ingest dir - cannot have update/delete events
continue|continue
name|deltaLoop
continue|;
block|}
if|if
condition|(
name|needsCompaction
argument_list|(
name|bucket
argument_list|,
name|fs
argument_list|)
condition|)
block|{
comment|//found delete events - this 'location' needs compacting
name|compactionMetaInfo
operator|.
name|numberOfBytes
operator|+=
name|getDataSize
argument_list|(
name|location
argument_list|,
name|conf
argument_list|)
expr_stmt|;
comment|//todo: this is not remotely accurate if you have many (relevant) original files
return|return
literal|true
return|;
block|}
block|}
block|}
return|return
literal|false
return|;
block|}
comment|/**    *    * @param location - path to a partition (or table if not partitioned) dir    */
specifier|private
specifier|static
name|boolean
name|needsCompaction
parameter_list|(
name|Path
name|location
parameter_list|,
name|HiveConf
name|conf
parameter_list|,
name|CompactionMetaInfo
name|compactionMetaInfo
parameter_list|,
name|ValidTxnList
name|txns
parameter_list|)
throws|throws
name|IOException
block|{
name|FileSystem
name|fs
init|=
name|location
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|FileStatus
index|[]
name|deltas
init|=
name|fs
operator|.
name|listStatus
argument_list|(
name|location
argument_list|,
operator|new
name|PathFilter
argument_list|()
block|{
annotation|@
name|Override
specifier|public
name|boolean
name|accept
parameter_list|(
name|Path
name|path
parameter_list|)
block|{
comment|//checking for delete_delta is only so that this functionality can be exercised by code 3.0
comment|//which cannot produce any deltas with mix of update/insert events
return|return
name|path
operator|.
name|getName
argument_list|()
operator|.
name|startsWith
argument_list|(
literal|"delta_"
argument_list|)
operator|||
name|path
operator|.
name|getName
argument_list|()
operator|.
name|startsWith
argument_list|(
literal|"delete_delta_"
argument_list|)
return|;
block|}
block|}
argument_list|)
decl_stmt|;
if|if
condition|(
name|deltas
operator|==
literal|null
operator|||
name|deltas
operator|.
name|length
operator|==
literal|0
condition|)
block|{
comment|//base_n cannot contain update/delete.  Original files are all 'insert' and we need to compact
comment|//only if there are update/delete events.
return|return
literal|false
return|;
block|}
comment|/*getAcidState() is smart not to return any deltas in current if there is a base that covers     * them, i.e. if they were compacted but not yet cleaned.  This means re-checking if     * compaction is needed should cheap(er)*/
name|AcidUtils
operator|.
name|Directory
name|dir
init|=
name|AcidUtils
operator|.
name|getAcidState
argument_list|(
name|location
argument_list|,
name|conf
argument_list|,
name|txns
argument_list|)
decl_stmt|;
name|deltaLoop
label|:
for|for
control|(
name|AcidUtils
operator|.
name|ParsedDelta
name|delta
range|:
name|dir
operator|.
name|getCurrentDirectories
argument_list|()
control|)
block|{
name|FileStatus
index|[]
name|buckets
init|=
name|fs
operator|.
name|listStatus
argument_list|(
name|delta
operator|.
name|getPath
argument_list|()
argument_list|,
operator|new
name|PathFilter
argument_list|()
block|{
annotation|@
name|Override
specifier|public
name|boolean
name|accept
parameter_list|(
name|Path
name|path
parameter_list|)
block|{
comment|//since this is inside a delta dir created by Hive 2.x or earlier it can only contain
comment|//bucket_x or bucket_x__flush_length
return|return
name|path
operator|.
name|getName
argument_list|()
operator|.
name|startsWith
argument_list|(
literal|"bucket_"
argument_list|)
return|;
block|}
block|}
argument_list|)
decl_stmt|;
for|for
control|(
name|FileStatus
name|bucket
range|:
name|buckets
control|)
block|{
if|if
condition|(
name|bucket
operator|.
name|getPath
argument_list|()
operator|.
name|getName
argument_list|()
operator|.
name|endsWith
argument_list|(
literal|"_flush_length"
argument_list|)
condition|)
block|{
comment|//streaming ingest dir - cannot have update/delete events
continue|continue
name|deltaLoop
continue|;
block|}
if|if
condition|(
name|needsCompaction
argument_list|(
name|bucket
argument_list|,
name|fs
argument_list|)
condition|)
block|{
comment|//found delete events - this 'location' needs compacting
name|compactionMetaInfo
operator|.
name|numberOfBytes
operator|+=
name|getDataSize
argument_list|(
name|location
argument_list|,
name|conf
argument_list|)
expr_stmt|;
comment|//if there are un-compacted original files, they will be included in compaction, so
comment|//count at the size for 'cost' estimation later
for|for
control|(
name|HadoopShims
operator|.
name|HdfsFileStatusWithId
name|origFile
range|:
name|dir
operator|.
name|getOriginalFiles
argument_list|()
control|)
block|{
name|FileStatus
name|fileStatus
init|=
name|origFile
operator|.
name|getFileStatus
argument_list|()
decl_stmt|;
if|if
condition|(
name|fileStatus
operator|!=
literal|null
condition|)
block|{
name|compactionMetaInfo
operator|.
name|numberOfBytes
operator|+=
name|fileStatus
operator|.
name|getLen
argument_list|()
expr_stmt|;
block|}
block|}
return|return
literal|true
return|;
block|}
block|}
block|}
return|return
literal|false
return|;
block|}
comment|/**    * @param location - path to a partition (or table if not partitioned) dir    */
specifier|private
specifier|static
name|long
name|getDataSize
parameter_list|(
name|Path
name|location
parameter_list|,
name|HiveConf
name|conf
parameter_list|)
throws|throws
name|IOException
block|{
name|FileSystem
name|fs
init|=
name|location
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|ContentSummary
name|cs
init|=
name|fs
operator|.
name|getContentSummary
argument_list|(
name|location
argument_list|)
decl_stmt|;
return|return
name|cs
operator|.
name|getLength
argument_list|()
return|;
block|}
specifier|private
specifier|static
name|boolean
name|needsCompaction
parameter_list|(
name|FileStatus
name|bucket
parameter_list|,
name|FileSystem
name|fs
parameter_list|)
throws|throws
name|IOException
block|{
comment|//create reader, look at footer
comment|//no need to check side file since it can only be in a streaming ingest delta
name|Reader
name|orcReader
init|=
name|OrcFile
operator|.
name|createReader
argument_list|(
name|bucket
operator|.
name|getPath
argument_list|()
argument_list|,
name|OrcFile
operator|.
name|readerOptions
argument_list|(
name|fs
operator|.
name|getConf
argument_list|()
argument_list|)
operator|.
name|filesystem
argument_list|(
name|fs
argument_list|)
argument_list|)
decl_stmt|;
name|AcidStats
name|as
init|=
name|OrcAcidUtils
operator|.
name|parseAcidStats
argument_list|(
name|orcReader
argument_list|)
decl_stmt|;
if|if
condition|(
name|as
operator|==
literal|null
condition|)
block|{
comment|//should never happen since we are reading bucket_x written by acid write
throw|throw
operator|new
name|IllegalStateException
argument_list|(
literal|"AcidStats missing in "
operator|+
name|bucket
operator|.
name|getPath
argument_list|()
argument_list|)
throw|;
block|}
return|return
name|as
operator|.
name|deletes
operator|>
literal|0
operator|||
name|as
operator|.
name|updates
operator|>
literal|0
return|;
block|}
specifier|private
specifier|static
name|String
name|getCompactionCommand
parameter_list|(
name|Table
name|t
parameter_list|,
name|Partition
name|p
parameter_list|)
block|{
name|StringBuilder
name|sb
init|=
operator|new
name|StringBuilder
argument_list|(
literal|"ALTER TABLE "
argument_list|)
operator|.
name|append
argument_list|(
name|Warehouse
operator|.
name|getQualifiedName
argument_list|(
name|t
argument_list|)
argument_list|)
decl_stmt|;
if|if
condition|(
name|t
operator|.
name|getPartitionKeysSize
argument_list|()
operator|>
literal|0
condition|)
block|{
assert|assert
name|p
operator|!=
literal|null
operator|:
literal|"must supply partition for partitioned table "
operator|+
name|Warehouse
operator|.
name|getQualifiedName
argument_list|(
name|t
argument_list|)
assert|;
name|sb
operator|.
name|append
argument_list|(
literal|" PARTITION("
argument_list|)
expr_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|t
operator|.
name|getPartitionKeysSize
argument_list|()
condition|;
name|i
operator|++
control|)
block|{
name|sb
operator|.
name|append
argument_list|(
name|t
operator|.
name|getPartitionKeys
argument_list|()
operator|.
name|get
argument_list|(
name|i
argument_list|)
operator|.
name|getName
argument_list|()
argument_list|)
operator|.
name|append
argument_list|(
literal|'='
argument_list|)
operator|.
name|append
argument_list|(
name|genPartValueString
argument_list|(
name|t
operator|.
name|getPartitionKeys
argument_list|()
operator|.
name|get
argument_list|(
name|i
argument_list|)
operator|.
name|getType
argument_list|()
argument_list|,
name|p
operator|.
name|getValues
argument_list|()
operator|.
name|get
argument_list|(
name|i
argument_list|)
argument_list|)
argument_list|)
operator|.
name|append
argument_list|(
literal|","
argument_list|)
expr_stmt|;
block|}
name|sb
operator|.
name|setCharAt
argument_list|(
name|sb
operator|.
name|length
argument_list|()
operator|-
literal|1
argument_list|,
literal|')'
argument_list|)
expr_stmt|;
comment|//replace trailing ','
block|}
return|return
name|sb
operator|.
name|append
argument_list|(
literal|" COMPACT 'major'"
argument_list|)
operator|.
name|toString
argument_list|()
return|;
block|}
comment|/**    * This is copy-pasted from {@link org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer},    * which can't be refactored since this is linked against Hive 2.x    */
specifier|private
specifier|static
name|String
name|genPartValueString
parameter_list|(
name|String
name|partColType
parameter_list|,
name|String
name|partVal
parameter_list|)
block|{
name|String
name|returnVal
init|=
name|partVal
decl_stmt|;
if|if
condition|(
name|partColType
operator|.
name|equals
argument_list|(
name|serdeConstants
operator|.
name|STRING_TYPE_NAME
argument_list|)
operator|||
name|partColType
operator|.
name|contains
argument_list|(
name|serdeConstants
operator|.
name|VARCHAR_TYPE_NAME
argument_list|)
operator|||
name|partColType
operator|.
name|contains
argument_list|(
name|serdeConstants
operator|.
name|CHAR_TYPE_NAME
argument_list|)
condition|)
block|{
name|returnVal
operator|=
literal|"'"
operator|+
name|escapeSQLString
argument_list|(
name|partVal
argument_list|)
operator|+
literal|"'"
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|partColType
operator|.
name|equals
argument_list|(
name|serdeConstants
operator|.
name|TINYINT_TYPE_NAME
argument_list|)
condition|)
block|{
name|returnVal
operator|=
name|partVal
operator|+
literal|"Y"
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|partColType
operator|.
name|equals
argument_list|(
name|serdeConstants
operator|.
name|SMALLINT_TYPE_NAME
argument_list|)
condition|)
block|{
name|returnVal
operator|=
name|partVal
operator|+
literal|"S"
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|partColType
operator|.
name|equals
argument_list|(
name|serdeConstants
operator|.
name|INT_TYPE_NAME
argument_list|)
condition|)
block|{
name|returnVal
operator|=
name|partVal
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|partColType
operator|.
name|equals
argument_list|(
name|serdeConstants
operator|.
name|BIGINT_TYPE_NAME
argument_list|)
condition|)
block|{
name|returnVal
operator|=
name|partVal
operator|+
literal|"L"
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|partColType
operator|.
name|contains
argument_list|(
name|serdeConstants
operator|.
name|DECIMAL_TYPE_NAME
argument_list|)
condition|)
block|{
name|returnVal
operator|=
name|partVal
operator|+
literal|"BD"
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|partColType
operator|.
name|equals
argument_list|(
name|serdeConstants
operator|.
name|DATE_TYPE_NAME
argument_list|)
operator|||
name|partColType
operator|.
name|equals
argument_list|(
name|serdeConstants
operator|.
name|TIMESTAMP_TYPE_NAME
argument_list|)
condition|)
block|{
name|returnVal
operator|=
name|partColType
operator|+
literal|" '"
operator|+
name|escapeSQLString
argument_list|(
name|partVal
argument_list|)
operator|+
literal|"'"
expr_stmt|;
block|}
else|else
block|{
comment|//for other usually not used types, just quote the value
name|returnVal
operator|=
literal|"'"
operator|+
name|escapeSQLString
argument_list|(
name|partVal
argument_list|)
operator|+
literal|"'"
expr_stmt|;
block|}
return|return
name|returnVal
return|;
block|}
specifier|private
specifier|static
name|boolean
name|isFullAcidTable
parameter_list|(
name|Table
name|t
parameter_list|)
block|{
if|if
condition|(
name|t
operator|.
name|getParametersSize
argument_list|()
operator|<=
literal|0
condition|)
block|{
comment|//cannot be acid
return|return
literal|false
return|;
block|}
name|String
name|transacationalValue
init|=
name|t
operator|.
name|getParameters
argument_list|()
operator|.
name|get
argument_list|(
name|hive_metastoreConstants
operator|.
name|TABLE_IS_TRANSACTIONAL
argument_list|)
decl_stmt|;
if|if
condition|(
name|transacationalValue
operator|!=
literal|null
operator|&&
literal|"true"
operator|.
name|equalsIgnoreCase
argument_list|(
name|transacationalValue
argument_list|)
condition|)
block|{
name|System
operator|.
name|out
operator|.
name|println
argument_list|(
literal|"Found Acid table: "
operator|+
name|Warehouse
operator|.
name|getQualifiedName
argument_list|(
name|t
argument_list|)
argument_list|)
expr_stmt|;
return|return
literal|true
return|;
block|}
return|return
literal|false
return|;
block|}
specifier|private
specifier|static
name|boolean
name|isAcidEnabled
parameter_list|(
name|HiveConf
name|hiveConf
parameter_list|)
block|{
name|String
name|txnMgr
init|=
name|hiveConf
operator|.
name|getVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_TXN_MANAGER
argument_list|)
decl_stmt|;
name|boolean
name|concurrency
init|=
name|hiveConf
operator|.
name|getBoolVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_SUPPORT_CONCURRENCY
argument_list|)
decl_stmt|;
name|String
name|dbTxnMgr
init|=
literal|"org.apache.hadoop.hive.ql.lockmgr.DbTxnManager"
decl_stmt|;
return|return
name|txnMgr
operator|.
name|equals
argument_list|(
name|dbTxnMgr
argument_list|)
operator|&&
name|concurrency
return|;
block|}
specifier|private
specifier|static
class|class
name|CompactionMetaInfo
block|{
comment|/**      * total number of bytes to be compacted across all compaction commands      */
name|long
name|numberOfBytes
decl_stmt|;
comment|/**      * IDs of compactions launched by this utility      */
name|Set
argument_list|<
name|Long
argument_list|>
name|compactionIds
init|=
operator|new
name|HashSet
argument_list|<>
argument_list|()
decl_stmt|;
block|}
annotation|@
name|VisibleForTesting
specifier|static
specifier|abstract
class|class
name|Callback
block|{
comment|/**      * This is a hack enable Unit testing.  Derby can't handle multiple concurrent threads but      * somehow Compactor needs to run to test "execute" mode.  This callback can be used      * to run Worker.  For TESTING ONLY.      */
name|void
name|onWaitForCompaction
parameter_list|()
throws|throws
name|MetaException
block|{}
block|}
annotation|@
name|VisibleForTesting
specifier|static
name|Callback
name|callback
decl_stmt|;
annotation|@
name|VisibleForTesting
specifier|static
name|int
name|pollIntervalMs
init|=
literal|1000
operator|*
literal|30
decl_stmt|;
comment|/**    * Also to enable testing until I set up Maven profiles to be able to run with 3.0 jars    */
annotation|@
name|VisibleForTesting
specifier|static
name|boolean
name|isTestMode
init|=
literal|false
decl_stmt|;
comment|/**    * can set it from tests to test when config needs something other than default values    */
annotation|@
name|VisibleForTesting
specifier|static
name|HiveConf
name|hiveConf
init|=
literal|null
decl_stmt|;
block|}
end_class

end_unit

