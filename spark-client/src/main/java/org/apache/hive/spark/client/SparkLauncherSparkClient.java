begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/*  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *<p/>  * http://www.apache.org/licenses/LICENSE-2.0  *<p/>  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hive
operator|.
name|spark
operator|.
name|client
package|;
end_package

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|annotations
operator|.
name|VisibleForTesting
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|base
operator|.
name|Preconditions
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|collect
operator|.
name|Sets
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hive
operator|.
name|spark
operator|.
name|client
operator|.
name|rpc
operator|.
name|RpcServer
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|spark
operator|.
name|launcher
operator|.
name|AbstractLauncher
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|spark
operator|.
name|launcher
operator|.
name|InProcessLauncher
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|spark
operator|.
name|launcher
operator|.
name|SparkAppHandle
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|Logger
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|LoggerFactory
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Set
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|Callable
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|CountDownLatch
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|Future
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|FutureTask
import|;
end_import

begin_comment
comment|/**  * Extends the {@link AbstractSparkClient} and uses Spark's  * {@link org.apache.spark.launcher.SparkLauncher} to submit the HoS application. Specifically,  * it uses the {@link InProcessLauncher} to avoid spawning a sub-process to submit the Spark app.  * It uses a {@link Thread} to monitor when the Spark app has been successfully submitted. The  * thread can be interrupted, in which case the {@link RpcServer} client will be cancelled and  * the Spark app will be stopped.  */
end_comment

begin_class
specifier|public
class|class
name|SparkLauncherSparkClient
extends|extends
name|AbstractSparkClient
block|{
specifier|private
specifier|static
specifier|final
name|Logger
name|LOG
init|=
name|LoggerFactory
operator|.
name|getLogger
argument_list|(
name|SparkLauncherSparkClient
operator|.
name|class
operator|.
name|getName
argument_list|()
argument_list|)
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|long
name|serialVersionUID
init|=
literal|2153000661341457380L
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|Set
argument_list|<
name|SparkAppHandle
operator|.
name|State
argument_list|>
name|FAILED_SPARK_STATES
init|=
name|Sets
operator|.
name|newHashSet
argument_list|(
name|SparkAppHandle
operator|.
name|State
operator|.
name|FAILED
argument_list|,
name|SparkAppHandle
operator|.
name|State
operator|.
name|KILLED
argument_list|,
name|SparkAppHandle
operator|.
name|State
operator|.
name|LOST
argument_list|)
decl_stmt|;
specifier|private
specifier|transient
name|AbstractLauncher
argument_list|<
name|InProcessLauncher
argument_list|>
name|sparkLauncher
decl_stmt|;
name|SparkLauncherSparkClient
parameter_list|(
name|RpcServer
name|rpcServer
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|conf
parameter_list|,
name|HiveConf
name|hiveConf
parameter_list|,
name|String
name|sessionid
parameter_list|)
throws|throws
name|IOException
block|{
name|super
argument_list|(
name|rpcServer
argument_list|,
name|conf
argument_list|,
name|hiveConf
argument_list|,
name|sessionid
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
specifier|protected
name|Future
argument_list|<
name|Void
argument_list|>
name|launchDriver
parameter_list|(
name|String
name|isTesting
parameter_list|,
name|RpcServer
name|rpcServer
parameter_list|,
name|String
name|clientId
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
name|isTesting
operator|!=
literal|null
condition|)
block|{
name|System
operator|.
name|setProperty
argument_list|(
literal|"spark.testing"
argument_list|,
literal|"true"
argument_list|)
expr_stmt|;
block|}
comment|// Only allow the spark.master to be local in unit tests
if|if
condition|(
name|isTesting
operator|==
literal|null
condition|)
block|{
name|Preconditions
operator|.
name|checkArgument
argument_list|(
name|SparkClientUtilities
operator|.
name|isYarnClusterMode
argument_list|(
name|this
operator|.
name|conf
operator|.
name|get
argument_list|(
literal|"spark.master"
argument_list|)
argument_list|,
name|this
operator|.
name|conf
operator|.
name|get
argument_list|(
literal|"spark.submit.deployMode"
argument_list|)
argument_list|)
argument_list|,
name|getClass
argument_list|()
operator|.
name|getName
argument_list|()
operator|+
literal|" is only supported in yarn-cluster mode"
argument_list|)
expr_stmt|;
block|}
comment|// Monitors when the Spark app has been successfully started
name|CountDownLatch
name|shutdownLatch
init|=
operator|new
name|CountDownLatch
argument_list|(
literal|1
argument_list|)
decl_stmt|;
comment|// Submit the app
name|SparkAppHandle
name|sparkAppHandle
init|=
name|getSparkLauncher
argument_list|()
operator|.
name|startApplication
argument_list|(
operator|new
name|SparkAppListener
argument_list|(
name|shutdownLatch
argument_list|,
name|rpcServer
argument_list|,
name|clientId
argument_list|)
argument_list|)
decl_stmt|;
return|return
name|createSparkLauncherFuture
argument_list|(
name|shutdownLatch
argument_list|,
name|sparkAppHandle
argument_list|,
name|rpcServer
argument_list|,
name|clientId
argument_list|)
return|;
block|}
annotation|@
name|VisibleForTesting
specifier|static
name|Future
argument_list|<
name|Void
argument_list|>
name|createSparkLauncherFuture
parameter_list|(
name|CountDownLatch
name|shutdownLatch
parameter_list|,
name|SparkAppHandle
name|sparkAppHandle
parameter_list|,
name|RpcServer
name|rpcServer
parameter_list|,
name|String
name|clientId
parameter_list|)
block|{
comment|// Monitor the countdown latch
name|Callable
argument_list|<
name|Void
argument_list|>
name|runnable
init|=
parameter_list|()
lambda|->
block|{
try|try
block|{
name|shutdownLatch
operator|.
name|await
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|InterruptedException
name|e
parameter_list|)
block|{
name|rpcServer
operator|.
name|cancelClient
argument_list|(
name|clientId
argument_list|,
literal|"Spark app launcher interrupted"
argument_list|)
expr_stmt|;
name|sparkAppHandle
operator|.
name|stop
argument_list|()
expr_stmt|;
block|}
return|return
literal|null
return|;
block|}
decl_stmt|;
name|FutureTask
argument_list|<
name|Void
argument_list|>
name|futureTask
init|=
operator|new
name|FutureTask
argument_list|<>
argument_list|(
name|runnable
argument_list|)
decl_stmt|;
name|Thread
name|driverThread
init|=
operator|new
name|Thread
argument_list|(
name|futureTask
argument_list|)
decl_stmt|;
name|driverThread
operator|.
name|setDaemon
argument_list|(
literal|true
argument_list|)
expr_stmt|;
name|driverThread
operator|.
name|setName
argument_list|(
literal|"SparkLauncherMonitor"
argument_list|)
expr_stmt|;
name|driverThread
operator|.
name|start
argument_list|()
expr_stmt|;
return|return
name|futureTask
return|;
block|}
annotation|@
name|Override
specifier|protected
name|String
name|getSparkHome
parameter_list|()
block|{
return|return
literal|null
return|;
block|}
annotation|@
name|Override
specifier|protected
name|void
name|addAppArg
parameter_list|(
name|String
name|arg
parameter_list|)
block|{
name|getSparkLauncher
argument_list|()
operator|.
name|addAppArgs
argument_list|(
name|arg
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
specifier|protected
name|void
name|addExecutableJar
parameter_list|(
name|String
name|jar
parameter_list|)
block|{
name|getSparkLauncher
argument_list|()
operator|.
name|setAppResource
argument_list|(
name|jar
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
specifier|protected
name|void
name|addPropertiesFile
parameter_list|(
name|String
name|absolutePath
parameter_list|)
block|{
name|getSparkLauncher
argument_list|()
operator|.
name|setPropertiesFile
argument_list|(
name|absolutePath
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
specifier|protected
name|void
name|addClass
parameter_list|(
name|String
name|name
parameter_list|)
block|{
name|getSparkLauncher
argument_list|()
operator|.
name|setMainClass
argument_list|(
name|name
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
specifier|protected
name|void
name|addJars
parameter_list|(
name|String
name|jars
parameter_list|)
block|{
name|getSparkLauncher
argument_list|()
operator|.
name|addJar
argument_list|(
name|jars
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
specifier|protected
name|void
name|addProxyUser
parameter_list|(
name|String
name|proxyUser
parameter_list|)
block|{
throw|throw
operator|new
name|UnsupportedOperationException
argument_list|()
throw|;
comment|//    getSparkLauncher().addSparkArg("--proxy-user", proxyUser);
block|}
annotation|@
name|Override
specifier|protected
name|void
name|addKeytabAndPrincipal
parameter_list|(
name|boolean
name|isDoAsEnabled
parameter_list|,
name|String
name|keyTabFile
parameter_list|,
name|String
name|principal
parameter_list|)
block|{
throw|throw
operator|new
name|UnsupportedOperationException
argument_list|()
throw|;
comment|//    getSparkLauncher().addSparkArg("--principal", principal);
comment|//    getSparkLauncher().addSparkArg("--keytab", keyTabFile);
block|}
annotation|@
name|Override
specifier|protected
name|void
name|addNumExecutors
parameter_list|(
name|String
name|numOfExecutors
parameter_list|)
block|{
name|getSparkLauncher
argument_list|()
operator|.
name|addSparkArg
argument_list|(
literal|"--num-executors"
argument_list|,
name|numOfExecutors
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
specifier|protected
name|void
name|addExecutorMemory
parameter_list|(
name|String
name|executorMemory
parameter_list|)
block|{
name|getSparkLauncher
argument_list|()
operator|.
name|addSparkArg
argument_list|(
literal|"--executor-memory"
argument_list|,
name|executorMemory
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
specifier|protected
name|void
name|addExecutorCores
parameter_list|(
name|String
name|executorCores
parameter_list|)
block|{
name|getSparkLauncher
argument_list|()
operator|.
name|addSparkArg
argument_list|(
literal|"--executor-cores"
argument_list|,
name|executorCores
argument_list|)
expr_stmt|;
block|}
specifier|private
name|AbstractLauncher
argument_list|<
name|InProcessLauncher
argument_list|>
name|getSparkLauncher
parameter_list|()
block|{
if|if
condition|(
name|this
operator|.
name|sparkLauncher
operator|==
literal|null
condition|)
block|{
name|this
operator|.
name|sparkLauncher
operator|=
operator|new
name|InProcessLauncher
argument_list|()
expr_stmt|;
block|}
return|return
name|this
operator|.
name|sparkLauncher
return|;
block|}
annotation|@
name|VisibleForTesting
specifier|static
specifier|final
class|class
name|SparkAppListener
implements|implements
name|SparkAppHandle
operator|.
name|Listener
block|{
specifier|private
specifier|final
name|CountDownLatch
name|shutdownLatch
decl_stmt|;
specifier|private
specifier|final
name|RpcServer
name|rpcServer
decl_stmt|;
specifier|private
specifier|final
name|String
name|clientId
decl_stmt|;
name|SparkAppListener
parameter_list|(
name|CountDownLatch
name|shutdownLatch
parameter_list|,
name|RpcServer
name|rpcServer
parameter_list|,
name|String
name|clientId
parameter_list|)
block|{
name|this
operator|.
name|shutdownLatch
operator|=
name|shutdownLatch
expr_stmt|;
name|this
operator|.
name|rpcServer
operator|=
name|rpcServer
expr_stmt|;
name|this
operator|.
name|clientId
operator|=
name|clientId
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|stateChanged
parameter_list|(
name|SparkAppHandle
name|sparkAppHandle
parameter_list|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Spark app transitioned to state = "
operator|+
name|sparkAppHandle
operator|.
name|getState
argument_list|()
argument_list|)
expr_stmt|;
if|if
condition|(
name|sparkAppHandle
operator|.
name|getState
argument_list|()
operator|.
name|isFinal
argument_list|()
operator|||
name|sparkAppHandle
operator|.
name|getState
argument_list|()
operator|.
name|equals
argument_list|(
name|SparkAppHandle
operator|.
name|State
operator|.
name|RUNNING
argument_list|)
condition|)
block|{
name|this
operator|.
name|shutdownLatch
operator|.
name|countDown
argument_list|()
expr_stmt|;
name|sparkAppHandle
operator|.
name|disconnect
argument_list|()
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Successfully disconnected from Spark app handle"
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|FAILED_SPARK_STATES
operator|.
name|contains
argument_list|(
name|sparkAppHandle
operator|.
name|getState
argument_list|()
argument_list|)
condition|)
block|{
name|this
operator|.
name|rpcServer
operator|.
name|cancelClient
argument_list|(
name|this
operator|.
name|clientId
argument_list|,
literal|"Spark app launcher failed,"
operator|+
literal|" transitioned to state "
operator|+
name|sparkAppHandle
operator|.
name|getState
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
annotation|@
name|Override
specifier|public
name|void
name|infoChanged
parameter_list|(
name|SparkAppHandle
name|sparkAppHandle
parameter_list|)
block|{
comment|// Do nothing
block|}
block|}
block|}
end_class

end_unit

