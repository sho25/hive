begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/*  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|kafka
package|;
end_package

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|collect
operator|.
name|ImmutableMap
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configuration
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|SerializationUtilities
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|ExprNodeGenericFuncDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|TableScanDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|io
operator|.
name|NullWritable
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|InputSplit
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|JobConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|RecordReader
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|Reporter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|InputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|JobContext
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|TaskAttemptContext
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|kafka
operator|.
name|clients
operator|.
name|consumer
operator|.
name|KafkaConsumer
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|kafka
operator|.
name|common
operator|.
name|PartitionInfo
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|kafka
operator|.
name|common
operator|.
name|TopicPartition
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|Logger
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|LoggerFactory
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|Callable
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|ExecutionException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|ExecutorService
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|Executors
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|Future
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|TimeUnit
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|TimeoutException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|stream
operator|.
name|Collectors
import|;
end_import

begin_comment
comment|/**  * Kafka puller input format to read records from a Kafka Queue.  * The input split will contain the set of topic partition and start/end offsets.  * Records will be returned as bytes array.  */
end_comment

begin_class
specifier|public
class|class
name|KafkaPullerInputFormat
extends|extends
name|InputFormat
argument_list|<
name|NullWritable
argument_list|,
name|KafkaRecordWritable
argument_list|>
implements|implements
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|InputFormat
argument_list|<
name|NullWritable
argument_list|,
name|KafkaRecordWritable
argument_list|>
block|{
specifier|private
specifier|static
specifier|final
name|Logger
name|LOG
init|=
name|LoggerFactory
operator|.
name|getLogger
argument_list|(
name|KafkaPullerInputFormat
operator|.
name|class
argument_list|)
decl_stmt|;
annotation|@
name|Override
specifier|public
name|InputSplit
index|[]
name|getSplits
parameter_list|(
name|JobConf
name|jobConf
parameter_list|,
name|int
name|i
parameter_list|)
throws|throws
name|IOException
block|{
name|List
argument_list|<
name|KafkaPullerInputSplit
argument_list|>
name|inputSplits
decl_stmt|;
try|try
block|{
name|inputSplits
operator|=
name|computeSplits
argument_list|(
name|jobConf
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|InterruptedException
name|e
parameter_list|)
block|{
name|Thread
operator|.
name|currentThread
argument_list|()
operator|.
name|interrupt
argument_list|()
expr_stmt|;
throw|throw
operator|new
name|IOException
argument_list|(
name|e
argument_list|)
throw|;
block|}
name|InputSplit
index|[]
name|inputSplitsArray
init|=
operator|new
name|InputSplit
index|[
name|inputSplits
operator|.
name|size
argument_list|()
index|]
decl_stmt|;
return|return
name|inputSplits
operator|.
name|toArray
argument_list|(
name|inputSplitsArray
argument_list|)
return|;
block|}
comment|/**    * Build a full scan using Kafka list partition then beginning/end offsets.    * This function might block duo to calls like:    * org.apache.kafka.clients.consumer.KafkaConsumer#beginningOffsets(java.util.Collection)    *    * @param topic      kafka topic    * @param consumer   initialized kafka consumer    * @param tablePaths hive table path    *    * @return full scan input split collection based on Kafka metadata APIs    */
specifier|private
specifier|static
name|List
argument_list|<
name|KafkaPullerInputSplit
argument_list|>
name|buildFullScanFromKafka
parameter_list|(
name|String
name|topic
parameter_list|,
name|KafkaConsumer
argument_list|<
name|byte
index|[]
argument_list|,
name|byte
index|[]
argument_list|>
name|consumer
parameter_list|,
name|Path
index|[]
name|tablePaths
parameter_list|)
block|{
specifier|final
name|Map
argument_list|<
name|TopicPartition
argument_list|,
name|Long
argument_list|>
name|starOffsetsMap
decl_stmt|;
specifier|final
name|Map
argument_list|<
name|TopicPartition
argument_list|,
name|Long
argument_list|>
name|endOffsetsMap
decl_stmt|;
specifier|final
name|List
argument_list|<
name|TopicPartition
argument_list|>
name|topicPartitions
decl_stmt|;
name|topicPartitions
operator|=
name|fetchTopicPartitions
argument_list|(
name|topic
argument_list|,
name|consumer
argument_list|)
expr_stmt|;
name|starOffsetsMap
operator|=
name|consumer
operator|.
name|beginningOffsets
argument_list|(
name|topicPartitions
argument_list|)
expr_stmt|;
name|endOffsetsMap
operator|=
name|consumer
operator|.
name|endOffsets
argument_list|(
name|topicPartitions
argument_list|)
expr_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Found the following partitions [{}]"
argument_list|,
name|topicPartitions
operator|.
name|stream
argument_list|()
operator|.
name|map
argument_list|(
name|TopicPartition
operator|::
name|toString
argument_list|)
operator|.
name|collect
argument_list|(
name|Collectors
operator|.
name|joining
argument_list|(
literal|","
argument_list|)
argument_list|)
argument_list|)
expr_stmt|;
name|starOffsetsMap
operator|.
name|forEach
argument_list|(
parameter_list|(
name|tp
parameter_list|,
name|start
parameter_list|)
lambda|->
name|LOG
operator|.
name|info
argument_list|(
literal|"TPartition [{}],Start offsets [{}]"
argument_list|,
name|tp
argument_list|,
name|start
argument_list|)
argument_list|)
expr_stmt|;
name|endOffsetsMap
operator|.
name|forEach
argument_list|(
parameter_list|(
name|tp
parameter_list|,
name|end
parameter_list|)
lambda|->
name|LOG
operator|.
name|info
argument_list|(
literal|"TPartition [{}],End offsets [{}]"
argument_list|,
name|tp
argument_list|,
name|end
argument_list|)
argument_list|)
expr_stmt|;
block|}
return|return
name|topicPartitions
operator|.
name|stream
argument_list|()
operator|.
name|map
argument_list|(
name|topicPartition
lambda|->
operator|new
name|KafkaPullerInputSplit
argument_list|(
name|topicPartition
operator|.
name|topic
argument_list|()
argument_list|,
name|topicPartition
operator|.
name|partition
argument_list|()
argument_list|,
name|starOffsetsMap
operator|.
name|get
argument_list|(
name|topicPartition
argument_list|)
argument_list|,
name|endOffsetsMap
operator|.
name|get
argument_list|(
name|topicPartition
argument_list|)
argument_list|,
name|tablePaths
index|[
literal|0
index|]
argument_list|)
argument_list|)
operator|.
name|collect
argument_list|(
name|Collectors
operator|.
name|toList
argument_list|()
argument_list|)
return|;
block|}
specifier|private
name|List
argument_list|<
name|KafkaPullerInputSplit
argument_list|>
name|computeSplits
parameter_list|(
name|Configuration
name|configuration
parameter_list|)
throws|throws
name|IOException
throws|,
name|InterruptedException
block|{
comment|// this will be used to harness some KAFKA blocking calls
specifier|final
name|ExecutorService
name|execService
init|=
name|Executors
operator|.
name|newSingleThreadExecutor
argument_list|()
decl_stmt|;
try|try
init|(
name|KafkaConsumer
name|consumer
init|=
operator|new
name|KafkaConsumer
argument_list|(
name|KafkaStreamingUtils
operator|.
name|consumerProperties
argument_list|(
name|configuration
argument_list|)
argument_list|)
init|)
block|{
specifier|final
name|String
name|topic
init|=
name|configuration
operator|.
name|get
argument_list|(
name|KafkaStreamingUtils
operator|.
name|HIVE_KAFKA_TOPIC
argument_list|)
decl_stmt|;
specifier|final
name|long
name|timeoutMs
init|=
name|configuration
operator|.
name|getLong
argument_list|(
name|KafkaStreamingUtils
operator|.
name|HIVE_KAFKA_POLL_TIMEOUT
argument_list|,
name|KafkaStreamingUtils
operator|.
name|DEFAULT_CONSUMER_POLL_TIMEOUT_MS
argument_list|)
decl_stmt|;
comment|// hive depends on FileSplits
name|JobConf
name|jobConf
init|=
operator|new
name|JobConf
argument_list|(
name|configuration
argument_list|)
decl_stmt|;
name|Path
index|[]
name|tablePaths
init|=
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|FileInputFormat
operator|.
name|getInputPaths
argument_list|(
name|jobConf
argument_list|)
decl_stmt|;
comment|//noinspection unchecked
name|Future
argument_list|<
name|List
argument_list|<
name|KafkaPullerInputSplit
argument_list|>
argument_list|>
name|futureFullHouse
init|=
name|execService
operator|.
name|submit
argument_list|(
parameter_list|()
lambda|->
name|buildFullScanFromKafka
argument_list|(
name|topic
argument_list|,
name|consumer
argument_list|,
name|tablePaths
argument_list|)
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|KafkaPullerInputSplit
argument_list|>
name|fullHouse
decl_stmt|;
try|try
block|{
name|fullHouse
operator|=
name|futureFullHouse
operator|.
name|get
argument_list|(
name|timeoutMs
argument_list|,
name|TimeUnit
operator|.
name|MILLISECONDS
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|TimeoutException
decl||
name|ExecutionException
name|e
parameter_list|)
block|{
name|futureFullHouse
operator|.
name|cancel
argument_list|(
literal|true
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|error
argument_list|(
literal|"can not generate full scan split"
argument_list|,
name|e
argument_list|)
expr_stmt|;
comment|// at this point we can not go further fail split generation
throw|throw
operator|new
name|IOException
argument_list|(
name|e
argument_list|)
throw|;
block|}
annotation|@
name|SuppressWarnings
argument_list|(
literal|"unchecked"
argument_list|)
specifier|final
name|ImmutableMap
operator|.
name|Builder
argument_list|<
name|TopicPartition
argument_list|,
name|KafkaPullerInputSplit
argument_list|>
name|fullHouseMapBuilder
init|=
operator|new
name|ImmutableMap
operator|.
name|Builder
argument_list|()
decl_stmt|;
name|fullHouse
operator|.
name|forEach
argument_list|(
name|input
lambda|->
name|fullHouseMapBuilder
operator|.
name|put
argument_list|(
operator|new
name|TopicPartition
argument_list|(
name|input
operator|.
name|getTopic
argument_list|()
argument_list|,
name|input
operator|.
name|getPartition
argument_list|()
argument_list|)
argument_list|,
name|input
argument_list|)
argument_list|)
expr_stmt|;
specifier|final
name|KafkaScanTrimmer
name|kafkaScanTrimmer
init|=
operator|new
name|KafkaScanTrimmer
argument_list|(
name|fullHouseMapBuilder
operator|.
name|build
argument_list|()
argument_list|,
name|consumer
argument_list|)
decl_stmt|;
specifier|final
name|String
name|filterExprSerialized
init|=
name|configuration
operator|.
name|get
argument_list|(
name|TableScanDesc
operator|.
name|FILTER_EXPR_CONF_STR
argument_list|)
decl_stmt|;
if|if
condition|(
name|filterExprSerialized
operator|!=
literal|null
operator|&&
operator|!
name|filterExprSerialized
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|ExprNodeGenericFuncDesc
name|filterExpr
init|=
name|SerializationUtilities
operator|.
name|deserializeExpression
argument_list|(
name|filterExprSerialized
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Kafka trimmer working on Filter tree {}"
argument_list|,
name|filterExpr
operator|.
name|getExprString
argument_list|()
argument_list|)
expr_stmt|;
name|Callable
argument_list|<
name|List
argument_list|<
name|KafkaPullerInputSplit
argument_list|>
argument_list|>
name|trimmerWorker
init|=
parameter_list|()
lambda|->
name|kafkaScanTrimmer
operator|.
name|computeOptimizedScan
argument_list|(
name|filterExpr
argument_list|)
operator|.
name|entrySet
argument_list|()
operator|.
name|stream
argument_list|()
operator|.
name|map
argument_list|(
name|Map
operator|.
name|Entry
operator|::
name|getValue
argument_list|)
operator|.
name|collect
argument_list|(
name|Collectors
operator|.
name|toList
argument_list|()
argument_list|)
decl_stmt|;
name|Future
argument_list|<
name|List
argument_list|<
name|KafkaPullerInputSplit
argument_list|>
argument_list|>
name|futureTinyHouse
init|=
name|execService
operator|.
name|submit
argument_list|(
name|trimmerWorker
argument_list|)
decl_stmt|;
try|try
block|{
return|return
name|futureTinyHouse
operator|.
name|get
argument_list|(
name|timeoutMs
argument_list|,
name|TimeUnit
operator|.
name|MILLISECONDS
argument_list|)
operator|.
name|stream
argument_list|()
comment|// filter out empty splits
operator|.
name|filter
argument_list|(
name|split
lambda|->
name|split
operator|.
name|getStartOffset
argument_list|()
operator|<
name|split
operator|.
name|getEndOffset
argument_list|()
argument_list|)
operator|.
name|collect
argument_list|(
name|Collectors
operator|.
name|toList
argument_list|()
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|ExecutionException
decl||
name|TimeoutException
name|e
parameter_list|)
block|{
name|futureTinyHouse
operator|.
name|cancel
argument_list|(
literal|true
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|error
argument_list|(
literal|"Had issue with trimmer will return full scan "
argument_list|,
name|e
argument_list|)
expr_stmt|;
return|return
name|fullHouse
return|;
block|}
block|}
comment|//Case null: it can be filter evaluated to false or no filter at all thus return full scan
return|return
name|fullHouse
return|;
block|}
finally|finally
block|{
name|execService
operator|.
name|shutdown
argument_list|()
expr_stmt|;
block|}
block|}
specifier|private
specifier|static
name|List
argument_list|<
name|TopicPartition
argument_list|>
name|fetchTopicPartitions
parameter_list|(
name|String
name|topic
parameter_list|,
name|KafkaConsumer
argument_list|<
name|byte
index|[]
argument_list|,
name|byte
index|[]
argument_list|>
name|consumer
parameter_list|)
block|{
comment|// this will block till REQUEST_TIMEOUT_MS_CONFIG = "request.timeout.ms"
comment|// then throws org.apache.kafka.common.errors.TimeoutException if can not fetch metadata
comment|// @TODO add retry logic maybe
name|List
argument_list|<
name|PartitionInfo
argument_list|>
name|partitions
init|=
name|consumer
operator|.
name|partitionsFor
argument_list|(
name|topic
argument_list|)
decl_stmt|;
return|return
name|partitions
operator|.
name|stream
argument_list|()
operator|.
name|map
argument_list|(
name|p
lambda|->
operator|new
name|TopicPartition
argument_list|(
name|topic
argument_list|,
name|p
operator|.
name|partition
argument_list|()
argument_list|)
argument_list|)
operator|.
name|collect
argument_list|(
name|Collectors
operator|.
name|toList
argument_list|()
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|public
name|RecordReader
argument_list|<
name|NullWritable
argument_list|,
name|KafkaRecordWritable
argument_list|>
name|getRecordReader
parameter_list|(
name|InputSplit
name|inputSplit
parameter_list|,
name|JobConf
name|jobConf
parameter_list|,
name|Reporter
name|reporter
parameter_list|)
block|{
return|return
operator|new
name|KafkaPullerRecordReader
argument_list|(
operator|(
name|KafkaPullerInputSplit
operator|)
name|inputSplit
argument_list|,
name|jobConf
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|public
name|List
argument_list|<
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|InputSplit
argument_list|>
name|getSplits
parameter_list|(
name|JobContext
name|jobContext
parameter_list|)
throws|throws
name|IOException
throws|,
name|InterruptedException
block|{
return|return
name|computeSplits
argument_list|(
name|jobContext
operator|.
name|getConfiguration
argument_list|()
argument_list|)
operator|.
name|stream
argument_list|()
operator|.
name|map
argument_list|(
name|kafkaPullerInputSplit
lambda|->
operator|(
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|InputSplit
operator|)
name|kafkaPullerInputSplit
argument_list|)
operator|.
name|collect
argument_list|(
name|Collectors
operator|.
name|toList
argument_list|()
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|public
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|RecordReader
argument_list|<
name|NullWritable
argument_list|,
name|KafkaRecordWritable
argument_list|>
name|createRecordReader
parameter_list|(
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapreduce
operator|.
name|InputSplit
name|inputSplit
parameter_list|,
name|TaskAttemptContext
name|taskAttemptContext
parameter_list|)
block|{
return|return
operator|new
name|KafkaPullerRecordReader
argument_list|()
return|;
block|}
block|}
end_class

end_unit

