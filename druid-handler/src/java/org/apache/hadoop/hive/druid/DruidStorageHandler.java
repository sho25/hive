begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/*  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|druid
package|;
end_package

begin_import
import|import
name|com
operator|.
name|fasterxml
operator|.
name|jackson
operator|.
name|core
operator|.
name|type
operator|.
name|TypeReference
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|annotations
operator|.
name|VisibleForTesting
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|base
operator|.
name|Preconditions
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|base
operator|.
name|Strings
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|base
operator|.
name|Supplier
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|base
operator|.
name|Suppliers
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|base
operator|.
name|Throwables
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|collect
operator|.
name|ImmutableList
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|collect
operator|.
name|ImmutableSet
import|;
end_import

begin_import
import|import
name|com
operator|.
name|google
operator|.
name|common
operator|.
name|collect
operator|.
name|Sets
import|;
end_import

begin_import
import|import
name|io
operator|.
name|druid
operator|.
name|data
operator|.
name|input
operator|.
name|impl
operator|.
name|DimensionSchema
import|;
end_import

begin_import
import|import
name|io
operator|.
name|druid
operator|.
name|data
operator|.
name|input
operator|.
name|impl
operator|.
name|DimensionsSpec
import|;
end_import

begin_import
import|import
name|io
operator|.
name|druid
operator|.
name|data
operator|.
name|input
operator|.
name|impl
operator|.
name|InputRowParser
import|;
end_import

begin_import
import|import
name|io
operator|.
name|druid
operator|.
name|data
operator|.
name|input
operator|.
name|impl
operator|.
name|TimestampSpec
import|;
end_import

begin_import
import|import
name|io
operator|.
name|druid
operator|.
name|java
operator|.
name|util
operator|.
name|common
operator|.
name|Pair
import|;
end_import

begin_import
import|import
name|io
operator|.
name|druid
operator|.
name|java
operator|.
name|util
operator|.
name|common
operator|.
name|RetryUtils
import|;
end_import

begin_import
import|import
name|io
operator|.
name|druid
operator|.
name|java
operator|.
name|util
operator|.
name|common
operator|.
name|lifecycle
operator|.
name|Lifecycle
import|;
end_import

begin_import
import|import
name|io
operator|.
name|druid
operator|.
name|java
operator|.
name|util
operator|.
name|http
operator|.
name|client
operator|.
name|HttpClient
import|;
end_import

begin_import
import|import
name|io
operator|.
name|druid
operator|.
name|java
operator|.
name|util
operator|.
name|http
operator|.
name|client
operator|.
name|HttpClientConfig
import|;
end_import

begin_import
import|import
name|io
operator|.
name|druid
operator|.
name|java
operator|.
name|util
operator|.
name|http
operator|.
name|client
operator|.
name|HttpClientInit
import|;
end_import

begin_import
import|import
name|io
operator|.
name|druid
operator|.
name|java
operator|.
name|util
operator|.
name|http
operator|.
name|client
operator|.
name|Request
import|;
end_import

begin_import
import|import
name|io
operator|.
name|druid
operator|.
name|java
operator|.
name|util
operator|.
name|http
operator|.
name|client
operator|.
name|response
operator|.
name|FullResponseHandler
import|;
end_import

begin_import
import|import
name|io
operator|.
name|druid
operator|.
name|java
operator|.
name|util
operator|.
name|http
operator|.
name|client
operator|.
name|response
operator|.
name|FullResponseHolder
import|;
end_import

begin_import
import|import
name|io
operator|.
name|druid
operator|.
name|metadata
operator|.
name|MetadataStorageConnectorConfig
import|;
end_import

begin_import
import|import
name|io
operator|.
name|druid
operator|.
name|metadata
operator|.
name|MetadataStorageTablesConfig
import|;
end_import

begin_import
import|import
name|io
operator|.
name|druid
operator|.
name|metadata
operator|.
name|SQLMetadataConnector
import|;
end_import

begin_import
import|import
name|io
operator|.
name|druid
operator|.
name|metadata
operator|.
name|storage
operator|.
name|derby
operator|.
name|DerbyConnector
import|;
end_import

begin_import
import|import
name|io
operator|.
name|druid
operator|.
name|metadata
operator|.
name|storage
operator|.
name|derby
operator|.
name|DerbyMetadataStorage
import|;
end_import

begin_import
import|import
name|io
operator|.
name|druid
operator|.
name|metadata
operator|.
name|storage
operator|.
name|mysql
operator|.
name|MySQLConnector
import|;
end_import

begin_import
import|import
name|io
operator|.
name|druid
operator|.
name|metadata
operator|.
name|storage
operator|.
name|mysql
operator|.
name|MySQLConnectorConfig
import|;
end_import

begin_import
import|import
name|io
operator|.
name|druid
operator|.
name|metadata
operator|.
name|storage
operator|.
name|postgresql
operator|.
name|PostgreSQLConnector
import|;
end_import

begin_import
import|import
name|io
operator|.
name|druid
operator|.
name|query
operator|.
name|aggregation
operator|.
name|AggregatorFactory
import|;
end_import

begin_import
import|import
name|io
operator|.
name|druid
operator|.
name|segment
operator|.
name|IndexSpec
import|;
end_import

begin_import
import|import
name|io
operator|.
name|druid
operator|.
name|segment
operator|.
name|indexing
operator|.
name|DataSchema
import|;
end_import

begin_import
import|import
name|io
operator|.
name|druid
operator|.
name|segment
operator|.
name|indexing
operator|.
name|granularity
operator|.
name|GranularitySpec
import|;
end_import

begin_import
import|import
name|io
operator|.
name|druid
operator|.
name|segment
operator|.
name|loading
operator|.
name|DataSegmentPusher
import|;
end_import

begin_import
import|import
name|io
operator|.
name|druid
operator|.
name|segment
operator|.
name|loading
operator|.
name|SegmentLoadingException
import|;
end_import

begin_import
import|import
name|io
operator|.
name|druid
operator|.
name|storage
operator|.
name|hdfs
operator|.
name|HdfsDataSegmentPusher
import|;
end_import

begin_import
import|import
name|io
operator|.
name|druid
operator|.
name|storage
operator|.
name|hdfs
operator|.
name|HdfsDataSegmentPusherConfig
import|;
end_import

begin_import
import|import
name|io
operator|.
name|druid
operator|.
name|timeline
operator|.
name|DataSegment
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|lang3
operator|.
name|StringUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configuration
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|common
operator|.
name|TableName
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|Constants
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|druid
operator|.
name|conf
operator|.
name|DruidConstants
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|druid
operator|.
name|io
operator|.
name|DruidOutputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|druid
operator|.
name|io
operator|.
name|DruidQueryBasedInputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|druid
operator|.
name|io
operator|.
name|DruidRecordWriter
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|druid
operator|.
name|json
operator|.
name|KafkaSupervisorReport
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|druid
operator|.
name|json
operator|.
name|KafkaSupervisorSpec
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|druid
operator|.
name|security
operator|.
name|KerberosHttpClient
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|druid
operator|.
name|serde
operator|.
name|DruidSerDe
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|DefaultHiveMetaHook
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|HiveMetaHook
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|EnvironmentContext
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|FieldSchema
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|LockType
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|MetaException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|api
operator|.
name|Table
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|utils
operator|.
name|MetaStoreUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|hooks
operator|.
name|WriteEntity
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|HiveStorageHandler
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|StorageHandlerInfo
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|TableDesc
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|security
operator|.
name|authorization
operator|.
name|DefaultHiveAuthorizationProvider
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|security
operator|.
name|authorization
operator|.
name|HiveAuthorizationProvider
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|session
operator|.
name|SessionState
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|AbstractSerDe
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|typeinfo
operator|.
name|TypeInfo
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|serde2
operator|.
name|typeinfo
operator|.
name|TypeInfoUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|InputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|JobConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|mapred
operator|.
name|OutputFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|security
operator|.
name|UserGroupInformation
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hive
operator|.
name|common
operator|.
name|util
operator|.
name|ShutdownHookManager
import|;
end_import

begin_import
import|import
name|org
operator|.
name|jboss
operator|.
name|netty
operator|.
name|handler
operator|.
name|codec
operator|.
name|http
operator|.
name|HttpMethod
import|;
end_import

begin_import
import|import
name|org
operator|.
name|jboss
operator|.
name|netty
operator|.
name|handler
operator|.
name|codec
operator|.
name|http
operator|.
name|HttpResponseStatus
import|;
end_import

begin_import
import|import
name|org
operator|.
name|joda
operator|.
name|time
operator|.
name|DateTime
import|;
end_import

begin_import
import|import
name|org
operator|.
name|joda
operator|.
name|time
operator|.
name|Period
import|;
end_import

begin_import
import|import
name|org
operator|.
name|skife
operator|.
name|jdbi
operator|.
name|v2
operator|.
name|exceptions
operator|.
name|CallbackFailedException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|Logger
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|LoggerFactory
import|;
end_import

begin_import
import|import
name|javax
operator|.
name|annotation
operator|.
name|Nullable
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|net
operator|.
name|MalformedURLException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|net
operator|.
name|URL
import|;
end_import

begin_import
import|import
name|java
operator|.
name|nio
operator|.
name|charset
operator|.
name|Charset
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Collection
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Collections
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Properties
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Set
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|concurrent
operator|.
name|ExecutionException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|stream
operator|.
name|Collectors
import|;
end_import

begin_import
import|import static
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|druid
operator|.
name|DruidStorageHandlerUtils
operator|.
name|JSON_MAPPER
import|;
end_import

begin_comment
comment|/**  * DruidStorageHandler provides a HiveStorageHandler implementation for Druid.  */
end_comment

begin_class
annotation|@
name|SuppressWarnings
argument_list|(
block|{
literal|"rawtypes"
block|}
argument_list|)
specifier|public
class|class
name|DruidStorageHandler
extends|extends
name|DefaultHiveMetaHook
implements|implements
name|HiveStorageHandler
block|{
specifier|private
specifier|static
specifier|final
name|Logger
name|LOG
init|=
name|LoggerFactory
operator|.
name|getLogger
argument_list|(
name|DruidStorageHandler
operator|.
name|class
argument_list|)
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|SessionState
operator|.
name|LogHelper
name|CONSOLE
init|=
operator|new
name|SessionState
operator|.
name|LogHelper
argument_list|(
name|LOG
argument_list|)
decl_stmt|;
specifier|public
specifier|static
specifier|final
name|String
name|SEGMENTS_DESCRIPTOR_DIR_NAME
init|=
literal|"segmentsDescriptorDir"
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|String
name|INTERMEDIATE_SEGMENT_DIR_NAME
init|=
literal|"intermediateSegmentDir"
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|HttpClient
name|HTTP_CLIENT
decl_stmt|;
specifier|private
specifier|static
specifier|final
name|List
argument_list|<
name|String
argument_list|>
name|ALLOWED_ALTER_TYPES
init|=
name|ImmutableList
operator|.
name|of
argument_list|(
literal|"ADDPROPS"
argument_list|,
literal|"DROPPROPS"
argument_list|,
literal|"ADDCOLS"
argument_list|)
decl_stmt|;
static|static
block|{
specifier|final
name|Lifecycle
name|lifecycle
init|=
operator|new
name|Lifecycle
argument_list|()
decl_stmt|;
try|try
block|{
name|lifecycle
operator|.
name|start
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Issues with lifecycle start"
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
name|HTTP_CLIENT
operator|=
name|makeHttpClient
argument_list|(
name|lifecycle
argument_list|)
expr_stmt|;
name|ShutdownHookManager
operator|.
name|addShutdownHook
argument_list|(
name|lifecycle
operator|::
name|stop
argument_list|)
expr_stmt|;
block|}
specifier|private
name|SQLMetadataConnector
name|connector
decl_stmt|;
specifier|private
name|MetadataStorageTablesConfig
name|druidMetadataStorageTablesConfig
init|=
literal|null
decl_stmt|;
specifier|private
name|String
name|uniqueId
init|=
literal|null
decl_stmt|;
specifier|private
name|String
name|rootWorkingDir
init|=
literal|null
decl_stmt|;
specifier|private
name|Configuration
name|conf
decl_stmt|;
specifier|public
name|DruidStorageHandler
parameter_list|()
block|{   }
annotation|@
name|VisibleForTesting
specifier|public
name|DruidStorageHandler
parameter_list|(
name|SQLMetadataConnector
name|connector
parameter_list|,
name|MetadataStorageTablesConfig
name|druidMetadataStorageTablesConfig
parameter_list|)
block|{
name|this
operator|.
name|connector
operator|=
name|connector
expr_stmt|;
name|this
operator|.
name|druidMetadataStorageTablesConfig
operator|=
name|druidMetadataStorageTablesConfig
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|Class
argument_list|<
name|?
extends|extends
name|InputFormat
argument_list|>
name|getInputFormatClass
parameter_list|()
block|{
return|return
name|DruidQueryBasedInputFormat
operator|.
name|class
return|;
block|}
annotation|@
name|Override
specifier|public
name|Class
argument_list|<
name|?
extends|extends
name|OutputFormat
argument_list|>
name|getOutputFormatClass
parameter_list|()
block|{
return|return
name|DruidOutputFormat
operator|.
name|class
return|;
block|}
annotation|@
name|Override
specifier|public
name|Class
argument_list|<
name|?
extends|extends
name|AbstractSerDe
argument_list|>
name|getSerDeClass
parameter_list|()
block|{
return|return
name|DruidSerDe
operator|.
name|class
return|;
block|}
annotation|@
name|Override
specifier|public
name|HiveMetaHook
name|getMetaHook
parameter_list|()
block|{
return|return
name|this
return|;
block|}
annotation|@
name|Override
specifier|public
name|HiveAuthorizationProvider
name|getAuthorizationProvider
parameter_list|()
block|{
return|return
operator|new
name|DefaultHiveAuthorizationProvider
argument_list|()
return|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|configureInputJobProperties
parameter_list|(
name|TableDesc
name|tableDesc
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|jobProperties
parameter_list|)
block|{    }
annotation|@
name|Override
specifier|public
name|void
name|configureInputJobCredentials
parameter_list|(
name|TableDesc
name|tableDesc
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|jobSecrets
parameter_list|)
block|{    }
annotation|@
name|Override
specifier|public
name|void
name|preCreateTable
parameter_list|(
name|Table
name|table
parameter_list|)
throws|throws
name|MetaException
block|{
if|if
condition|(
operator|!
name|StringUtils
operator|.
name|isEmpty
argument_list|(
name|table
operator|.
name|getSd
argument_list|()
operator|.
name|getLocation
argument_list|()
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|MetaException
argument_list|(
literal|"LOCATION may not be specified for Druid"
argument_list|)
throw|;
block|}
if|if
condition|(
name|table
operator|.
name|getPartitionKeysSize
argument_list|()
operator|!=
literal|0
condition|)
block|{
throw|throw
operator|new
name|MetaException
argument_list|(
literal|"PARTITIONED BY may not be specified for Druid"
argument_list|)
throw|;
block|}
if|if
condition|(
name|table
operator|.
name|getSd
argument_list|()
operator|.
name|getBucketColsSize
argument_list|()
operator|!=
literal|0
condition|)
block|{
throw|throw
operator|new
name|MetaException
argument_list|(
literal|"CLUSTERED BY may not be specified for Druid"
argument_list|)
throw|;
block|}
name|String
name|dataSourceName
init|=
name|table
operator|.
name|getParameters
argument_list|()
operator|.
name|get
argument_list|(
name|Constants
operator|.
name|DRUID_DATA_SOURCE
argument_list|)
decl_stmt|;
if|if
condition|(
name|dataSourceName
operator|!=
literal|null
condition|)
block|{
comment|// Already Existing datasource in Druid.
return|return;
block|}
comment|// create dataSourceName based on Hive Table name
name|dataSourceName
operator|=
name|TableName
operator|.
name|getDbTable
argument_list|(
name|table
operator|.
name|getDbName
argument_list|()
argument_list|,
name|table
operator|.
name|getTableName
argument_list|()
argument_list|)
expr_stmt|;
try|try
block|{
comment|// NOTE: This just created druid_segments table in Druid metastore.
comment|// This is needed for the case when hive is started before any of druid services
comment|// and druid_segments table has not been created yet.
name|getConnector
argument_list|()
operator|.
name|createSegmentTable
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Exception while trying to create druid segments table"
argument_list|,
name|e
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|MetaException
argument_list|(
name|e
operator|.
name|getMessage
argument_list|()
argument_list|)
throw|;
block|}
name|Collection
argument_list|<
name|String
argument_list|>
name|existingDataSources
init|=
name|DruidStorageHandlerUtils
operator|.
name|getAllDataSourceNames
argument_list|(
name|getConnector
argument_list|()
argument_list|,
name|getDruidMetadataStorageTablesConfig
argument_list|()
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"pre-create data source with name {}"
argument_list|,
name|dataSourceName
argument_list|)
expr_stmt|;
comment|// Check for existence of for the datasource we are going to create in druid_segments table.
if|if
condition|(
name|existingDataSources
operator|.
name|contains
argument_list|(
name|dataSourceName
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|MetaException
argument_list|(
name|String
operator|.
name|format
argument_list|(
literal|"Data source [%s] already existing"
argument_list|,
name|dataSourceName
argument_list|)
argument_list|)
throw|;
block|}
name|table
operator|.
name|getParameters
argument_list|()
operator|.
name|put
argument_list|(
name|Constants
operator|.
name|DRUID_DATA_SOURCE
argument_list|,
name|dataSourceName
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|rollbackCreateTable
parameter_list|(
name|Table
name|table
parameter_list|)
block|{
name|cleanWorkingDir
argument_list|()
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|commitCreateTable
parameter_list|(
name|Table
name|table
parameter_list|)
throws|throws
name|MetaException
block|{
if|if
condition|(
name|DruidKafkaUtils
operator|.
name|isKafkaStreamingTable
argument_list|(
name|table
argument_list|)
condition|)
block|{
name|updateKafkaIngestion
argument_list|(
name|table
argument_list|)
expr_stmt|;
block|}
comment|// For CTAS queries when user has explicitly specified the datasource.
comment|// We will append the data to existing druid datasource.
name|this
operator|.
name|commitInsertTable
argument_list|(
name|table
argument_list|,
literal|false
argument_list|)
expr_stmt|;
block|}
specifier|private
name|void
name|updateKafkaIngestion
parameter_list|(
name|Table
name|table
parameter_list|)
block|{
specifier|final
name|String
name|overlordAddress
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|getConf
argument_list|()
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_DRUID_OVERLORD_DEFAULT_ADDRESS
argument_list|)
decl_stmt|;
specifier|final
name|String
name|dataSourceName
init|=
name|Preconditions
operator|.
name|checkNotNull
argument_list|(
name|DruidStorageHandlerUtils
operator|.
name|getTableProperty
argument_list|(
name|table
argument_list|,
name|Constants
operator|.
name|DRUID_DATA_SOURCE
argument_list|)
argument_list|,
literal|"Druid datasource name is null"
argument_list|)
decl_stmt|;
specifier|final
name|String
name|kafkaTopic
init|=
name|Preconditions
operator|.
name|checkNotNull
argument_list|(
name|DruidStorageHandlerUtils
operator|.
name|getTableProperty
argument_list|(
name|table
argument_list|,
name|DruidConstants
operator|.
name|KAFKA_TOPIC
argument_list|)
argument_list|,
literal|"kafka topic is null"
argument_list|)
decl_stmt|;
specifier|final
name|String
name|kafkaServers
init|=
name|Preconditions
operator|.
name|checkNotNull
argument_list|(
name|DruidStorageHandlerUtils
operator|.
name|getTableProperty
argument_list|(
name|table
argument_list|,
name|DruidConstants
operator|.
name|KAFKA_BOOTSTRAP_SERVERS
argument_list|)
argument_list|,
literal|"kafka connect string is null"
argument_list|)
decl_stmt|;
name|Properties
name|tableProperties
init|=
operator|new
name|Properties
argument_list|()
decl_stmt|;
name|tableProperties
operator|.
name|putAll
argument_list|(
name|table
operator|.
name|getParameters
argument_list|()
argument_list|)
expr_stmt|;
specifier|final
name|GranularitySpec
name|granularitySpec
init|=
name|DruidStorageHandlerUtils
operator|.
name|getGranularitySpec
argument_list|(
name|getConf
argument_list|()
argument_list|,
name|tableProperties
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|FieldSchema
argument_list|>
name|columns
init|=
name|table
operator|.
name|getSd
argument_list|()
operator|.
name|getCols
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|String
argument_list|>
name|columnNames
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|(
name|columns
operator|.
name|size
argument_list|()
argument_list|)
decl_stmt|;
name|List
argument_list|<
name|TypeInfo
argument_list|>
name|columnTypes
init|=
operator|new
name|ArrayList
argument_list|<>
argument_list|(
name|columns
operator|.
name|size
argument_list|()
argument_list|)
decl_stmt|;
for|for
control|(
name|FieldSchema
name|schema
range|:
name|columns
control|)
block|{
name|columnNames
operator|.
name|add
argument_list|(
name|schema
operator|.
name|getName
argument_list|()
argument_list|)
expr_stmt|;
name|columnTypes
operator|.
name|add
argument_list|(
name|TypeInfoUtils
operator|.
name|getTypeInfoFromTypeString
argument_list|(
name|schema
operator|.
name|getType
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
name|Pair
argument_list|<
name|List
argument_list|<
name|DimensionSchema
argument_list|>
argument_list|,
name|AggregatorFactory
index|[]
argument_list|>
name|dimensionsAndAggregates
init|=
name|DruidStorageHandlerUtils
operator|.
name|getDimensionsAndAggregates
argument_list|(
name|columnNames
argument_list|,
name|columnTypes
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|columnNames
operator|.
name|contains
argument_list|(
name|DruidConstants
operator|.
name|DEFAULT_TIMESTAMP_COLUMN
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|IllegalStateException
argument_list|(
literal|"Timestamp column (' "
operator|+
name|DruidConstants
operator|.
name|DEFAULT_TIMESTAMP_COLUMN
operator|+
literal|"') not specified in create table; list of columns is : "
operator|+
name|columnNames
argument_list|)
throw|;
block|}
name|DimensionsSpec
name|dimensionsSpec
init|=
operator|new
name|DimensionsSpec
argument_list|(
name|dimensionsAndAggregates
operator|.
name|lhs
argument_list|,
literal|null
argument_list|,
literal|null
argument_list|)
decl_stmt|;
name|String
name|timestampFormat
init|=
name|DruidStorageHandlerUtils
operator|.
name|getTableProperty
argument_list|(
name|table
argument_list|,
name|DruidConstants
operator|.
name|DRUID_TIMESTAMP_FORMAT
argument_list|)
decl_stmt|;
name|String
name|timestampColumnName
init|=
name|DruidStorageHandlerUtils
operator|.
name|getTableProperty
argument_list|(
name|table
argument_list|,
name|DruidConstants
operator|.
name|DRUID_TIMESTAMP_COLUMN
argument_list|)
decl_stmt|;
if|if
condition|(
name|timestampColumnName
operator|==
literal|null
condition|)
block|{
name|timestampColumnName
operator|=
name|DruidConstants
operator|.
name|DEFAULT_TIMESTAMP_COLUMN
expr_stmt|;
block|}
specifier|final
name|TimestampSpec
name|timestampSpec
init|=
operator|new
name|TimestampSpec
argument_list|(
name|timestampColumnName
argument_list|,
name|timestampFormat
argument_list|,
literal|null
argument_list|)
decl_stmt|;
specifier|final
name|InputRowParser
name|inputRowParser
init|=
name|DruidKafkaUtils
operator|.
name|getInputRowParser
argument_list|(
name|table
argument_list|,
name|timestampSpec
argument_list|,
name|dimensionsSpec
argument_list|)
decl_stmt|;
specifier|final
name|Map
argument_list|<
name|String
argument_list|,
name|Object
argument_list|>
name|inputParser
init|=
name|JSON_MAPPER
operator|.
name|convertValue
argument_list|(
name|inputRowParser
argument_list|,
operator|new
name|TypeReference
argument_list|<
name|Map
argument_list|<
name|String
argument_list|,
name|Object
argument_list|>
argument_list|>
argument_list|()
block|{         }
argument_list|)
decl_stmt|;
specifier|final
name|DataSchema
name|dataSchema
init|=
operator|new
name|DataSchema
argument_list|(
name|dataSourceName
argument_list|,
name|inputParser
argument_list|,
name|dimensionsAndAggregates
operator|.
name|rhs
argument_list|,
name|granularitySpec
argument_list|,
literal|null
argument_list|,
name|DruidStorageHandlerUtils
operator|.
name|JSON_MAPPER
argument_list|)
decl_stmt|;
name|IndexSpec
name|indexSpec
init|=
name|DruidStorageHandlerUtils
operator|.
name|getIndexSpec
argument_list|(
name|getConf
argument_list|()
argument_list|)
decl_stmt|;
name|KafkaSupervisorSpec
name|spec
init|=
name|DruidKafkaUtils
operator|.
name|createKafkaSupervisorSpec
argument_list|(
name|table
argument_list|,
name|kafkaTopic
argument_list|,
name|kafkaServers
argument_list|,
name|dataSchema
argument_list|,
name|indexSpec
argument_list|)
decl_stmt|;
comment|// Fetch existing Ingestion Spec from Druid, if any
name|KafkaSupervisorSpec
name|existingSpec
init|=
name|fetchKafkaIngestionSpec
argument_list|(
name|table
argument_list|)
decl_stmt|;
name|String
name|targetState
init|=
name|DruidStorageHandlerUtils
operator|.
name|getTableProperty
argument_list|(
name|table
argument_list|,
name|DruidConstants
operator|.
name|DRUID_KAFKA_INGESTION
argument_list|)
decl_stmt|;
if|if
condition|(
name|targetState
operator|==
literal|null
condition|)
block|{
comment|// Case when user has not specified any ingestion state in the current command
comment|// if there is a kafka supervisor running then keep it last known state is START otherwise STOP.
name|targetState
operator|=
name|existingSpec
operator|==
literal|null
condition|?
literal|"STOP"
else|:
literal|"START"
expr_stmt|;
block|}
if|if
condition|(
literal|"STOP"
operator|.
name|equalsIgnoreCase
argument_list|(
name|targetState
argument_list|)
condition|)
block|{
if|if
condition|(
name|existingSpec
operator|!=
literal|null
condition|)
block|{
name|stopKafkaIngestion
argument_list|(
name|overlordAddress
argument_list|,
name|dataSourceName
argument_list|)
expr_stmt|;
block|}
block|}
elseif|else
if|if
condition|(
literal|"START"
operator|.
name|equalsIgnoreCase
argument_list|(
name|targetState
argument_list|)
condition|)
block|{
if|if
condition|(
name|existingSpec
operator|==
literal|null
operator|||
operator|!
name|existingSpec
operator|.
name|equals
argument_list|(
name|spec
argument_list|)
condition|)
block|{
name|DruidKafkaUtils
operator|.
name|updateKafkaIngestionSpec
argument_list|(
name|overlordAddress
argument_list|,
name|spec
argument_list|)
expr_stmt|;
block|}
block|}
elseif|else
if|if
condition|(
literal|"RESET"
operator|.
name|equalsIgnoreCase
argument_list|(
name|targetState
argument_list|)
condition|)
block|{
comment|// Case when there are changes in multiple table properties.
if|if
condition|(
name|existingSpec
operator|!=
literal|null
operator|&&
operator|!
name|existingSpec
operator|.
name|equals
argument_list|(
name|spec
argument_list|)
condition|)
block|{
name|DruidKafkaUtils
operator|.
name|updateKafkaIngestionSpec
argument_list|(
name|overlordAddress
argument_list|,
name|spec
argument_list|)
expr_stmt|;
block|}
name|resetKafkaIngestion
argument_list|(
name|overlordAddress
argument_list|,
name|dataSourceName
argument_list|)
expr_stmt|;
block|}
else|else
block|{
throw|throw
operator|new
name|IllegalArgumentException
argument_list|(
name|String
operator|.
name|format
argument_list|(
literal|"Invalid value for property [%s], Valid values are [START, STOP, RESET]"
argument_list|,
name|DruidConstants
operator|.
name|DRUID_KAFKA_INGESTION
argument_list|)
argument_list|)
throw|;
block|}
comment|// We do not want to keep state in two separate places so remove from hive table properties.
name|table
operator|.
name|getParameters
argument_list|()
operator|.
name|remove
argument_list|(
name|DruidConstants
operator|.
name|DRUID_KAFKA_INGESTION
argument_list|)
expr_stmt|;
block|}
specifier|private
name|void
name|resetKafkaIngestion
parameter_list|(
name|String
name|overlordAddress
parameter_list|,
name|String
name|dataSourceName
parameter_list|)
block|{
try|try
block|{
name|FullResponseHolder
name|response
init|=
name|RetryUtils
operator|.
name|retry
argument_list|(
parameter_list|()
lambda|->
name|DruidStorageHandlerUtils
operator|.
name|getResponseFromCurrentLeader
argument_list|(
name|getHttpClient
argument_list|()
argument_list|,
operator|new
name|Request
argument_list|(
name|HttpMethod
operator|.
name|POST
argument_list|,
operator|new
name|URL
argument_list|(
name|String
operator|.
name|format
argument_list|(
literal|"http://%s/druid/indexer/v1/supervisor/%s/reset"
argument_list|,
name|overlordAddress
argument_list|,
name|dataSourceName
argument_list|)
argument_list|)
argument_list|)
argument_list|,
operator|new
name|FullResponseHandler
argument_list|(
name|Charset
operator|.
name|forName
argument_list|(
literal|"UTF-8"
argument_list|)
argument_list|)
argument_list|)
argument_list|,
name|input
lambda|->
name|input
operator|instanceof
name|IOException
argument_list|,
name|getMaxRetryCount
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|response
operator|.
name|getStatus
argument_list|()
operator|.
name|equals
argument_list|(
name|HttpResponseStatus
operator|.
name|OK
argument_list|)
condition|)
block|{
name|CONSOLE
operator|.
name|printInfo
argument_list|(
literal|"Druid Kafka Ingestion Reset successful."
argument_list|)
expr_stmt|;
block|}
else|else
block|{
throw|throw
operator|new
name|IOException
argument_list|(
name|String
operator|.
name|format
argument_list|(
literal|"Unable to reset Kafka Ingestion Druid status [%d] full response [%s]"
argument_list|,
name|response
operator|.
name|getStatus
argument_list|()
operator|.
name|getCode
argument_list|()
argument_list|,
name|response
operator|.
name|getContent
argument_list|()
argument_list|)
argument_list|)
throw|;
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|private
name|void
name|stopKafkaIngestion
parameter_list|(
name|String
name|overlordAddress
parameter_list|,
name|String
name|dataSourceName
parameter_list|)
block|{
try|try
block|{
name|FullResponseHolder
name|response
init|=
name|RetryUtils
operator|.
name|retry
argument_list|(
parameter_list|()
lambda|->
name|DruidStorageHandlerUtils
operator|.
name|getResponseFromCurrentLeader
argument_list|(
name|getHttpClient
argument_list|()
argument_list|,
operator|new
name|Request
argument_list|(
name|HttpMethod
operator|.
name|POST
argument_list|,
operator|new
name|URL
argument_list|(
name|String
operator|.
name|format
argument_list|(
literal|"http://%s/druid/indexer/v1/supervisor/%s/shutdown"
argument_list|,
name|overlordAddress
argument_list|,
name|dataSourceName
argument_list|)
argument_list|)
argument_list|)
argument_list|,
operator|new
name|FullResponseHandler
argument_list|(
name|Charset
operator|.
name|forName
argument_list|(
literal|"UTF-8"
argument_list|)
argument_list|)
argument_list|)
argument_list|,
name|input
lambda|->
name|input
operator|instanceof
name|IOException
argument_list|,
name|getMaxRetryCount
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|response
operator|.
name|getStatus
argument_list|()
operator|.
name|equals
argument_list|(
name|HttpResponseStatus
operator|.
name|OK
argument_list|)
condition|)
block|{
name|CONSOLE
operator|.
name|printInfo
argument_list|(
literal|"Druid Kafka Ingestion shutdown successful."
argument_list|)
expr_stmt|;
block|}
else|else
block|{
throw|throw
operator|new
name|IOException
argument_list|(
name|String
operator|.
name|format
argument_list|(
literal|"Unable to stop Kafka Ingestion Druid status [%d] full response [%s]"
argument_list|,
name|response
operator|.
name|getStatus
argument_list|()
operator|.
name|getCode
argument_list|()
argument_list|,
name|response
operator|.
name|getContent
argument_list|()
argument_list|)
argument_list|)
throw|;
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
specifier|private
name|KafkaSupervisorSpec
name|fetchKafkaIngestionSpec
parameter_list|(
name|Table
name|table
parameter_list|)
block|{
comment|// Stop Kafka Ingestion first
specifier|final
name|String
name|overlordAddress
init|=
name|Preconditions
operator|.
name|checkNotNull
argument_list|(
name|HiveConf
operator|.
name|getVar
argument_list|(
name|getConf
argument_list|()
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_DRUID_OVERLORD_DEFAULT_ADDRESS
argument_list|)
argument_list|,
literal|"Druid Overlord Address is null"
argument_list|)
decl_stmt|;
name|String
name|dataSourceName
init|=
name|Preconditions
operator|.
name|checkNotNull
argument_list|(
name|DruidStorageHandlerUtils
operator|.
name|getTableProperty
argument_list|(
name|table
argument_list|,
name|Constants
operator|.
name|DRUID_DATA_SOURCE
argument_list|)
argument_list|,
literal|"Druid Datasource name is null"
argument_list|)
decl_stmt|;
try|try
block|{
name|FullResponseHolder
name|response
init|=
name|RetryUtils
operator|.
name|retry
argument_list|(
parameter_list|()
lambda|->
name|DruidStorageHandlerUtils
operator|.
name|getResponseFromCurrentLeader
argument_list|(
name|getHttpClient
argument_list|()
argument_list|,
operator|new
name|Request
argument_list|(
name|HttpMethod
operator|.
name|GET
argument_list|,
operator|new
name|URL
argument_list|(
name|String
operator|.
name|format
argument_list|(
literal|"http://%s/druid/indexer/v1/supervisor/%s"
argument_list|,
name|overlordAddress
argument_list|,
name|dataSourceName
argument_list|)
argument_list|)
argument_list|)
argument_list|,
operator|new
name|FullResponseHandler
argument_list|(
name|Charset
operator|.
name|forName
argument_list|(
literal|"UTF-8"
argument_list|)
argument_list|)
argument_list|)
argument_list|,
name|input
lambda|->
name|input
operator|instanceof
name|IOException
argument_list|,
name|getMaxRetryCount
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|response
operator|.
name|getStatus
argument_list|()
operator|.
name|equals
argument_list|(
name|HttpResponseStatus
operator|.
name|OK
argument_list|)
condition|)
block|{
return|return
name|JSON_MAPPER
operator|.
name|readValue
argument_list|(
name|response
operator|.
name|getContent
argument_list|()
argument_list|,
name|KafkaSupervisorSpec
operator|.
name|class
argument_list|)
return|;
comment|// Druid Returns 400 Bad Request when not found.
block|}
elseif|else
if|if
condition|(
name|response
operator|.
name|getStatus
argument_list|()
operator|.
name|equals
argument_list|(
name|HttpResponseStatus
operator|.
name|NOT_FOUND
argument_list|)
operator|||
name|response
operator|.
name|getStatus
argument_list|()
operator|.
name|equals
argument_list|(
name|HttpResponseStatus
operator|.
name|BAD_REQUEST
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"No Kafka Supervisor found for datasource[%s]"
argument_list|,
name|dataSourceName
argument_list|)
expr_stmt|;
return|return
literal|null
return|;
block|}
else|else
block|{
throw|throw
operator|new
name|IOException
argument_list|(
name|String
operator|.
name|format
argument_list|(
literal|"Unable to fetch Kafka Ingestion Spec from Druid status [%d] full response [%s]"
argument_list|,
name|response
operator|.
name|getStatus
argument_list|()
operator|.
name|getCode
argument_list|()
argument_list|,
name|response
operator|.
name|getContent
argument_list|()
argument_list|)
argument_list|)
throw|;
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
literal|"Exception while fetching kafka ingestion spec from druid"
argument_list|,
name|e
argument_list|)
throw|;
block|}
block|}
comment|/**    * Fetches kafka supervisor status report from druid overlord. This method will return null if can not fetch report    *    * @param table object.    * @return kafka supervisor report or null when druid overlord is unreachable.    */
annotation|@
name|Nullable
specifier|private
name|KafkaSupervisorReport
name|fetchKafkaSupervisorReport
parameter_list|(
name|Table
name|table
parameter_list|)
block|{
specifier|final
name|String
name|overlordAddress
init|=
name|Preconditions
operator|.
name|checkNotNull
argument_list|(
name|HiveConf
operator|.
name|getVar
argument_list|(
name|getConf
argument_list|()
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_DRUID_OVERLORD_DEFAULT_ADDRESS
argument_list|)
argument_list|,
literal|"Druid Overlord Address is null"
argument_list|)
decl_stmt|;
specifier|final
name|String
name|dataSourceName
init|=
name|Preconditions
operator|.
name|checkNotNull
argument_list|(
name|DruidStorageHandlerUtils
operator|.
name|getTableProperty
argument_list|(
name|table
argument_list|,
name|Constants
operator|.
name|DRUID_DATA_SOURCE
argument_list|)
argument_list|,
literal|"Druid Datasource name is null"
argument_list|)
decl_stmt|;
try|try
block|{
name|FullResponseHolder
name|response
init|=
name|RetryUtils
operator|.
name|retry
argument_list|(
parameter_list|()
lambda|->
name|DruidStorageHandlerUtils
operator|.
name|getResponseFromCurrentLeader
argument_list|(
name|getHttpClient
argument_list|()
argument_list|,
operator|new
name|Request
argument_list|(
name|HttpMethod
operator|.
name|GET
argument_list|,
operator|new
name|URL
argument_list|(
name|String
operator|.
name|format
argument_list|(
literal|"http://%s/druid/indexer/v1/supervisor/%s/status"
argument_list|,
name|overlordAddress
argument_list|,
name|dataSourceName
argument_list|)
argument_list|)
argument_list|)
argument_list|,
operator|new
name|FullResponseHandler
argument_list|(
name|Charset
operator|.
name|forName
argument_list|(
literal|"UTF-8"
argument_list|)
argument_list|)
argument_list|)
argument_list|,
name|input
lambda|->
name|input
operator|instanceof
name|IOException
argument_list|,
name|getMaxRetryCount
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
name|response
operator|.
name|getStatus
argument_list|()
operator|.
name|equals
argument_list|(
name|HttpResponseStatus
operator|.
name|OK
argument_list|)
condition|)
block|{
return|return
name|DruidStorageHandlerUtils
operator|.
name|JSON_MAPPER
operator|.
name|readValue
argument_list|(
name|response
operator|.
name|getContent
argument_list|()
argument_list|,
name|KafkaSupervisorReport
operator|.
name|class
argument_list|)
return|;
comment|// Druid Returns 400 Bad Request when not found.
block|}
elseif|else
if|if
condition|(
name|response
operator|.
name|getStatus
argument_list|()
operator|.
name|equals
argument_list|(
name|HttpResponseStatus
operator|.
name|NOT_FOUND
argument_list|)
operator|||
name|response
operator|.
name|getStatus
argument_list|()
operator|.
name|equals
argument_list|(
name|HttpResponseStatus
operator|.
name|BAD_REQUEST
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"No Kafka Supervisor found for datasource[%s]"
argument_list|,
name|dataSourceName
argument_list|)
expr_stmt|;
return|return
literal|null
return|;
block|}
else|else
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Unable to fetch Kafka Supervisor status [%d] full response [%s]"
argument_list|,
name|response
operator|.
name|getStatus
argument_list|()
operator|.
name|getCode
argument_list|()
argument_list|,
name|response
operator|.
name|getContent
argument_list|()
argument_list|)
expr_stmt|;
return|return
literal|null
return|;
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Exception while fetching kafka ingestion spec from druid"
argument_list|,
name|e
argument_list|)
expr_stmt|;
return|return
literal|null
return|;
block|}
block|}
comment|/**    * Creates metadata moves then commit the Segment's metadata to Druid metadata store in one TxN.    *    * @param table Hive table    * @param overwrite true if it is an insert overwrite table.    */
specifier|private
name|List
argument_list|<
name|DataSegment
argument_list|>
name|loadAndCommitDruidSegments
parameter_list|(
name|Table
name|table
parameter_list|,
name|boolean
name|overwrite
parameter_list|,
name|List
argument_list|<
name|DataSegment
argument_list|>
name|segmentsToLoad
parameter_list|)
throws|throws
name|IOException
throws|,
name|CallbackFailedException
block|{
specifier|final
name|String
name|dataSourceName
init|=
name|table
operator|.
name|getParameters
argument_list|()
operator|.
name|get
argument_list|(
name|Constants
operator|.
name|DRUID_DATA_SOURCE
argument_list|)
decl_stmt|;
specifier|final
name|String
name|segmentDirectory
init|=
name|table
operator|.
name|getParameters
argument_list|()
operator|.
name|get
argument_list|(
name|DruidConstants
operator|.
name|DRUID_SEGMENT_DIRECTORY
argument_list|)
operator|!=
literal|null
condition|?
name|table
operator|.
name|getParameters
argument_list|()
operator|.
name|get
argument_list|(
name|DruidConstants
operator|.
name|DRUID_SEGMENT_DIRECTORY
argument_list|)
else|:
name|HiveConf
operator|.
name|getVar
argument_list|(
name|getConf
argument_list|()
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|DRUID_SEGMENT_DIRECTORY
argument_list|)
decl_stmt|;
specifier|final
name|HdfsDataSegmentPusherConfig
name|hdfsSegmentPusherConfig
init|=
operator|new
name|HdfsDataSegmentPusherConfig
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|DataSegment
argument_list|>
name|publishedDataSegmentList
decl_stmt|;
name|LOG
operator|.
name|info
argument_list|(
name|String
operator|.
name|format
argument_list|(
literal|"Moving [%s] Druid segments from staging directory [%s] to Deep storage [%s]"
argument_list|,
name|segmentsToLoad
operator|.
name|size
argument_list|()
argument_list|,
name|getStagingWorkingDir
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|,
name|segmentDirectory
argument_list|)
argument_list|)
expr_stmt|;
name|hdfsSegmentPusherConfig
operator|.
name|setStorageDirectory
argument_list|(
name|segmentDirectory
argument_list|)
expr_stmt|;
name|DataSegmentPusher
name|dataSegmentPusher
init|=
operator|new
name|HdfsDataSegmentPusher
argument_list|(
name|hdfsSegmentPusherConfig
argument_list|,
name|getConf
argument_list|()
argument_list|,
name|JSON_MAPPER
argument_list|)
decl_stmt|;
name|publishedDataSegmentList
operator|=
name|DruidStorageHandlerUtils
operator|.
name|publishSegmentsAndCommit
argument_list|(
name|getConnector
argument_list|()
argument_list|,
name|getDruidMetadataStorageTablesConfig
argument_list|()
argument_list|,
name|dataSourceName
argument_list|,
name|segmentsToLoad
argument_list|,
name|overwrite
argument_list|,
name|getConf
argument_list|()
argument_list|,
name|dataSegmentPusher
argument_list|)
expr_stmt|;
return|return
name|publishedDataSegmentList
return|;
block|}
comment|/**    * This function checks the load status of Druid segments by polling druid coordinator.    * @param segments List of druid segments to check for    */
specifier|private
name|void
name|checkLoadStatus
parameter_list|(
name|List
argument_list|<
name|DataSegment
argument_list|>
name|segments
parameter_list|)
block|{
specifier|final
name|String
name|coordinatorAddress
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|getConf
argument_list|()
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_DRUID_COORDINATOR_DEFAULT_ADDRESS
argument_list|)
decl_stmt|;
name|int
name|maxTries
init|=
name|getMaxRetryCount
argument_list|()
decl_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"checking load status from coordinator {}"
argument_list|,
name|coordinatorAddress
argument_list|)
expr_stmt|;
name|String
name|coordinatorResponse
decl_stmt|;
try|try
block|{
name|coordinatorResponse
operator|=
name|RetryUtils
operator|.
name|retry
argument_list|(
parameter_list|()
lambda|->
name|DruidStorageHandlerUtils
operator|.
name|getResponseFromCurrentLeader
argument_list|(
name|getHttpClient
argument_list|()
argument_list|,
operator|new
name|Request
argument_list|(
name|HttpMethod
operator|.
name|GET
argument_list|,
operator|new
name|URL
argument_list|(
name|String
operator|.
name|format
argument_list|(
literal|"http://%s/status"
argument_list|,
name|coordinatorAddress
argument_list|)
argument_list|)
argument_list|)
argument_list|,
operator|new
name|FullResponseHandler
argument_list|(
name|Charset
operator|.
name|forName
argument_list|(
literal|"UTF-8"
argument_list|)
argument_list|)
argument_list|)
operator|.
name|getContent
argument_list|()
argument_list|,
name|input
lambda|->
name|input
operator|instanceof
name|IOException
argument_list|,
name|maxTries
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|CONSOLE
operator|.
name|printInfo
argument_list|(
literal|"Will skip waiting for data loading, coordinator unavailable"
argument_list|)
expr_stmt|;
return|return;
block|}
if|if
condition|(
name|Strings
operator|.
name|isNullOrEmpty
argument_list|(
name|coordinatorResponse
argument_list|)
condition|)
block|{
name|CONSOLE
operator|.
name|printInfo
argument_list|(
literal|"Will skip waiting for data loading empty response from coordinator"
argument_list|)
expr_stmt|;
block|}
name|CONSOLE
operator|.
name|printInfo
argument_list|(
name|String
operator|.
name|format
argument_list|(
literal|"Waiting for the loading of [%s] segments"
argument_list|,
name|segments
operator|.
name|size
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
name|long
name|passiveWaitTimeMs
init|=
name|HiveConf
operator|.
name|getLongVar
argument_list|(
name|getConf
argument_list|()
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_DRUID_PASSIVE_WAIT_TIME
argument_list|)
decl_stmt|;
name|Set
argument_list|<
name|URL
argument_list|>
name|urlsOfUnloadedSegments
init|=
name|segments
operator|.
name|stream
argument_list|()
operator|.
name|map
argument_list|(
name|dataSegment
lambda|->
block|{
try|try
block|{
comment|//Need to make sure that we are using segment identifier
return|return
operator|new
name|URL
argument_list|(
name|String
operator|.
name|format
argument_list|(
literal|"http://%s/druid/coordinator/v1/datasources/%s/segments/%s"
argument_list|,
name|coordinatorAddress
argument_list|,
name|dataSegment
operator|.
name|getDataSource
argument_list|()
argument_list|,
name|dataSegment
operator|.
name|getIdentifier
argument_list|()
argument_list|)
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|MalformedURLException
name|e
parameter_list|)
block|{
name|Throwables
operator|.
name|propagate
argument_list|(
name|e
argument_list|)
expr_stmt|;
block|}
return|return
literal|null
return|;
block|}
argument_list|)
operator|.
name|collect
argument_list|(
name|Collectors
operator|.
name|toSet
argument_list|()
argument_list|)
decl_stmt|;
name|int
name|numRetries
init|=
literal|0
decl_stmt|;
while|while
condition|(
name|numRetries
operator|++
operator|<
name|maxTries
operator|&&
operator|!
name|urlsOfUnloadedSegments
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|urlsOfUnloadedSegments
operator|=
name|ImmutableSet
operator|.
name|copyOf
argument_list|(
name|Sets
operator|.
name|filter
argument_list|(
name|urlsOfUnloadedSegments
argument_list|,
name|input
lambda|->
block|{
try|try
block|{
name|String
name|result
init|=
name|DruidStorageHandlerUtils
operator|.
name|getResponseFromCurrentLeader
argument_list|(
name|getHttpClient
argument_list|()
argument_list|,
operator|new
name|Request
argument_list|(
name|HttpMethod
operator|.
name|GET
argument_list|,
name|input
argument_list|)
argument_list|,
operator|new
name|FullResponseHandler
argument_list|(
name|Charset
operator|.
name|forName
argument_list|(
literal|"UTF-8"
argument_list|)
argument_list|)
argument_list|)
operator|.
name|getContent
argument_list|()
decl_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Checking segment [{}] response is [{}]"
argument_list|,
name|input
argument_list|,
name|result
argument_list|)
expr_stmt|;
return|return
name|Strings
operator|.
name|isNullOrEmpty
argument_list|(
name|result
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|InterruptedException
decl||
name|ExecutionException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
name|String
operator|.
name|format
argument_list|(
literal|"Error while checking URL [%s]"
argument_list|,
name|input
argument_list|)
argument_list|,
name|e
argument_list|)
expr_stmt|;
return|return
literal|true
return|;
block|}
block|}
argument_list|)
argument_list|)
expr_stmt|;
try|try
block|{
if|if
condition|(
operator|!
name|urlsOfUnloadedSegments
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|Thread
operator|.
name|sleep
argument_list|(
name|passiveWaitTimeMs
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|InterruptedException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
name|e
argument_list|)
throw|;
block|}
block|}
if|if
condition|(
operator|!
name|urlsOfUnloadedSegments
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
comment|// We are not Throwing an exception since it might be a transient issue that is blocking loading
name|CONSOLE
operator|.
name|printError
argument_list|(
name|String
operator|.
name|format
argument_list|(
literal|"Wait time exhausted and we have [%s] out of [%s] segments not loaded yet"
argument_list|,
name|urlsOfUnloadedSegments
operator|.
name|size
argument_list|()
argument_list|,
name|segments
operator|.
name|size
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
annotation|@
name|VisibleForTesting
name|void
name|deleteSegment
parameter_list|(
name|DataSegment
name|segment
parameter_list|)
throws|throws
name|SegmentLoadingException
block|{
specifier|final
name|Path
name|path
init|=
name|DruidStorageHandlerUtils
operator|.
name|getPath
argument_list|(
name|segment
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"removing segment {}, located at path {}"
argument_list|,
name|segment
operator|.
name|getIdentifier
argument_list|()
argument_list|,
name|path
argument_list|)
expr_stmt|;
try|try
block|{
if|if
condition|(
name|path
operator|.
name|getName
argument_list|()
operator|.
name|endsWith
argument_list|(
literal|".zip"
argument_list|)
condition|)
block|{
specifier|final
name|FileSystem
name|fs
init|=
name|path
operator|.
name|getFileSystem
argument_list|(
name|getConf
argument_list|()
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|fs
operator|.
name|exists
argument_list|(
name|path
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
literal|"Segment Path {} does not exist. It appears to have been deleted already."
argument_list|,
name|path
argument_list|)
expr_stmt|;
return|return;
block|}
comment|// path format --> .../dataSource/interval/version/partitionNum/xxx.zip
name|Path
name|partitionNumDir
init|=
name|path
operator|.
name|getParent
argument_list|()
decl_stmt|;
if|if
condition|(
operator|!
name|fs
operator|.
name|delete
argument_list|(
name|partitionNumDir
argument_list|,
literal|true
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|SegmentLoadingException
argument_list|(
literal|"Unable to kill segment, failed to delete dir [%s]"
argument_list|,
name|partitionNumDir
operator|.
name|toString
argument_list|()
argument_list|)
throw|;
block|}
comment|//try to delete other directories if possible
name|Path
name|versionDir
init|=
name|partitionNumDir
operator|.
name|getParent
argument_list|()
decl_stmt|;
if|if
condition|(
name|safeNonRecursiveDelete
argument_list|(
name|fs
argument_list|,
name|versionDir
argument_list|)
condition|)
block|{
name|Path
name|intervalDir
init|=
name|versionDir
operator|.
name|getParent
argument_list|()
decl_stmt|;
if|if
condition|(
name|safeNonRecursiveDelete
argument_list|(
name|fs
argument_list|,
name|intervalDir
argument_list|)
condition|)
block|{
name|Path
name|dataSourceDir
init|=
name|intervalDir
operator|.
name|getParent
argument_list|()
decl_stmt|;
name|safeNonRecursiveDelete
argument_list|(
name|fs
argument_list|,
name|dataSourceDir
argument_list|)
expr_stmt|;
block|}
block|}
block|}
else|else
block|{
throw|throw
operator|new
name|SegmentLoadingException
argument_list|(
literal|"Unknown file type[%s]"
argument_list|,
name|path
argument_list|)
throw|;
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|SegmentLoadingException
argument_list|(
name|e
argument_list|,
literal|"Unable to kill segment"
argument_list|)
throw|;
block|}
block|}
specifier|private
specifier|static
name|boolean
name|safeNonRecursiveDelete
parameter_list|(
name|FileSystem
name|fs
parameter_list|,
name|Path
name|path
parameter_list|)
block|{
try|try
block|{
return|return
name|fs
operator|.
name|delete
argument_list|(
name|path
argument_list|,
literal|false
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|ex
parameter_list|)
block|{
return|return
literal|false
return|;
block|}
block|}
annotation|@
name|Override
specifier|public
name|void
name|preDropTable
parameter_list|(
name|Table
name|table
parameter_list|)
block|{
comment|// Nothing to do
block|}
annotation|@
name|Override
specifier|public
name|void
name|rollbackDropTable
parameter_list|(
name|Table
name|table
parameter_list|)
block|{
comment|// Nothing to do
block|}
annotation|@
name|Override
specifier|public
name|void
name|commitDropTable
parameter_list|(
name|Table
name|table
parameter_list|,
name|boolean
name|deleteData
parameter_list|)
block|{
if|if
condition|(
name|DruidKafkaUtils
operator|.
name|isKafkaStreamingTable
argument_list|(
name|table
argument_list|)
condition|)
block|{
comment|// Stop Kafka Ingestion first
specifier|final
name|String
name|overlordAddress
init|=
name|Preconditions
operator|.
name|checkNotNull
argument_list|(
name|HiveConf
operator|.
name|getVar
argument_list|(
name|getConf
argument_list|()
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_DRUID_OVERLORD_DEFAULT_ADDRESS
argument_list|)
argument_list|,
literal|"Druid Overlord Address is null"
argument_list|)
decl_stmt|;
name|String
name|dataSourceName
init|=
name|Preconditions
operator|.
name|checkNotNull
argument_list|(
name|DruidStorageHandlerUtils
operator|.
name|getTableProperty
argument_list|(
name|table
argument_list|,
name|Constants
operator|.
name|DRUID_DATA_SOURCE
argument_list|)
argument_list|,
literal|"Druid Datasource name is null"
argument_list|)
decl_stmt|;
name|stopKafkaIngestion
argument_list|(
name|overlordAddress
argument_list|,
name|dataSourceName
argument_list|)
expr_stmt|;
block|}
name|String
name|dataSourceName
init|=
name|Preconditions
operator|.
name|checkNotNull
argument_list|(
name|table
operator|.
name|getParameters
argument_list|()
operator|.
name|get
argument_list|(
name|Constants
operator|.
name|DRUID_DATA_SOURCE
argument_list|)
argument_list|,
literal|"DataSource name is null !"
argument_list|)
decl_stmt|;
comment|// Move MetaStoreUtils.isExternalTablePurge(table) calls to a common place for all StorageHandlers
comment|// deleteData flag passed down to StorageHandler should be true only if
comment|// MetaStoreUtils.isExternalTablePurge(table) returns true.
if|if
condition|(
name|deleteData
operator|&&
name|MetaStoreUtils
operator|.
name|isExternalTablePurge
argument_list|(
name|table
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Dropping with purge all the data for data source {}"
argument_list|,
name|dataSourceName
argument_list|)
expr_stmt|;
name|List
argument_list|<
name|DataSegment
argument_list|>
name|dataSegmentList
init|=
name|DruidStorageHandlerUtils
operator|.
name|getDataSegmentList
argument_list|(
name|getConnector
argument_list|()
argument_list|,
name|getDruidMetadataStorageTablesConfig
argument_list|()
argument_list|,
name|dataSourceName
argument_list|)
decl_stmt|;
if|if
condition|(
name|dataSegmentList
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Nothing to delete for data source {}"
argument_list|,
name|dataSourceName
argument_list|)
expr_stmt|;
return|return;
block|}
for|for
control|(
name|DataSegment
name|dataSegment
range|:
name|dataSegmentList
control|)
block|{
try|try
block|{
name|deleteSegment
argument_list|(
name|dataSegment
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|SegmentLoadingException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
name|String
operator|.
name|format
argument_list|(
literal|"Error while deleting segment [%s]"
argument_list|,
name|dataSegment
operator|.
name|getIdentifier
argument_list|()
argument_list|)
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
block|}
block|}
if|if
condition|(
name|DruidStorageHandlerUtils
operator|.
name|disableDataSource
argument_list|(
name|getConnector
argument_list|()
argument_list|,
name|getDruidMetadataStorageTablesConfig
argument_list|()
argument_list|,
name|dataSourceName
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Successfully dropped druid data source {}"
argument_list|,
name|dataSourceName
argument_list|)
expr_stmt|;
block|}
block|}
annotation|@
name|Override
specifier|public
name|void
name|commitInsertTable
parameter_list|(
name|Table
name|table
parameter_list|,
name|boolean
name|overwrite
parameter_list|)
throws|throws
name|MetaException
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"commit insert into table {} overwrite {}"
argument_list|,
name|table
operator|.
name|getTableName
argument_list|()
argument_list|,
name|overwrite
argument_list|)
expr_stmt|;
try|try
block|{
comment|// Check if there segments to load
specifier|final
name|Path
name|segmentDescriptorDir
init|=
name|getSegmentDescriptorDir
argument_list|()
decl_stmt|;
specifier|final
name|List
argument_list|<
name|DataSegment
argument_list|>
name|segmentsToLoad
init|=
name|fetchSegmentsMetadata
argument_list|(
name|segmentDescriptorDir
argument_list|)
decl_stmt|;
specifier|final
name|String
name|dataSourceName
init|=
name|table
operator|.
name|getParameters
argument_list|()
operator|.
name|get
argument_list|(
name|Constants
operator|.
name|DRUID_DATA_SOURCE
argument_list|)
decl_stmt|;
comment|//No segments to load still need to honer overwrite
if|if
condition|(
name|segmentsToLoad
operator|.
name|isEmpty
argument_list|()
operator|&&
name|overwrite
condition|)
block|{
comment|//disable datasource
comment|//Case it is an insert overwrite we have to disable the existing Druid DataSource
name|DruidStorageHandlerUtils
operator|.
name|disableDataSource
argument_list|(
name|getConnector
argument_list|()
argument_list|,
name|getDruidMetadataStorageTablesConfig
argument_list|()
argument_list|,
name|dataSourceName
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
operator|!
name|segmentsToLoad
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
comment|// at this point we have Druid segments from reducers but we need to atomically
comment|// rename and commit to metadata
comment|// Moving Druid segments and committing to druid metadata as one transaction.
name|checkLoadStatus
argument_list|(
name|loadAndCommitDruidSegments
argument_list|(
name|table
argument_list|,
name|overwrite
argument_list|,
name|segmentsToLoad
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|MetaException
argument_list|(
name|e
operator|.
name|getMessage
argument_list|()
argument_list|)
throw|;
block|}
catch|catch
parameter_list|(
name|CallbackFailedException
name|c
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Error while committing transaction to druid metadata storage"
argument_list|,
name|c
argument_list|)
expr_stmt|;
throw|throw
operator|new
name|MetaException
argument_list|(
name|c
operator|.
name|getCause
argument_list|()
operator|.
name|getMessage
argument_list|()
argument_list|)
throw|;
block|}
finally|finally
block|{
name|cleanWorkingDir
argument_list|()
expr_stmt|;
block|}
block|}
specifier|private
name|List
argument_list|<
name|DataSegment
argument_list|>
name|fetchSegmentsMetadata
parameter_list|(
name|Path
name|segmentDescriptorDir
parameter_list|)
throws|throws
name|IOException
block|{
if|if
condition|(
operator|!
name|segmentDescriptorDir
operator|.
name|getFileSystem
argument_list|(
name|getConf
argument_list|()
argument_list|)
operator|.
name|exists
argument_list|(
name|segmentDescriptorDir
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Directory {} does not exist, ignore this if it is create statement or inserts of 0 rows,"
operator|+
literal|" no Druid segments to move, cleaning working directory {}"
argument_list|,
name|segmentDescriptorDir
operator|.
name|toString
argument_list|()
argument_list|,
name|getStagingWorkingDir
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
return|return
name|Collections
operator|.
name|emptyList
argument_list|()
return|;
block|}
return|return
name|DruidStorageHandlerUtils
operator|.
name|getCreatedSegments
argument_list|(
name|segmentDescriptorDir
argument_list|,
name|getConf
argument_list|()
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|preInsertTable
parameter_list|(
name|Table
name|table
parameter_list|,
name|boolean
name|overwrite
parameter_list|)
block|{    }
annotation|@
name|Override
specifier|public
name|void
name|rollbackInsertTable
parameter_list|(
name|Table
name|table
parameter_list|,
name|boolean
name|overwrite
parameter_list|)
block|{
comment|// do nothing
block|}
annotation|@
name|Override
specifier|public
name|void
name|configureOutputJobProperties
parameter_list|(
name|TableDesc
name|tableDesc
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|jobProperties
parameter_list|)
block|{
name|jobProperties
operator|.
name|put
argument_list|(
name|Constants
operator|.
name|DRUID_DATA_SOURCE
argument_list|,
name|tableDesc
operator|.
name|getTableName
argument_list|()
argument_list|)
expr_stmt|;
name|jobProperties
operator|.
name|put
argument_list|(
name|DruidConstants
operator|.
name|DRUID_SEGMENT_VERSION
argument_list|,
operator|new
name|DateTime
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
name|jobProperties
operator|.
name|put
argument_list|(
name|DruidConstants
operator|.
name|DRUID_JOB_WORKING_DIRECTORY
argument_list|,
name|getStagingWorkingDir
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
comment|// DruidOutputFormat will write segments in an intermediate directory
name|jobProperties
operator|.
name|put
argument_list|(
name|DruidConstants
operator|.
name|DRUID_SEGMENT_INTERMEDIATE_DIRECTORY
argument_list|,
name|getIntermediateSegmentDir
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|configureTableJobProperties
parameter_list|(
name|TableDesc
name|tableDesc
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|jobProperties
parameter_list|)
block|{    }
annotation|@
name|Override
specifier|public
name|void
name|configureJobConf
parameter_list|(
name|TableDesc
name|tableDesc
parameter_list|,
name|JobConf
name|jobConf
parameter_list|)
block|{
if|if
condition|(
name|UserGroupInformation
operator|.
name|isSecurityEnabled
argument_list|()
condition|)
block|{
comment|// AM can not do Kerberos Auth so will do the input split generation in the HS2
name|LOG
operator|.
name|debug
argument_list|(
literal|"Setting {} to {} to enable split generation on HS2"
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_AM_SPLIT_GENERATION
operator|.
name|toString
argument_list|()
argument_list|,
name|Boolean
operator|.
name|FALSE
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
name|jobConf
operator|.
name|set
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_AM_SPLIT_GENERATION
operator|.
name|toString
argument_list|()
argument_list|,
name|Boolean
operator|.
name|FALSE
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
try|try
block|{
name|DruidStorageHandlerUtils
operator|.
name|addDependencyJars
argument_list|(
name|jobConf
argument_list|,
name|DruidRecordWriter
operator|.
name|class
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|Throwables
operator|.
name|propagate
argument_list|(
name|e
argument_list|)
expr_stmt|;
block|}
block|}
annotation|@
name|Override
specifier|public
name|void
name|setConf
parameter_list|(
name|Configuration
name|conf
parameter_list|)
block|{
name|this
operator|.
name|conf
operator|=
name|conf
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|Configuration
name|getConf
parameter_list|()
block|{
return|return
name|conf
return|;
block|}
annotation|@
name|Override
specifier|public
name|LockType
name|getLockType
parameter_list|(
name|WriteEntity
name|writeEntity
parameter_list|)
block|{
if|if
condition|(
name|writeEntity
operator|.
name|getWriteType
argument_list|()
operator|.
name|equals
argument_list|(
name|WriteEntity
operator|.
name|WriteType
operator|.
name|INSERT
argument_list|)
condition|)
block|{
return|return
name|LockType
operator|.
name|SHARED_READ
return|;
block|}
return|return
name|LockType
operator|.
name|SHARED_WRITE
return|;
block|}
annotation|@
name|Override
specifier|public
name|String
name|toString
parameter_list|()
block|{
return|return
name|Constants
operator|.
name|DRUID_HIVE_STORAGE_HANDLER_ID
return|;
block|}
specifier|private
name|String
name|getUniqueId
parameter_list|()
block|{
if|if
condition|(
name|uniqueId
operator|==
literal|null
condition|)
block|{
name|uniqueId
operator|=
name|Preconditions
operator|.
name|checkNotNull
argument_list|(
name|Strings
operator|.
name|emptyToNull
argument_list|(
name|HiveConf
operator|.
name|getVar
argument_list|(
name|getConf
argument_list|()
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVEQUERYID
argument_list|)
argument_list|)
argument_list|,
literal|"Hive query id is null"
argument_list|)
expr_stmt|;
block|}
return|return
name|uniqueId
return|;
block|}
specifier|private
name|Path
name|getStagingWorkingDir
parameter_list|()
block|{
return|return
operator|new
name|Path
argument_list|(
name|getRootWorkingDir
argument_list|()
argument_list|,
name|makeStagingName
argument_list|()
argument_list|)
return|;
block|}
specifier|private
name|MetadataStorageTablesConfig
name|getDruidMetadataStorageTablesConfig
parameter_list|()
block|{
if|if
condition|(
name|druidMetadataStorageTablesConfig
operator|!=
literal|null
condition|)
block|{
return|return
name|druidMetadataStorageTablesConfig
return|;
block|}
specifier|final
name|String
name|base
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|getConf
argument_list|()
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|DRUID_METADATA_BASE
argument_list|)
decl_stmt|;
name|druidMetadataStorageTablesConfig
operator|=
name|MetadataStorageTablesConfig
operator|.
name|fromBase
argument_list|(
name|base
argument_list|)
expr_stmt|;
return|return
name|druidMetadataStorageTablesConfig
return|;
block|}
specifier|private
name|SQLMetadataConnector
name|getConnector
parameter_list|()
block|{
return|return
name|Suppliers
operator|.
name|memoize
argument_list|(
name|this
operator|::
name|buildConnector
argument_list|)
operator|.
name|get
argument_list|()
return|;
block|}
specifier|private
name|SQLMetadataConnector
name|buildConnector
parameter_list|()
block|{
if|if
condition|(
name|connector
operator|!=
literal|null
condition|)
block|{
return|return
name|connector
return|;
block|}
specifier|final
name|String
name|dbType
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|getConf
argument_list|()
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|DRUID_METADATA_DB_TYPE
argument_list|)
decl_stmt|;
specifier|final
name|String
name|username
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|getConf
argument_list|()
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|DRUID_METADATA_DB_USERNAME
argument_list|)
decl_stmt|;
specifier|final
name|String
name|password
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|getConf
argument_list|()
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|DRUID_METADATA_DB_PASSWORD
argument_list|)
decl_stmt|;
specifier|final
name|String
name|uri
init|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|getConf
argument_list|()
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|DRUID_METADATA_DB_URI
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Supplying SQL Connector with DB type {}, URI {}, User {}"
argument_list|,
name|dbType
argument_list|,
name|uri
argument_list|,
name|username
argument_list|)
expr_stmt|;
annotation|@
name|SuppressWarnings
argument_list|(
literal|"Guava"
argument_list|)
specifier|final
name|Supplier
argument_list|<
name|MetadataStorageConnectorConfig
argument_list|>
name|storageConnectorConfigSupplier
init|=
name|Suppliers
operator|.
name|ofInstance
argument_list|(
operator|new
name|MetadataStorageConnectorConfig
argument_list|()
block|{
annotation|@
name|Override
specifier|public
name|String
name|getConnectURI
parameter_list|()
block|{
return|return
name|uri
return|;
block|}
annotation|@
name|Override
specifier|public
name|String
name|getUser
parameter_list|()
block|{
return|return
name|Strings
operator|.
name|emptyToNull
argument_list|(
name|username
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|public
name|String
name|getPassword
parameter_list|()
block|{
return|return
name|Strings
operator|.
name|emptyToNull
argument_list|(
name|password
argument_list|)
return|;
block|}
block|}
argument_list|)
decl_stmt|;
switch|switch
condition|(
name|dbType
condition|)
block|{
case|case
literal|"mysql"
case|:
name|connector
operator|=
operator|new
name|MySQLConnector
argument_list|(
name|storageConnectorConfigSupplier
argument_list|,
name|Suppliers
operator|.
name|ofInstance
argument_list|(
name|getDruidMetadataStorageTablesConfig
argument_list|()
argument_list|)
argument_list|,
operator|new
name|MySQLConnectorConfig
argument_list|()
argument_list|)
expr_stmt|;
break|break;
case|case
literal|"postgresql"
case|:
name|connector
operator|=
operator|new
name|PostgreSQLConnector
argument_list|(
name|storageConnectorConfigSupplier
argument_list|,
name|Suppliers
operator|.
name|ofInstance
argument_list|(
name|getDruidMetadataStorageTablesConfig
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
break|break;
case|case
literal|"derby"
case|:
name|connector
operator|=
operator|new
name|DerbyConnector
argument_list|(
operator|new
name|DerbyMetadataStorage
argument_list|(
name|storageConnectorConfigSupplier
operator|.
name|get
argument_list|()
argument_list|)
argument_list|,
name|storageConnectorConfigSupplier
argument_list|,
name|Suppliers
operator|.
name|ofInstance
argument_list|(
name|getDruidMetadataStorageTablesConfig
argument_list|()
argument_list|)
argument_list|)
expr_stmt|;
break|break;
default|default:
throw|throw
operator|new
name|IllegalStateException
argument_list|(
name|String
operator|.
name|format
argument_list|(
literal|"Unknown metadata storage type [%s]"
argument_list|,
name|dbType
argument_list|)
argument_list|)
throw|;
block|}
return|return
name|connector
return|;
block|}
annotation|@
name|VisibleForTesting
name|String
name|makeStagingName
parameter_list|()
block|{
return|return
literal|".staging-"
operator|.
name|concat
argument_list|(
name|getUniqueId
argument_list|()
operator|.
name|replace
argument_list|(
literal|":"
argument_list|,
literal|""
argument_list|)
argument_list|)
return|;
block|}
specifier|private
name|Path
name|getSegmentDescriptorDir
parameter_list|()
block|{
return|return
operator|new
name|Path
argument_list|(
name|getStagingWorkingDir
argument_list|()
argument_list|,
name|SEGMENTS_DESCRIPTOR_DIR_NAME
argument_list|)
return|;
block|}
specifier|private
name|Path
name|getIntermediateSegmentDir
parameter_list|()
block|{
return|return
operator|new
name|Path
argument_list|(
name|getStagingWorkingDir
argument_list|()
argument_list|,
name|INTERMEDIATE_SEGMENT_DIR_NAME
argument_list|)
return|;
block|}
specifier|private
name|void
name|cleanWorkingDir
parameter_list|()
block|{
specifier|final
name|FileSystem
name|fileSystem
decl_stmt|;
try|try
block|{
name|fileSystem
operator|=
name|getStagingWorkingDir
argument_list|()
operator|.
name|getFileSystem
argument_list|(
name|getConf
argument_list|()
argument_list|)
expr_stmt|;
name|fileSystem
operator|.
name|delete
argument_list|(
name|getStagingWorkingDir
argument_list|()
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|IOException
name|e
parameter_list|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Got Exception while cleaning working directory"
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
block|}
specifier|private
name|String
name|getRootWorkingDir
parameter_list|()
block|{
if|if
condition|(
name|Strings
operator|.
name|isNullOrEmpty
argument_list|(
name|rootWorkingDir
argument_list|)
condition|)
block|{
name|rootWorkingDir
operator|=
name|HiveConf
operator|.
name|getVar
argument_list|(
name|getConf
argument_list|()
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|DRUID_WORKING_DIR
argument_list|)
expr_stmt|;
block|}
return|return
name|rootWorkingDir
return|;
block|}
specifier|private
specifier|static
name|HttpClient
name|makeHttpClient
parameter_list|(
name|Lifecycle
name|lifecycle
parameter_list|)
block|{
specifier|final
name|int
name|numConnection
init|=
name|HiveConf
operator|.
name|getIntVar
argument_list|(
name|SessionState
operator|.
name|getSessionConf
argument_list|()
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_DRUID_NUM_HTTP_CONNECTION
argument_list|)
decl_stmt|;
specifier|final
name|Period
name|readTimeout
init|=
operator|new
name|Period
argument_list|(
name|HiveConf
operator|.
name|getVar
argument_list|(
name|SessionState
operator|.
name|getSessionConf
argument_list|()
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_DRUID_HTTP_READ_TIMEOUT
argument_list|)
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Creating Druid HTTP client with {} max parallel connections and {}ms read timeout"
argument_list|,
name|numConnection
argument_list|,
name|readTimeout
operator|.
name|toStandardDuration
argument_list|()
operator|.
name|getMillis
argument_list|()
argument_list|)
expr_stmt|;
specifier|final
name|HttpClient
name|httpClient
init|=
name|HttpClientInit
operator|.
name|createClient
argument_list|(
name|HttpClientConfig
operator|.
name|builder
argument_list|()
operator|.
name|withNumConnections
argument_list|(
name|numConnection
argument_list|)
operator|.
name|withReadTimeout
argument_list|(
operator|new
name|Period
argument_list|(
name|readTimeout
argument_list|)
operator|.
name|toStandardDuration
argument_list|()
argument_list|)
operator|.
name|build
argument_list|()
argument_list|,
name|lifecycle
argument_list|)
decl_stmt|;
if|if
condition|(
name|UserGroupInformation
operator|.
name|isSecurityEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"building Kerberos Http Client"
argument_list|)
expr_stmt|;
return|return
operator|new
name|KerberosHttpClient
argument_list|(
name|httpClient
argument_list|)
return|;
block|}
return|return
name|httpClient
return|;
block|}
specifier|public
specifier|static
name|HttpClient
name|getHttpClient
parameter_list|()
block|{
return|return
name|HTTP_CLIENT
return|;
block|}
annotation|@
name|Override
specifier|public
name|void
name|preAlterTable
parameter_list|(
name|Table
name|table
parameter_list|,
name|EnvironmentContext
name|context
parameter_list|)
throws|throws
name|MetaException
block|{
name|String
name|alterOpType
init|=
name|context
operator|==
literal|null
condition|?
literal|null
else|:
name|context
operator|.
name|getProperties
argument_list|()
operator|.
name|get
argument_list|(
name|ALTER_TABLE_OPERATION_TYPE
argument_list|)
decl_stmt|;
comment|// alterOpType is null in case of stats update
if|if
condition|(
name|alterOpType
operator|!=
literal|null
operator|&&
operator|!
name|ALLOWED_ALTER_TYPES
operator|.
name|contains
argument_list|(
name|alterOpType
argument_list|)
condition|)
block|{
throw|throw
operator|new
name|MetaException
argument_list|(
literal|"ALTER TABLE can not be used for "
operator|+
name|alterOpType
operator|+
literal|" to a non-native table "
argument_list|)
throw|;
block|}
if|if
condition|(
name|DruidKafkaUtils
operator|.
name|isKafkaStreamingTable
argument_list|(
name|table
argument_list|)
condition|)
block|{
name|updateKafkaIngestion
argument_list|(
name|table
argument_list|)
expr_stmt|;
block|}
block|}
specifier|private
name|int
name|getMaxRetryCount
parameter_list|()
block|{
return|return
name|HiveConf
operator|.
name|getIntVar
argument_list|(
name|getConf
argument_list|()
argument_list|,
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HIVE_DRUID_MAX_TRIES
argument_list|)
return|;
block|}
annotation|@
name|Override
specifier|public
name|StorageHandlerInfo
name|getStorageHandlerInfo
parameter_list|(
name|Table
name|table
parameter_list|)
throws|throws
name|MetaException
block|{
if|if
condition|(
name|DruidKafkaUtils
operator|.
name|isKafkaStreamingTable
argument_list|(
name|table
argument_list|)
condition|)
block|{
name|KafkaSupervisorReport
name|kafkaSupervisorReport
init|=
name|fetchKafkaSupervisorReport
argument_list|(
name|table
argument_list|)
decl_stmt|;
if|if
condition|(
name|kafkaSupervisorReport
operator|==
literal|null
condition|)
block|{
return|return
name|DruidStorageHandlerInfo
operator|.
name|UNREACHABLE
return|;
block|}
return|return
operator|new
name|DruidStorageHandlerInfo
argument_list|(
name|kafkaSupervisorReport
argument_list|)
return|;
block|}
else|else
block|{
comment|// Currently we do not expose any runtime info for non-streaming tables.
comment|// In future extend this add more information regarding table status.
comment|// e.g. Total size of segments in druid, load status of table on historical nodes etc.
return|return
literal|null
return|;
block|}
block|}
block|}
end_class

end_unit

