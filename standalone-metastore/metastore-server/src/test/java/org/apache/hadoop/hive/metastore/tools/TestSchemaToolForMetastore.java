begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/*  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|tools
package|;
end_package

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|BufferedWriter
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|ByteArrayOutputStream
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|File
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|FileWriter
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|OutputStream
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|PrintStream
import|;
end_import

begin_import
import|import
name|java
operator|.
name|net
operator|.
name|URI
import|;
end_import

begin_import
import|import
name|java
operator|.
name|sql
operator|.
name|Connection
import|;
end_import

begin_import
import|import
name|java
operator|.
name|sql
operator|.
name|DatabaseMetaData
import|;
end_import

begin_import
import|import
name|java
operator|.
name|sql
operator|.
name|SQLException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Random
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|dbcp
operator|.
name|DelegatingConnection
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|io
operator|.
name|FileUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|lang
operator|.
name|text
operator|.
name|StrTokenizer
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configuration
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|HiveMetaException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|IMetaStoreSchemaInfo
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|MetaStoreSchemaInfoFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|annotation
operator|.
name|MetastoreCheckinTest
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|conf
operator|.
name|MetastoreConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|conf
operator|.
name|MetastoreConf
operator|.
name|ConfVars
import|;
end_import

begin_import
import|import
name|org
operator|.
name|junit
operator|.
name|After
import|;
end_import

begin_import
import|import
name|org
operator|.
name|junit
operator|.
name|Assert
import|;
end_import

begin_import
import|import
name|org
operator|.
name|junit
operator|.
name|Before
import|;
end_import

begin_import
import|import
name|org
operator|.
name|junit
operator|.
name|Test
import|;
end_import

begin_import
import|import
name|org
operator|.
name|junit
operator|.
name|experimental
operator|.
name|categories
operator|.
name|Category
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|Logger
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|LoggerFactory
import|;
end_import

begin_class
annotation|@
name|Category
argument_list|(
name|MetastoreCheckinTest
operator|.
name|class
argument_list|)
specifier|public
class|class
name|TestSchemaToolForMetastore
block|{
specifier|private
specifier|static
specifier|final
name|Logger
name|LOG
init|=
name|LoggerFactory
operator|.
name|getLogger
argument_list|(
name|TestMetastoreSchemaTool
operator|.
name|class
argument_list|)
decl_stmt|;
specifier|private
name|MetastoreSchemaTool
name|schemaTool
decl_stmt|;
specifier|private
name|Connection
name|conn
decl_stmt|;
specifier|private
name|Configuration
name|conf
decl_stmt|;
specifier|private
name|String
name|testMetastoreDB
decl_stmt|;
specifier|private
name|PrintStream
name|errStream
decl_stmt|;
specifier|private
name|PrintStream
name|outStream
decl_stmt|;
specifier|private
name|String
name|argsBase
decl_stmt|;
specifier|private
name|SchemaToolTaskValidate
name|validator
decl_stmt|;
annotation|@
name|Before
specifier|public
name|void
name|setUp
parameter_list|()
throws|throws
name|HiveMetaException
throws|,
name|IOException
block|{
name|testMetastoreDB
operator|=
name|System
operator|.
name|getProperty
argument_list|(
literal|"java.io.tmpdir"
argument_list|)
operator|+
name|File
operator|.
name|separator
operator|+
literal|"test_metastore-"
operator|+
operator|new
name|Random
argument_list|()
operator|.
name|nextInt
argument_list|()
expr_stmt|;
name|System
operator|.
name|setProperty
argument_list|(
name|ConfVars
operator|.
name|CONNECT_URL_KEY
operator|.
name|toString
argument_list|()
argument_list|,
literal|"jdbc:derby:"
operator|+
name|testMetastoreDB
operator|+
literal|";create=true"
argument_list|)
expr_stmt|;
name|conf
operator|=
name|MetastoreConf
operator|.
name|newMetastoreConf
argument_list|()
expr_stmt|;
name|schemaTool
operator|=
operator|new
name|MetastoreSchemaTool
argument_list|()
expr_stmt|;
name|schemaTool
operator|.
name|init
argument_list|(
name|System
operator|.
name|getProperty
argument_list|(
literal|"test.tmp.dir"
argument_list|,
literal|"target/tmp"
argument_list|)
argument_list|,
operator|new
name|String
index|[]
block|{
literal|"-dbType"
block|,
literal|"derby"
block|,
literal|"--info"
block|}
argument_list|,
literal|null
argument_list|,
name|conf
argument_list|)
expr_stmt|;
name|String
name|userName
init|=
name|MetastoreConf
operator|.
name|getVar
argument_list|(
name|schemaTool
operator|.
name|getConf
argument_list|()
argument_list|,
name|ConfVars
operator|.
name|CONNECTION_USER_NAME
argument_list|)
decl_stmt|;
name|String
name|passWord
init|=
name|MetastoreConf
operator|.
name|getPassword
argument_list|(
name|schemaTool
operator|.
name|getConf
argument_list|()
argument_list|,
name|ConfVars
operator|.
name|PWD
argument_list|)
decl_stmt|;
name|schemaTool
operator|.
name|setUserName
argument_list|(
name|userName
argument_list|)
expr_stmt|;
name|schemaTool
operator|.
name|setPassWord
argument_list|(
name|passWord
argument_list|)
expr_stmt|;
name|argsBase
operator|=
literal|"-dbType derby -userName "
operator|+
name|userName
operator|+
literal|" -passWord "
operator|+
name|passWord
operator|+
literal|" "
expr_stmt|;
name|System
operator|.
name|setProperty
argument_list|(
literal|"beeLine.system.exit"
argument_list|,
literal|"true"
argument_list|)
expr_stmt|;
name|errStream
operator|=
name|System
operator|.
name|err
expr_stmt|;
name|outStream
operator|=
name|System
operator|.
name|out
expr_stmt|;
name|conn
operator|=
name|schemaTool
operator|.
name|getConnectionToMetastore
argument_list|(
literal|false
argument_list|)
expr_stmt|;
name|validator
operator|=
operator|new
name|SchemaToolTaskValidate
argument_list|()
expr_stmt|;
name|validator
operator|.
name|setHiveSchemaTool
argument_list|(
name|schemaTool
argument_list|)
expr_stmt|;
block|}
annotation|@
name|After
specifier|public
name|void
name|tearDown
parameter_list|()
throws|throws
name|IOException
throws|,
name|SQLException
block|{
name|File
name|metaStoreDir
init|=
operator|new
name|File
argument_list|(
name|testMetastoreDB
argument_list|)
decl_stmt|;
if|if
condition|(
name|metaStoreDir
operator|.
name|exists
argument_list|()
condition|)
block|{
name|FileUtils
operator|.
name|forceDeleteOnExit
argument_list|(
name|metaStoreDir
argument_list|)
expr_stmt|;
block|}
name|System
operator|.
name|setOut
argument_list|(
name|outStream
argument_list|)
expr_stmt|;
name|System
operator|.
name|setErr
argument_list|(
name|errStream
argument_list|)
expr_stmt|;
if|if
condition|(
name|conn
operator|!=
literal|null
condition|)
block|{
name|conn
operator|.
name|close
argument_list|()
expr_stmt|;
block|}
block|}
comment|/*    * Test the sequence validation functionality    */
annotation|@
name|Test
specifier|public
name|void
name|testValidateSequences
parameter_list|()
throws|throws
name|Exception
block|{
name|execute
argument_list|(
operator|new
name|SchemaToolTaskInit
argument_list|()
argument_list|,
literal|"-initSchema"
argument_list|)
expr_stmt|;
comment|// Test empty database
name|boolean
name|isValid
init|=
name|validator
operator|.
name|validateSequences
argument_list|(
name|conn
argument_list|)
decl_stmt|;
name|Assert
operator|.
name|assertTrue
argument_list|(
name|isValid
argument_list|)
expr_stmt|;
name|String
name|time
init|=
name|String
operator|.
name|valueOf
argument_list|(
name|System
operator|.
name|currentTimeMillis
argument_list|()
operator|/
literal|1000
argument_list|)
decl_stmt|;
comment|// Test valid case
name|String
index|[]
name|scripts
init|=
operator|new
name|String
index|[]
block|{
literal|"insert into CTLGS values(99, 'test_cat_1', 'description', 'hdfs://myhost.com:8020/user/hive/warehouse/mydb', "
operator|+
name|time
operator|+
literal|");"
block|,
literal|"insert into SEQUENCE_TABLE values('org.apache.hadoop.hive.metastore.model.MDatabase', 100);"
block|,
literal|"insert into DBS values(99, 'test db1', 'hdfs:///tmp', 'db1', 'test', 'test', 'test_cat_1', "
operator|+
name|time
operator|+
literal|");"
block|}
decl_stmt|;
name|File
name|scriptFile
init|=
name|generateTestScript
argument_list|(
name|scripts
argument_list|)
decl_stmt|;
name|schemaTool
operator|.
name|execSql
argument_list|(
name|scriptFile
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
name|isValid
operator|=
name|validator
operator|.
name|validateSequences
argument_list|(
name|conn
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertTrue
argument_list|(
name|isValid
argument_list|)
expr_stmt|;
comment|// Test invalid case
name|scripts
operator|=
operator|new
name|String
index|[]
block|{
literal|"delete from SEQUENCE_TABLE;"
block|,
literal|"delete from DBS;"
block|,
literal|"insert into SEQUENCE_TABLE values('org.apache.hadoop.hive.metastore.model.MDatabase', 100);"
block|,
literal|"insert into DBS values(102, 'test db1', 'hdfs:///tmp', 'db1', 'test', 'test', 'test_cat_1', "
operator|+
name|time
operator|+
literal|");"
block|}
expr_stmt|;
name|scriptFile
operator|=
name|generateTestScript
argument_list|(
name|scripts
argument_list|)
expr_stmt|;
name|schemaTool
operator|.
name|execSql
argument_list|(
name|scriptFile
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
name|isValid
operator|=
name|validator
operator|.
name|validateSequences
argument_list|(
name|conn
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertFalse
argument_list|(
name|isValid
argument_list|)
expr_stmt|;
block|}
comment|/*    * Test to validate that all tables exist in the HMS metastore.    */
annotation|@
name|Test
specifier|public
name|void
name|testValidateSchemaTables
parameter_list|()
throws|throws
name|Exception
block|{
name|execute
argument_list|(
operator|new
name|SchemaToolTaskInit
argument_list|()
argument_list|,
literal|"-initSchemaTo 1.2.0"
argument_list|)
expr_stmt|;
name|boolean
name|isValid
init|=
name|validator
operator|.
name|validateSchemaTables
argument_list|(
name|conn
argument_list|)
decl_stmt|;
name|Assert
operator|.
name|assertTrue
argument_list|(
name|isValid
argument_list|)
expr_stmt|;
comment|// upgrade from 1.2.0 schema and re-validate
name|execute
argument_list|(
operator|new
name|SchemaToolTaskUpgrade
argument_list|()
argument_list|,
literal|"-upgradeSchemaFrom 1.2.0"
argument_list|)
expr_stmt|;
name|isValid
operator|=
name|validator
operator|.
name|validateSchemaTables
argument_list|(
name|conn
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertTrue
argument_list|(
name|isValid
argument_list|)
expr_stmt|;
comment|// Simulate a missing table scenario by renaming a couple of tables
name|String
index|[]
name|scripts
init|=
operator|new
name|String
index|[]
block|{
literal|"RENAME TABLE SEQUENCE_TABLE to SEQUENCE_TABLE_RENAMED;"
block|,
literal|"RENAME TABLE NUCLEUS_TABLES to NUCLEUS_TABLES_RENAMED;"
block|}
decl_stmt|;
name|File
name|scriptFile
init|=
name|generateTestScript
argument_list|(
name|scripts
argument_list|)
decl_stmt|;
name|schemaTool
operator|.
name|execSql
argument_list|(
name|scriptFile
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
name|isValid
operator|=
name|validator
operator|.
name|validateSchemaTables
argument_list|(
name|conn
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertFalse
argument_list|(
name|isValid
argument_list|)
expr_stmt|;
comment|// Restored the renamed tables
name|scripts
operator|=
operator|new
name|String
index|[]
block|{
literal|"RENAME TABLE SEQUENCE_TABLE_RENAMED to SEQUENCE_TABLE;"
block|,
literal|"RENAME TABLE NUCLEUS_TABLES_RENAMED to NUCLEUS_TABLES;"
block|}
expr_stmt|;
name|scriptFile
operator|=
name|generateTestScript
argument_list|(
name|scripts
argument_list|)
expr_stmt|;
name|schemaTool
operator|.
name|execSql
argument_list|(
name|scriptFile
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
name|isValid
operator|=
name|validator
operator|.
name|validateSchemaTables
argument_list|(
name|conn
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertTrue
argument_list|(
name|isValid
argument_list|)
expr_stmt|;
comment|// Check that an exception from getMetaData() is reported correctly
try|try
block|{
comment|// Make a Connection object that will throw an exception
name|BadMetaDataConnection
name|bad
init|=
operator|new
name|BadMetaDataConnection
argument_list|(
name|conn
argument_list|)
decl_stmt|;
name|validator
operator|.
name|validateSchemaTables
argument_list|(
name|bad
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|fail
argument_list|(
literal|"did not get expected exception"
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|HiveMetaException
name|hme
parameter_list|)
block|{
name|String
name|message
init|=
name|hme
operator|.
name|getMessage
argument_list|()
decl_stmt|;
name|Assert
operator|.
name|assertTrue
argument_list|(
literal|"Bad HiveMetaException message :"
operator|+
name|message
argument_list|,
name|message
operator|.
name|contains
argument_list|(
literal|"Failed to retrieve schema tables from Hive Metastore DB"
argument_list|)
argument_list|)
expr_stmt|;
name|Throwable
name|cause
init|=
name|hme
operator|.
name|getCause
argument_list|()
decl_stmt|;
name|Assert
operator|.
name|assertNotNull
argument_list|(
literal|"HiveMetaException did not contain a cause"
argument_list|,
name|cause
argument_list|)
expr_stmt|;
name|String
name|causeMessage
init|=
name|cause
operator|.
name|getMessage
argument_list|()
decl_stmt|;
name|Assert
operator|.
name|assertTrue
argument_list|(
literal|"Bad SQLException message: "
operator|+
name|causeMessage
argument_list|,
name|causeMessage
operator|.
name|contains
argument_list|(
name|BadMetaDataConnection
operator|.
name|FAILURE_TEXT
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
comment|// Test the validation of incorrect NULL values in the tables
annotation|@
name|Test
specifier|public
name|void
name|testValidateNullValues
parameter_list|()
throws|throws
name|Exception
block|{
name|execute
argument_list|(
operator|new
name|SchemaToolTaskInit
argument_list|()
argument_list|,
literal|"-initSchema"
argument_list|)
expr_stmt|;
comment|// Test empty database
name|boolean
name|isValid
init|=
name|validator
operator|.
name|validateColumnNullValues
argument_list|(
name|conn
argument_list|)
decl_stmt|;
name|Assert
operator|.
name|assertTrue
argument_list|(
name|isValid
argument_list|)
expr_stmt|;
comment|// Test valid case
name|createTestHiveTableSchemas
argument_list|()
expr_stmt|;
name|isValid
operator|=
name|validator
operator|.
name|validateColumnNullValues
argument_list|(
name|conn
argument_list|)
expr_stmt|;
comment|// Test invalid case
name|String
index|[]
name|scripts
init|=
operator|new
name|String
index|[]
block|{
literal|"update TBLS set SD_ID=null"
block|}
decl_stmt|;
name|File
name|scriptFile
init|=
name|generateTestScript
argument_list|(
name|scripts
argument_list|)
decl_stmt|;
name|schemaTool
operator|.
name|execSql
argument_list|(
name|scriptFile
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
name|isValid
operator|=
name|validator
operator|.
name|validateColumnNullValues
argument_list|(
name|conn
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertFalse
argument_list|(
name|isValid
argument_list|)
expr_stmt|;
block|}
comment|// Test dryrun of schema initialization
annotation|@
name|Test
specifier|public
name|void
name|testSchemaInitDryRun
parameter_list|()
throws|throws
name|Exception
block|{
name|schemaTool
operator|.
name|setDryRun
argument_list|(
literal|true
argument_list|)
expr_stmt|;
name|execute
argument_list|(
operator|new
name|SchemaToolTaskInit
argument_list|()
argument_list|,
literal|"-initSchemaTo 1.2.0"
argument_list|)
expr_stmt|;
name|schemaTool
operator|.
name|setDryRun
argument_list|(
literal|false
argument_list|)
expr_stmt|;
try|try
block|{
name|schemaTool
operator|.
name|verifySchemaVersion
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|HiveMetaException
name|e
parameter_list|)
block|{
comment|// The connection should fail since it the dry run
return|return;
block|}
name|Assert
operator|.
name|fail
argument_list|(
literal|"Dry run shouldn't create actual metastore"
argument_list|)
expr_stmt|;
block|}
comment|// Test dryrun of schema upgrade
annotation|@
name|Test
specifier|public
name|void
name|testSchemaUpgradeDryRun
parameter_list|()
throws|throws
name|Exception
block|{
name|execute
argument_list|(
operator|new
name|SchemaToolTaskInit
argument_list|()
argument_list|,
literal|"-initSchemaTo 1.2.0"
argument_list|)
expr_stmt|;
name|schemaTool
operator|.
name|setDryRun
argument_list|(
literal|true
argument_list|)
expr_stmt|;
name|execute
argument_list|(
operator|new
name|SchemaToolTaskUpgrade
argument_list|()
argument_list|,
literal|"-upgradeSchemaFrom 1.2.0"
argument_list|)
expr_stmt|;
name|schemaTool
operator|.
name|setDryRun
argument_list|(
literal|false
argument_list|)
expr_stmt|;
try|try
block|{
name|schemaTool
operator|.
name|verifySchemaVersion
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|HiveMetaException
name|e
parameter_list|)
block|{
comment|// The connection should fail since it the dry run
return|return;
block|}
name|Assert
operator|.
name|fail
argument_list|(
literal|"Dry run shouldn't upgrade metastore schema"
argument_list|)
expr_stmt|;
block|}
comment|/**    * Test schema initialization    */
annotation|@
name|Test
specifier|public
name|void
name|testSchemaInit
parameter_list|()
throws|throws
name|Exception
block|{
name|IMetaStoreSchemaInfo
name|metastoreSchemaInfo
init|=
name|MetaStoreSchemaInfoFactory
operator|.
name|get
argument_list|(
name|conf
argument_list|,
name|System
operator|.
name|getProperty
argument_list|(
literal|"test.tmp.dir"
argument_list|,
literal|"target/tmp"
argument_list|)
argument_list|,
literal|"derby"
argument_list|)
decl_stmt|;
name|execute
argument_list|(
operator|new
name|SchemaToolTaskInit
argument_list|()
argument_list|,
literal|"-initSchemaTo "
operator|+
name|metastoreSchemaInfo
operator|.
name|getHiveSchemaVersion
argument_list|()
argument_list|)
expr_stmt|;
name|schemaTool
operator|.
name|verifySchemaVersion
argument_list|()
expr_stmt|;
block|}
comment|/**    * initOrUpgrade takes init path    */
annotation|@
name|Test
specifier|public
name|void
name|testSchemaInitOrUgrade1
parameter_list|()
throws|throws
name|Exception
block|{
name|execute
argument_list|(
operator|new
name|SchemaToolTaskInit
argument_list|()
argument_list|,
literal|"-initOrUpgradeSchema"
argument_list|)
expr_stmt|;
name|schemaTool
operator|.
name|verifySchemaVersion
argument_list|()
expr_stmt|;
block|}
comment|/**    * initOrUpgrade takes upgrade path    */
annotation|@
name|Test
specifier|public
name|void
name|testSchemaInitOrUgrade2
parameter_list|()
throws|throws
name|Exception
block|{
name|execute
argument_list|(
operator|new
name|SchemaToolTaskInit
argument_list|()
argument_list|,
literal|"-initSchemaTo 1.2.0"
argument_list|)
expr_stmt|;
name|schemaTool
operator|.
name|setDryRun
argument_list|(
literal|true
argument_list|)
expr_stmt|;
name|execute
argument_list|(
operator|new
name|SchemaToolTaskUpgrade
argument_list|()
argument_list|,
literal|"-initOrUpgradeSchema"
argument_list|)
expr_stmt|;
name|schemaTool
operator|.
name|setDryRun
argument_list|(
literal|false
argument_list|)
expr_stmt|;
try|try
block|{
name|schemaTool
operator|.
name|verifySchemaVersion
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|HiveMetaException
name|e
parameter_list|)
block|{
comment|// The connection should fail since it the dry run
return|return;
block|}
name|Assert
operator|.
name|fail
argument_list|(
literal|"Dry run shouldn't upgrade metastore schema"
argument_list|)
expr_stmt|;
block|}
comment|/**   * Test validation for schema versions   */
annotation|@
name|Test
specifier|public
name|void
name|testValidateSchemaVersions
parameter_list|()
throws|throws
name|Exception
block|{
name|execute
argument_list|(
operator|new
name|SchemaToolTaskInit
argument_list|()
argument_list|,
literal|"-initSchema"
argument_list|)
expr_stmt|;
name|boolean
name|isValid
init|=
name|validator
operator|.
name|validateSchemaVersions
argument_list|()
decl_stmt|;
comment|// Test an invalid case with multiple versions
name|String
index|[]
name|scripts
init|=
operator|new
name|String
index|[]
block|{
literal|"insert into VERSION values(100, '2.2.0', 'Hive release version 2.2.0')"
block|}
decl_stmt|;
name|File
name|scriptFile
init|=
name|generateTestScript
argument_list|(
name|scripts
argument_list|)
decl_stmt|;
name|schemaTool
operator|.
name|execSql
argument_list|(
name|scriptFile
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
name|isValid
operator|=
name|validator
operator|.
name|validateSchemaVersions
argument_list|()
expr_stmt|;
name|Assert
operator|.
name|assertFalse
argument_list|(
name|isValid
argument_list|)
expr_stmt|;
name|scripts
operator|=
operator|new
name|String
index|[]
block|{
literal|"delete from VERSION where VER_ID = 100"
block|}
expr_stmt|;
name|scriptFile
operator|=
name|generateTestScript
argument_list|(
name|scripts
argument_list|)
expr_stmt|;
name|schemaTool
operator|.
name|execSql
argument_list|(
name|scriptFile
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
name|isValid
operator|=
name|validator
operator|.
name|validateSchemaVersions
argument_list|()
expr_stmt|;
name|Assert
operator|.
name|assertTrue
argument_list|(
name|isValid
argument_list|)
expr_stmt|;
comment|// Test an invalid case without version
name|scripts
operator|=
operator|new
name|String
index|[]
block|{
literal|"delete from VERSION"
block|}
expr_stmt|;
name|scriptFile
operator|=
name|generateTestScript
argument_list|(
name|scripts
argument_list|)
expr_stmt|;
name|schemaTool
operator|.
name|execSql
argument_list|(
name|scriptFile
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
name|isValid
operator|=
name|validator
operator|.
name|validateSchemaVersions
argument_list|()
expr_stmt|;
name|Assert
operator|.
name|assertFalse
argument_list|(
name|isValid
argument_list|)
expr_stmt|;
block|}
comment|/**    * Test schema upgrade    */
annotation|@
name|Test
specifier|public
name|void
name|testSchemaUpgrade
parameter_list|()
throws|throws
name|Exception
block|{
name|boolean
name|foundException
init|=
literal|false
decl_stmt|;
comment|// Initialize 1.2.0 schema
name|execute
argument_list|(
operator|new
name|SchemaToolTaskInit
argument_list|()
argument_list|,
literal|"-initSchemaTo 1.2.0"
argument_list|)
expr_stmt|;
comment|// verify that driver fails due to older version schema
try|try
block|{
name|schemaTool
operator|.
name|verifySchemaVersion
argument_list|()
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|HiveMetaException
name|e
parameter_list|)
block|{
comment|// Expected to fail due to old schema
name|foundException
operator|=
literal|true
expr_stmt|;
block|}
if|if
condition|(
operator|!
name|foundException
condition|)
block|{
throw|throw
operator|new
name|Exception
argument_list|(
literal|"Hive operations shouldn't pass with older version schema"
argument_list|)
throw|;
block|}
comment|// Generate dummy pre-upgrade script with errors
name|String
name|invalidPreUpgradeScript
init|=
name|writeDummyPreUpgradeScript
argument_list|(
literal|0
argument_list|,
literal|"upgrade-2.3.0-to-3.0.0.derby.sql"
argument_list|,
literal|"foo bar;"
argument_list|)
decl_stmt|;
comment|// Generate dummy pre-upgrade scripts with valid SQL
name|String
name|validPreUpgradeScript0
init|=
name|writeDummyPreUpgradeScript
argument_list|(
literal|1
argument_list|,
literal|"upgrade-2.3.0-to-3.0.0.derby.sql"
argument_list|,
literal|"CREATE TABLE schema_test0 (id integer);"
argument_list|)
decl_stmt|;
name|String
name|validPreUpgradeScript1
init|=
name|writeDummyPreUpgradeScript
argument_list|(
literal|2
argument_list|,
literal|"upgrade-2.3.0-to-3.0.0.derby.sql"
argument_list|,
literal|"CREATE TABLE schema_test1 (id integer);"
argument_list|)
decl_stmt|;
comment|// Capture system out and err
name|schemaTool
operator|.
name|setVerbose
argument_list|(
literal|true
argument_list|)
expr_stmt|;
name|OutputStream
name|stderr
init|=
operator|new
name|ByteArrayOutputStream
argument_list|()
decl_stmt|;
name|PrintStream
name|errPrintStream
init|=
operator|new
name|PrintStream
argument_list|(
name|stderr
argument_list|)
decl_stmt|;
name|System
operator|.
name|setErr
argument_list|(
name|errPrintStream
argument_list|)
expr_stmt|;
name|OutputStream
name|stdout
init|=
operator|new
name|ByteArrayOutputStream
argument_list|()
decl_stmt|;
name|PrintStream
name|outPrintStream
init|=
operator|new
name|PrintStream
argument_list|(
name|stdout
argument_list|)
decl_stmt|;
name|System
operator|.
name|setOut
argument_list|(
name|outPrintStream
argument_list|)
expr_stmt|;
comment|// Upgrade schema from 0.7.0 to latest
name|execute
argument_list|(
operator|new
name|SchemaToolTaskUpgrade
argument_list|()
argument_list|,
literal|"-upgradeSchemaFrom 1.2.0"
argument_list|)
expr_stmt|;
comment|// Verify that the schemaTool ran pre-upgrade scripts and ignored errors
name|Assert
operator|.
name|assertTrue
argument_list|(
name|stderr
operator|.
name|toString
argument_list|()
operator|.
name|contains
argument_list|(
name|invalidPreUpgradeScript
argument_list|)
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertTrue
argument_list|(
name|stderr
operator|.
name|toString
argument_list|()
operator|.
name|contains
argument_list|(
literal|"foo"
argument_list|)
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertFalse
argument_list|(
name|stderr
operator|.
name|toString
argument_list|()
operator|.
name|contains
argument_list|(
name|validPreUpgradeScript0
argument_list|)
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertFalse
argument_list|(
name|stderr
operator|.
name|toString
argument_list|()
operator|.
name|contains
argument_list|(
name|validPreUpgradeScript1
argument_list|)
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertTrue
argument_list|(
name|stdout
operator|.
name|toString
argument_list|()
operator|.
name|contains
argument_list|(
name|validPreUpgradeScript0
argument_list|)
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertTrue
argument_list|(
name|stdout
operator|.
name|toString
argument_list|()
operator|.
name|contains
argument_list|(
name|validPreUpgradeScript1
argument_list|)
argument_list|)
expr_stmt|;
comment|// Verify that driver works fine with latest schema
name|schemaTool
operator|.
name|verifySchemaVersion
argument_list|()
expr_stmt|;
block|}
comment|/**    * Test validate uri of locations    */
annotation|@
name|Test
specifier|public
name|void
name|testValidateLocations
parameter_list|()
throws|throws
name|Exception
block|{
name|execute
argument_list|(
operator|new
name|SchemaToolTaskInit
argument_list|()
argument_list|,
literal|"-initSchema"
argument_list|)
expr_stmt|;
name|URI
name|defaultRoot
init|=
operator|new
name|URI
argument_list|(
literal|"hdfs://myhost.com:8020"
argument_list|)
decl_stmt|;
name|URI
name|defaultRoot2
init|=
operator|new
name|URI
argument_list|(
literal|"s3://myhost2.com:8888"
argument_list|)
decl_stmt|;
comment|//check empty DB
name|boolean
name|isValid
init|=
name|validator
operator|.
name|validateLocations
argument_list|(
name|conn
argument_list|,
literal|null
argument_list|)
decl_stmt|;
name|Assert
operator|.
name|assertTrue
argument_list|(
name|isValid
argument_list|)
expr_stmt|;
name|isValid
operator|=
name|validator
operator|.
name|validateLocations
argument_list|(
name|conn
argument_list|,
operator|new
name|URI
index|[]
block|{
name|defaultRoot
block|,
name|defaultRoot2
block|}
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertTrue
argument_list|(
name|isValid
argument_list|)
expr_stmt|;
name|String
name|time
init|=
name|String
operator|.
name|valueOf
argument_list|(
name|System
operator|.
name|currentTimeMillis
argument_list|()
operator|/
literal|1000
argument_list|)
decl_stmt|;
comment|// Test valid case
name|String
index|[]
name|scripts
init|=
operator|new
name|String
index|[]
block|{
literal|"insert into CTLGS values(3, 'test_cat_2', 'description', 'hdfs://myhost.com:8020/user/hive/warehouse/mydb', "
operator|+
name|time
operator|+
literal|");"
block|,
literal|"insert into DBS values(2, 'my db', 'hdfs://myhost.com:8020/user/hive/warehouse/mydb', 'mydb', 'public', 'role', 'test_cat_2', "
operator|+
name|time
operator|+
literal|");"
block|,
literal|"insert into DBS values(7, 'db with bad port', 'hdfs://myhost.com:8020/', 'haDB', 'public', 'role', 'test_cat_2', "
operator|+
name|time
operator|+
literal|");"
block|,
literal|"insert into SDS(SD_ID,CD_ID,INPUT_FORMAT,IS_COMPRESSED,IS_STOREDASSUBDIRECTORIES,LOCATION,NUM_BUCKETS,OUTPUT_FORMAT,SERDE_ID) values (1,null,'org.apache.hadoop.mapred.TextInputFormat','N','N','hdfs://myhost.com:8020/user/hive/warehouse/mydb',-1,'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat',null);"
block|,
literal|"insert into SDS(SD_ID,CD_ID,INPUT_FORMAT,IS_COMPRESSED,IS_STOREDASSUBDIRECTORIES,LOCATION,NUM_BUCKETS,OUTPUT_FORMAT,SERDE_ID) values (2,null,'org.apache.hadoop.mapred.TextInputFormat','N','N','hdfs://myhost.com:8020/user/admin/2015_11_18',-1,'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat',null);"
block|,
literal|"insert into SDS(SD_ID,CD_ID,INPUT_FORMAT,IS_COMPRESSED,IS_STOREDASSUBDIRECTORIES,LOCATION,NUM_BUCKETS,OUTPUT_FORMAT,SERDE_ID) values (3,null,'org.apache.hadoop.mapred.TextInputFormat','N','N',null,-1,'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat',null);"
block|,
literal|"insert into SDS(SD_ID,CD_ID,INPUT_FORMAT,IS_COMPRESSED,IS_STOREDASSUBDIRECTORIES,LOCATION,NUM_BUCKETS,OUTPUT_FORMAT,SERDE_ID) values (4000,null,'org.apache.hadoop.mapred.TextInputFormat','N','N','hdfs://myhost.com:8020/',-1,'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat',null);"
block|,
literal|"insert into TBLS(TBL_ID,CREATE_TIME,DB_ID,LAST_ACCESS_TIME,OWNER,RETENTION,SD_ID,TBL_NAME,TBL_TYPE,VIEW_EXPANDED_TEXT,VIEW_ORIGINAL_TEXT,IS_REWRITE_ENABLED) values (2 ,1435255431,2,0 ,'hive',0,1,'mytal','MANAGED_TABLE',NULL,NULL,'n');"
block|,
literal|"insert into TBLS(TBL_ID,CREATE_TIME,DB_ID,LAST_ACCESS_TIME,OWNER,RETENTION,SD_ID,TBL_NAME,TBL_TYPE,VIEW_EXPANDED_TEXT,VIEW_ORIGINAL_TEXT,IS_REWRITE_ENABLED) values (3 ,1435255431,2,0 ,'hive',0,3,'myView','VIRTUAL_VIEW','select a.col1,a.col2 from foo','select * from foo','n');"
block|,
literal|"insert into TBLS(TBL_ID,CREATE_TIME,DB_ID,LAST_ACCESS_TIME,OWNER,RETENTION,SD_ID,TBL_NAME,TBL_TYPE,VIEW_EXPANDED_TEXT,VIEW_ORIGINAL_TEXT,IS_REWRITE_ENABLED) values (4012 ,1435255431,7,0 ,'hive',0,4000,'mytal4012','MANAGED_TABLE',NULL,NULL,'n');"
block|,
literal|"insert into PARTITIONS(PART_ID,CREATE_TIME,LAST_ACCESS_TIME, PART_NAME,SD_ID,TBL_ID) values(1, 1441402388,0, 'd1=1/d2=1',2,2);"
block|,
literal|"insert into SKEWED_STRING_LIST values(1);"
block|,
literal|"insert into SKEWED_STRING_LIST values(2);"
block|,
literal|"insert into SKEWED_COL_VALUE_LOC_MAP values(1,1,'hdfs://myhost.com:8020/user/hive/warehouse/mytal/HIVE_DEFAULT_LIST_BUCKETING_DIR_NAME/');"
block|,
literal|"insert into SKEWED_COL_VALUE_LOC_MAP values(2,2,'s3://myhost.com:8020/user/hive/warehouse/mytal/HIVE_DEFAULT_LIST_BUCKETING_DIR_NAME/');"
block|}
decl_stmt|;
name|File
name|scriptFile
init|=
name|generateTestScript
argument_list|(
name|scripts
argument_list|)
decl_stmt|;
name|schemaTool
operator|.
name|execSql
argument_list|(
name|scriptFile
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
name|isValid
operator|=
name|validator
operator|.
name|validateLocations
argument_list|(
name|conn
argument_list|,
literal|null
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertTrue
argument_list|(
name|isValid
argument_list|)
expr_stmt|;
name|isValid
operator|=
name|validator
operator|.
name|validateLocations
argument_list|(
name|conn
argument_list|,
operator|new
name|URI
index|[]
block|{
name|defaultRoot
block|,
name|defaultRoot2
block|}
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertTrue
argument_list|(
name|isValid
argument_list|)
expr_stmt|;
name|scripts
operator|=
operator|new
name|String
index|[]
block|{
literal|"delete from SKEWED_COL_VALUE_LOC_MAP;"
block|,
literal|"delete from SKEWED_STRING_LIST;"
block|,
literal|"delete from PARTITIONS;"
block|,
literal|"delete from TBLS;"
block|,
literal|"delete from SDS;"
block|,
literal|"delete from DBS;"
block|,
literal|"insert into DBS values(2, 'my db', '/user/hive/warehouse/mydb', 'mydb', 'public', 'role', 'test_cat_2', "
operator|+
name|time
operator|+
literal|");"
block|,
literal|"insert into DBS values(4, 'my db2', 'hdfs://myhost.com:8020', '', 'public', 'role', 'test_cat_2', "
operator|+
name|time
operator|+
literal|");"
block|,
literal|"insert into DBS values(6, 'db with bad port', 'hdfs://myhost.com:8020:', 'zDB', 'public', 'role', 'test_cat_2', "
operator|+
name|time
operator|+
literal|");"
block|,
literal|"insert into DBS values(7, 'db with bad port', 'hdfs://mynameservice.com/', 'haDB', 'public', 'role', 'test_cat_2', "
operator|+
name|time
operator|+
literal|");"
block|,
literal|"insert into SDS(SD_ID,CD_ID,INPUT_FORMAT,IS_COMPRESSED,IS_STOREDASSUBDIRECTORIES,LOCATION,NUM_BUCKETS,OUTPUT_FORMAT,SERDE_ID) values (1,null,'org.apache.hadoop.mapred.TextInputFormat','N','N','hdfs://yourhost.com:8020/user/hive/warehouse/mydb',-1,'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat',null);"
block|,
literal|"insert into SDS(SD_ID,CD_ID,INPUT_FORMAT,IS_COMPRESSED,IS_STOREDASSUBDIRECTORIES,LOCATION,NUM_BUCKETS,OUTPUT_FORMAT,SERDE_ID) values (2,null,'org.apache.hadoop.mapred.TextInputFormat','N','N','file:///user/admin/2015_11_18',-1,'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat',null);"
block|,
literal|"insert into TBLS(TBL_ID,CREATE_TIME,DB_ID,LAST_ACCESS_TIME,OWNER,RETENTION,SD_ID,TBL_NAME,TBL_TYPE,VIEW_EXPANDED_TEXT,VIEW_ORIGINAL_TEXT,IS_REWRITE_ENABLED) values (2 ,1435255431,2,0 ,'hive',0,1,'mytal','MANAGED_TABLE',NULL,NULL,'n');"
block|,
literal|"insert into PARTITIONS(PART_ID,CREATE_TIME,LAST_ACCESS_TIME, PART_NAME,SD_ID,TBL_ID) values(1, 1441402388,0, 'd1=1/d2=1',2,2);"
block|,
literal|"insert into SDS(SD_ID,CD_ID,INPUT_FORMAT,IS_COMPRESSED,IS_STOREDASSUBDIRECTORIES,LOCATION,NUM_BUCKETS,OUTPUT_FORMAT,SERDE_ID) values (3000,null,'org.apache.hadoop.mapred.TextInputFormat','N','N','yourhost.com:8020/user/hive/warehouse/mydb',-1,'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat',null);"
block|,
literal|"insert into SDS(SD_ID,CD_ID,INPUT_FORMAT,IS_COMPRESSED,IS_STOREDASSUBDIRECTORIES,LOCATION,NUM_BUCKETS,OUTPUT_FORMAT,SERDE_ID) values (4000,null,'org.apache.hadoop.mapred.TextInputFormat','N','N','hdfs://myhost.com:8020/',-1,'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat',null);"
block|,
literal|"insert into SDS(SD_ID,CD_ID,INPUT_FORMAT,IS_COMPRESSED,IS_STOREDASSUBDIRECTORIES,LOCATION,NUM_BUCKETS,OUTPUT_FORMAT,SERDE_ID) values (4001,null,'org.apache.hadoop.mapred.TextInputFormat','N','N','hdfs://myhost.com:8020',-1,'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat',null);"
block|,
literal|"insert into SDS(SD_ID,CD_ID,INPUT_FORMAT,IS_COMPRESSED,IS_STOREDASSUBDIRECTORIES,LOCATION,NUM_BUCKETS,OUTPUT_FORMAT,SERDE_ID) values (4003,null,'org.apache.hadoop.mapred.TextInputFormat','N','N','hdfs://myhost.com:8020',-1,'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat',null);"
block|,
literal|"insert into SDS(SD_ID,CD_ID,INPUT_FORMAT,IS_COMPRESSED,IS_STOREDASSUBDIRECTORIES,LOCATION,NUM_BUCKETS,OUTPUT_FORMAT,SERDE_ID) values (4004,null,'org.apache.hadoop.mapred.TextInputFormat','N','N','hdfs://myhost.com:8020',-1,'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat',null);"
block|,
literal|"insert into SDS(SD_ID,CD_ID,INPUT_FORMAT,IS_COMPRESSED,IS_STOREDASSUBDIRECTORIES,LOCATION,NUM_BUCKETS,OUTPUT_FORMAT,SERDE_ID) values (4002,null,'org.apache.hadoop.mapred.TextInputFormat','N','N','hdfs://myhost.com:8020/',-1,'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat',null);"
block|,
literal|"insert into SDS(SD_ID,CD_ID,INPUT_FORMAT,IS_COMPRESSED,IS_STOREDASSUBDIRECTORIES,LOCATION,NUM_BUCKETS,OUTPUT_FORMAT,SERDE_ID) values (5000,null,'org.apache.hadoop.mapred.TextInputFormat','N','N','file:///user/admin/2016_11_18',-1,'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat',null);"
block|,
literal|"insert into TBLS(TBL_ID,CREATE_TIME,DB_ID,LAST_ACCESS_TIME,OWNER,RETENTION,SD_ID,TBL_NAME,TBL_TYPE,VIEW_EXPANDED_TEXT,VIEW_ORIGINAL_TEXT,IS_REWRITE_ENABLED) values (3000 ,1435255431,2,0 ,'hive',0,3000,'mytal3000','MANAGED_TABLE',NULL,NULL,'n');"
block|,
literal|"insert into TBLS(TBL_ID,CREATE_TIME,DB_ID,LAST_ACCESS_TIME,OWNER,RETENTION,SD_ID,TBL_NAME,TBL_TYPE,VIEW_EXPANDED_TEXT,VIEW_ORIGINAL_TEXT,IS_REWRITE_ENABLED) values (4011 ,1435255431,4,0 ,'hive',0,4001,'mytal4011','MANAGED_TABLE',NULL,NULL,'n');"
block|,
literal|"insert into TBLS(TBL_ID,CREATE_TIME,DB_ID,LAST_ACCESS_TIME,OWNER,RETENTION,SD_ID,TBL_NAME,TBL_TYPE,VIEW_EXPANDED_TEXT,VIEW_ORIGINAL_TEXT,IS_REWRITE_ENABLED) values (4012 ,1435255431,4,0 ,'hive',0,4002,'','MANAGED_TABLE',NULL,NULL,'n');"
block|,
literal|"insert into TBLS(TBL_ID,CREATE_TIME,DB_ID,LAST_ACCESS_TIME,OWNER,RETENTION,SD_ID,TBL_NAME,TBL_TYPE,VIEW_EXPANDED_TEXT,VIEW_ORIGINAL_TEXT,IS_REWRITE_ENABLED) values (4013 ,1435255431,4,0 ,'hive',0,4003,'mytal4013','MANAGED_TABLE',NULL,NULL,'n');"
block|,
literal|"insert into TBLS(TBL_ID,CREATE_TIME,DB_ID,LAST_ACCESS_TIME,OWNER,RETENTION,SD_ID,TBL_NAME,TBL_TYPE,VIEW_EXPANDED_TEXT,VIEW_ORIGINAL_TEXT,IS_REWRITE_ENABLED) values (4014 ,1435255431,2,0 ,'hive',0,4003,'','MANAGED_TABLE',NULL,NULL,'n');"
block|,
literal|"insert into PARTITIONS(PART_ID,CREATE_TIME,LAST_ACCESS_TIME, PART_NAME,SD_ID,TBL_ID) values(4001, 1441402388,0, 'd1=1/d2=4001',4001,4011);"
block|,
literal|"insert into PARTITIONS(PART_ID,CREATE_TIME,LAST_ACCESS_TIME, PART_NAME,SD_ID,TBL_ID) values(4002, 1441402388,0, 'd1=1/d2=4002',4002,4012);"
block|,
literal|"insert into PARTITIONS(PART_ID,CREATE_TIME,LAST_ACCESS_TIME, PART_NAME,SD_ID,TBL_ID) values(4003, 1441402388,0, 'd1=1/d2=4003',4003,4013);"
block|,
literal|"insert into PARTITIONS(PART_ID,CREATE_TIME,LAST_ACCESS_TIME, PART_NAME,SD_ID,TBL_ID) values(4004, 1441402388,0, 'd1=1/d2=4004',4004,4014);"
block|,
literal|"insert into PARTITIONS(PART_ID,CREATE_TIME,LAST_ACCESS_TIME, PART_NAME,SD_ID,TBL_ID) values(5000, 1441402388,0, 'd1=1/d2=5000',5000,2);"
block|,
literal|"insert into SKEWED_STRING_LIST values(1);"
block|,
literal|"insert into SKEWED_STRING_LIST values(2);"
block|,
literal|"insert into SKEWED_COL_VALUE_LOC_MAP values(1,1,'hdfs://yourhost.com:8020/user/hive/warehouse/mytal/HIVE_DEFAULT_LIST_BUCKETING_DIR_NAME/');"
block|,
literal|"insert into SKEWED_COL_VALUE_LOC_MAP values(2,2,'file:///user/admin/warehouse/mytal/HIVE_DEFAULT_LIST_BUCKETING_DIR_NAME/');"
block|}
expr_stmt|;
name|scriptFile
operator|=
name|generateTestScript
argument_list|(
name|scripts
argument_list|)
expr_stmt|;
name|schemaTool
operator|.
name|execSql
argument_list|(
name|scriptFile
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
name|isValid
operator|=
name|validator
operator|.
name|validateLocations
argument_list|(
name|conn
argument_list|,
literal|null
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertFalse
argument_list|(
name|isValid
argument_list|)
expr_stmt|;
name|isValid
operator|=
name|validator
operator|.
name|validateLocations
argument_list|(
name|conn
argument_list|,
operator|new
name|URI
index|[]
block|{
name|defaultRoot
block|,
name|defaultRoot2
block|}
argument_list|)
expr_stmt|;
name|Assert
operator|.
name|assertFalse
argument_list|(
name|isValid
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Test
specifier|public
name|void
name|testHiveMetastoreDbPropertiesTable
parameter_list|()
throws|throws
name|HiveMetaException
throws|,
name|IOException
block|{
name|execute
argument_list|(
operator|new
name|SchemaToolTaskInit
argument_list|()
argument_list|,
literal|"-initSchemaTo 3.0.0"
argument_list|)
expr_stmt|;
name|validateMetastoreDbPropertiesTable
argument_list|()
expr_stmt|;
block|}
annotation|@
name|Test
specifier|public
name|void
name|testMetastoreDbPropertiesAfterUpgrade
parameter_list|()
throws|throws
name|HiveMetaException
throws|,
name|IOException
block|{
name|execute
argument_list|(
operator|new
name|SchemaToolTaskInit
argument_list|()
argument_list|,
literal|"-initSchemaTo 1.2.0"
argument_list|)
expr_stmt|;
name|execute
argument_list|(
operator|new
name|SchemaToolTaskUpgrade
argument_list|()
argument_list|,
literal|"-upgradeSchema"
argument_list|)
expr_stmt|;
name|validateMetastoreDbPropertiesTable
argument_list|()
expr_stmt|;
block|}
specifier|private
name|File
name|generateTestScript
parameter_list|(
name|String
index|[]
name|stmts
parameter_list|)
throws|throws
name|IOException
block|{
name|File
name|testScriptFile
init|=
name|File
operator|.
name|createTempFile
argument_list|(
literal|"schematest"
argument_list|,
literal|".sql"
argument_list|)
decl_stmt|;
name|testScriptFile
operator|.
name|deleteOnExit
argument_list|()
expr_stmt|;
name|FileWriter
name|fstream
init|=
operator|new
name|FileWriter
argument_list|(
name|testScriptFile
operator|.
name|getPath
argument_list|()
argument_list|)
decl_stmt|;
name|BufferedWriter
name|out
init|=
operator|new
name|BufferedWriter
argument_list|(
name|fstream
argument_list|)
decl_stmt|;
for|for
control|(
name|String
name|line
range|:
name|stmts
control|)
block|{
name|out
operator|.
name|write
argument_list|(
name|line
argument_list|)
expr_stmt|;
name|out
operator|.
name|newLine
argument_list|()
expr_stmt|;
block|}
name|out
operator|.
name|close
argument_list|()
expr_stmt|;
return|return
name|testScriptFile
return|;
block|}
specifier|private
name|void
name|validateMetastoreDbPropertiesTable
parameter_list|()
throws|throws
name|HiveMetaException
throws|,
name|IOException
block|{
name|boolean
name|isValid
init|=
operator|(
name|boolean
operator|)
name|validator
operator|.
name|validateSchemaTables
argument_list|(
name|conn
argument_list|)
decl_stmt|;
name|Assert
operator|.
name|assertTrue
argument_list|(
name|isValid
argument_list|)
expr_stmt|;
comment|// adding same property key twice should throw unique key constraint violation exception
name|String
index|[]
name|scripts
init|=
operator|new
name|String
index|[]
block|{
literal|"insert into METASTORE_DB_PROPERTIES values ('guid', 'test-uuid-1', 'dummy uuid 1')"
block|,
literal|"insert into METASTORE_DB_PROPERTIES values ('guid', 'test-uuid-2', 'dummy uuid 2')"
block|, }
decl_stmt|;
name|File
name|scriptFile
init|=
name|generateTestScript
argument_list|(
name|scripts
argument_list|)
decl_stmt|;
name|Exception
name|ex
init|=
literal|null
decl_stmt|;
try|try
block|{
name|schemaTool
operator|.
name|execSql
argument_list|(
name|scriptFile
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|iox
parameter_list|)
block|{
name|ex
operator|=
name|iox
expr_stmt|;
block|}
name|Assert
operator|.
name|assertTrue
argument_list|(
name|ex
operator|!=
literal|null
operator|&&
name|ex
operator|instanceof
name|IOException
argument_list|)
expr_stmt|;
block|}
comment|/**    * Write out a dummy pre-upgrade script with given SQL statement.    */
specifier|private
name|String
name|writeDummyPreUpgradeScript
parameter_list|(
name|int
name|index
parameter_list|,
name|String
name|upgradeScriptName
parameter_list|,
name|String
name|sql
parameter_list|)
throws|throws
name|Exception
block|{
name|String
name|preUpgradeScript
init|=
literal|"pre-"
operator|+
name|index
operator|+
literal|"-"
operator|+
name|upgradeScriptName
decl_stmt|;
name|String
name|dummyPreScriptPath
init|=
name|System
operator|.
name|getProperty
argument_list|(
literal|"test.tmp.dir"
argument_list|,
literal|"target/tmp"
argument_list|)
operator|+
name|File
operator|.
name|separatorChar
operator|+
literal|"scripts"
operator|+
name|File
operator|.
name|separatorChar
operator|+
literal|"metastore"
operator|+
name|File
operator|.
name|separatorChar
operator|+
literal|"upgrade"
operator|+
name|File
operator|.
name|separatorChar
operator|+
literal|"derby"
operator|+
name|File
operator|.
name|separatorChar
operator|+
name|preUpgradeScript
decl_stmt|;
name|FileWriter
name|fstream
init|=
operator|new
name|FileWriter
argument_list|(
name|dummyPreScriptPath
argument_list|)
decl_stmt|;
name|BufferedWriter
name|out
init|=
operator|new
name|BufferedWriter
argument_list|(
name|fstream
argument_list|)
decl_stmt|;
name|out
operator|.
name|write
argument_list|(
name|sql
operator|+
name|System
operator|.
name|getProperty
argument_list|(
literal|"line.separator"
argument_list|)
argument_list|)
expr_stmt|;
name|out
operator|.
name|close
argument_list|()
expr_stmt|;
return|return
name|preUpgradeScript
return|;
block|}
comment|// Insert the records in DB to simulate a hive table
specifier|private
name|void
name|createTestHiveTableSchemas
parameter_list|()
throws|throws
name|IOException
block|{
name|String
name|time
init|=
name|String
operator|.
name|valueOf
argument_list|(
name|System
operator|.
name|currentTimeMillis
argument_list|()
operator|/
literal|1000
argument_list|)
decl_stmt|;
name|String
index|[]
name|scripts
init|=
operator|new
name|String
index|[]
block|{
literal|"insert into CTLGS values (1, 'mycat', 'my description', 'hdfs://myhost.com:8020/user/hive/warehouse', "
operator|+
name|time
operator|+
literal|");"
block|,
literal|"insert into DBS values(2, 'my db', 'hdfs://myhost.com:8020/user/hive/warehouse/mydb', 'mydb', 'public', 'role', 'mycat', "
operator|+
name|time
operator|+
literal|");"
block|,
literal|"insert into SDS(SD_ID,CD_ID,INPUT_FORMAT,IS_COMPRESSED,IS_STOREDASSUBDIRECTORIES,LOCATION,NUM_BUCKETS,OUTPUT_FORMAT,SERDE_ID) values (1,null,'org.apache.hadoop.mapred.TextInputFormat','N','N','hdfs://myhost.com:8020/user/hive/warehouse/mydb',-1,'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat',null);"
block|,
literal|"insert into SDS(SD_ID,CD_ID,INPUT_FORMAT,IS_COMPRESSED,IS_STOREDASSUBDIRECTORIES,LOCATION,NUM_BUCKETS,OUTPUT_FORMAT,SERDE_ID) values (2,null,'org.apache.hadoop.mapred.TextInputFormat','N','N','hdfs://myhost.com:8020/user/admin/2015_11_18',-1,'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat',null);"
block|,
literal|"insert into TBLS(TBL_ID,CREATE_TIME,DB_ID,LAST_ACCESS_TIME,OWNER,RETENTION,SD_ID,TBL_NAME,TBL_TYPE,VIEW_EXPANDED_TEXT,VIEW_ORIGINAL_TEXT,IS_REWRITE_ENABLED) values (2 ,1435255431,2,0 ,'hive',0,1,'mytal','MANAGED_TABLE',NULL,NULL,'n');"
block|,
literal|"insert into TBLS(TBL_ID,CREATE_TIME,DB_ID,LAST_ACCESS_TIME,OWNER,RETENTION,SD_ID,TBL_NAME,TBL_TYPE,VIEW_EXPANDED_TEXT,VIEW_ORIGINAL_TEXT,IS_REWRITE_ENABLED) values (3 ,1435255431,2,0 ,'hive',0,2,'aTable','MANAGED_TABLE',NULL,NULL,'n');"
block|,
literal|"insert into PARTITIONS(PART_ID,CREATE_TIME,LAST_ACCESS_TIME, PART_NAME,SD_ID,TBL_ID) values(1, 1441402388,0, 'd1=1/d2=1',2,2);"
block|}
decl_stmt|;
name|File
name|scriptFile
init|=
name|generateTestScript
argument_list|(
name|scripts
argument_list|)
decl_stmt|;
name|schemaTool
operator|.
name|execSql
argument_list|(
name|scriptFile
operator|.
name|getPath
argument_list|()
argument_list|)
expr_stmt|;
block|}
comment|/**    * A mock Connection class that throws an exception out of getMetaData().    */
class|class
name|BadMetaDataConnection
extends|extends
name|DelegatingConnection
block|{
specifier|static
specifier|final
name|String
name|FAILURE_TEXT
init|=
literal|"fault injected"
decl_stmt|;
name|BadMetaDataConnection
parameter_list|(
name|Connection
name|connection
parameter_list|)
block|{
name|super
argument_list|(
name|connection
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|DatabaseMetaData
name|getMetaData
parameter_list|()
throws|throws
name|SQLException
block|{
throw|throw
operator|new
name|SQLException
argument_list|(
name|FAILURE_TEXT
argument_list|)
throw|;
block|}
block|}
specifier|private
name|void
name|execute
parameter_list|(
name|SchemaToolTask
name|task
parameter_list|,
name|String
name|taskArgs
parameter_list|)
throws|throws
name|HiveMetaException
block|{
try|try
block|{
name|StrTokenizer
name|tokenizer
init|=
operator|new
name|StrTokenizer
argument_list|(
name|argsBase
operator|+
name|taskArgs
argument_list|,
literal|' '
argument_list|,
literal|'\"'
argument_list|)
decl_stmt|;
name|SchemaToolCommandLine
name|cl
init|=
operator|new
name|SchemaToolCommandLine
argument_list|(
name|tokenizer
operator|.
name|getTokenArray
argument_list|()
argument_list|,
literal|null
argument_list|)
decl_stmt|;
name|task
operator|.
name|setCommandLineArguments
argument_list|(
name|cl
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
throw|throw
operator|new
name|IllegalStateException
argument_list|(
literal|"Could not parse comman line \n"
operator|+
name|argsBase
operator|+
name|taskArgs
argument_list|,
name|e
argument_list|)
throw|;
block|}
name|task
operator|.
name|setHiveSchemaTool
argument_list|(
name|schemaTool
argument_list|)
expr_stmt|;
name|task
operator|.
name|execute
argument_list|()
expr_stmt|;
block|}
block|}
end_class

end_unit

