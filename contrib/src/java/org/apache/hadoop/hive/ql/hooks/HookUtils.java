begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/**  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|hooks
package|;
end_package

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|IOException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|Serializable
import|;
end_import

begin_import
import|import
name|java
operator|.
name|net
operator|.
name|URI
import|;
end_import

begin_import
import|import
name|java
operator|.
name|sql
operator|.
name|Connection
import|;
end_import

begin_import
import|import
name|java
operator|.
name|sql
operator|.
name|DriverManager
import|;
end_import

begin_import
import|import
name|java
operator|.
name|sql
operator|.
name|PreparedStatement
import|;
end_import

begin_import
import|import
name|java
operator|.
name|sql
operator|.
name|ResultSet
import|;
end_import

begin_import
import|import
name|java
operator|.
name|sql
operator|.
name|SQLException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|ArrayList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Arrays
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Collection
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Random
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Set
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|Log
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|LogFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configuration
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|ContentSummary
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|FileSystem
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|Path
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|metastore
operator|.
name|TableType
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|Task
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|Hive
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|Partition
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|metadata
operator|.
name|Table
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|plan
operator|.
name|MapredWork
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|util
operator|.
name|ReflectionUtils
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|hooks
operator|.
name|conf
operator|.
name|FBHiveConf
import|;
end_import

begin_comment
comment|/**  * Utilities for writing hooks.  */
end_comment

begin_class
specifier|public
class|class
name|HookUtils
block|{
specifier|static
specifier|final
specifier|private
name|Log
name|LOG
init|=
name|LogFactory
operator|.
name|getLog
argument_list|(
name|HookUtils
operator|.
name|class
operator|.
name|getName
argument_list|()
argument_list|)
decl_stmt|;
specifier|public
specifier|static
specifier|final
name|String
name|TABLE_CREATION_CLUSTER
init|=
literal|"creation_cluster"
decl_stmt|;
specifier|static
specifier|final
name|String
name|POST_HOOK_DB_MAX_RETRY_VAR_NAME
init|=
literal|"fbhive.posthook.mysql.max_retries"
decl_stmt|;
comment|// The default value is to retry 20 times with maximum retry interval
comment|// 60 seconds. The expectation is about 22 minutes. After 7 retries, it
comment|// reaches 60 seconds.
specifier|static
specifier|final
name|int
name|DEFAULT_SQL_NUM_RETRIES
init|=
literal|20
decl_stmt|;
specifier|static
specifier|final
name|int
name|DEFAULT_RETRY_MAX_INTERVAL_SEC
init|=
literal|60
decl_stmt|;
specifier|private
specifier|static
name|Connection
name|getConnection
parameter_list|(
name|HiveConf
name|conf
parameter_list|,
name|String
name|url
parameter_list|)
throws|throws
name|SQLException
block|{
return|return
name|DriverManager
operator|.
name|getConnection
argument_list|(
name|url
argument_list|,
name|conf
operator|.
name|get
argument_list|(
name|FBHiveConf
operator|.
name|FBHIVE_DB_USERNAME
argument_list|)
argument_list|,
name|conf
operator|.
name|get
argument_list|(
name|FBHiveConf
operator|.
name|FBHIVE_DB_PASSWORD
argument_list|)
argument_list|)
return|;
block|}
specifier|public
specifier|static
name|int
name|getSqlNumRetry
parameter_list|(
name|HiveConf
name|conf
parameter_list|)
block|{
return|return
name|conf
operator|.
name|getInt
argument_list|(
name|POST_HOOK_DB_MAX_RETRY_VAR_NAME
argument_list|,
literal|30
argument_list|)
return|;
block|}
specifier|public
specifier|static
name|void
name|runInsert
parameter_list|(
name|HiveConf
name|conf
parameter_list|,
name|ConnectionUrlFactory
name|urlFactory
parameter_list|,
name|String
name|sql
parameter_list|,
name|List
argument_list|<
name|Object
argument_list|>
name|sqlParams
parameter_list|)
throws|throws
name|Exception
block|{
name|runInsert
argument_list|(
name|conf
argument_list|,
name|urlFactory
argument_list|,
name|sql
argument_list|,
name|sqlParams
argument_list|,
name|DEFAULT_SQL_NUM_RETRIES
argument_list|)
expr_stmt|;
block|}
specifier|public
specifier|static
name|List
argument_list|<
name|List
argument_list|<
name|Object
argument_list|>
argument_list|>
name|runInsertSelect
parameter_list|(
name|HiveConf
name|conf
parameter_list|,
name|ConnectionUrlFactory
name|urlFactory
parameter_list|,
name|String
name|sql
parameter_list|,
name|List
argument_list|<
name|Object
argument_list|>
name|sqlParams
parameter_list|)
throws|throws
name|Exception
block|{
return|return
name|runInsertSelect
argument_list|(
name|conf
argument_list|,
name|urlFactory
argument_list|,
name|sql
argument_list|,
name|sqlParams
argument_list|,
literal|true
argument_list|)
return|;
block|}
specifier|public
specifier|static
name|List
argument_list|<
name|List
argument_list|<
name|Object
argument_list|>
argument_list|>
name|runInsertSelect
parameter_list|(
name|HiveConf
name|conf
parameter_list|,
name|ConnectionUrlFactory
name|urlFactory
parameter_list|,
name|String
name|sql
parameter_list|,
name|List
argument_list|<
name|Object
argument_list|>
name|sqlParams
parameter_list|,
name|boolean
name|isWrite
parameter_list|)
throws|throws
name|Exception
block|{
return|return
name|runInsertSelect
argument_list|(
name|conf
argument_list|,
name|urlFactory
argument_list|,
name|sql
argument_list|,
name|sqlParams
argument_list|,
name|isWrite
argument_list|,
name|DEFAULT_SQL_NUM_RETRIES
argument_list|,
name|DEFAULT_RETRY_MAX_INTERVAL_SEC
argument_list|,
literal|false
argument_list|)
return|;
block|}
specifier|public
specifier|static
name|void
name|runInsert
parameter_list|(
name|HiveConf
name|conf
parameter_list|,
name|ConnectionUrlFactory
name|urlFactory
parameter_list|,
name|String
name|sql
parameter_list|,
name|List
argument_list|<
name|Object
argument_list|>
name|sqlParams
parameter_list|,
name|int
name|numRetries
parameter_list|)
throws|throws
name|Exception
block|{
name|runInsertSelect
argument_list|(
name|conf
argument_list|,
name|urlFactory
argument_list|,
name|sql
argument_list|,
name|sqlParams
argument_list|,
literal|true
argument_list|,
name|numRetries
argument_list|,
name|DEFAULT_RETRY_MAX_INTERVAL_SEC
argument_list|,
literal|true
argument_list|)
expr_stmt|;
block|}
comment|/*    * @param conf -    * @param parentTierName - the factory to create    * @param tierName - the factory to create    * @param tierParam1Name - the name of the first parameter    * @param tierParam2Name - the name of the second parameter    */
specifier|public
specifier|static
name|ConnectionUrlFactory
name|getUrlFactory
parameter_list|(
name|HiveConf
name|conf
parameter_list|,
name|String
name|parentTierName
parameter_list|,
name|String
name|childTierName
parameter_list|,
name|String
name|tierParam1Name
parameter_list|,
name|String
name|tierParam2Name
parameter_list|)
block|{
return|return
name|getUrlFactory
argument_list|(
name|conf
argument_list|,
name|parentTierName
argument_list|,
name|childTierName
argument_list|,
name|tierParam1Name
argument_list|,
name|tierParam2Name
argument_list|,
literal|null
argument_list|)
return|;
block|}
comment|/*    * @param conf -    * @param parentTierName - the factory to create    * @param tierName - the factory to create    * @param tierParam1Name - the name of the first parameter    * @param tierParam2Name - the name of the second parameter    */
specifier|public
specifier|static
name|ConnectionUrlFactory
name|getUrlFactory
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|String
name|parentTierName
parameter_list|,
name|String
name|childTierName
parameter_list|,
name|String
name|tierParam1Name
parameter_list|,
name|String
name|tierParam2Name
parameter_list|,
name|String
name|commonParam
parameter_list|)
block|{
name|String
name|parentTierValue
init|=
name|parentTierName
operator|==
literal|null
condition|?
literal|null
else|:
name|conf
operator|.
name|get
argument_list|(
name|parentTierName
argument_list|)
decl_stmt|;
name|String
name|childTierValue
init|=
name|childTierName
operator|==
literal|null
condition|?
literal|null
else|:
name|conf
operator|.
name|get
argument_list|(
name|childTierName
argument_list|)
decl_stmt|;
name|String
name|tierValue
init|=
name|childTierValue
operator|!=
literal|null
operator|&&
operator|!
name|childTierValue
operator|.
name|isEmpty
argument_list|()
condition|?
name|childTierValue
else|:
operator|(
name|parentTierValue
operator|!=
literal|null
operator|&&
operator|!
name|parentTierValue
operator|.
name|isEmpty
argument_list|()
condition|?
name|parentTierValue
else|:
literal|null
operator|)
decl_stmt|;
if|if
condition|(
name|tierValue
operator|==
literal|null
condition|)
block|{
return|return
literal|null
return|;
block|}
name|ConnectionUrlFactory
name|conn
init|=
operator|(
name|ConnectionUrlFactory
operator|)
name|getObject
argument_list|(
name|conf
argument_list|,
name|tierValue
argument_list|)
decl_stmt|;
name|String
name|tierParamValue
init|=
name|tierParam1Name
operator|==
literal|null
condition|?
literal|null
else|:
name|conf
operator|.
name|get
argument_list|(
name|tierParam1Name
argument_list|)
decl_stmt|;
if|if
condition|(
operator|(
name|tierParamValue
operator|==
literal|null
operator|)
operator|||
name|tierParamValue
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|tierParamValue
operator|=
name|tierParam2Name
operator|==
literal|null
condition|?
literal|null
else|:
name|conf
operator|.
name|get
argument_list|(
name|tierParam2Name
argument_list|)
expr_stmt|;
block|}
name|String
name|commonParamValue
init|=
name|commonParam
operator|==
literal|null
condition|?
literal|null
else|:
name|conf
operator|.
name|get
argument_list|(
name|commonParam
argument_list|)
decl_stmt|;
name|conn
operator|.
name|init
argument_list|(
name|tierParamValue
argument_list|,
name|commonParamValue
argument_list|)
expr_stmt|;
return|return
name|conn
return|;
block|}
comment|// In the case of a select returns a list of lists, where each inner list represents a row
comment|// returned by the query.  In the case of an insert, returns null.
specifier|public
specifier|static
name|List
argument_list|<
name|List
argument_list|<
name|Object
argument_list|>
argument_list|>
name|runInsertSelect
parameter_list|(
name|HiveConf
name|conf
parameter_list|,
name|ConnectionUrlFactory
name|urlFactory
parameter_list|,
name|String
name|sql
parameter_list|,
name|List
argument_list|<
name|Object
argument_list|>
name|sqlParams
parameter_list|,
name|boolean
name|isWrite
parameter_list|,
name|int
name|numRetries
parameter_list|,
name|int
name|retryMaxInternalSec
parameter_list|,
name|boolean
name|insert
parameter_list|)
throws|throws
name|Exception
block|{
comment|// throwing an exception
name|int
name|waitMS
init|=
literal|300
decl_stmt|;
comment|// wait for at least 300 msec before next retry.
name|Random
name|rand
init|=
operator|new
name|Random
argument_list|()
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|numRetries
condition|;
operator|++
name|i
control|)
block|{
try|try
block|{
name|String
name|url
init|=
name|urlFactory
operator|.
name|getUrl
argument_list|(
name|isWrite
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"Attepting connection with URL "
operator|+
name|url
argument_list|)
expr_stmt|;
name|Connection
name|conn
init|=
name|getConnection
argument_list|(
name|conf
argument_list|,
name|url
argument_list|)
decl_stmt|;
name|PreparedStatement
name|pstmt
init|=
name|conn
operator|.
name|prepareStatement
argument_list|(
name|sql
argument_list|)
decl_stmt|;
name|int
name|pos
init|=
literal|1
decl_stmt|;
for|for
control|(
name|Object
name|param
range|:
name|sqlParams
control|)
block|{
if|if
condition|(
name|param
operator|instanceof
name|Integer
condition|)
block|{
name|pstmt
operator|.
name|setInt
argument_list|(
name|pos
operator|++
argument_list|,
operator|(
operator|(
name|Integer
operator|)
name|param
operator|)
operator|.
name|intValue
argument_list|()
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|pstmt
operator|.
name|setString
argument_list|(
name|pos
operator|++
argument_list|,
operator|(
name|String
operator|)
name|param
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
name|insert
condition|)
block|{
name|int
name|recordsUpdated
init|=
name|pstmt
operator|.
name|executeUpdate
argument_list|()
decl_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"rows inserted: "
operator|+
name|recordsUpdated
operator|+
literal|" sql: "
operator|+
name|sql
argument_list|)
expr_stmt|;
name|pstmt
operator|.
name|close
argument_list|()
expr_stmt|;
return|return
literal|null
return|;
block|}
else|else
block|{
name|ResultSet
name|result
init|=
name|pstmt
operator|.
name|executeQuery
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|List
argument_list|<
name|Object
argument_list|>
argument_list|>
name|results
init|=
operator|new
name|ArrayList
argument_list|<
name|List
argument_list|<
name|Object
argument_list|>
argument_list|>
argument_list|()
decl_stmt|;
name|int
name|numColumns
init|=
name|result
operator|.
name|getMetaData
argument_list|()
operator|.
name|getColumnCount
argument_list|()
decl_stmt|;
while|while
condition|(
name|result
operator|.
name|next
argument_list|()
condition|)
block|{
name|List
argument_list|<
name|Object
argument_list|>
name|row
init|=
operator|new
name|ArrayList
argument_list|<
name|Object
argument_list|>
argument_list|()
decl_stmt|;
name|results
operator|.
name|add
argument_list|(
name|row
argument_list|)
expr_stmt|;
for|for
control|(
name|int
name|index
init|=
literal|1
init|;
name|index
operator|<=
name|numColumns
condition|;
name|index
operator|++
control|)
block|{
name|row
operator|.
name|add
argument_list|(
name|result
operator|.
name|getObject
argument_list|(
name|index
argument_list|)
argument_list|)
expr_stmt|;
block|}
block|}
name|pstmt
operator|.
name|clearBatch
argument_list|()
expr_stmt|;
name|pstmt
operator|.
name|close
argument_list|()
expr_stmt|;
name|LOG
operator|.
name|info
argument_list|(
literal|"rows selected: "
operator|+
name|results
operator|.
name|size
argument_list|()
operator|+
literal|" sql: "
operator|+
name|sql
argument_list|)
expr_stmt|;
return|return
name|results
return|;
block|}
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
comment|// We should catch a better exception than Exception, but since
comment|// ConnectionUrlFactory.getUrl() defines throws Exception, it's hard
comment|// for us to figure out the complete set it can throw. We follow
comment|// ConnectionUrlFactory.getUrl()'s definition to catch Exception.
comment|// It shouldn't be a big problem as after numRetries, we anyway exit.
name|LOG
operator|.
name|info
argument_list|(
literal|"Exception "
operator|+
name|e
operator|+
literal|". Will retry "
operator|+
operator|(
name|numRetries
operator|-
name|i
operator|)
operator|+
literal|" times."
argument_list|)
expr_stmt|;
comment|// Introducing a random factor to the wait time before another retry.
comment|// The wait time is dependent on # of failures and a random factor.
comment|// At the first time of getting a SQLException, the wait time
comment|// is a random number between [0,300] msec. If the first retry
comment|// still fails, we will wait 300 msec grace period before the 2nd retry.
comment|// Also at the second retry, the waiting window is expanded to 600 msec
comment|// alleviating the request rate from the server. Similarly the 3rd retry
comment|// will wait 600 msec grace period before retry and the waiting window
comment|// is
comment|// expanded to 1200 msec.
name|waitMS
operator|+=
name|waitMS
expr_stmt|;
if|if
condition|(
name|waitMS
operator|>
name|retryMaxInternalSec
operator|*
literal|1000
condition|)
block|{
name|waitMS
operator|=
name|retryMaxInternalSec
operator|*
literal|1000
expr_stmt|;
block|}
name|double
name|waitTime
init|=
name|waitMS
operator|+
name|waitMS
operator|*
name|rand
operator|.
name|nextDouble
argument_list|()
decl_stmt|;
name|Thread
operator|.
name|sleep
argument_list|(
operator|(
name|long
operator|)
name|waitTime
argument_list|)
expr_stmt|;
if|if
condition|(
name|i
operator|+
literal|1
operator|==
name|numRetries
condition|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"Still got Exception after "
operator|+
name|numRetries
operator|+
literal|"  retries."
argument_list|,
name|e
argument_list|)
expr_stmt|;
throw|throw
name|e
throw|;
block|}
block|}
block|}
return|return
literal|null
return|;
block|}
comment|/**    * Populates inputToCS with a mapping from the input paths to their respective ContentSummary    * objects.  If an input is in a subdirectory of another's location, or in the same location,    * the input is not included in the total size of the inputs.  If it is not already present in    * the mapping, it will not be added.    *    * @param inputs    * @param inputToCS    * @param conf    * @throws IOException    * @throws Exception    */
specifier|public
specifier|static
name|long
name|getInputSize
parameter_list|(
name|Set
argument_list|<
name|ReadEntity
argument_list|>
name|inputs
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|ContentSummary
argument_list|>
name|inputToCS
parameter_list|,
name|HiveConf
name|conf
parameter_list|)
throws|throws
name|IOException
throws|,
name|Exception
block|{
name|URI
name|defaultPathUri
init|=
operator|new
name|URI
argument_list|(
name|conf
operator|.
name|getVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|METASTOREWAREHOUSE
argument_list|)
argument_list|)
decl_stmt|;
name|String
name|defaultPath
init|=
name|defaultPathUri
operator|.
name|getPath
argument_list|()
decl_stmt|;
name|String
name|defaultPrefix
init|=
name|defaultPathUri
operator|.
name|toString
argument_list|()
operator|.
name|substring
argument_list|(
literal|0
argument_list|,
name|defaultPathUri
operator|.
name|toString
argument_list|()
operator|.
name|lastIndexOf
argument_list|(
name|defaultPath
argument_list|)
argument_list|)
decl_stmt|;
comment|// A mapping from the location as a String, formatted as a String for sorting, to the original
comment|// path of the object
name|Map
argument_list|<
name|String
argument_list|,
name|Path
argument_list|>
name|locationToPath
init|=
operator|new
name|HashMap
argument_list|<
name|String
argument_list|,
name|Path
argument_list|>
argument_list|()
decl_stmt|;
for|for
control|(
name|ReadEntity
name|re
range|:
name|inputs
control|)
block|{
name|Path
name|p
init|=
literal|null
decl_stmt|;
switch|switch
condition|(
name|re
operator|.
name|getType
argument_list|()
condition|)
block|{
case|case
name|TABLE
case|:
name|Table
name|table
init|=
name|re
operator|.
name|getTable
argument_list|()
decl_stmt|;
if|if
condition|(
name|table
operator|.
name|isPartitioned
argument_list|()
condition|)
block|{
comment|// If the input is a partitioned table, do not include its content summary, as data will
comment|// never be read from a partitioned table, only its partitions, so it must be a metadata
comment|// change to the table.
continue|continue;
block|}
if|if
condition|(
name|table
operator|.
name|isView
argument_list|()
condition|)
block|{
comment|// If the input is a view, it does not have a content summary as it is only a logical
comment|// construct.
continue|continue;
block|}
name|p
operator|=
name|table
operator|.
name|getPath
argument_list|()
expr_stmt|;
break|break;
case|case
name|PARTITION
case|:
name|Partition
name|partition
init|=
name|re
operator|.
name|getPartition
argument_list|()
decl_stmt|;
if|if
condition|(
name|partition
operator|.
name|getTable
argument_list|()
operator|.
name|isView
argument_list|()
condition|)
block|{
comment|// If the input is a partition of a view, it does not have a content summary as it is
comment|// only a logical construct.
continue|continue;
block|}
name|p
operator|=
name|partition
operator|.
name|getPartitionPath
argument_list|()
expr_stmt|;
break|break;
default|default:
continue|continue;
block|}
name|String
name|location
init|=
name|re
operator|.
name|getLocation
argument_list|()
operator|.
name|toString
argument_list|()
decl_stmt|;
comment|// If the location is something like /user/facebook/warehouse/ we want it to start with
comment|// hdfs://... to make ensure using prefixes we can identify subdirectories
if|if
condition|(
name|location
operator|.
name|equals
argument_list|(
name|defaultPath
argument_list|)
operator|||
name|location
operator|.
name|startsWith
argument_list|(
name|defaultPath
operator|.
name|endsWith
argument_list|(
literal|"/"
argument_list|)
condition|?
name|defaultPath
else|:
name|defaultPath
operator|+
literal|"/"
argument_list|)
condition|)
block|{
name|location
operator|=
name|defaultPrefix
operator|+
name|location
expr_stmt|;
block|}
comment|// If the location does not end with / add it, this ensures /a/b/cd is not considered a
comment|// subdirectory of /a/b/c
if|if
condition|(
operator|!
name|location
operator|.
name|endsWith
argument_list|(
literal|"/"
argument_list|)
condition|)
block|{
name|location
operator|+=
literal|"/"
expr_stmt|;
block|}
name|locationToPath
operator|.
name|put
argument_list|(
name|location
argument_list|,
name|p
argument_list|)
expr_stmt|;
block|}
name|String
index|[]
name|locations
init|=
operator|new
name|String
index|[
name|locationToPath
operator|.
name|size
argument_list|()
index|]
decl_stmt|;
name|locations
operator|=
name|locationToPath
operator|.
name|keySet
argument_list|()
operator|.
name|toArray
argument_list|(
name|locations
argument_list|)
expr_stmt|;
name|Arrays
operator|.
name|sort
argument_list|(
name|locations
argument_list|)
expr_stmt|;
name|String
name|lastLocation
init|=
literal|null
decl_stmt|;
name|long
name|totalInputSize
init|=
literal|0
decl_stmt|;
for|for
control|(
name|String
name|formattedLocation
range|:
name|locations
control|)
block|{
comment|// Since the locations have been sorted, if this location is a subdirectory of another, that
comment|// directory must either be immediately before this location, or every location in between is
comment|// also a subdirectory
if|if
condition|(
name|lastLocation
operator|!=
literal|null
operator|&&
name|formattedLocation
operator|.
name|startsWith
argument_list|(
name|lastLocation
argument_list|)
condition|)
block|{
continue|continue;
block|}
name|Path
name|p
init|=
name|locationToPath
operator|.
name|get
argument_list|(
name|formattedLocation
argument_list|)
decl_stmt|;
name|lastLocation
operator|=
name|formattedLocation
expr_stmt|;
name|String
name|pathStr
init|=
name|p
operator|.
name|toString
argument_list|()
decl_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Finding from cache Content Summary for "
operator|+
name|pathStr
argument_list|)
expr_stmt|;
block|}
name|ContentSummary
name|cs
init|=
operator|(
name|inputToCS
operator|==
literal|null
operator|)
condition|?
literal|null
else|:
name|inputToCS
operator|.
name|get
argument_list|(
name|pathStr
argument_list|)
decl_stmt|;
if|if
condition|(
name|cs
operator|==
literal|null
condition|)
block|{
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Fetch Content Summary for "
operator|+
name|pathStr
argument_list|)
expr_stmt|;
block|}
name|FileSystem
name|fs
init|=
name|p
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|cs
operator|=
name|fs
operator|.
name|getContentSummary
argument_list|(
name|p
argument_list|)
expr_stmt|;
name|inputToCS
operator|.
name|put
argument_list|(
name|pathStr
argument_list|,
name|cs
argument_list|)
expr_stmt|;
block|}
name|totalInputSize
operator|+=
name|cs
operator|.
name|getLength
argument_list|()
expr_stmt|;
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Length for file: "
operator|+
name|pathStr
operator|+
literal|" = "
operator|+
name|cs
operator|.
name|getLength
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|totalInputSize
return|;
block|}
comment|/**    * Goes through the list of tasks, and populates a map from each path used    * by a mapRedTask to the highest percentage to which it is sampled, or 100    * if it is ever not sampled.    *    * Also, if a task is not a map reduce task or has a null or empty    * NameToSplitSample map, it adds all of its inputs to a    * set so they can be treated as unsampled.    *    * Calls itself recursively on each task's list of dependent tasks    *    * @return whether or not there is any sampling performed in the query    */
specifier|static
specifier|public
name|boolean
name|checkForSamplingTasks
parameter_list|(
name|List
argument_list|<
name|Task
argument_list|<
name|?
extends|extends
name|Serializable
argument_list|>
argument_list|>
name|tasks
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|Double
argument_list|>
name|topPercentages
parameter_list|,
name|Set
argument_list|<
name|ReadEntity
argument_list|>
name|nonSampledInputs
parameter_list|)
block|{
name|boolean
name|isThereSampling
init|=
literal|false
decl_stmt|;
for|for
control|(
name|Task
argument_list|<
name|?
extends|extends
name|Serializable
argument_list|>
name|task
range|:
name|tasks
control|)
block|{
name|MapredWork
name|work
decl_stmt|;
comment|// Only look for sampled inputs in MapRedTasks with non-null, non-empty
comment|// NameToSplitSample maps
if|if
condition|(
name|task
operator|.
name|getWork
argument_list|()
operator|instanceof
name|MapredWork
operator|&&
operator|(
name|work
operator|=
operator|(
name|MapredWork
operator|)
name|task
operator|.
name|getWork
argument_list|()
operator|)
operator|.
name|getNameToSplitSample
argument_list|()
operator|!=
literal|null
operator|&&
operator|!
name|work
operator|.
name|getNameToSplitSample
argument_list|()
operator|.
name|isEmpty
argument_list|()
condition|)
block|{
name|isThereSampling
operator|=
literal|true
expr_stmt|;
comment|// If the task is a map reduce task, go through each of the paths
comment|// used by its work, if it is sampled check if it is the highest
comment|// sampling percentage yet seen for that path.  If it is not
comment|// sampled, set the highest percentage to 100.
for|for
control|(
name|Map
operator|.
name|Entry
argument_list|<
name|String
argument_list|,
name|ArrayList
argument_list|<
name|String
argument_list|>
argument_list|>
name|entry
range|:
name|work
operator|.
name|getPathToAliases
argument_list|()
operator|.
name|entrySet
argument_list|()
control|)
block|{
name|double
name|percentage
init|=
literal|0
decl_stmt|;
for|for
control|(
name|String
name|alias
range|:
name|entry
operator|.
name|getValue
argument_list|()
control|)
block|{
if|if
condition|(
name|work
operator|.
name|getNameToSplitSample
argument_list|()
operator|.
name|containsKey
argument_list|(
name|alias
argument_list|)
condition|)
block|{
if|if
condition|(
name|work
operator|.
name|getNameToSplitSample
argument_list|()
operator|.
name|get
argument_list|(
name|alias
argument_list|)
operator|.
name|getPercent
argument_list|()
operator|>
name|percentage
condition|)
block|{
name|percentage
operator|=
name|work
operator|.
name|getNameToSplitSample
argument_list|()
operator|.
name|get
argument_list|(
name|alias
argument_list|)
operator|.
name|getPercent
argument_list|()
expr_stmt|;
block|}
block|}
else|else
block|{
name|percentage
operator|=
literal|100
expr_stmt|;
break|break;
block|}
block|}
name|String
name|path
init|=
name|entry
operator|.
name|getKey
argument_list|()
decl_stmt|;
if|if
condition|(
operator|!
name|topPercentages
operator|.
name|containsKey
argument_list|(
name|path
argument_list|)
operator|||
name|percentage
operator|>
name|topPercentages
operator|.
name|get
argument_list|(
name|path
argument_list|)
condition|)
block|{
name|topPercentages
operator|.
name|put
argument_list|(
name|path
argument_list|,
name|percentage
argument_list|)
expr_stmt|;
block|}
block|}
block|}
elseif|else
if|if
condition|(
name|task
operator|.
name|getQueryPlan
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|nonSampledInputs
operator|.
name|addAll
argument_list|(
name|task
operator|.
name|getQueryPlan
argument_list|()
operator|.
name|getInputs
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|task
operator|.
name|getDependentTasks
argument_list|()
operator|!=
literal|null
condition|)
block|{
name|isThereSampling
operator||=
name|checkForSamplingTasks
argument_list|(
name|task
operator|.
name|getDependentTasks
argument_list|()
argument_list|,
name|topPercentages
argument_list|,
name|nonSampledInputs
argument_list|)
expr_stmt|;
block|}
block|}
return|return
name|isThereSampling
return|;
block|}
comment|/**    * Helper class used to pass from getObjectSize back to the caller.    * This contains the total size of the objects passed in, as well as    * the type, size and number of files for each object. For eg, if a query    * references 2 partitions T1@p1 and T1@p2 of size 10 and 20, and 2 and 5    * files respectively, the totalSize will be 30, and the object map will be    * like:    * T1@p1 -><MANAGED_TABLE, 10, 2)    * T1@p2 -><MANAGED_TABLE, 20, 5)    * Currently, this is logged in the job_stats_log table, and used for    * downstream analysis.    */
specifier|public
specifier|static
class|class
name|ObjectSize
block|{
name|long
name|totalSize
decl_stmt|;
name|Map
argument_list|<
name|String
argument_list|,
name|Triple
argument_list|<
name|String
argument_list|,
name|String
argument_list|,
name|String
argument_list|>
argument_list|>
name|objectTypeLengths
decl_stmt|;
name|ObjectSize
parameter_list|()
block|{     }
name|ObjectSize
parameter_list|(
name|long
name|totalSize
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|Triple
argument_list|<
name|String
argument_list|,
name|String
argument_list|,
name|String
argument_list|>
argument_list|>
name|objectTypeLengths
parameter_list|)
block|{
name|this
operator|.
name|totalSize
operator|=
name|totalSize
expr_stmt|;
name|this
operator|.
name|objectTypeLengths
operator|=
name|objectTypeLengths
expr_stmt|;
block|}
name|long
name|getTotalSize
parameter_list|()
block|{
return|return
name|totalSize
return|;
block|}
name|void
name|setTotalSize
parameter_list|(
name|long
name|totalSize
parameter_list|)
block|{
name|this
operator|.
name|totalSize
operator|=
name|totalSize
expr_stmt|;
block|}
name|Map
argument_list|<
name|String
argument_list|,
name|Triple
argument_list|<
name|String
argument_list|,
name|String
argument_list|,
name|String
argument_list|>
argument_list|>
name|getObjectTypeLengths
parameter_list|()
block|{
return|return
name|objectTypeLengths
return|;
block|}
name|void
name|setObjectTypeLengths
parameter_list|(
name|Map
argument_list|<
name|String
argument_list|,
name|Triple
argument_list|<
name|String
argument_list|,
name|String
argument_list|,
name|String
argument_list|>
argument_list|>
name|objectTypeLengths
parameter_list|)
block|{
name|this
operator|.
name|objectTypeLengths
operator|=
name|objectTypeLengths
expr_stmt|;
block|}
block|}
specifier|static
specifier|public
name|HookUtils
operator|.
name|ObjectSize
name|getObjectSize
parameter_list|(
name|HiveConf
name|conf
parameter_list|,
name|Set
argument_list|<
name|Entity
argument_list|>
name|objects
parameter_list|,
name|boolean
name|loadObjects
parameter_list|)
throws|throws
name|Exception
block|{
comment|// The objects may need to be loaded again since StatsTask is executed after
comment|// the move task, and the object in the write entity may not have the size
name|long
name|totalSize
init|=
literal|0
decl_stmt|;
name|Map
argument_list|<
name|String
argument_list|,
name|Triple
argument_list|<
name|String
argument_list|,
name|String
argument_list|,
name|String
argument_list|>
argument_list|>
name|objectLengths
init|=
operator|new
name|HashMap
argument_list|<
name|String
argument_list|,
name|Triple
argument_list|<
name|String
argument_list|,
name|String
argument_list|,
name|String
argument_list|>
argument_list|>
argument_list|()
decl_stmt|;
name|Hive
name|db
init|=
literal|null
decl_stmt|;
if|if
condition|(
name|loadObjects
condition|)
block|{
name|db
operator|=
name|Hive
operator|.
name|get
argument_list|()
expr_stmt|;
block|}
for|for
control|(
name|Entity
name|object
range|:
name|objects
control|)
block|{
comment|// We are computing sizes only for tables and partitions
name|Entity
operator|.
name|Type
name|objectType
init|=
name|object
operator|.
name|getTyp
argument_list|()
decl_stmt|;
name|Table
name|table
init|=
literal|null
decl_stmt|;
name|String
name|size
init|=
literal|null
decl_stmt|;
name|String
name|numFiles
init|=
literal|null
decl_stmt|;
name|Path
name|path
init|=
literal|null
decl_stmt|;
switch|switch
condition|(
name|objectType
condition|)
block|{
case|case
name|TABLE
case|:
name|table
operator|=
name|object
operator|.
name|getTable
argument_list|()
expr_stmt|;
if|if
condition|(
name|table
operator|.
name|isPartitioned
argument_list|()
operator|&&
operator|!
name|table
operator|.
name|isView
argument_list|()
condition|)
block|{
comment|// If the input is a partitioned table, do not include its content summary, as data will
comment|// never be read from a partitioned table, only its partitions, so it must be a metadata
comment|// change to the table.
comment|//
comment|// However, if the table is a view, a view's partitions are not included in the inputs,
comment|// so do not skip it so that we have some record of it.
continue|continue;
block|}
if|if
condition|(
name|loadObjects
condition|)
block|{
name|table
operator|=
name|db
operator|.
name|getTable
argument_list|(
name|table
operator|.
name|getTableName
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|table
operator|.
name|isView
argument_list|()
condition|)
block|{
comment|// Views are logical, so they have no size or files
name|path
operator|=
literal|null
expr_stmt|;
name|size
operator|=
literal|"0"
expr_stmt|;
name|numFiles
operator|=
literal|"0"
expr_stmt|;
block|}
else|else
block|{
name|path
operator|=
name|table
operator|.
name|getPath
argument_list|()
expr_stmt|;
name|size
operator|=
name|table
operator|.
name|getProperty
argument_list|(
literal|"totalSize"
argument_list|)
expr_stmt|;
name|numFiles
operator|=
name|table
operator|.
name|getProperty
argument_list|(
literal|"numFiles"
argument_list|)
expr_stmt|;
block|}
break|break;
case|case
name|PARTITION
case|:
name|Partition
name|partition
init|=
name|object
operator|.
name|getPartition
argument_list|()
decl_stmt|;
if|if
condition|(
name|loadObjects
condition|)
block|{
name|partition
operator|=
name|db
operator|.
name|getPartition
argument_list|(
name|partition
operator|.
name|getTable
argument_list|()
argument_list|,
name|partition
operator|.
name|getSpec
argument_list|()
argument_list|,
literal|false
argument_list|)
expr_stmt|;
block|}
name|table
operator|=
name|partition
operator|.
name|getTable
argument_list|()
expr_stmt|;
if|if
condition|(
name|table
operator|.
name|isView
argument_list|()
condition|)
block|{
comment|// Views are logical, so they have no size or files
comment|// Currently view partitions are not included in the inputs, but this is included so
comment|// that if that changes in open source, it will not cause an NPE.  It should not cause
comment|// any double counting as the size of the view and its partitions are both 0.
name|path
operator|=
literal|null
expr_stmt|;
name|size
operator|=
literal|"0"
expr_stmt|;
name|numFiles
operator|=
literal|"0"
expr_stmt|;
block|}
else|else
block|{
name|path
operator|=
name|partition
operator|.
name|getPartitionPath
argument_list|()
expr_stmt|;
name|size
operator|=
name|partition
operator|.
name|getParameters
argument_list|()
operator|.
name|get
argument_list|(
literal|"totalSize"
argument_list|)
expr_stmt|;
name|numFiles
operator|=
name|partition
operator|.
name|getParameters
argument_list|()
operator|.
name|get
argument_list|(
literal|"numFiles"
argument_list|)
expr_stmt|;
block|}
break|break;
default|default:
comment|// nothing to do
break|break;
block|}
comment|// Counting needed
if|if
condition|(
name|table
operator|!=
literal|null
condition|)
block|{
if|if
condition|(
name|size
operator|==
literal|null
condition|)
block|{
comment|// If the size is not present in the metastore (old
comment|// legacy tables), get it from hdfs
name|FileSystem
name|fs
init|=
name|path
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|size
operator|=
name|String
operator|.
name|valueOf
argument_list|(
name|fs
operator|.
name|getContentSummary
argument_list|(
name|path
argument_list|)
operator|.
name|getLength
argument_list|()
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|numFiles
operator|==
literal|null
condition|)
block|{
name|numFiles
operator|=
name|String
operator|.
name|valueOf
argument_list|(
literal|0
argument_list|)
expr_stmt|;
block|}
name|Triple
argument_list|<
name|String
argument_list|,
name|String
argument_list|,
name|String
argument_list|>
name|triple
init|=
operator|new
name|Triple
argument_list|<
name|String
argument_list|,
name|String
argument_list|,
name|String
argument_list|>
argument_list|(
name|table
operator|.
name|getTableType
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|,
name|size
argument_list|,
name|numFiles
argument_list|)
decl_stmt|;
name|objectLengths
operator|.
name|put
argument_list|(
name|object
operator|.
name|getName
argument_list|()
argument_list|,
name|triple
argument_list|)
expr_stmt|;
comment|// If the input/output is a external table or a view, dont add it to
comment|// the total size. The managed tables/partitions for those locations
comment|// should also be part of the object list passed in. It is true for
comment|// inputs, whereas outputs should not external tables or views in a
comment|// query. So, the totalSize may be less than the sum of all individual
comment|// sizes
if|if
condition|(
operator|(
name|table
operator|.
name|getTableType
argument_list|()
operator|!=
name|TableType
operator|.
name|EXTERNAL_TABLE
operator|)
operator|&&
operator|(
name|table
operator|.
name|getTableType
argument_list|()
operator|!=
name|TableType
operator|.
name|VIRTUAL_VIEW
operator|)
condition|)
block|{
name|totalSize
operator|+=
name|Long
operator|.
name|valueOf
argument_list|(
name|size
argument_list|)
expr_stmt|;
block|}
block|}
block|}
name|ObjectSize
name|objectSize
init|=
operator|new
name|ObjectSize
argument_list|(
name|totalSize
argument_list|,
name|objectLengths
argument_list|)
decl_stmt|;
return|return
name|objectSize
return|;
block|}
comment|/**    * A helper class used to pass info from getInputInfo back to the caller.    */
specifier|public
specifier|static
class|class
name|InputInfo
block|{
name|long
name|size
decl_stmt|;
name|long
name|fileCount
decl_stmt|;
name|long
name|directoryCount
decl_stmt|;
name|double
name|estimatedNumSplits
decl_stmt|;
name|InputInfo
parameter_list|(
name|long
name|size
parameter_list|,
name|long
name|fileCount
parameter_list|,
name|long
name|directoryCount
parameter_list|,
name|double
name|estimatedNumSplits
parameter_list|)
block|{
name|this
operator|.
name|size
operator|=
name|size
expr_stmt|;
name|this
operator|.
name|fileCount
operator|=
name|fileCount
expr_stmt|;
name|this
operator|.
name|directoryCount
operator|=
name|directoryCount
expr_stmt|;
name|this
operator|.
name|estimatedNumSplits
operator|=
name|estimatedNumSplits
expr_stmt|;
block|}
name|long
name|getSize
parameter_list|()
block|{
return|return
name|size
return|;
block|}
name|long
name|getFileCount
parameter_list|()
block|{
return|return
name|fileCount
return|;
block|}
name|long
name|getDirectoryCount
parameter_list|()
block|{
return|return
name|directoryCount
return|;
block|}
name|double
name|getEstimatedNumSplits
parameter_list|()
block|{
return|return
name|estimatedNumSplits
return|;
block|}
block|}
comment|/**    * Returns the sizes of the inputs while taking sampling into account.    *    * @param inputs - entities used for the query input    * @param inputToCS - already known mappings from paths to content summaries.    * If a path is not in this mapping, it will be looked up    * @param conf - hadoop conf for constructing filesystem    * @param isThereSampling - whether the query includes sampling    * @param pathToTopPercentage - a mapping from the path to the highest    * sampled percentage. If not in the map, defaults to 100%    * @param nonSampledInputs - entities that are not sampled    * @param maxSplits - if the number of splits exceeds this number as the    * splits are incrementally summed, return early    * @param maxSize - if the size exceeds this number as the sizes are being    * incrementally summed, return early    * @return an InputInfo object about the net input    * @throws IOException    */
specifier|static
specifier|public
name|InputInfo
name|getInputInfo
parameter_list|(
name|Collection
argument_list|<
name|ReadEntity
argument_list|>
name|inputs
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|ContentSummary
argument_list|>
name|inputToCS
parameter_list|,
name|Configuration
name|conf
parameter_list|,
name|boolean
name|isThereSampling
parameter_list|,
name|Map
argument_list|<
name|String
argument_list|,
name|Double
argument_list|>
name|pathToTopPercentage
parameter_list|,
name|Set
argument_list|<
name|ReadEntity
argument_list|>
name|nonSampledInputs
parameter_list|,
name|long
name|maxSplits
parameter_list|,
name|long
name|maxSize
parameter_list|)
throws|throws
name|IOException
block|{
name|double
name|estimatedNumSplits
init|=
literal|0
decl_stmt|;
name|long
name|size
init|=
literal|0
decl_stmt|;
name|long
name|fileCount
init|=
literal|0
decl_stmt|;
name|long
name|directoryCount
init|=
literal|0
decl_stmt|;
comment|// Go over the input paths and calculate size
for|for
control|(
name|ReadEntity
name|re
range|:
name|inputs
control|)
block|{
name|Path
name|p
init|=
literal|null
decl_stmt|;
switch|switch
condition|(
name|re
operator|.
name|getType
argument_list|()
condition|)
block|{
case|case
name|TABLE
case|:
name|p
operator|=
name|re
operator|.
name|getTable
argument_list|()
operator|.
name|getPath
argument_list|()
expr_stmt|;
break|break;
case|case
name|PARTITION
case|:
name|p
operator|=
name|re
operator|.
name|getPartition
argument_list|()
operator|.
name|getPartitionPath
argument_list|()
expr_stmt|;
break|break;
default|default:
break|break;
block|}
if|if
condition|(
name|p
operator|!=
literal|null
condition|)
block|{
name|String
name|pathStr
init|=
name|p
operator|.
name|toString
argument_list|()
decl_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Finding from cache Content Summary for "
operator|+
name|pathStr
argument_list|)
expr_stmt|;
name|ContentSummary
name|cs
init|=
operator|(
name|inputToCS
operator|==
literal|null
operator|)
condition|?
literal|null
else|:
name|inputToCS
operator|.
name|get
argument_list|(
name|pathStr
argument_list|)
decl_stmt|;
if|if
condition|(
name|cs
operator|==
literal|null
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Fetch Content Summary for "
operator|+
name|pathStr
argument_list|)
expr_stmt|;
name|FileSystem
name|fs
init|=
name|p
operator|.
name|getFileSystem
argument_list|(
name|conf
argument_list|)
decl_stmt|;
name|cs
operator|=
name|fs
operator|.
name|getContentSummary
argument_list|(
name|p
argument_list|)
expr_stmt|;
name|inputToCS
operator|.
name|put
argument_list|(
name|pathStr
argument_list|,
name|cs
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
name|isThereSampling
condition|)
block|{
comment|// If the input is used in a map reduce task get the highest
comment|// percentage to which it is sampled, otherwise, set the
comment|// sampling percentage to 100
name|double
name|samplePercentage
init|=
literal|100
decl_stmt|;
if|if
condition|(
name|pathToTopPercentage
operator|.
name|containsKey
argument_list|(
name|pathStr
argument_list|)
operator|&&
operator|!
name|nonSampledInputs
operator|.
name|contains
argument_list|(
name|re
argument_list|)
condition|)
block|{
name|samplePercentage
operator|=
name|pathToTopPercentage
operator|.
name|get
argument_list|(
name|pathStr
argument_list|)
expr_stmt|;
block|}
name|size
operator|+=
call|(
name|long
call|)
argument_list|(
name|cs
operator|.
name|getLength
argument_list|()
operator|*
name|samplePercentage
operator|/
literal|100D
argument_list|)
expr_stmt|;
name|estimatedNumSplits
operator|+=
name|samplePercentage
operator|/
literal|100
expr_stmt|;
if|if
condition|(
name|estimatedNumSplits
operator|>
name|maxSplits
condition|)
block|{
break|break;
block|}
block|}
else|else
block|{
name|size
operator|+=
name|cs
operator|.
name|getLength
argument_list|()
expr_stmt|;
name|fileCount
operator|+=
name|cs
operator|.
name|getFileCount
argument_list|()
expr_stmt|;
name|directoryCount
operator|+=
name|cs
operator|.
name|getDirectoryCount
argument_list|()
expr_stmt|;
block|}
if|if
condition|(
name|LOG
operator|.
name|isDebugEnabled
argument_list|()
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Length for file: "
operator|+
name|p
operator|.
name|toString
argument_list|()
operator|+
literal|" = "
operator|+
name|cs
operator|.
name|getLength
argument_list|()
argument_list|)
expr_stmt|;
block|}
block|}
if|if
condition|(
name|size
operator|>
name|maxSize
condition|)
block|{
break|break;
block|}
block|}
return|return
operator|new
name|InputInfo
argument_list|(
name|size
argument_list|,
name|fileCount
argument_list|,
name|directoryCount
argument_list|,
name|estimatedNumSplits
argument_list|)
return|;
block|}
comment|//Returns true approximately<percentageObj*100>% of the time
specifier|public
specifier|static
name|boolean
name|rollDice
parameter_list|(
name|float
name|percentage
parameter_list|)
throws|throws
name|Exception
block|{
name|Random
name|randGen
init|=
operator|new
name|Random
argument_list|()
decl_stmt|;
name|float
name|randVal
init|=
name|randGen
operator|.
name|nextFloat
argument_list|()
decl_stmt|;
if|if
condition|(
name|percentage
argument_list|<
literal|0
operator|||
name|percentage
argument_list|>
literal|1
condition|)
block|{
throw|throw
operator|new
name|Exception
argument_list|(
literal|"Percentages must be>=0% and<= 100%. Got "
operator|+
name|percentage
argument_list|)
throw|;
block|}
if|if
condition|(
name|randVal
operator|<
name|percentage
condition|)
block|{
return|return
literal|true
return|;
block|}
return|return
literal|false
return|;
block|}
specifier|public
specifier|static
parameter_list|<
name|T
parameter_list|>
name|T
name|getObject
parameter_list|(
name|Configuration
name|conf
parameter_list|,
name|String
name|className
parameter_list|)
block|{
if|if
condition|(
operator|(
name|className
operator|==
literal|null
operator|)
operator|||
operator|(
name|className
operator|.
name|isEmpty
argument_list|()
operator|)
condition|)
block|{
return|return
literal|null
return|;
block|}
name|T
name|clazz
init|=
literal|null
decl_stmt|;
try|try
block|{
name|clazz
operator|=
operator|(
name|T
operator|)
name|ReflectionUtils
operator|.
name|newInstance
argument_list|(
name|conf
operator|.
name|getClassByName
argument_list|(
name|className
argument_list|)
argument_list|,
name|conf
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|ClassNotFoundException
name|e
parameter_list|)
block|{
return|return
literal|null
return|;
block|}
return|return
name|clazz
return|;
block|}
block|}
end_class

end_unit

