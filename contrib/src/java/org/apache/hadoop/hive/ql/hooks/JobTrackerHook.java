begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_comment
comment|/**  * Licensed to the Apache Software Foundation (ASF) under one  * or more contributor license agreements.  See the NOTICE file  * distributed with this work for additional information  * regarding copyright ownership.  The ASF licenses this file  * to you under the Apache License, Version 2.0 (the  * "License"); you may not use this file except in compliance  * with the License.  You may obtain a copy of the License at  *  *     http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an "AS IS" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */
end_comment

begin_package
package|package
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|hooks
package|;
end_package

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|Serializable
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashMap
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashSet
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Iterator
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|List
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
operator|.
name|Entry
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Random
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Set
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|Log
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|commons
operator|.
name|logging
operator|.
name|LogFactory
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|conf
operator|.
name|Configuration
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|fs
operator|.
name|ContentSummary
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|conf
operator|.
name|HiveConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|QueryPlan
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|DDLTask
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|exec
operator|.
name|Task
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|hooks
operator|.
name|HookUtils
operator|.
name|InputInfo
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|hooks
operator|.
name|conf
operator|.
name|FBHiveConf
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|hive
operator|.
name|ql
operator|.
name|session
operator|.
name|SessionState
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|hadoop
operator|.
name|security
operator|.
name|UserGroupInformation
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|thrift
operator|.
name|TException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|json
operator|.
name|JSONArray
import|;
end_import

begin_import
import|import
name|org
operator|.
name|json
operator|.
name|JSONException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|json
operator|.
name|JSONObject
import|;
end_import

begin_comment
comment|/**  * Implementation of a pre execute hook that decides what  * cluster to send a given query to based on the size of  * query inputs  *  * TODO: this needs to be optimized once HIVE-1507 is in  * place to reuse the patch->summary cache maintained in hive  *  * TODO: this encodes hadoop cluster info in code. very  * undesirable. Need to figure this out better (SMC?)  */
end_comment

begin_class
specifier|public
class|class
name|JobTrackerHook
block|{
specifier|static
specifier|final
specifier|private
name|Log
name|LOG
init|=
name|LogFactory
operator|.
name|getLog
argument_list|(
name|JobTrackerHook
operator|.
name|class
operator|.
name|getName
argument_list|()
argument_list|)
decl_stmt|;
comment|// store the prior location of the hadoop executable. switching this doesn't
comment|// matter unless we are using the 'submit via child' feature
specifier|private
specifier|static
name|String
name|preHadoopBin
init|=
literal|null
decl_stmt|;
specifier|private
specifier|static
name|String
name|preJobTracker
init|=
literal|null
decl_stmt|;
specifier|private
specifier|static
name|Map
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|savedValues
init|=
literal|null
decl_stmt|;
specifier|public
specifier|static
class|class
name|PreExec
implements|implements
name|ExecuteWithHookContext
block|{
specifier|private
specifier|final
name|String
name|dislike
init|=
literal|"Not choosing Bronze/Corona because "
decl_stmt|;
specifier|static
specifier|final
specifier|private
name|String
name|POOLS
init|=
literal|"pools"
decl_stmt|;
comment|/**      * If the job is on an SLA pool, do not redirect this job.      *      * @return True if the pool matches an SLA pool, false otherwise      */
specifier|private
name|boolean
name|isOnSlaPool
parameter_list|(
name|HiveConf
name|conf
parameter_list|)
block|{
name|String
name|pool
init|=
name|conf
operator|.
name|get
argument_list|(
literal|"mapred.fairscheduler.pool"
argument_list|)
decl_stmt|;
comment|// Nothing to be done if pool is not specified
if|if
condition|(
operator|(
name|pool
operator|==
literal|null
operator|)
operator|||
operator|(
name|pool
operator|.
name|isEmpty
argument_list|()
operator|)
condition|)
block|{
return|return
literal|false
return|;
block|}
comment|// Make sure that SLA jobs are not redirected
name|String
index|[]
name|slaPoolArray
init|=
name|conf
operator|.
name|getStrings
argument_list|(
literal|"mapred.jobtracker.hook.sla.pools"
argument_list|)
decl_stmt|;
if|if
condition|(
operator|(
name|slaPoolArray
operator|==
literal|null
operator|)
operator|||
operator|(
name|slaPoolArray
operator|.
name|length
operator|==
literal|0
operator|)
condition|)
block|{
name|slaPoolArray
operator|=
operator|new
name|String
index|[]
block|{
literal|"rootsla"
block|,
literal|"incrementalscraping"
block|}
expr_stmt|;
block|}
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|slaPoolArray
operator|.
name|length
condition|;
operator|++
name|i
control|)
block|{
if|if
condition|(
name|slaPoolArray
index|[
name|i
index|]
operator|.
name|equals
argument_list|(
name|pool
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|debug
argument_list|(
literal|"Pool "
operator|+
name|pool
operator|+
literal|" is on an sla pool"
argument_list|)
expr_stmt|;
return|return
literal|true
return|;
block|}
block|}
name|LOG
operator|.
name|debug
argument_list|(
literal|"Pool "
operator|+
name|pool
operator|+
literal|" is not on an sla pool"
argument_list|)
expr_stmt|;
return|return
literal|false
return|;
block|}
comment|/*      * The user has specified a mapping table in hive.configs, which is      * essentially of the form: pool -><cluster, hadoopHome, jobTracker>      * Since, cluster will be repeated a lot in these scenarios, the exact      * mapping is: cluster -><hadoopHome, jobTracker, array of pools>      * Going forward, multiple clusters will be used in these mappings, once      * silver is broken into silver and silver2. No code changes will be      * required, only configuration change.      * @return Whether to use the cluster from the smc      */
specifier|private
name|boolean
name|useClusterFromSmcConfig
parameter_list|(
name|HiveConf
name|conf
parameter_list|)
block|{
try|try
block|{
name|String
name|pool
init|=
name|conf
operator|.
name|get
argument_list|(
literal|"mapred.fairscheduler.pool"
argument_list|)
decl_stmt|;
comment|// Nothing to be done if pool is not specified
if|if
condition|(
operator|(
name|pool
operator|==
literal|null
operator|)
operator|||
operator|(
name|pool
operator|.
name|isEmpty
argument_list|()
operator|)
condition|)
block|{
return|return
literal|false
return|;
block|}
name|ConnectionUrlFactory
name|connectionUrlFactory
init|=
name|HookUtils
operator|.
name|getUrlFactory
argument_list|(
name|conf
argument_list|,
name|FBHiveConf
operator|.
name|CONNECTION_FACTORY
argument_list|,
name|FBHiveConf
operator|.
name|JOBTRACKER_CONNECTION_FACTORY
argument_list|,
name|FBHiveConf
operator|.
name|JOBTRACKER_MYSQL_TIER_VAR_NAME
argument_list|,
name|FBHiveConf
operator|.
name|JOBTRACKER_HOST_DATABASE_VAR_NAME
argument_list|)
decl_stmt|;
if|if
condition|(
name|connectionUrlFactory
operator|==
literal|null
condition|)
block|{
return|return
literal|false
return|;
block|}
name|String
name|s
init|=
name|connectionUrlFactory
operator|.
name|getValue
argument_list|(
name|conf
operator|.
name|get
argument_list|(
name|FBHiveConf
operator|.
name|HIVE_CONFIG_TIER
argument_list|)
argument_list|,
name|POOLS
argument_list|)
decl_stmt|;
if|if
condition|(
name|s
operator|==
literal|null
condition|)
block|{
return|return
literal|false
return|;
block|}
name|JSONObject
name|poolsJSON
init|=
operator|new
name|JSONObject
argument_list|(
name|s
argument_list|)
decl_stmt|;
name|Iterator
argument_list|<
name|String
argument_list|>
name|i
init|=
operator|(
name|Iterator
argument_list|<
name|String
argument_list|>
operator|)
name|poolsJSON
operator|.
name|keys
argument_list|()
decl_stmt|;
while|while
condition|(
name|i
operator|.
name|hasNext
argument_list|()
condition|)
block|{
name|String
name|clusterName
init|=
name|i
operator|.
name|next
argument_list|()
decl_stmt|;
name|JSONObject
name|jo
init|=
operator|(
name|JSONObject
operator|)
name|poolsJSON
operator|.
name|get
argument_list|(
name|clusterName
argument_list|)
decl_stmt|;
name|String
name|hadoopHome
init|=
literal|null
decl_stmt|;
name|String
name|jobTracker
init|=
literal|null
decl_stmt|;
name|JSONArray
name|poolsObj
init|=
literal|null
decl_stmt|;
name|boolean
name|isCorona
init|=
literal|false
decl_stmt|;
if|if
condition|(
name|jo
operator|.
name|has
argument_list|(
literal|"isCorona"
argument_list|)
condition|)
block|{
name|isCorona
operator|=
name|jo
operator|.
name|getBoolean
argument_list|(
literal|"isCorona"
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
operator|!
name|jo
operator|.
name|has
argument_list|(
literal|"hadoopHome"
argument_list|)
operator|||
operator|!
name|jo
operator|.
name|has
argument_list|(
literal|"pools"
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"hadoopHome and pools need to be specified for "
operator|+
name|clusterName
argument_list|)
expr_stmt|;
return|return
literal|false
return|;
block|}
else|else
block|{
name|hadoopHome
operator|=
name|jo
operator|.
name|getString
argument_list|(
literal|"hadoopHome"
argument_list|)
expr_stmt|;
name|poolsObj
operator|=
operator|(
name|JSONArray
operator|)
name|jo
operator|.
name|get
argument_list|(
literal|"pools"
argument_list|)
expr_stmt|;
block|}
if|if
condition|(
operator|!
name|isCorona
operator|&&
operator|!
name|jo
operator|.
name|has
argument_list|(
literal|"jobTracker"
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|error
argument_list|(
literal|"jobTracker needs to be specified for non-corona cluster "
operator|+
name|clusterName
argument_list|)
expr_stmt|;
return|return
literal|false
return|;
block|}
else|else
block|{
if|if
condition|(
name|jo
operator|.
name|has
argument_list|(
literal|"jobTracker"
argument_list|)
condition|)
block|{
name|jobTracker
operator|=
name|jo
operator|.
name|getString
argument_list|(
literal|"jobTracker"
argument_list|)
expr_stmt|;
block|}
block|}
comment|// Do the pool match
for|for
control|(
name|int
name|idx
init|=
literal|0
init|;
name|idx
operator|<
name|poolsObj
operator|.
name|length
argument_list|()
condition|;
name|idx
operator|++
control|)
block|{
if|if
condition|(
name|pool
operator|.
name|equals
argument_list|(
name|poolsObj
operator|.
name|getString
argument_list|(
name|idx
argument_list|)
argument_list|)
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
literal|"Run it on "
operator|+
name|clusterName
operator|+
literal|" due to pool "
operator|+
name|pool
argument_list|)
expr_stmt|;
if|if
condition|(
name|isCorona
condition|)
block|{
comment|// Parameters are taken from configuration.
name|runCorona
argument_list|(
name|conf
argument_list|,
name|hadoopHome
argument_list|)
expr_stmt|;
block|}
else|else
block|{
comment|// Run it on "clusterName"
name|preHadoopBin
operator|=
name|conf
operator|.
name|getVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HADOOPBIN
argument_list|)
expr_stmt|;
name|conf
operator|.
name|setVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HADOOPBIN
argument_list|,
name|hadoopHome
operator|+
literal|"/bin/hadoop"
argument_list|)
expr_stmt|;
name|preJobTracker
operator|=
name|conf
operator|.
name|getVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HADOOPJT
argument_list|)
expr_stmt|;
name|conf
operator|.
name|setVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HADOOPJT
argument_list|,
name|jobTracker
argument_list|)
expr_stmt|;
block|}
return|return
literal|true
return|;
block|}
block|}
block|}
comment|// Found nothing
return|return
literal|false
return|;
block|}
catch|catch
parameter_list|(
name|TException
name|e
parameter_list|)
block|{
return|return
literal|false
return|;
block|}
catch|catch
parameter_list|(
name|JSONException
name|e
parameter_list|)
block|{
return|return
literal|false
return|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
return|return
literal|false
return|;
block|}
block|}
annotation|@
name|Override
specifier|public
name|void
name|run
parameter_list|(
name|HookContext
name|hookContext
parameter_list|)
throws|throws
name|Exception
block|{
assert|assert
operator|(
name|hookContext
operator|.
name|getHookType
argument_list|()
operator|==
name|HookContext
operator|.
name|HookType
operator|.
name|PRE_EXEC_HOOK
operator|)
assert|;
name|SessionState
name|sess
init|=
name|SessionState
operator|.
name|get
argument_list|()
decl_stmt|;
name|Set
argument_list|<
name|ReadEntity
argument_list|>
name|inputs
init|=
name|hookContext
operator|.
name|getInputs
argument_list|()
decl_stmt|;
name|Set
argument_list|<
name|WriteEntity
argument_list|>
name|outputs
init|=
name|hookContext
operator|.
name|getOutputs
argument_list|()
decl_stmt|;
name|UserGroupInformation
name|ugi
init|=
name|hookContext
operator|.
name|getUgi
argument_list|()
decl_stmt|;
name|Map
argument_list|<
name|String
argument_list|,
name|ContentSummary
argument_list|>
name|inputToCS
init|=
name|hookContext
operator|.
name|getInputPathToContentSummary
argument_list|()
decl_stmt|;
name|QueryPlan
name|queryPlan
init|=
name|hookContext
operator|.
name|getQueryPlan
argument_list|()
decl_stmt|;
name|List
argument_list|<
name|Task
argument_list|<
name|?
extends|extends
name|Serializable
argument_list|>
argument_list|>
name|rootTasks
init|=
name|queryPlan
operator|.
name|getRootTasks
argument_list|()
decl_stmt|;
comment|// If it is a pure DDL task,
if|if
condition|(
name|rootTasks
operator|==
literal|null
condition|)
block|{
return|return;
block|}
if|if
condition|(
name|rootTasks
operator|.
name|size
argument_list|()
operator|==
literal|1
condition|)
block|{
name|Task
argument_list|<
name|?
extends|extends
name|Serializable
argument_list|>
name|tsk
init|=
name|rootTasks
operator|.
name|get
argument_list|(
literal|0
argument_list|)
decl_stmt|;
if|if
condition|(
name|tsk
operator|instanceof
name|DDLTask
condition|)
block|{
return|return;
block|}
block|}
name|HiveConf
name|conf
init|=
name|sess
operator|.
name|getConf
argument_list|()
decl_stmt|;
comment|// In case posthook of the previous query was not triggered,
comment|// we revert job tracker to clean state first.
if|if
condition|(
name|preHadoopBin
operator|!=
literal|null
condition|)
block|{
name|conf
operator|.
name|setVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HADOOPBIN
argument_list|,
name|preHadoopBin
argument_list|)
expr_stmt|;
name|preHadoopBin
operator|=
literal|null
expr_stmt|;
block|}
if|if
condition|(
name|preJobTracker
operator|!=
literal|null
condition|)
block|{
name|conf
operator|.
name|setVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HADOOPJT
argument_list|,
name|preJobTracker
argument_list|)
expr_stmt|;
name|preJobTracker
operator|=
literal|null
expr_stmt|;
block|}
comment|// A map from a path to the highest percentage that it is sampled by a
comment|// map reduce task.  If any map reduce task which uses this path does not
comment|// sample, this percentage is 100.
name|Map
argument_list|<
name|String
argument_list|,
name|Double
argument_list|>
name|pathToTopPercentage
init|=
operator|new
name|HashMap
argument_list|<
name|String
argument_list|,
name|Double
argument_list|>
argument_list|()
decl_stmt|;
comment|// A set of inputs we know were not sampled for some task, so we should
comment|// ignore any entries for them in pathToTopPercentage
name|Set
argument_list|<
name|ReadEntity
argument_list|>
name|nonSampledInputs
init|=
operator|new
name|HashSet
argument_list|<
name|ReadEntity
argument_list|>
argument_list|()
decl_stmt|;
name|boolean
name|isThereSampling
init|=
literal|false
decl_stmt|;
if|if
condition|(
operator|!
name|hookContext
operator|.
name|getQueryPlan
argument_list|()
operator|.
name|getQueryStr
argument_list|()
operator|.
name|toUpperCase
argument_list|()
operator|.
name|contains
argument_list|(
literal|" JOIN "
argument_list|)
condition|)
block|{
name|isThereSampling
operator|=
name|HookUtils
operator|.
name|checkForSamplingTasks
argument_list|(
name|hookContext
operator|.
name|getQueryPlan
argument_list|()
operator|.
name|getRootTasks
argument_list|()
argument_list|,
name|pathToTopPercentage
argument_list|,
name|nonSampledInputs
argument_list|)
expr_stmt|;
block|}
comment|// if we are set on local mode execution (via user or auto) bail
if|if
condition|(
literal|"local"
operator|.
name|equals
argument_list|(
name|conf
operator|.
name|getVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HADOOPJT
argument_list|)
argument_list|)
condition|)
block|{
return|return;
block|}
comment|// The smc hive.configs contains a mapping of pools to the map-reduce
comment|// cluster. If the user has specified a pool, and the pool belongs to one
comment|// of the clusters for the smc, use that cluster
if|if
condition|(
name|useClusterFromSmcConfig
argument_list|(
name|conf
argument_list|)
condition|)
block|{
return|return;
block|}
comment|// If this is an SLA pool, bail
if|if
condition|(
name|isOnSlaPool
argument_list|(
name|conf
argument_list|)
condition|)
block|{
return|return;
block|}
comment|// check if we need to run at all
if|if
condition|(
operator|!
literal|"true"
operator|.
name|equals
argument_list|(
name|conf
operator|.
name|get
argument_list|(
literal|"fbhive.jobtracker.auto"
argument_list|,
literal|""
argument_list|)
argument_list|)
condition|)
block|{
return|return;
block|}
name|int
name|bronzePercentage
init|=
name|conf
operator|.
name|getInt
argument_list|(
literal|"fbhive.jobtracker.bronze.percentage"
argument_list|,
literal|0
argument_list|)
decl_stmt|;
name|boolean
name|isCoronaEnabled
init|=
name|conf
operator|.
name|getBoolean
argument_list|(
literal|"fbhive.jobtracker.corona.enabled"
argument_list|,
literal|false
argument_list|)
decl_stmt|;
name|int
name|coronaPercentage
init|=
literal|0
decl_stmt|;
if|if
condition|(
name|isCoronaEnabled
condition|)
block|{
name|coronaPercentage
operator|=
name|conf
operator|.
name|getInt
argument_list|(
literal|"fbhive.jobtracker.corona.percentage"
argument_list|,
literal|0
argument_list|)
expr_stmt|;
block|}
name|int
name|percents
index|[]
init|=
block|{
name|bronzePercentage
block|,
name|coronaPercentage
block|}
decl_stmt|;
name|int
name|roll
init|=
name|rollDice
argument_list|(
name|percents
argument_list|)
decl_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Dice roll is "
operator|+
name|roll
argument_list|)
expr_stmt|;
name|boolean
name|tryBronze
init|=
literal|false
decl_stmt|;
name|boolean
name|tryCorona
init|=
literal|false
decl_stmt|;
if|if
condition|(
name|roll
operator|==
operator|-
literal|1
condition|)
block|{
comment|// Don't run bronze/corona
name|LOG
operator|.
name|info
argument_list|(
name|dislike
operator|+
literal|"because the coin toss said so"
argument_list|)
expr_stmt|;
return|return;
block|}
elseif|else
if|if
condition|(
name|roll
operator|==
literal|0
condition|)
block|{
name|tryBronze
operator|=
literal|true
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|roll
operator|==
literal|1
condition|)
block|{
name|tryCorona
operator|=
literal|true
expr_stmt|;
block|}
else|else
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
literal|"Invalid roll! Roll was "
operator|+
name|roll
argument_list|)
throw|;
block|}
name|int
name|maxGigaBytes
init|=
name|conf
operator|.
name|getInt
argument_list|(
literal|"fbhive.jobtracker.bronze.maxGigaBytes"
argument_list|,
literal|0
argument_list|)
decl_stmt|;
if|if
condition|(
name|maxGigaBytes
operator|==
literal|0
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
name|dislike
operator|+
literal|"maxGigaBytes = 0"
argument_list|)
expr_stmt|;
return|return;
block|}
name|long
name|maxBytes
init|=
name|maxGigaBytes
operator|*
literal|1024L
operator|*
literal|1024
operator|*
literal|1024
decl_stmt|;
if|if
condition|(
name|maxGigaBytes
operator|<
literal|0
condition|)
block|{
name|LOG
operator|.
name|warn
argument_list|(
name|dislike
operator|+
literal|"maxGigaBytes value of "
operator|+
name|maxGigaBytes
operator|+
literal|"is invalid"
argument_list|)
expr_stmt|;
return|return;
block|}
name|String
name|bronzeHadoopHome
init|=
name|conf
operator|.
name|get
argument_list|(
literal|"fbhive.jobtracker.bronze.hadoopHome"
argument_list|,
literal|"/mnt/vol/hive/sites/bronze/hadoop"
argument_list|)
decl_stmt|;
name|String
name|bronzeJobTracker
init|=
name|conf
operator|.
name|get
argument_list|(
literal|"fbhive.jobtracker.bronze.tracker"
argument_list|,
name|conf
operator|.
name|get
argument_list|(
name|FBHiveConf
operator|.
name|FBHIVE_BRONZE_JOBTRACKER
argument_list|)
argument_list|)
decl_stmt|;
comment|// assuming we are using combinehiveinputformat - we know the # of splits will _at least_
comment|// be>= number of partitions/tables. by indicating the max input size - the
comment|// admin is also signalling the max # of splits (maxGig*1000/256MB). So we limit the number
comment|// of partitions to the max # of splits.
name|int
name|maxSplits
init|=
name|conf
operator|.
name|getInt
argument_list|(
literal|"fbhive.jobtracker.bronze.maxPartitions"
argument_list|,
name|maxGigaBytes
operator|*
literal|4
argument_list|)
decl_stmt|;
if|if
condition|(
operator|!
name|isThereSampling
operator|&&
name|inputs
operator|.
name|size
argument_list|()
operator|>
name|maxSplits
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
name|dislike
operator|+
literal|"number of input tables/partitions: "
operator|+
name|inputs
operator|.
name|size
argument_list|()
operator|+
literal|" exceeded max splits: "
operator|+
name|maxSplits
argument_list|)
expr_stmt|;
return|return;
block|}
if|if
condition|(
name|conf
operator|.
name|getIntVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HADOOPNUMREDUCERS
argument_list|)
operator|>
name|maxSplits
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
name|dislike
operator|+
literal|"number of reducers: "
operator|+
name|conf
operator|.
name|getVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HADOOPNUMREDUCERS
argument_list|)
operator|+
literal|" exceeded max reducers: "
operator|+
name|maxSplits
argument_list|)
expr_stmt|;
return|return;
block|}
name|InputInfo
name|info
init|=
name|HookUtils
operator|.
name|getInputInfo
argument_list|(
name|inputs
argument_list|,
name|inputToCS
argument_list|,
name|conf
argument_list|,
name|isThereSampling
argument_list|,
name|pathToTopPercentage
argument_list|,
name|nonSampledInputs
argument_list|,
name|maxSplits
argument_list|,
name|maxBytes
argument_list|)
decl_stmt|;
if|if
condition|(
name|info
operator|.
name|getEstimatedNumSplits
argument_list|()
operator|>
name|maxSplits
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
name|dislike
operator|+
literal|"the estimated number of input "
operator|+
literal|"tables/partitions exceeded max splits: "
operator|+
name|maxSplits
argument_list|)
expr_stmt|;
return|return;
block|}
if|if
condition|(
name|info
operator|.
name|getSize
argument_list|()
operator|>
name|maxBytes
condition|)
block|{
name|LOG
operator|.
name|info
argument_list|(
name|dislike
operator|+
literal|"input length of "
operator|+
name|info
operator|.
name|getSize
argument_list|()
operator|+
literal|" is more than "
operator|+
name|maxBytes
argument_list|)
expr_stmt|;
return|return;
block|}
comment|// we have met all the conditions to switch to bronze/corona cluster
if|if
condition|(
name|tryBronze
condition|)
block|{
comment|// Run it on Bronze
name|preHadoopBin
operator|=
name|conf
operator|.
name|getVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HADOOPBIN
argument_list|)
expr_stmt|;
name|conf
operator|.
name|setVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HADOOPBIN
argument_list|,
name|bronzeHadoopHome
operator|+
literal|"/bin/hadoop"
argument_list|)
expr_stmt|;
name|preJobTracker
operator|=
name|conf
operator|.
name|getVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HADOOPJT
argument_list|)
expr_stmt|;
name|conf
operator|.
name|setVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HADOOPJT
argument_list|,
name|bronzeJobTracker
argument_list|)
expr_stmt|;
block|}
elseif|else
if|if
condition|(
name|tryCorona
condition|)
block|{
name|String
name|coronaHadoopHome
init|=
name|conf
operator|.
name|get
argument_list|(
literal|"fbhive.jobtracker.corona.hadoopHome"
argument_list|,
literal|"/mnt/vol/hive/sites/corona/hadoop"
argument_list|)
decl_stmt|;
name|runCorona
argument_list|(
name|conf
argument_list|,
name|coronaHadoopHome
argument_list|)
expr_stmt|;
block|}
block|}
specifier|private
name|void
name|runCorona
parameter_list|(
name|HiveConf
name|conf
parameter_list|,
name|String
name|hadoopHome
parameter_list|)
block|{
comment|// Run it on Corona
name|preHadoopBin
operator|=
name|conf
operator|.
name|getVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HADOOPBIN
argument_list|)
expr_stmt|;
name|conf
operator|.
name|setVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HADOOPBIN
argument_list|,
name|hadoopHome
operator|+
literal|"/bin/hadoop"
argument_list|)
expr_stmt|;
comment|// No need to set the JT as it's done through the conf
name|Configuration
name|coronaConf
init|=
operator|new
name|Configuration
argument_list|(
literal|false
argument_list|)
decl_stmt|;
comment|// Read the configuration, save old values, replace with new ones
name|coronaConf
operator|.
name|addResource
argument_list|(
literal|"mapred-site-corona.xml"
argument_list|)
expr_stmt|;
name|savedValues
operator|=
operator|new
name|HashMap
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
argument_list|()
expr_stmt|;
for|for
control|(
name|Entry
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|e
range|:
name|coronaConf
control|)
block|{
name|String
name|key
init|=
name|e
operator|.
name|getKey
argument_list|()
decl_stmt|;
name|String
name|value
init|=
name|e
operator|.
name|getValue
argument_list|()
decl_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Saving "
operator|+
name|key
operator|+
literal|"("
operator|+
name|conf
operator|.
name|get
argument_list|(
name|key
argument_list|)
operator|+
literal|")"
argument_list|)
expr_stmt|;
name|savedValues
operator|.
name|put
argument_list|(
name|key
argument_list|,
name|conf
operator|.
name|get
argument_list|(
name|key
argument_list|)
argument_list|)
expr_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Setting "
operator|+
name|key
operator|+
literal|"("
operator|+
name|key
operator|+
literal|")"
argument_list|)
expr_stmt|;
name|conf
operator|.
name|set
argument_list|(
name|key
argument_list|,
name|value
argument_list|)
expr_stmt|;
block|}
block|}
block|}
comment|/**    * Randomly picks an index with chance that is indicated by the value in    * percentages. Returns -1 for the remaining percentage    *    * E.g. [60, 20] will return 0 (60% of the time) and 1 (20% of the time) and    * -1 (20% of the time)    * @param percentages    * @return    */
specifier|private
specifier|static
name|int
name|rollDice
parameter_list|(
name|int
index|[]
name|percentages
parameter_list|)
block|{
name|Random
name|randGen
init|=
operator|new
name|Random
argument_list|()
decl_stmt|;
name|int
name|randVal
init|=
name|randGen
operator|.
name|nextInt
argument_list|(
literal|100
argument_list|)
operator|+
literal|1
decl_stmt|;
comment|// Make sure that percentages add up to<= 100%
name|int
name|sum
init|=
literal|0
decl_stmt|;
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|percentages
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
name|sum
operator|+=
name|percentages
index|[
name|i
index|]
expr_stmt|;
if|if
condition|(
name|percentages
index|[
name|i
index|]
operator|<
literal|0
condition|)
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
literal|"Percentages must be>=0. Got "
operator|+
name|percentages
index|[
name|i
index|]
argument_list|)
throw|;
block|}
block|}
if|if
condition|(
name|sum
operator|>
literal|100
condition|)
block|{
throw|throw
operator|new
name|RuntimeException
argument_list|(
literal|"Percentages add up to> 100!"
argument_list|)
throw|;
block|}
for|for
control|(
name|int
name|i
init|=
literal|0
init|;
name|i
operator|<
name|percentages
operator|.
name|length
condition|;
name|i
operator|++
control|)
block|{
if|if
condition|(
name|randVal
operator|<=
name|percentages
index|[
name|i
index|]
condition|)
block|{
return|return
name|i
return|;
block|}
name|randVal
operator|=
name|randVal
operator|-
name|percentages
index|[
name|i
index|]
expr_stmt|;
block|}
return|return
operator|-
literal|1
return|;
block|}
specifier|public
specifier|static
class|class
name|PostExec
implements|implements
name|ExecuteWithHookContext
block|{
annotation|@
name|Override
specifier|public
name|void
name|run
parameter_list|(
name|HookContext
name|hookContext
parameter_list|)
throws|throws
name|Exception
block|{
assert|assert
operator|(
name|hookContext
operator|.
name|getHookType
argument_list|()
operator|==
name|HookContext
operator|.
name|HookType
operator|.
name|POST_EXEC_HOOK
operator|)
assert|;
name|SessionState
name|ss
init|=
name|SessionState
operator|.
name|get
argument_list|()
decl_stmt|;
name|Set
argument_list|<
name|ReadEntity
argument_list|>
name|inputs
init|=
name|hookContext
operator|.
name|getInputs
argument_list|()
decl_stmt|;
name|Set
argument_list|<
name|WriteEntity
argument_list|>
name|outputs
init|=
name|hookContext
operator|.
name|getOutputs
argument_list|()
decl_stmt|;
name|LineageInfo
name|linfo
init|=
name|hookContext
operator|.
name|getLinfo
argument_list|()
decl_stmt|;
name|UserGroupInformation
name|ugi
init|=
name|hookContext
operator|.
name|getUgi
argument_list|()
decl_stmt|;
name|this
operator|.
name|run
argument_list|(
name|ss
argument_list|,
name|inputs
argument_list|,
name|outputs
argument_list|,
name|linfo
argument_list|,
name|ugi
argument_list|)
expr_stmt|;
block|}
specifier|public
name|void
name|run
parameter_list|(
name|SessionState
name|sess
parameter_list|,
name|Set
argument_list|<
name|ReadEntity
argument_list|>
name|inputs
parameter_list|,
name|Set
argument_list|<
name|WriteEntity
argument_list|>
name|outputs
parameter_list|,
name|LineageInfo
name|lInfo
parameter_list|,
name|UserGroupInformation
name|ugi
parameter_list|)
throws|throws
name|Exception
block|{
name|HiveConf
name|conf
init|=
name|sess
operator|.
name|getConf
argument_list|()
decl_stmt|;
if|if
condition|(
name|preHadoopBin
operator|!=
literal|null
condition|)
block|{
name|conf
operator|.
name|setVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HADOOPBIN
argument_list|,
name|preHadoopBin
argument_list|)
expr_stmt|;
name|preHadoopBin
operator|=
literal|null
expr_stmt|;
block|}
if|if
condition|(
name|preJobTracker
operator|!=
literal|null
condition|)
block|{
name|conf
operator|.
name|setVar
argument_list|(
name|HiveConf
operator|.
name|ConfVars
operator|.
name|HADOOPJT
argument_list|,
name|preJobTracker
argument_list|)
expr_stmt|;
name|preJobTracker
operator|=
literal|null
expr_stmt|;
block|}
comment|// Restore values set for Corona
if|if
condition|(
name|savedValues
operator|!=
literal|null
condition|)
block|{
for|for
control|(
name|Entry
argument_list|<
name|String
argument_list|,
name|String
argument_list|>
name|e
range|:
name|savedValues
operator|.
name|entrySet
argument_list|()
control|)
block|{
name|String
name|key
init|=
name|e
operator|.
name|getKey
argument_list|()
decl_stmt|;
name|String
name|value
init|=
name|e
operator|.
name|getValue
argument_list|()
decl_stmt|;
name|LOG
operator|.
name|debug
argument_list|(
literal|"Restoring "
operator|+
name|key
operator|+
literal|"("
operator|+
name|value
operator|+
literal|")"
argument_list|)
expr_stmt|;
if|if
condition|(
name|value
operator|!=
literal|null
condition|)
block|{
name|conf
operator|.
name|set
argument_list|(
name|key
argument_list|,
name|value
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|conf
operator|.
name|set
argument_list|(
name|key
argument_list|,
literal|""
argument_list|)
expr_stmt|;
block|}
block|}
block|}
block|}
block|}
block|}
end_class

end_unit

